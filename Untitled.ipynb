{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b95c4e2-c5b9-4618-8f67-0d791d7e05d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a25f4d4b-7789-4b7e-82ac-284c02f53e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check for CUDA availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b9b29fb-66af-4059-bcd4-909ecbc85e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0 Ticker Symbol Period Ending  Accounts Payable  \\\n",
      "0           0           AAL    2012-12-31      3.068000e+09   \n",
      "1           1           AAL    2013-12-31      4.975000e+09   \n",
      "2           2           AAL    2014-12-31      4.668000e+09   \n",
      "3           3           AAL    2015-12-31      5.102000e+09   \n",
      "4           4           AAP    2012-12-29      2.409453e+09   \n",
      "\n",
      "   Accounts Receivable  Add'l income/expense items  After Tax ROE  \\\n",
      "0         -222000000.0               -1.961000e+09           23.0   \n",
      "1          -93000000.0               -2.723000e+09           67.0   \n",
      "2         -160000000.0               -1.500000e+08          143.0   \n",
      "3          352000000.0               -7.080000e+08          135.0   \n",
      "4          -89482000.0                6.000000e+05           32.0   \n",
      "\n",
      "   Capital Expenditures  Capital Surplus  Cash Ratio  ...  \\\n",
      "0         -1.888000e+09     4.695000e+09        53.0  ...   \n",
      "1         -3.114000e+09     1.059200e+10        75.0  ...   \n",
      "2         -5.311000e+09     1.513500e+10        60.0  ...   \n",
      "3         -6.151000e+09     1.159100e+10        51.0  ...   \n",
      "4         -2.711820e+08     5.202150e+08        23.0  ...   \n",
      "\n",
      "   Total Current Assets  Total Current Liabilities  Total Equity  \\\n",
      "0          7.072000e+09               9.011000e+09 -7.987000e+09   \n",
      "1          1.432300e+10               1.380600e+10 -2.731000e+09   \n",
      "2          1.175000e+10               1.340400e+10  2.021000e+09   \n",
      "3          9.985000e+09               1.360500e+10  5.635000e+09   \n",
      "4          3.184200e+09               2.559638e+09  1.210694e+09   \n",
      "\n",
      "   Total Liabilities  Total Liabilities & Equity  Total Revenue  \\\n",
      "0       2.489100e+10                1.690400e+10   2.485500e+10   \n",
      "1       4.500900e+10                4.227800e+10   2.674300e+10   \n",
      "2       4.120400e+10                4.322500e+10   4.265000e+10   \n",
      "3       4.278000e+10                4.841500e+10   4.099000e+10   \n",
      "4       3.403120e+09                4.613814e+09   6.205003e+09   \n",
      "\n",
      "   Treasury Stock  For Year  Earnings Per Share  Estimated Shares Outstanding  \n",
      "0    -367000000.0    2012.0               -5.60                  3.350000e+08  \n",
      "1             0.0    2013.0              -11.25                  1.630222e+08  \n",
      "2             0.0    2014.0                4.02                  7.169154e+08  \n",
      "3             0.0    2015.0               11.39                  6.681299e+08  \n",
      "4     -27095000.0    2012.0                5.29                  7.328355e+07  \n",
      "\n",
      "[5 rows x 79 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load Dataset\n",
    "df = pd.read_csv(\"./fundamentals.csv\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8975751-25ee-4d2f-a2cf-4ebf462d69da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop non-numeric columns and handle missing values\n",
    "df = df.select_dtypes(include=[np.number]).drop(columns=['Unnamed: 0']).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf761c35-07ba-482c-a00c-49ab7b7e4782",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Exploratory Data Analysis (EDA)\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.heatmap(df.corr(), cmap='coolwarm', annot=False)\n",
    "plt.title(\"Feature Correlation Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef5e1343-d312-479f-8d25-28fb2f842a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the target variable (Earnings Per Share)\n",
    "X = df.drop(columns=['Earnings Per Share'])\n",
    "y = df['Earnings Per Share']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1de1126-20af-4074-ac9a-5f222845d97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95c4b6f1-3318-45a1-8051-5f017a4c5c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardization\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5eb9d07b-6adb-47db-a817-1b9634dde4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Torch Tensors and Move to GPU\n",
    "X_train_tensor = torch.FloatTensor(X_train).to(device)\n",
    "y_train_tensor = torch.FloatTensor(y_train.values).view(-1, 1).to(device)\n",
    "X_test_tensor = torch.FloatTensor(X_test).to(device)\n",
    "y_test_tensor = torch.FloatTensor(y_test.values).view(-1, 1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fee98985-e5f9-4fb0-aecd-9accc10f5e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define PyTorch Model\n",
    "class RegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden1=64, hidden2=32):\n",
    "        super(RegressionModel, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden1, hidden2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden2, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "109598e9-909f-41d8-b913-4cc58cacff0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Train the Model\n",
    "def train_model(model, X_train, y_train, X_test, y_test, criterion, optimizer, epochs=100):\n",
    "    train_losses, test_losses, train_r2, test_r2 = [], [], [], []\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train)\n",
    "        loss = criterion(outputs, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        with torch.no_grad():\n",
    "            test_output = model(X_test)\n",
    "            test_loss = criterion(test_output, y_test).item()\n",
    "            train_r2.append(r2_score(y_train.cpu().numpy(), outputs.cpu().detach().numpy()))\n",
    "            test_r2.append(r2_score(y_test.cpu().numpy(), test_output.cpu().detach().numpy()))\n",
    "        \n",
    "        train_losses.append(loss.item())\n",
    "        test_losses.append(test_loss)\n",
    "        \n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {loss.item():.4f}, Test Loss: {test_loss:.4f}')\n",
    "    \n",
    "    return train_losses, test_losses, train_r2, test_r2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d87db31a-b7dd-4ca8-be40-bbc1fe0cd43d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m43.8589\u001b[0m       \u001b[32m46.0523\u001b[0m  0.4645\n",
      "      2       \u001b[36m43.5128\u001b[0m       \u001b[32m45.5827\u001b[0m  0.0843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      3       \u001b[36m43.1459\u001b[0m       \u001b[32m45.0493\u001b[0m  0.0914\n",
      "      4       \u001b[36m42.7337\u001b[0m       \u001b[32m44.4466\u001b[0m  0.0910\n",
      "      5       \u001b[36m42.2639\u001b[0m       \u001b[32m43.7650\u001b[0m  0.0891\n",
      "      6       \u001b[36m41.7371\u001b[0m       \u001b[32m43.0246\u001b[0m  0.0792\n",
      "      7       \u001b[36m41.1668\u001b[0m       \u001b[32m42.2444\u001b[0m  0.0957\n",
      "      8       \u001b[36m40.5638\u001b[0m       \u001b[32m41.4193\u001b[0m  0.0676\n",
      "      9       \u001b[36m39.9052\u001b[0m       \u001b[32m40.4962\u001b[0m  0.1063\n",
      "     10       \u001b[36m39.1545\u001b[0m       \u001b[32m39.4323\u001b[0m  0.0641\n",
      "     11       \u001b[36m38.2961\u001b[0m       \u001b[32m38.2192\u001b[0m  0.1013\n",
      "     12       \u001b[36m37.3454\u001b[0m       \u001b[32m36.8779\u001b[0m  0.0897\n",
      "     13       \u001b[36m36.3586\u001b[0m       \u001b[32m35.4857\u001b[0m  0.0935\n",
      "     14       \u001b[36m35.4223\u001b[0m       \u001b[32m34.1503\u001b[0m  0.0806\n",
      "     15       \u001b[36m34.6479\u001b[0m       \u001b[32m33.0137\u001b[0m  0.0922\n",
      "     16       \u001b[36m34.1297\u001b[0m       \u001b[32m32.1934\u001b[0m  0.0923\n",
      "     17       \u001b[36m33.8803\u001b[0m       \u001b[32m31.7107\u001b[0m  0.0820\n",
      "     18       \u001b[36m33.7944\u001b[0m       \u001b[32m31.4691\u001b[0m  0.0922\n",
      "     19       \u001b[36m33.7238\u001b[0m       \u001b[32m31.3587\u001b[0m  0.0734\n",
      "     20       \u001b[36m33.6132\u001b[0m       \u001b[32m31.3268\u001b[0m  0.0554\n",
      "     21       \u001b[36m33.4915\u001b[0m       31.3295  0.0494\n",
      "     22       \u001b[36m33.3866\u001b[0m       \u001b[32m31.3250\u001b[0m  0.0462\n",
      "     23       \u001b[36m33.3005\u001b[0m       \u001b[32m31.2849\u001b[0m  0.0365\n",
      "     24       \u001b[36m33.2244\u001b[0m       \u001b[32m31.2086\u001b[0m  0.0281\n",
      "     25       \u001b[36m33.1536\u001b[0m       \u001b[32m31.1131\u001b[0m  0.0308\n",
      "     26       \u001b[36m33.0866\u001b[0m       \u001b[32m31.0170\u001b[0m  0.0291\n",
      "     27       \u001b[36m33.0257\u001b[0m       \u001b[32m30.9307\u001b[0m  0.0270\n",
      "     28       \u001b[36m32.9712\u001b[0m       \u001b[32m30.8601\u001b[0m  0.0275\n",
      "     29       \u001b[36m32.9222\u001b[0m       \u001b[32m30.8038\u001b[0m  0.0400\n",
      "     30       \u001b[36m32.8774\u001b[0m       \u001b[32m30.7602\u001b[0m  0.1244\n",
      "     31       \u001b[36m32.8353\u001b[0m       \u001b[32m30.7271\u001b[0m  0.1007\n",
      "     32       \u001b[36m32.7963\u001b[0m       \u001b[32m30.7034\u001b[0m  0.1340\n",
      "     33       \u001b[36m32.7598\u001b[0m       \u001b[32m30.6857\u001b[0m  0.1350\n",
      "     34       \u001b[36m32.7258\u001b[0m       \u001b[32m30.6697\u001b[0m  0.1133\n",
      "     35       \u001b[36m32.6940\u001b[0m       \u001b[32m30.6529\u001b[0m  0.0599\n",
      "     36       \u001b[36m32.6639\u001b[0m       \u001b[32m30.6386\u001b[0m  0.1084\n",
      "     37       \u001b[36m32.6357\u001b[0m       \u001b[32m30.6261\u001b[0m  0.0883\n",
      "     38       \u001b[36m32.6093\u001b[0m       \u001b[32m30.6136\u001b[0m  0.0658\n",
      "     39       \u001b[36m32.5847\u001b[0m       \u001b[32m30.6044\u001b[0m  0.1008\n",
      "     40       \u001b[36m32.5619\u001b[0m       \u001b[32m30.5993\u001b[0m  0.0609\n",
      "     41       \u001b[36m32.5404\u001b[0m       \u001b[32m30.5920\u001b[0m  0.1048\n",
      "     42       \u001b[36m32.5201\u001b[0m       \u001b[32m30.5867\u001b[0m  0.0776\n",
      "     43       \u001b[36m32.5008\u001b[0m       \u001b[32m30.5855\u001b[0m  0.0999\n",
      "     44       \u001b[36m32.4826\u001b[0m       \u001b[32m30.5832\u001b[0m  0.0455\n",
      "     45       \u001b[36m32.4655\u001b[0m       \u001b[32m30.5788\u001b[0m  0.1110\n",
      "     46       \u001b[36m32.4496\u001b[0m       \u001b[32m30.5753\u001b[0m  0.0631\n",
      "     47       \u001b[36m32.4348\u001b[0m       \u001b[32m30.5737\u001b[0m  0.1005\n",
      "     48       \u001b[36m32.4208\u001b[0m       \u001b[32m30.5696\u001b[0m  0.0684\n",
      "     49       \u001b[36m32.4077\u001b[0m       \u001b[32m30.5642\u001b[0m  0.0963\n",
      "     50       \u001b[36m32.3954\u001b[0m       \u001b[32m30.5610\u001b[0m  0.0683\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m33.0913\u001b[0m       \u001b[32m31.8408\u001b[0m  0.0529\n",
      "      2       \u001b[36m32.7078\u001b[0m       \u001b[32m31.5761\u001b[0m  0.0580\n",
      "      3       \u001b[36m32.3855\u001b[0m       \u001b[32m31.3218\u001b[0m  0.0827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      4       \u001b[36m32.0468\u001b[0m       \u001b[32m31.0469\u001b[0m  0.0777\n",
      "      5       \u001b[36m31.6634\u001b[0m       \u001b[32m30.7313\u001b[0m  0.1118\n",
      "      6       \u001b[36m31.2158\u001b[0m       \u001b[32m30.3584\u001b[0m  0.0592\n",
      "      7       \u001b[36m30.6740\u001b[0m       \u001b[32m29.9107\u001b[0m  0.0850\n",
      "      8       \u001b[36m30.0269\u001b[0m       \u001b[32m29.4002\u001b[0m  0.0929\n",
      "      9       \u001b[36m29.2969\u001b[0m       \u001b[32m28.8638\u001b[0m  0.0838\n",
      "     10       \u001b[36m28.5317\u001b[0m       \u001b[32m28.3368\u001b[0m  0.0834\n",
      "     11       \u001b[36m27.7667\u001b[0m       \u001b[32m27.8371\u001b[0m  0.0814\n",
      "     12       \u001b[36m26.9917\u001b[0m       \u001b[32m27.3757\u001b[0m  0.0814\n",
      "     13       \u001b[36m26.2270\u001b[0m       \u001b[32m27.0498\u001b[0m  0.0810\n",
      "     14       \u001b[36m25.5822\u001b[0m       \u001b[32m26.9481\u001b[0m  0.0840\n",
      "     15       \u001b[36m25.1466\u001b[0m       27.0673  0.0826\n",
      "     16       \u001b[36m24.9259\u001b[0m       27.2769  0.1004\n",
      "     17       \u001b[36m24.8374\u001b[0m       27.4081  0.0828\n",
      "     18       \u001b[36m24.7753\u001b[0m       27.3844  0.0894\n",
      "     19       \u001b[36m24.6907\u001b[0m       27.2566  0.0832\n",
      "     20       \u001b[36m24.5948\u001b[0m       27.1061  0.0841\n",
      "     21       \u001b[36m24.5076\u001b[0m       26.9808  0.0823\n",
      "     22       \u001b[36m24.4303\u001b[0m       \u001b[32m26.8935\u001b[0m  0.0817\n",
      "     23       \u001b[36m24.3560\u001b[0m       \u001b[32m26.8414\u001b[0m  0.0697\n",
      "     24       \u001b[36m24.2825\u001b[0m       \u001b[32m26.8150\u001b[0m  0.0970\n",
      "     25       \u001b[36m24.2095\u001b[0m       \u001b[32m26.8055\u001b[0m  0.0794\n",
      "     26       \u001b[36m24.1376\u001b[0m       \u001b[32m26.8051\u001b[0m  0.0811\n",
      "     27       \u001b[36m24.0693\u001b[0m       26.8069  0.0849\n",
      "     28       \u001b[36m24.0051\u001b[0m       26.8071  0.0935\n",
      "     29       \u001b[36m23.9447\u001b[0m       \u001b[32m26.8049\u001b[0m  0.0734\n",
      "     30       \u001b[36m23.8883\u001b[0m       \u001b[32m26.8004\u001b[0m  0.0885\n",
      "     31       \u001b[36m23.8351\u001b[0m       \u001b[32m26.7957\u001b[0m  0.0744\n",
      "     32       \u001b[36m23.7851\u001b[0m       \u001b[32m26.7932\u001b[0m  0.0457\n",
      "     33       \u001b[36m23.7378\u001b[0m       26.7942  0.0564\n",
      "     34       \u001b[36m23.6933\u001b[0m       26.7987  0.0439\n",
      "     35       \u001b[36m23.6522\u001b[0m       26.8057  0.0590\n",
      "     36       \u001b[36m23.6143\u001b[0m       26.8126  0.0487\n",
      "     37       \u001b[36m23.5797\u001b[0m       26.8177  0.0592\n",
      "     38       \u001b[36m23.5478\u001b[0m       26.8227  0.0435\n",
      "     39       \u001b[36m23.5176\u001b[0m       26.8293  0.0592\n",
      "     40       \u001b[36m23.4899\u001b[0m       26.8338  0.0490\n",
      "     41       \u001b[36m23.4643\u001b[0m       26.8354  0.0594\n",
      "     42       \u001b[36m23.4408\u001b[0m       26.8368  0.0480\n",
      "     43       \u001b[36m23.4187\u001b[0m       26.8388  0.0628\n",
      "     44       \u001b[36m23.3980\u001b[0m       26.8401  0.0521\n",
      "     45       \u001b[36m23.3788\u001b[0m       26.8408  0.0640\n",
      "     46       \u001b[36m23.3610\u001b[0m       26.8416  0.0555\n",
      "     47       \u001b[36m23.3446\u001b[0m       26.8432  0.0595\n",
      "     48       \u001b[36m23.3292\u001b[0m       26.8437  0.0484\n",
      "     49       \u001b[36m23.3148\u001b[0m       26.8435  0.0571\n",
      "     50       \u001b[36m23.3014\u001b[0m       26.8450  0.0476\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m39.3753\u001b[0m       \u001b[32m31.4035\u001b[0m  0.0423\n",
      "      2       \u001b[36m38.8865\u001b[0m       \u001b[32m31.0696\u001b[0m  0.0469\n",
      "      3       \u001b[36m38.3465\u001b[0m       \u001b[32m30.6897\u001b[0m  0.0557\n",
      "      4       \u001b[36m37.7744\u001b[0m       \u001b[32m30.2800\u001b[0m  0.0509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      5       \u001b[36m37.1885\u001b[0m       \u001b[32m29.8201\u001b[0m  0.0751\n",
      "      6       \u001b[36m36.5189\u001b[0m       \u001b[32m29.2760\u001b[0m  0.0542\n",
      "      7       \u001b[36m35.7051\u001b[0m       \u001b[32m28.6549\u001b[0m  0.0458\n",
      "      8       \u001b[36m34.7420\u001b[0m       \u001b[32m28.0041\u001b[0m  0.0580\n",
      "      9       \u001b[36m33.6509\u001b[0m       \u001b[32m27.4145\u001b[0m  0.0440\n",
      "     10       \u001b[36m32.5122\u001b[0m       \u001b[32m27.0425\u001b[0m  0.0584\n",
      "     11       \u001b[36m31.4810\u001b[0m       \u001b[32m27.0394\u001b[0m  0.0577\n",
      "     12       \u001b[36m30.7296\u001b[0m       27.4729  0.0587\n",
      "     13       \u001b[36m30.3795\u001b[0m       28.0817  0.0514\n",
      "     14       \u001b[36m30.2988\u001b[0m       28.3618  0.0598\n",
      "     15       \u001b[36m30.2002\u001b[0m       28.1532  0.0498\n",
      "     16       \u001b[36m29.9981\u001b[0m       27.7284  0.0601\n",
      "     17       \u001b[36m29.7840\u001b[0m       27.3629  0.0483\n",
      "     18       \u001b[36m29.6150\u001b[0m       27.1456  0.0580\n",
      "     19       \u001b[36m29.4891\u001b[0m       27.0623  0.0493\n",
      "     20       \u001b[36m29.3858\u001b[0m       27.0707  0.0577\n",
      "     21       \u001b[36m29.2968\u001b[0m       27.1298  0.0462\n",
      "     22       \u001b[36m29.2219\u001b[0m       27.2039  0.0578\n",
      "     23       \u001b[36m29.1577\u001b[0m       27.2663  0.0476\n",
      "     24       \u001b[36m29.0987\u001b[0m       27.3033  0.0603\n",
      "     25       \u001b[36m29.0416\u001b[0m       27.3169  0.0481\n",
      "     26       \u001b[36m28.9871\u001b[0m       27.3179  0.0592\n",
      "     27       \u001b[36m28.9371\u001b[0m       27.3190  0.0485\n",
      "     28       \u001b[36m28.8928\u001b[0m       27.3263  0.0563\n",
      "     29       \u001b[36m28.8536\u001b[0m       27.3399  0.0603\n",
      "     30       \u001b[36m28.8190\u001b[0m       27.3582  0.0642\n",
      "     31       \u001b[36m28.7887\u001b[0m       27.3746  0.0527\n",
      "     32       \u001b[36m28.7614\u001b[0m       27.3866  0.0654\n",
      "     33       \u001b[36m28.7367\u001b[0m       27.3945  0.0503\n",
      "     34       \u001b[36m28.7141\u001b[0m       27.3983  0.0589\n",
      "     35       \u001b[36m28.6930\u001b[0m       27.4008  0.0482\n",
      "     36       \u001b[36m28.6736\u001b[0m       27.4011  0.0575\n",
      "     37       \u001b[36m28.6557\u001b[0m       27.4029  0.0493\n",
      "     38       \u001b[36m28.6397\u001b[0m       27.4065  0.0634\n",
      "     39       \u001b[36m28.6253\u001b[0m       27.4132  0.0475\n",
      "     40       \u001b[36m28.6121\u001b[0m       27.4206  0.0624\n",
      "     41       \u001b[36m28.6002\u001b[0m       27.4241  0.0488\n",
      "     42       \u001b[36m28.5892\u001b[0m       27.4254  0.0591\n",
      "     43       \u001b[36m28.5791\u001b[0m       27.4265  0.0534\n",
      "     44       \u001b[36m28.5698\u001b[0m       27.4261  0.0628\n",
      "     45       \u001b[36m28.5610\u001b[0m       27.4231  0.0604\n",
      "     46       \u001b[36m28.5526\u001b[0m       27.4196  0.0685\n",
      "     47       \u001b[36m28.5449\u001b[0m       27.4160  0.0590\n",
      "     48       \u001b[36m28.5376\u001b[0m       27.4128  0.0430\n",
      "     49       \u001b[36m28.5309\u001b[0m       27.4107  0.0396\n",
      "     50       \u001b[36m28.5245\u001b[0m       27.4079  0.0333\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m43.7033\u001b[0m       \u001b[32m45.8263\u001b[0m  0.0586\n",
      "      2       \u001b[36m43.3842\u001b[0m       \u001b[32m45.4393\u001b[0m  0.0295\n",
      "      3       \u001b[36m43.0748\u001b[0m       \u001b[32m45.0630\u001b[0m  0.0259\n",
      "      4       \u001b[36m42.7740\u001b[0m       \u001b[32m44.6964\u001b[0m  0.0484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      5       \u001b[36m42.4809\u001b[0m       \u001b[32m44.3382\u001b[0m  0.1156\n",
      "      6       \u001b[36m42.1948\u001b[0m       \u001b[32m43.9868\u001b[0m  0.1084\n",
      "      7       \u001b[36m41.9148\u001b[0m       \u001b[32m43.6417\u001b[0m  0.0452\n",
      "      8       \u001b[36m41.6403\u001b[0m       \u001b[32m43.3022\u001b[0m  0.0578\n",
      "      9       \u001b[36m41.3702\u001b[0m       \u001b[32m42.9675\u001b[0m  0.0444\n",
      "     10       \u001b[36m41.1041\u001b[0m       \u001b[32m42.6370\u001b[0m  0.0552\n",
      "     11       \u001b[36m40.8421\u001b[0m       \u001b[32m42.3106\u001b[0m  0.0466\n",
      "     12       \u001b[36m40.5838\u001b[0m       \u001b[32m41.9871\u001b[0m  0.0528\n",
      "     13       \u001b[36m40.3284\u001b[0m       \u001b[32m41.6660\u001b[0m  0.0570\n",
      "     14       \u001b[36m40.0757\u001b[0m       \u001b[32m41.3472\u001b[0m  0.0667\n",
      "     15       \u001b[36m39.8257\u001b[0m       \u001b[32m41.0309\u001b[0m  0.0535\n",
      "     16       \u001b[36m39.5786\u001b[0m       \u001b[32m40.7161\u001b[0m  0.0615\n",
      "     17       \u001b[36m39.3340\u001b[0m       \u001b[32m40.4021\u001b[0m  0.0743\n",
      "     18       \u001b[36m39.0916\u001b[0m       \u001b[32m40.0890\u001b[0m  0.0442\n",
      "     19       \u001b[36m38.8514\u001b[0m       \u001b[32m39.7772\u001b[0m  0.0582\n",
      "     20       \u001b[36m38.6128\u001b[0m       \u001b[32m39.4665\u001b[0m  0.0505\n",
      "     21       \u001b[36m38.3761\u001b[0m       \u001b[32m39.1567\u001b[0m  0.0586\n",
      "     22       \u001b[36m38.1417\u001b[0m       \u001b[32m38.8479\u001b[0m  0.0479\n",
      "     23       \u001b[36m37.9089\u001b[0m       \u001b[32m38.5399\u001b[0m  0.0561\n",
      "     24       \u001b[36m37.6777\u001b[0m       \u001b[32m38.2321\u001b[0m  0.0552\n",
      "     25       \u001b[36m37.4480\u001b[0m       \u001b[32m37.9254\u001b[0m  0.0592\n",
      "     26       \u001b[36m37.2203\u001b[0m       \u001b[32m37.6198\u001b[0m  0.0497\n",
      "     27       \u001b[36m36.9947\u001b[0m       \u001b[32m37.3154\u001b[0m  0.0592\n",
      "     28       \u001b[36m36.7716\u001b[0m       \u001b[32m37.0122\u001b[0m  0.0490\n",
      "     29       \u001b[36m36.5508\u001b[0m       \u001b[32m36.7106\u001b[0m  0.0548\n",
      "     30       \u001b[36m36.3328\u001b[0m       \u001b[32m36.4112\u001b[0m  0.0580\n",
      "     31       \u001b[36m36.1180\u001b[0m       \u001b[32m36.1139\u001b[0m  0.0683\n",
      "     32       \u001b[36m35.9065\u001b[0m       \u001b[32m35.8195\u001b[0m  0.0537\n",
      "     33       \u001b[36m35.6987\u001b[0m       \u001b[32m35.5282\u001b[0m  0.0597\n",
      "     34       \u001b[36m35.4952\u001b[0m       \u001b[32m35.2406\u001b[0m  0.0476\n",
      "     35       \u001b[36m35.2964\u001b[0m       \u001b[32m34.9579\u001b[0m  0.0554\n",
      "     36       \u001b[36m35.1032\u001b[0m       \u001b[32m34.6809\u001b[0m  0.0425\n",
      "     37       \u001b[36m34.9160\u001b[0m       \u001b[32m34.4106\u001b[0m  0.0593\n",
      "     38       \u001b[36m34.7354\u001b[0m       \u001b[32m34.1475\u001b[0m  0.0443\n",
      "     39       \u001b[36m34.5618\u001b[0m       \u001b[32m33.8923\u001b[0m  0.0570\n",
      "     40       \u001b[36m34.3957\u001b[0m       \u001b[32m33.6457\u001b[0m  0.0467\n",
      "     41       \u001b[36m34.2374\u001b[0m       \u001b[32m33.4079\u001b[0m  0.0553\n",
      "     42       \u001b[36m34.0871\u001b[0m       \u001b[32m33.1796\u001b[0m  0.0494\n",
      "     43       \u001b[36m33.9453\u001b[0m       \u001b[32m32.9611\u001b[0m  0.0566\n",
      "     44       \u001b[36m33.8121\u001b[0m       \u001b[32m32.7524\u001b[0m  0.0456\n",
      "     45       \u001b[36m33.6874\u001b[0m       \u001b[32m32.5543\u001b[0m  0.0560\n",
      "     46       \u001b[36m33.5714\u001b[0m       \u001b[32m32.3671\u001b[0m  0.0500\n",
      "     47       \u001b[36m33.4640\u001b[0m       \u001b[32m32.1906\u001b[0m  0.0560\n",
      "     48       \u001b[36m33.3650\u001b[0m       \u001b[32m32.0250\u001b[0m  0.0442\n",
      "     49       \u001b[36m33.2743\u001b[0m       \u001b[32m31.8701\u001b[0m  0.0632\n",
      "     50       \u001b[36m33.1916\u001b[0m       \u001b[32m31.7259\u001b[0m  0.0514\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m33.4212\u001b[0m       \u001b[32m32.0482\u001b[0m  0.0462\n",
      "      2       \u001b[36m33.1050\u001b[0m       \u001b[32m31.8084\u001b[0m  0.0420\n",
      "      3       \u001b[36m32.7950\u001b[0m       \u001b[32m31.5735\u001b[0m  0.0623\n",
      "      4       \u001b[36m32.4901\u001b[0m       \u001b[32m31.3433\u001b[0m  0.0462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      5       \u001b[36m32.1900\u001b[0m       \u001b[32m31.1174\u001b[0m  0.0774\n",
      "      6       \u001b[36m31.8945\u001b[0m       \u001b[32m30.8955\u001b[0m  0.0528\n",
      "      7       \u001b[36m31.6032\u001b[0m       \u001b[32m30.6771\u001b[0m  0.0417\n",
      "      8       \u001b[36m31.3152\u001b[0m       \u001b[32m30.4616\u001b[0m  0.0489\n",
      "      9       \u001b[36m31.0296\u001b[0m       \u001b[32m30.2488\u001b[0m  0.0497\n",
      "     10       \u001b[36m30.7459\u001b[0m       \u001b[32m30.0381\u001b[0m  0.0488\n",
      "     11       \u001b[36m30.4632\u001b[0m       \u001b[32m29.8293\u001b[0m  0.0487\n",
      "     12       \u001b[36m30.1814\u001b[0m       \u001b[32m29.6228\u001b[0m  0.0490\n",
      "     13       \u001b[36m29.9008\u001b[0m       \u001b[32m29.4184\u001b[0m  0.0474\n",
      "     14       \u001b[36m29.6209\u001b[0m       \u001b[32m29.2161\u001b[0m  0.0500\n",
      "     15       \u001b[36m29.3418\u001b[0m       \u001b[32m29.0160\u001b[0m  0.0520\n",
      "     16       \u001b[36m29.0632\u001b[0m       \u001b[32m28.8181\u001b[0m  0.0487\n",
      "     17       \u001b[36m28.7855\u001b[0m       \u001b[32m28.6230\u001b[0m  0.0733\n",
      "     18       \u001b[36m28.5088\u001b[0m       \u001b[32m28.4310\u001b[0m  0.0654\n",
      "     19       \u001b[36m28.2336\u001b[0m       \u001b[32m28.2425\u001b[0m  0.0517\n",
      "     20       \u001b[36m27.9603\u001b[0m       \u001b[32m28.0582\u001b[0m  0.0614\n",
      "     21       \u001b[36m27.6894\u001b[0m       \u001b[32m27.8785\u001b[0m  0.0442\n",
      "     22       \u001b[36m27.4213\u001b[0m       \u001b[32m27.7044\u001b[0m  0.0549\n",
      "     23       \u001b[36m27.1572\u001b[0m       \u001b[32m27.5363\u001b[0m  0.0475\n",
      "     24       \u001b[36m26.8978\u001b[0m       \u001b[32m27.3752\u001b[0m  0.0564\n",
      "     25       \u001b[36m26.6442\u001b[0m       \u001b[32m27.2219\u001b[0m  0.0487\n",
      "     26       \u001b[36m26.3971\u001b[0m       \u001b[32m27.0769\u001b[0m  0.0568\n",
      "     27       \u001b[36m26.1574\u001b[0m       \u001b[32m26.9412\u001b[0m  0.0471\n",
      "     28       \u001b[36m25.9261\u001b[0m       \u001b[32m26.8154\u001b[0m  0.0557\n",
      "     29       \u001b[36m25.7041\u001b[0m       \u001b[32m26.7000\u001b[0m  0.0447\n",
      "     30       \u001b[36m25.4923\u001b[0m       \u001b[32m26.5954\u001b[0m  0.0590\n",
      "     31       \u001b[36m25.2915\u001b[0m       \u001b[32m26.5021\u001b[0m  0.0481\n",
      "     32       \u001b[36m25.1022\u001b[0m       \u001b[32m26.4203\u001b[0m  0.0585\n",
      "     33       \u001b[36m24.9249\u001b[0m       \u001b[32m26.3501\u001b[0m  0.0416\n",
      "     34       \u001b[36m24.7598\u001b[0m       \u001b[32m26.2911\u001b[0m  0.0509\n",
      "     35       \u001b[36m24.6071\u001b[0m       \u001b[32m26.2430\u001b[0m  0.0584\n",
      "     36       \u001b[36m24.4669\u001b[0m       \u001b[32m26.2053\u001b[0m  0.0618\n",
      "     37       \u001b[36m24.3390\u001b[0m       \u001b[32m26.1774\u001b[0m  0.0476\n",
      "     38       \u001b[36m24.2231\u001b[0m       \u001b[32m26.1584\u001b[0m  0.0672\n",
      "     39       \u001b[36m24.1187\u001b[0m       \u001b[32m26.1475\u001b[0m  0.0433\n",
      "     40       \u001b[36m24.0252\u001b[0m       \u001b[32m26.1437\u001b[0m  0.0675\n",
      "     41       \u001b[36m23.9421\u001b[0m       26.1461  0.0482\n",
      "     42       \u001b[36m23.8684\u001b[0m       26.1537  0.0562\n",
      "     43       \u001b[36m23.8035\u001b[0m       26.1656  0.0483\n",
      "     44       \u001b[36m23.7466\u001b[0m       26.1808  0.0586\n",
      "     45       \u001b[36m23.6967\u001b[0m       26.1985  0.0407\n",
      "     46       \u001b[36m23.6533\u001b[0m       26.2180  0.0585\n",
      "     47       \u001b[36m23.6155\u001b[0m       26.2387  0.0461\n",
      "     48       \u001b[36m23.5827\u001b[0m       26.2599  0.0535\n",
      "     49       \u001b[36m23.5541\u001b[0m       26.2812  0.0474\n",
      "     50       \u001b[36m23.5293\u001b[0m       26.3022  0.0591\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m41.8096\u001b[0m       \u001b[32m33.1755\u001b[0m  0.0334\n",
      "      2       \u001b[36m41.4408\u001b[0m       \u001b[32m32.9120\u001b[0m  0.0544\n",
      "      3       \u001b[36m41.0762\u001b[0m       \u001b[32m32.6525\u001b[0m  0.0545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      4       \u001b[36m40.7155\u001b[0m       \u001b[32m32.3963\u001b[0m  0.0632\n",
      "      5       \u001b[36m40.3572\u001b[0m       \u001b[32m32.1425\u001b[0m  0.0723\n",
      "      6       \u001b[36m40.0008\u001b[0m       \u001b[32m31.8913\u001b[0m  0.0526\n",
      "      7       \u001b[36m39.6456\u001b[0m       \u001b[32m31.6426\u001b[0m  0.0504\n",
      "      8       \u001b[36m39.2916\u001b[0m       \u001b[32m31.3959\u001b[0m  0.0512\n",
      "      9       \u001b[36m38.9378\u001b[0m       \u001b[32m31.1506\u001b[0m  0.0476\n",
      "     10       \u001b[36m38.5835\u001b[0m       \u001b[32m30.9061\u001b[0m  0.0547\n",
      "     11       \u001b[36m38.2275\u001b[0m       \u001b[32m30.6625\u001b[0m  0.0560\n",
      "     12       \u001b[36m37.8700\u001b[0m       \u001b[32m30.4196\u001b[0m  0.0495\n",
      "     13       \u001b[36m37.5104\u001b[0m       \u001b[32m30.1771\u001b[0m  0.0574\n",
      "     14       \u001b[36m37.1493\u001b[0m       \u001b[32m29.9352\u001b[0m  0.0430\n",
      "     15       \u001b[36m36.7865\u001b[0m       \u001b[32m29.6941\u001b[0m  0.0544\n",
      "     16       \u001b[36m36.4224\u001b[0m       \u001b[32m29.4544\u001b[0m  0.0423\n",
      "     17       \u001b[36m36.0570\u001b[0m       \u001b[32m29.2158\u001b[0m  0.0581\n",
      "     18       \u001b[36m35.6902\u001b[0m       \u001b[32m28.9793\u001b[0m  0.0406\n",
      "     19       \u001b[36m35.3229\u001b[0m       \u001b[32m28.7455\u001b[0m  0.0495\n",
      "     20       \u001b[36m34.9560\u001b[0m       \u001b[32m28.5153\u001b[0m  0.0480\n",
      "     21       \u001b[36m34.5903\u001b[0m       \u001b[32m28.2894\u001b[0m  0.0582\n",
      "     22       \u001b[36m34.2263\u001b[0m       \u001b[32m28.0685\u001b[0m  0.0564\n",
      "     23       \u001b[36m33.8650\u001b[0m       \u001b[32m27.8538\u001b[0m  0.0622\n",
      "     24       \u001b[36m33.5075\u001b[0m       \u001b[32m27.6463\u001b[0m  0.0491\n",
      "     25       \u001b[36m33.1550\u001b[0m       \u001b[32m27.4475\u001b[0m  0.0601\n",
      "     26       \u001b[36m32.8093\u001b[0m       \u001b[32m27.2586\u001b[0m  0.0482\n",
      "     27       \u001b[36m32.4719\u001b[0m       \u001b[32m27.0811\u001b[0m  0.0575\n",
      "     28       \u001b[36m32.1443\u001b[0m       \u001b[32m26.9163\u001b[0m  0.0434\n",
      "     29       \u001b[36m31.8281\u001b[0m       \u001b[32m26.7656\u001b[0m  0.0495\n",
      "     30       \u001b[36m31.5251\u001b[0m       \u001b[32m26.6304\u001b[0m  0.0482\n",
      "     31       \u001b[36m31.2371\u001b[0m       \u001b[32m26.5112\u001b[0m  0.0490\n",
      "     32       \u001b[36m30.9652\u001b[0m       \u001b[32m26.4095\u001b[0m  0.0479\n",
      "     33       \u001b[36m30.7115\u001b[0m       \u001b[32m26.3256\u001b[0m  0.0494\n",
      "     34       \u001b[36m30.4769\u001b[0m       \u001b[32m26.2599\u001b[0m  0.0551\n",
      "     35       \u001b[36m30.2623\u001b[0m       \u001b[32m26.2119\u001b[0m  0.0558\n",
      "     36       \u001b[36m30.0679\u001b[0m       \u001b[32m26.1814\u001b[0m  0.0472\n",
      "     37       \u001b[36m29.8938\u001b[0m       \u001b[32m26.1670\u001b[0m  0.0518\n",
      "     38       \u001b[36m29.7396\u001b[0m       26.1671  0.0461\n",
      "     39       \u001b[36m29.6045\u001b[0m       26.1800  0.0573\n",
      "     40       \u001b[36m29.4875\u001b[0m       26.2038  0.0566\n",
      "     41       \u001b[36m29.3871\u001b[0m       26.2366  0.0615\n",
      "     42       \u001b[36m29.3018\u001b[0m       26.2761  0.0515\n",
      "     43       \u001b[36m29.2299\u001b[0m       26.3205  0.0652\n",
      "     44       \u001b[36m29.1697\u001b[0m       26.3679  0.0496\n",
      "     45       \u001b[36m29.1195\u001b[0m       26.4168  0.0614\n",
      "     46       \u001b[36m29.0779\u001b[0m       26.4658  0.0506\n",
      "     47       \u001b[36m29.0433\u001b[0m       26.5138  0.0588\n",
      "     48       \u001b[36m29.0146\u001b[0m       26.5601  0.0482\n",
      "     49       \u001b[36m28.9906\u001b[0m       26.6041  0.0618\n",
      "     50       \u001b[36m28.9703\u001b[0m       26.6453  0.0514\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m43.0339\u001b[0m       \u001b[32m44.8608\u001b[0m  0.0511\n",
      "      2       \u001b[36m42.5068\u001b[0m       \u001b[32m44.1944\u001b[0m  0.0523\n",
      "      3       \u001b[36m41.9820\u001b[0m       \u001b[32m43.4858\u001b[0m  0.0636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      4       \u001b[36m41.4306\u001b[0m       \u001b[32m42.7143\u001b[0m  0.0531\n",
      "      5       \u001b[36m40.8351\u001b[0m       \u001b[32m41.8590\u001b[0m  0.0726\n",
      "      6       \u001b[36m40.1841\u001b[0m       \u001b[32m40.9280\u001b[0m  0.0561\n",
      "      7       \u001b[36m39.4751\u001b[0m       \u001b[32m39.9170\u001b[0m  0.0645\n",
      "      8       \u001b[36m38.6951\u001b[0m       \u001b[32m38.7726\u001b[0m  0.0498\n",
      "      9       \u001b[36m37.8049\u001b[0m       \u001b[32m37.4438\u001b[0m  0.0933\n",
      "     10       \u001b[36m36.8026\u001b[0m       \u001b[32m35.9688\u001b[0m  0.0927\n",
      "     11       \u001b[36m35.7662\u001b[0m       \u001b[32m34.4627\u001b[0m  0.0478\n",
      "     12       \u001b[36m34.8386\u001b[0m       \u001b[32m33.1255\u001b[0m  0.0572\n",
      "     13       \u001b[36m34.1916\u001b[0m       \u001b[32m32.1755\u001b[0m  0.0465\n",
      "     14       \u001b[36m33.9031\u001b[0m       \u001b[32m31.6621\u001b[0m  0.0594\n",
      "     15       \u001b[36m33.8284\u001b[0m       \u001b[32m31.4316\u001b[0m  0.0482\n",
      "     16       \u001b[36m33.7466\u001b[0m       \u001b[32m31.3330\u001b[0m  0.0548\n",
      "     17       \u001b[36m33.6044\u001b[0m       \u001b[32m31.3168\u001b[0m  0.0473\n",
      "     18       \u001b[36m33.4641\u001b[0m       31.3383  0.0600\n",
      "     19       \u001b[36m33.3577\u001b[0m       31.3333  0.0448\n",
      "     20       \u001b[36m33.2701\u001b[0m       \u001b[32m31.2757\u001b[0m  0.0575\n",
      "     21       \u001b[36m33.1882\u001b[0m       \u001b[32m31.1741\u001b[0m  0.0474\n",
      "     22       \u001b[36m33.1098\u001b[0m       \u001b[32m31.0535\u001b[0m  0.0544\n",
      "     23       \u001b[36m33.0381\u001b[0m       \u001b[32m30.9369\u001b[0m  0.0493\n",
      "     24       \u001b[36m32.9739\u001b[0m       \u001b[32m30.8373\u001b[0m  0.0604\n",
      "     25       \u001b[36m32.9166\u001b[0m       \u001b[32m30.7585\u001b[0m  0.1172\n",
      "     26       \u001b[36m32.8665\u001b[0m       \u001b[32m30.6974\u001b[0m  0.0969\n",
      "     27       \u001b[36m32.8219\u001b[0m       \u001b[32m30.6485\u001b[0m  0.0677\n",
      "     28       \u001b[36m32.7819\u001b[0m       \u001b[32m30.6051\u001b[0m  0.0890\n",
      "     29       \u001b[36m32.7458\u001b[0m       \u001b[32m30.5638\u001b[0m  0.0346\n",
      "     30       \u001b[36m32.7135\u001b[0m       \u001b[32m30.5278\u001b[0m  0.0382\n",
      "     31       \u001b[36m32.6843\u001b[0m       \u001b[32m30.4979\u001b[0m  0.0338\n",
      "     32       \u001b[36m32.6569\u001b[0m       \u001b[32m30.4727\u001b[0m  0.0314\n",
      "     33       \u001b[36m32.6316\u001b[0m       \u001b[32m30.4502\u001b[0m  0.0295\n",
      "     34       \u001b[36m32.6080\u001b[0m       \u001b[32m30.4282\u001b[0m  0.0311\n",
      "     35       \u001b[36m32.5860\u001b[0m       \u001b[32m30.4084\u001b[0m  0.0315\n",
      "     36       \u001b[36m32.5657\u001b[0m       \u001b[32m30.3937\u001b[0m  0.0304\n",
      "     37       \u001b[36m32.5469\u001b[0m       \u001b[32m30.3798\u001b[0m  0.0331\n",
      "     38       \u001b[36m32.5292\u001b[0m       \u001b[32m30.3654\u001b[0m  0.0301\n",
      "     39       \u001b[36m32.5127\u001b[0m       \u001b[32m30.3521\u001b[0m  0.0310\n",
      "     40       \u001b[36m32.4970\u001b[0m       \u001b[32m30.3408\u001b[0m  0.0302\n",
      "     41       \u001b[36m32.4824\u001b[0m       \u001b[32m30.3316\u001b[0m  0.0361\n",
      "     42       \u001b[36m32.4684\u001b[0m       \u001b[32m30.3223\u001b[0m  0.0318\n",
      "     43       \u001b[36m32.4552\u001b[0m       \u001b[32m30.3122\u001b[0m  0.0316\n",
      "     44       \u001b[36m32.4428\u001b[0m       \u001b[32m30.3029\u001b[0m  0.0318\n",
      "     45       \u001b[36m32.4312\u001b[0m       \u001b[32m30.2946\u001b[0m  0.0425\n",
      "     46       \u001b[36m32.4199\u001b[0m       \u001b[32m30.2865\u001b[0m  0.0401\n",
      "     47       \u001b[36m32.4093\u001b[0m       \u001b[32m30.2798\u001b[0m  0.0373\n",
      "     48       \u001b[36m32.3992\u001b[0m       \u001b[32m30.2736\u001b[0m  0.0403\n",
      "     49       \u001b[36m32.3895\u001b[0m       \u001b[32m30.2655\u001b[0m  0.0390\n",
      "     50       \u001b[36m32.3804\u001b[0m       \u001b[32m30.2564\u001b[0m  0.0291\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m33.9587\u001b[0m       \u001b[32m32.4707\u001b[0m  0.0273\n",
      "      2       \u001b[36m33.3857\u001b[0m       \u001b[32m32.0430\u001b[0m  0.0306\n",
      "      3       \u001b[36m32.7702\u001b[0m       \u001b[32m31.5557\u001b[0m  0.0293\n",
      "      4       \u001b[36m32.0793\u001b[0m       \u001b[32m31.0065\u001b[0m  0.0304\n",
      "      5       \u001b[36m31.3121\u001b[0m       \u001b[32m30.3910\u001b[0m  0.0314\n",
      "      6       \u001b[36m30.4733\u001b[0m       \u001b[32m29.7064\u001b[0m  0.0298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      7       \u001b[36m29.5461\u001b[0m       \u001b[32m28.9551\u001b[0m  0.0330\n",
      "      8       \u001b[36m28.5087\u001b[0m       \u001b[32m28.1880\u001b[0m  0.0298\n",
      "      9       \u001b[36m27.4064\u001b[0m       \u001b[32m27.5284\u001b[0m  0.0293\n",
      "     10       \u001b[36m26.3559\u001b[0m       \u001b[32m27.1520\u001b[0m  0.0296\n",
      "     11       \u001b[36m25.5213\u001b[0m       27.1680  0.0285\n",
      "     12       \u001b[36m25.0335\u001b[0m       27.4769  0.0296\n",
      "     13       \u001b[36m24.8509\u001b[0m       27.6973  0.0310\n",
      "     14       \u001b[36m24.7421\u001b[0m       27.5809  0.0309\n",
      "     15       \u001b[36m24.5793\u001b[0m       27.2773  0.0306\n",
      "     16       \u001b[36m24.4143\u001b[0m       \u001b[32m26.9973\u001b[0m  0.0353\n",
      "     17       \u001b[36m24.2887\u001b[0m       \u001b[32m26.8168\u001b[0m  0.0304\n",
      "     18       \u001b[36m24.1912\u001b[0m       \u001b[32m26.7212\u001b[0m  0.0312\n",
      "     19       \u001b[36m24.1008\u001b[0m       \u001b[32m26.6886\u001b[0m  0.0323\n",
      "     20       \u001b[36m24.0114\u001b[0m       26.7004  0.0309\n",
      "     21       \u001b[36m23.9304\u001b[0m       26.7338  0.0308\n",
      "     22       \u001b[36m23.8603\u001b[0m       26.7690  0.0310\n",
      "     23       \u001b[36m23.7993\u001b[0m       26.7954  0.0417\n",
      "     24       \u001b[36m23.7452\u001b[0m       26.8131  0.0438\n",
      "     25       \u001b[36m23.6966\u001b[0m       26.8275  0.0409\n",
      "     26       \u001b[36m23.6526\u001b[0m       26.8429  0.0418\n",
      "     27       \u001b[36m23.6130\u001b[0m       26.8622  0.0380\n",
      "     28       \u001b[36m23.5770\u001b[0m       26.8858  0.0373\n",
      "     29       \u001b[36m23.5439\u001b[0m       26.9101  0.0301\n",
      "     30       \u001b[36m23.5133\u001b[0m       26.9329  0.0316\n",
      "     31       \u001b[36m23.4854\u001b[0m       26.9517  0.0323\n",
      "     32       \u001b[36m23.4603\u001b[0m       26.9661  0.0304\n",
      "     33       \u001b[36m23.4372\u001b[0m       26.9784  0.0318\n",
      "     34       \u001b[36m23.4163\u001b[0m       26.9886  0.0317\n",
      "     35       \u001b[36m23.3971\u001b[0m       26.9962  0.0312\n",
      "     36       \u001b[36m23.3794\u001b[0m       27.0031  0.0316\n",
      "     37       \u001b[36m23.3631\u001b[0m       27.0102  0.0319\n",
      "     38       \u001b[36m23.3477\u001b[0m       27.0169  0.0327\n",
      "     39       \u001b[36m23.3334\u001b[0m       27.0255  0.0308\n",
      "     40       \u001b[36m23.3196\u001b[0m       27.0329  0.0316\n",
      "     41       \u001b[36m23.3067\u001b[0m       27.0359  0.0315\n",
      "     42       \u001b[36m23.2949\u001b[0m       27.0395  0.0318\n",
      "     43       \u001b[36m23.2836\u001b[0m       27.0429  0.0312\n",
      "     44       \u001b[36m23.2733\u001b[0m       27.0450  0.0506\n",
      "     45       \u001b[36m23.2636\u001b[0m       27.0470  0.0425\n",
      "     46       \u001b[36m23.2546\u001b[0m       27.0473  0.0310\n",
      "     47       \u001b[36m23.2460\u001b[0m       27.0473  0.0312\n",
      "     48       \u001b[36m23.2379\u001b[0m       27.0456  0.0304\n",
      "     49       \u001b[36m23.2304\u001b[0m       27.0452  0.0314\n",
      "     50       \u001b[36m23.2234\u001b[0m       27.0458  0.0306\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m40.4919\u001b[0m       \u001b[32m32.1878\u001b[0m  0.0294\n",
      "      2       \u001b[36m39.8253\u001b[0m       \u001b[32m31.7797\u001b[0m  0.0522\n",
      "      3       \u001b[36m39.1793\u001b[0m       \u001b[32m31.3835\u001b[0m  0.0464\n",
      "      4       \u001b[36m38.5995\u001b[0m       \u001b[32m31.0281\u001b[0m  0.0412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      5       \u001b[36m38.1060\u001b[0m       \u001b[32m30.6882\u001b[0m  0.0418\n",
      "      6       \u001b[36m37.6152\u001b[0m       \u001b[32m30.2985\u001b[0m  0.0343\n",
      "      7       \u001b[36m37.0257\u001b[0m       \u001b[32m29.8137\u001b[0m  0.0326\n",
      "      8       \u001b[36m36.3014\u001b[0m       \u001b[32m29.2321\u001b[0m  0.0313\n",
      "      9       \u001b[36m35.4356\u001b[0m       \u001b[32m28.5629\u001b[0m  0.0293\n",
      "     10       \u001b[36m34.4033\u001b[0m       \u001b[32m27.8313\u001b[0m  0.0299\n",
      "     11       \u001b[36m33.1991\u001b[0m       \u001b[32m27.1314\u001b[0m  0.0318\n",
      "     12       \u001b[36m31.9256\u001b[0m       \u001b[32m26.6774\u001b[0m  0.0300\n",
      "     13       \u001b[36m30.8056\u001b[0m       26.6999  0.0313\n",
      "     14       \u001b[36m30.0921\u001b[0m       27.2621  0.0307\n",
      "     15       \u001b[36m29.8734\u001b[0m       27.9643  0.0312\n",
      "     16       29.8879       28.2068  0.0304\n",
      "     17       \u001b[36m29.8012\u001b[0m       27.9562  0.0301\n",
      "     18       \u001b[36m29.6135\u001b[0m       27.5836  0.0330\n",
      "     19       \u001b[36m29.4483\u001b[0m       27.3203  0.0321\n",
      "     20       \u001b[36m29.3381\u001b[0m       27.2023  0.0318\n",
      "     21       \u001b[36m29.2573\u001b[0m       27.1980  0.0308\n",
      "     22       \u001b[36m29.1892\u001b[0m       27.2631  0.0356\n",
      "     23       \u001b[36m29.1303\u001b[0m       27.3534  0.0300\n",
      "     24       \u001b[36m29.0799\u001b[0m       27.4328  0.0314\n",
      "     25       \u001b[36m29.0335\u001b[0m       27.4809  0.0301\n",
      "     26       \u001b[36m28.9895\u001b[0m       27.4931  0.0327\n",
      "     27       \u001b[36m28.9475\u001b[0m       27.4798  0.0315\n",
      "     28       \u001b[36m28.9065\u001b[0m       27.4559  0.0315\n",
      "     29       \u001b[36m28.8691\u001b[0m       27.4320  0.0368\n",
      "     30       \u001b[36m28.8340\u001b[0m       27.4153  0.0354\n",
      "     31       \u001b[36m28.8028\u001b[0m       27.4035  0.0524\n",
      "     32       \u001b[36m28.7747\u001b[0m       27.3945  0.0445\n",
      "     33       \u001b[36m28.7485\u001b[0m       27.3875  0.0396\n",
      "     34       \u001b[36m28.7248\u001b[0m       27.3767  0.0428\n",
      "     35       \u001b[36m28.7025\u001b[0m       27.3673  0.0326\n",
      "     36       \u001b[36m28.6820\u001b[0m       27.3627  0.0293\n",
      "     37       \u001b[36m28.6631\u001b[0m       27.3577  0.0296\n",
      "     38       \u001b[36m28.6462\u001b[0m       27.3559  0.0308\n",
      "     39       \u001b[36m28.6311\u001b[0m       27.3555  0.0547\n",
      "     40       \u001b[36m28.6171\u001b[0m       27.3519  0.0342\n",
      "     41       \u001b[36m28.6041\u001b[0m       27.3478  0.0299\n",
      "     42       \u001b[36m28.5922\u001b[0m       27.3425  0.0307\n",
      "     43       \u001b[36m28.5812\u001b[0m       27.3371  0.0295\n",
      "     44       \u001b[36m28.5708\u001b[0m       27.3307  0.0294\n",
      "     45       \u001b[36m28.5610\u001b[0m       27.3239  0.0323\n",
      "     46       \u001b[36m28.5522\u001b[0m       27.3193  0.0312\n",
      "     47       \u001b[36m28.5441\u001b[0m       27.3180  0.0298\n",
      "     48       \u001b[36m28.5368\u001b[0m       27.3173  0.0304\n",
      "     49       \u001b[36m28.5299\u001b[0m       27.3157  0.0302\n",
      "     50       \u001b[36m28.5234\u001b[0m       27.3133  0.0298\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m43.7052\u001b[0m       \u001b[32m45.7428\u001b[0m  0.0278\n",
      "      2       \u001b[36m43.3687\u001b[0m       \u001b[32m45.3325\u001b[0m  0.0283\n",
      "      3       \u001b[36m43.0360\u001b[0m       \u001b[32m44.9258\u001b[0m  0.0285\n",
      "      4       \u001b[36m42.7069\u001b[0m       \u001b[32m44.5217\u001b[0m  0.0288\n",
      "      5       \u001b[36m42.3813\u001b[0m       \u001b[32m44.1198\u001b[0m  0.0294\n",
      "      6       \u001b[36m42.0582\u001b[0m       \u001b[32m43.7195\u001b[0m  0.0340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      7       \u001b[36m41.7374\u001b[0m       \u001b[32m43.3197\u001b[0m  0.0578\n",
      "      8       \u001b[36m41.4181\u001b[0m       \u001b[32m42.9206\u001b[0m  0.0664\n",
      "      9       \u001b[36m41.1006\u001b[0m       \u001b[32m42.5224\u001b[0m  0.0328\n",
      "     10       \u001b[36m40.7847\u001b[0m       \u001b[32m42.1242\u001b[0m  0.0398\n",
      "     11       \u001b[36m40.4702\u001b[0m       \u001b[32m41.7263\u001b[0m  0.0346\n",
      "     12       \u001b[36m40.1571\u001b[0m       \u001b[32m41.3286\u001b[0m  0.0344\n",
      "     13       \u001b[36m39.8455\u001b[0m       \u001b[32m40.9317\u001b[0m  0.0462\n",
      "     14       \u001b[36m39.5358\u001b[0m       \u001b[32m40.5357\u001b[0m  0.0311\n",
      "     15       \u001b[36m39.2282\u001b[0m       \u001b[32m40.1409\u001b[0m  0.0289\n",
      "     16       \u001b[36m38.9230\u001b[0m       \u001b[32m39.7474\u001b[0m  0.0296\n",
      "     17       \u001b[36m38.6202\u001b[0m       \u001b[32m39.3556\u001b[0m  0.0303\n",
      "     18       \u001b[36m38.3203\u001b[0m       \u001b[32m38.9661\u001b[0m  0.0304\n",
      "     19       \u001b[36m38.0235\u001b[0m       \u001b[32m38.5794\u001b[0m  0.0288\n",
      "     20       \u001b[36m37.7305\u001b[0m       \u001b[32m38.1960\u001b[0m  0.0291\n",
      "     21       \u001b[36m37.4418\u001b[0m       \u001b[32m37.8166\u001b[0m  0.0293\n",
      "     22       \u001b[36m37.1579\u001b[0m       \u001b[32m37.4418\u001b[0m  0.0293\n",
      "     23       \u001b[36m36.8792\u001b[0m       \u001b[32m37.0721\u001b[0m  0.0292\n",
      "     24       \u001b[36m36.6063\u001b[0m       \u001b[32m36.7080\u001b[0m  0.0302\n",
      "     25       \u001b[36m36.3399\u001b[0m       \u001b[32m36.3504\u001b[0m  0.0286\n",
      "     26       \u001b[36m36.0807\u001b[0m       \u001b[32m36.0002\u001b[0m  0.0296\n",
      "     27       \u001b[36m35.8291\u001b[0m       \u001b[32m35.6578\u001b[0m  0.0300\n",
      "     28       \u001b[36m35.5858\u001b[0m       \u001b[32m35.3238\u001b[0m  0.0296\n",
      "     29       \u001b[36m35.3512\u001b[0m       \u001b[32m34.9988\u001b[0m  0.0339\n",
      "     30       \u001b[36m35.1259\u001b[0m       \u001b[32m34.6832\u001b[0m  0.0301\n",
      "     31       \u001b[36m34.9102\u001b[0m       \u001b[32m34.3776\u001b[0m  0.0294\n",
      "     32       \u001b[36m34.7047\u001b[0m       \u001b[32m34.0829\u001b[0m  0.0298\n",
      "     33       \u001b[36m34.5099\u001b[0m       \u001b[32m33.7996\u001b[0m  0.0298\n",
      "     34       \u001b[36m34.3259\u001b[0m       \u001b[32m33.5283\u001b[0m  0.0295\n",
      "     35       \u001b[36m34.1531\u001b[0m       \u001b[32m33.2693\u001b[0m  0.0304\n",
      "     36       \u001b[36m33.9915\u001b[0m       \u001b[32m33.0229\u001b[0m  0.0300\n",
      "     37       \u001b[36m33.8413\u001b[0m       \u001b[32m32.7895\u001b[0m  0.0372\n",
      "     38       \u001b[36m33.7023\u001b[0m       \u001b[32m32.5695\u001b[0m  0.0416\n",
      "     39       \u001b[36m33.5745\u001b[0m       \u001b[32m32.3627\u001b[0m  0.0554\n",
      "     40       \u001b[36m33.4575\u001b[0m       \u001b[32m32.1691\u001b[0m  0.0563\n",
      "     41       \u001b[36m33.3512\u001b[0m       \u001b[32m31.9889\u001b[0m  0.0395\n",
      "     42       \u001b[36m33.2551\u001b[0m       \u001b[32m31.8217\u001b[0m  0.0521\n",
      "     43       \u001b[36m33.1687\u001b[0m       \u001b[32m31.6670\u001b[0m  0.0345\n",
      "     44       \u001b[36m33.0913\u001b[0m       \u001b[32m31.5246\u001b[0m  0.0296\n",
      "     45       \u001b[36m33.0224\u001b[0m       \u001b[32m31.3940\u001b[0m  0.0285\n",
      "     46       \u001b[36m32.9613\u001b[0m       \u001b[32m31.2745\u001b[0m  0.0295\n",
      "     47       \u001b[36m32.9072\u001b[0m       \u001b[32m31.1657\u001b[0m  0.0295\n",
      "     48       \u001b[36m32.8595\u001b[0m       \u001b[32m31.0666\u001b[0m  0.0303\n",
      "     49       \u001b[36m32.8175\u001b[0m       \u001b[32m30.9765\u001b[0m  0.0304\n",
      "     50       \u001b[36m32.7805\u001b[0m       \u001b[32m30.8948\u001b[0m  0.0296\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m35.5448\u001b[0m       \u001b[32m33.7714\u001b[0m  0.0274\n",
      "      2       \u001b[36m35.1929\u001b[0m       \u001b[32m33.5021\u001b[0m  0.0293\n",
      "      3       \u001b[36m34.8529\u001b[0m       \u001b[32m33.2419\u001b[0m  0.0310\n",
      "      4       \u001b[36m34.5232\u001b[0m       \u001b[32m32.9900\u001b[0m  0.0297\n",
      "      5       \u001b[36m34.2028\u001b[0m       \u001b[32m32.7455\u001b[0m  0.0306\n",
      "      6       \u001b[36m33.8904\u001b[0m       \u001b[32m32.5078\u001b[0m  0.0303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      7       \u001b[36m33.5853\u001b[0m       \u001b[32m32.2763\u001b[0m  0.0318\n",
      "      8       \u001b[36m33.2869\u001b[0m       \u001b[32m32.0502\u001b[0m  0.0298\n",
      "      9       \u001b[36m32.9940\u001b[0m       \u001b[32m31.8289\u001b[0m  0.0290\n",
      "     10       \u001b[36m32.7058\u001b[0m       \u001b[32m31.6118\u001b[0m  0.0302\n",
      "     11       \u001b[36m32.4214\u001b[0m       \u001b[32m31.3986\u001b[0m  0.0294\n",
      "     12       \u001b[36m32.1407\u001b[0m       \u001b[32m31.1891\u001b[0m  0.0291\n",
      "     13       \u001b[36m31.8636\u001b[0m       \u001b[32m30.9830\u001b[0m  0.0297\n",
      "     14       \u001b[36m31.5898\u001b[0m       \u001b[32m30.7804\u001b[0m  0.0335\n",
      "     15       \u001b[36m31.3190\u001b[0m       \u001b[32m30.5812\u001b[0m  0.0567\n",
      "     16       \u001b[36m31.0513\u001b[0m       \u001b[32m30.3854\u001b[0m  0.0639\n",
      "     17       \u001b[36m30.7866\u001b[0m       \u001b[32m30.1927\u001b[0m  0.0386\n",
      "     18       \u001b[36m30.5245\u001b[0m       \u001b[32m30.0030\u001b[0m  0.0381\n",
      "     19       \u001b[36m30.2648\u001b[0m       \u001b[32m29.8164\u001b[0m  0.0370\n",
      "     20       \u001b[36m30.0078\u001b[0m       \u001b[32m29.6332\u001b[0m  0.0433\n",
      "     21       \u001b[36m29.7539\u001b[0m       \u001b[32m29.4530\u001b[0m  0.0300\n",
      "     22       \u001b[36m29.5029\u001b[0m       \u001b[32m29.2761\u001b[0m  0.0291\n",
      "     23       \u001b[36m29.2550\u001b[0m       \u001b[32m29.1027\u001b[0m  0.0288\n",
      "     24       \u001b[36m29.0105\u001b[0m       \u001b[32m28.9331\u001b[0m  0.0295\n",
      "     25       \u001b[36m28.7696\u001b[0m       \u001b[32m28.7673\u001b[0m  0.0300\n",
      "     26       \u001b[36m28.5325\u001b[0m       \u001b[32m28.6055\u001b[0m  0.0282\n",
      "     27       \u001b[36m28.2993\u001b[0m       \u001b[32m28.4480\u001b[0m  0.0293\n",
      "     28       \u001b[36m28.0705\u001b[0m       \u001b[32m28.2948\u001b[0m  0.0284\n",
      "     29       \u001b[36m27.8461\u001b[0m       \u001b[32m28.1460\u001b[0m  0.0297\n",
      "     30       \u001b[36m27.6261\u001b[0m       \u001b[32m28.0016\u001b[0m  0.0292\n",
      "     31       \u001b[36m27.4106\u001b[0m       \u001b[32m27.8619\u001b[0m  0.0300\n",
      "     32       \u001b[36m27.1998\u001b[0m       \u001b[32m27.7269\u001b[0m  0.0298\n",
      "     33       \u001b[36m26.9939\u001b[0m       \u001b[32m27.5967\u001b[0m  0.0295\n",
      "     34       \u001b[36m26.7929\u001b[0m       \u001b[32m27.4713\u001b[0m  0.0292\n",
      "     35       \u001b[36m26.5971\u001b[0m       \u001b[32m27.3508\u001b[0m  0.0289\n",
      "     36       \u001b[36m26.4067\u001b[0m       \u001b[32m27.2359\u001b[0m  0.0388\n",
      "     37       \u001b[36m26.2225\u001b[0m       \u001b[32m27.1263\u001b[0m  0.0332\n",
      "     38       \u001b[36m26.0439\u001b[0m       \u001b[32m27.0220\u001b[0m  0.0345\n",
      "     39       \u001b[36m25.8707\u001b[0m       \u001b[32m26.9233\u001b[0m  0.0557\n",
      "     40       \u001b[36m25.7032\u001b[0m       \u001b[32m26.8302\u001b[0m  0.0808\n",
      "     41       \u001b[36m25.5415\u001b[0m       \u001b[32m26.7426\u001b[0m  0.0400\n",
      "     42       \u001b[36m25.3858\u001b[0m       \u001b[32m26.6611\u001b[0m  0.0452\n",
      "     43       \u001b[36m25.2360\u001b[0m       \u001b[32m26.5856\u001b[0m  0.0384\n",
      "     44       \u001b[36m25.0924\u001b[0m       \u001b[32m26.5161\u001b[0m  0.0404\n",
      "     45       \u001b[36m24.9550\u001b[0m       \u001b[32m26.4525\u001b[0m  0.0404\n",
      "     46       \u001b[36m24.8240\u001b[0m       \u001b[32m26.3951\u001b[0m  0.0533\n",
      "     47       \u001b[36m24.6995\u001b[0m       \u001b[32m26.3438\u001b[0m  0.0773\n",
      "     48       \u001b[36m24.5816\u001b[0m       \u001b[32m26.2984\u001b[0m  0.0323\n",
      "     49       \u001b[36m24.4701\u001b[0m       \u001b[32m26.2590\u001b[0m  0.0328\n",
      "     50       \u001b[36m24.3653\u001b[0m       \u001b[32m26.2253\u001b[0m  0.0328\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m40.9101\u001b[0m       \u001b[32m32.4620\u001b[0m  0.0302\n",
      "      2       \u001b[36m40.5190\u001b[0m       \u001b[32m32.1914\u001b[0m  0.0282\n",
      "      3       \u001b[36m40.1376\u001b[0m       \u001b[32m31.9274\u001b[0m  0.0282\n",
      "      4       \u001b[36m39.7639\u001b[0m       \u001b[32m31.6696\u001b[0m  0.0301\n",
      "      5       \u001b[36m39.3970\u001b[0m       \u001b[32m31.4171\u001b[0m  0.0293\n",
      "      6       \u001b[36m39.0351\u001b[0m       \u001b[32m31.1693\u001b[0m  0.0289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      7       \u001b[36m38.6777\u001b[0m       \u001b[32m30.9258\u001b[0m  0.0297\n",
      "      8       \u001b[36m38.3240\u001b[0m       \u001b[32m30.6862\u001b[0m  0.0282\n",
      "      9       \u001b[36m37.9733\u001b[0m       \u001b[32m30.4503\u001b[0m  0.0298\n",
      "     10       \u001b[36m37.6257\u001b[0m       \u001b[32m30.2178\u001b[0m  0.0290\n",
      "     11       \u001b[36m37.2810\u001b[0m       \u001b[32m29.9887\u001b[0m  0.0284\n",
      "     12       \u001b[36m36.9386\u001b[0m       \u001b[32m29.7629\u001b[0m  0.0284\n",
      "     13       \u001b[36m36.5986\u001b[0m       \u001b[32m29.5407\u001b[0m  0.0285\n",
      "     14       \u001b[36m36.2613\u001b[0m       \u001b[32m29.3221\u001b[0m  0.0761\n",
      "     15       \u001b[36m35.9265\u001b[0m       \u001b[32m29.1067\u001b[0m  0.0305\n",
      "     16       \u001b[36m35.5935\u001b[0m       \u001b[32m28.8948\u001b[0m  0.0290\n",
      "     17       \u001b[36m35.2628\u001b[0m       \u001b[32m28.6870\u001b[0m  0.0328\n",
      "     18       \u001b[36m34.9350\u001b[0m       \u001b[32m28.4836\u001b[0m  0.0531\n",
      "     19       \u001b[36m34.6106\u001b[0m       \u001b[32m28.2852\u001b[0m  0.0446\n",
      "     20       \u001b[36m34.2900\u001b[0m       \u001b[32m28.0920\u001b[0m  0.0409\n",
      "     21       \u001b[36m33.9735\u001b[0m       \u001b[32m27.9044\u001b[0m  0.0421\n",
      "     22       \u001b[36m33.6613\u001b[0m       \u001b[32m27.7232\u001b[0m  0.0372\n",
      "     23       \u001b[36m33.3543\u001b[0m       \u001b[32m27.5489\u001b[0m  0.0285\n",
      "     24       \u001b[36m33.0532\u001b[0m       \u001b[32m27.3821\u001b[0m  0.0276\n",
      "     25       \u001b[36m32.7583\u001b[0m       \u001b[32m27.2235\u001b[0m  0.0336\n",
      "     26       \u001b[36m32.4702\u001b[0m       \u001b[32m27.0736\u001b[0m  0.0412\n",
      "     27       \u001b[36m32.1895\u001b[0m       \u001b[32m26.9332\u001b[0m  0.0954\n",
      "     28       \u001b[36m31.9167\u001b[0m       \u001b[32m26.8029\u001b[0m  0.0341\n",
      "     29       \u001b[36m31.6525\u001b[0m       \u001b[32m26.6834\u001b[0m  0.0281\n",
      "     30       \u001b[36m31.3980\u001b[0m       \u001b[32m26.5754\u001b[0m  0.0284\n",
      "     31       \u001b[36m31.1539\u001b[0m       \u001b[32m26.4795\u001b[0m  0.0288\n",
      "     32       \u001b[36m30.9209\u001b[0m       \u001b[32m26.3960\u001b[0m  0.0298\n",
      "     33       \u001b[36m30.6997\u001b[0m       \u001b[32m26.3257\u001b[0m  0.0284\n",
      "     34       \u001b[36m30.4919\u001b[0m       \u001b[32m26.2685\u001b[0m  0.0288\n",
      "     35       \u001b[36m30.2975\u001b[0m       \u001b[32m26.2243\u001b[0m  0.0290\n",
      "     36       \u001b[36m30.1169\u001b[0m       \u001b[32m26.1932\u001b[0m  0.0281\n",
      "     37       \u001b[36m29.9501\u001b[0m       \u001b[32m26.1745\u001b[0m  0.0308\n",
      "     38       \u001b[36m29.7977\u001b[0m       \u001b[32m26.1678\u001b[0m  0.0284\n",
      "     39       \u001b[36m29.6594\u001b[0m       26.1722  0.0289\n",
      "     40       \u001b[36m29.5347\u001b[0m       26.1865  0.0288\n",
      "     41       \u001b[36m29.4235\u001b[0m       26.2096  0.0307\n",
      "     42       \u001b[36m29.3252\u001b[0m       26.2402  0.0317\n",
      "     43       \u001b[36m29.2390\u001b[0m       26.2770  0.0317\n",
      "     44       \u001b[36m29.1638\u001b[0m       26.3186  0.0689\n",
      "     45       \u001b[36m29.0988\u001b[0m       26.3638  0.1456\n",
      "     46       \u001b[36m29.0429\u001b[0m       26.4113  0.1460\n",
      "     47       \u001b[36m28.9951\u001b[0m       26.4601  0.1353\n",
      "     48       \u001b[36m28.9545\u001b[0m       26.5091  0.0618\n",
      "     49       \u001b[36m28.9201\u001b[0m       26.5575  0.0724\n",
      "     50       \u001b[36m28.8911\u001b[0m       26.6046  0.0481\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m42.6445\u001b[0m       \u001b[32m44.0464\u001b[0m  0.0565\n",
      "      2       \u001b[36m41.8662\u001b[0m       \u001b[32m43.0012\u001b[0m  0.0482\n",
      "      3       \u001b[36m41.0475\u001b[0m       \u001b[32m41.8883\u001b[0m  0.0581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      4       \u001b[36m40.1634\u001b[0m       \u001b[32m40.7071\u001b[0m  0.0587\n",
      "      5       \u001b[36m39.2040\u001b[0m       \u001b[32m39.3906\u001b[0m  0.1178\n",
      "      6       \u001b[36m38.1158\u001b[0m       \u001b[32m37.8193\u001b[0m  0.1225\n",
      "      7       \u001b[36m36.8664\u001b[0m       \u001b[32m35.9981\u001b[0m  0.1054\n",
      "      8       \u001b[36m35.5653\u001b[0m       \u001b[32m34.1343\u001b[0m  0.1306\n",
      "      9       \u001b[36m34.4595\u001b[0m       \u001b[32m32.5899\u001b[0m  0.1246\n",
      "     10       \u001b[36m33.8281\u001b[0m       \u001b[32m31.6810\u001b[0m  0.1098\n",
      "     11       \u001b[36m33.6869\u001b[0m       \u001b[32m31.3331\u001b[0m  0.0368\n",
      "     12       \u001b[36m33.6585\u001b[0m       \u001b[32m31.2280\u001b[0m  0.0357\n",
      "     13       \u001b[36m33.5026\u001b[0m       31.2461  0.0344\n",
      "     14       \u001b[36m33.3248\u001b[0m       31.3181  0.0387\n",
      "     15       \u001b[36m33.2058\u001b[0m       31.3308  0.0477\n",
      "     16       \u001b[36m33.1185\u001b[0m       31.2477  0.0376\n",
      "     17       \u001b[36m33.0380\u001b[0m       \u001b[32m31.1035\u001b[0m  0.0550\n",
      "     18       \u001b[36m32.9623\u001b[0m       \u001b[32m30.9503\u001b[0m  0.0417\n",
      "     19       \u001b[36m32.8966\u001b[0m       \u001b[32m30.8236\u001b[0m  0.0396\n",
      "     20       \u001b[36m32.8417\u001b[0m       \u001b[32m30.7360\u001b[0m  0.0462\n",
      "     21       \u001b[36m32.7947\u001b[0m       \u001b[32m30.6814\u001b[0m  0.0415\n",
      "     22       \u001b[36m32.7525\u001b[0m       \u001b[32m30.6429\u001b[0m  0.0391\n",
      "     23       \u001b[36m32.7131\u001b[0m       \u001b[32m30.6120\u001b[0m  0.0412\n",
      "     24       \u001b[36m32.6767\u001b[0m       \u001b[32m30.5820\u001b[0m  0.0485\n",
      "     25       \u001b[36m32.6433\u001b[0m       \u001b[32m30.5509\u001b[0m  0.1048\n",
      "     26       \u001b[36m32.6124\u001b[0m       \u001b[32m30.5195\u001b[0m  0.1355\n",
      "     27       \u001b[36m32.5843\u001b[0m       \u001b[32m30.4887\u001b[0m  0.1364\n",
      "     28       \u001b[36m32.5585\u001b[0m       \u001b[32m30.4599\u001b[0m  0.0805\n",
      "     29       \u001b[36m32.5348\u001b[0m       \u001b[32m30.4351\u001b[0m  0.0475\n",
      "     30       \u001b[36m32.5131\u001b[0m       \u001b[32m30.4137\u001b[0m  0.0555\n",
      "     31       \u001b[36m32.4929\u001b[0m       \u001b[32m30.3957\u001b[0m  0.0617\n",
      "     32       \u001b[36m32.4744\u001b[0m       \u001b[32m30.3805\u001b[0m  0.0445\n",
      "     33       \u001b[36m32.4572\u001b[0m       \u001b[32m30.3677\u001b[0m  0.0483\n",
      "     34       \u001b[36m32.4415\u001b[0m       \u001b[32m30.3570\u001b[0m  0.0924\n",
      "     35       \u001b[36m32.4269\u001b[0m       \u001b[32m30.3456\u001b[0m  0.1444\n",
      "     36       \u001b[36m32.4135\u001b[0m       \u001b[32m30.3334\u001b[0m  0.0998\n",
      "     37       \u001b[36m32.4007\u001b[0m       \u001b[32m30.3238\u001b[0m  0.0290\n",
      "     38       \u001b[36m32.3890\u001b[0m       \u001b[32m30.3165\u001b[0m  0.0277\n",
      "     39       \u001b[36m32.3780\u001b[0m       \u001b[32m30.3101\u001b[0m  0.0277\n",
      "     40       \u001b[36m32.3676\u001b[0m       \u001b[32m30.3034\u001b[0m  0.0282\n",
      "     41       \u001b[36m32.3580\u001b[0m       \u001b[32m30.2985\u001b[0m  0.0820\n",
      "     42       \u001b[36m32.3489\u001b[0m       \u001b[32m30.2927\u001b[0m  0.1305\n",
      "     43       \u001b[36m32.3403\u001b[0m       \u001b[32m30.2883\u001b[0m  0.1257\n",
      "     44       \u001b[36m32.3325\u001b[0m       \u001b[32m30.2858\u001b[0m  0.1025\n",
      "     45       \u001b[36m32.3252\u001b[0m       \u001b[32m30.2811\u001b[0m  0.0509\n",
      "     46       \u001b[36m32.3182\u001b[0m       \u001b[32m30.2750\u001b[0m  0.0518\n",
      "     47       \u001b[36m32.3117\u001b[0m       \u001b[32m30.2700\u001b[0m  0.0819\n",
      "     48       \u001b[36m32.3055\u001b[0m       \u001b[32m30.2651\u001b[0m  0.1324\n",
      "     49       \u001b[36m32.2996\u001b[0m       \u001b[32m30.2596\u001b[0m  0.1456\n",
      "     50       \u001b[36m32.2942\u001b[0m       \u001b[32m30.2557\u001b[0m  0.1022\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m34.0983\u001b[0m       \u001b[32m32.3393\u001b[0m  0.0568\n",
      "      2       \u001b[36m33.1359\u001b[0m       \u001b[32m31.6133\u001b[0m  0.0477\n",
      "      3       \u001b[36m32.1407\u001b[0m       \u001b[32m30.8473\u001b[0m  0.0459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      4       \u001b[36m31.0672\u001b[0m       \u001b[32m30.0450\u001b[0m  0.0546\n",
      "      5       \u001b[36m29.9290\u001b[0m       \u001b[32m29.2265\u001b[0m  0.0662\n",
      "      6       \u001b[36m28.7574\u001b[0m       \u001b[32m28.4023\u001b[0m  0.0504\n",
      "      7       \u001b[36m27.5486\u001b[0m       \u001b[32m27.6063\u001b[0m  0.0553\n",
      "      8       \u001b[36m26.3443\u001b[0m       \u001b[32m27.0252\u001b[0m  0.0268\n",
      "      9       \u001b[36m25.3391\u001b[0m       \u001b[32m26.9219\u001b[0m  0.0684\n",
      "     10       \u001b[36m24.7772\u001b[0m       27.2815  0.0569\n",
      "     11       \u001b[36m24.6227\u001b[0m       27.5767  0.0612\n",
      "     12       \u001b[36m24.5219\u001b[0m       27.4505  0.0820\n",
      "     13       \u001b[36m24.3207\u001b[0m       27.1335  0.1319\n",
      "     14       \u001b[36m24.1279\u001b[0m       \u001b[32m26.8916\u001b[0m  0.1098\n",
      "     15       \u001b[36m24.0032\u001b[0m       \u001b[32m26.7743\u001b[0m  0.0735\n",
      "     16       \u001b[36m23.9076\u001b[0m       \u001b[32m26.7446\u001b[0m  0.1105\n",
      "     17       \u001b[36m23.8120\u001b[0m       26.7758  0.1302\n",
      "     18       \u001b[36m23.7226\u001b[0m       26.8459  0.1001\n",
      "     19       \u001b[36m23.6498\u001b[0m       26.9162  0.0241\n",
      "     20       \u001b[36m23.5907\u001b[0m       26.9556  0.0578\n",
      "     21       \u001b[36m23.5400\u001b[0m       26.9642  0.0541\n",
      "     22       \u001b[36m23.4965\u001b[0m       26.9561  0.0432\n",
      "     23       \u001b[36m23.4587\u001b[0m       26.9466  0.1114\n",
      "     24       \u001b[36m23.4253\u001b[0m       26.9412  0.0928\n",
      "     25       \u001b[36m23.3954\u001b[0m       26.9389  0.0638\n",
      "     26       \u001b[36m23.3687\u001b[0m       26.9367  0.0445\n",
      "     27       \u001b[36m23.3442\u001b[0m       26.9350  0.0456\n",
      "     28       \u001b[36m23.3218\u001b[0m       26.9312  0.0555\n",
      "     29       \u001b[36m23.3013\u001b[0m       26.9244  0.0569\n",
      "     30       \u001b[36m23.2826\u001b[0m       26.9166  0.0485\n",
      "     31       \u001b[36m23.2655\u001b[0m       26.9103  0.0570\n",
      "     32       \u001b[36m23.2497\u001b[0m       26.9064  0.0591\n",
      "     33       \u001b[36m23.2351\u001b[0m       26.9056  0.0635\n",
      "     34       \u001b[36m23.2217\u001b[0m       26.9044  0.0416\n",
      "     35       \u001b[36m23.2093\u001b[0m       26.9037  0.0550\n",
      "     36       \u001b[36m23.1981\u001b[0m       26.9016  0.0633\n",
      "     37       \u001b[36m23.1878\u001b[0m       26.8989  0.0791\n",
      "     38       \u001b[36m23.1784\u001b[0m       26.8967  0.0682\n",
      "     39       \u001b[36m23.1697\u001b[0m       26.8936  0.0711\n",
      "     40       \u001b[36m23.1616\u001b[0m       26.8891  0.0672\n",
      "     41       \u001b[36m23.1542\u001b[0m       26.8853  0.0566\n",
      "     42       \u001b[36m23.1474\u001b[0m       26.8827  0.0628\n",
      "     43       \u001b[36m23.1411\u001b[0m       26.8785  0.0680\n",
      "     44       \u001b[36m23.1353\u001b[0m       26.8726  0.0649\n",
      "     45       \u001b[36m23.1299\u001b[0m       26.8683  0.0599\n",
      "     46       \u001b[36m23.1248\u001b[0m       26.8656  0.0459\n",
      "     47       \u001b[36m23.1200\u001b[0m       26.8648  0.0539\n",
      "     48       \u001b[36m23.1154\u001b[0m       26.8614  0.0609\n",
      "     49       \u001b[36m23.1111\u001b[0m       26.8573  0.0529\n",
      "     50       \u001b[36m23.1070\u001b[0m       26.8528  0.0620\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m40.6024\u001b[0m       \u001b[32m32.1594\u001b[0m  0.0568\n",
      "      2       \u001b[36m39.8055\u001b[0m       \u001b[32m31.6569\u001b[0m  0.0613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      3       \u001b[36m39.0131\u001b[0m       \u001b[32m31.1505\u001b[0m  0.1029\n",
      "      4       \u001b[36m38.2271\u001b[0m       \u001b[32m30.6235\u001b[0m  0.1051\n",
      "      5       \u001b[36m37.4490\u001b[0m       \u001b[32m30.0237\u001b[0m  0.0535\n",
      "      6       \u001b[36m36.5393\u001b[0m       \u001b[32m29.2765\u001b[0m  0.0626\n",
      "      7       \u001b[36m35.3973\u001b[0m       \u001b[32m28.4074\u001b[0m  0.0511\n",
      "      8       \u001b[36m34.0386\u001b[0m       \u001b[32m27.5237\u001b[0m  0.0342\n",
      "      9       \u001b[36m32.5668\u001b[0m       \u001b[32m26.8641\u001b[0m  0.0730\n",
      "     10       \u001b[36m31.1969\u001b[0m       \u001b[32m26.7492\u001b[0m  0.0465\n",
      "     11       \u001b[36m30.2553\u001b[0m       27.3691  0.0584\n",
      "     12       \u001b[36m29.9619\u001b[0m       28.2422  0.0599\n",
      "     13       29.9947       28.4690  0.0506\n",
      "     14       \u001b[36m29.8634\u001b[0m       28.0381  0.0372\n",
      "     15       \u001b[36m29.5985\u001b[0m       27.5160  0.0799\n",
      "     16       \u001b[36m29.3956\u001b[0m       27.1967  0.0546\n",
      "     17       \u001b[36m29.2711\u001b[0m       27.0781  0.0571\n",
      "     18       \u001b[36m29.1814\u001b[0m       27.0990  0.0499\n",
      "     19       \u001b[36m29.1069\u001b[0m       27.2003  0.0567\n",
      "     20       \u001b[36m29.0480\u001b[0m       27.3235  0.0643\n",
      "     21       \u001b[36m29.0013\u001b[0m       27.4170  0.0735\n",
      "     22       \u001b[36m28.9588\u001b[0m       27.4507  0.0761\n",
      "     23       \u001b[36m28.9141\u001b[0m       27.4338  0.0886\n",
      "     24       \u001b[36m28.8686\u001b[0m       27.3945  0.0509\n",
      "     25       \u001b[36m28.8264\u001b[0m       27.3618  0.0575\n",
      "     26       \u001b[36m28.7894\u001b[0m       27.3483  0.0496\n",
      "     27       \u001b[36m28.7583\u001b[0m       27.3484  0.0580\n",
      "     28       \u001b[36m28.7313\u001b[0m       27.3541  0.0486\n",
      "     29       \u001b[36m28.7068\u001b[0m       27.3616  0.0578\n",
      "     30       \u001b[36m28.6843\u001b[0m       27.3682  0.0567\n",
      "     31       \u001b[36m28.6636\u001b[0m       27.3725  0.0632\n",
      "     32       \u001b[36m28.6442\u001b[0m       27.3751  0.0529\n",
      "     33       \u001b[36m28.6260\u001b[0m       27.3775  0.0601\n",
      "     34       \u001b[36m28.6092\u001b[0m       27.3817  0.0493\n",
      "     35       \u001b[36m28.5940\u001b[0m       27.3890  0.0595\n",
      "     36       \u001b[36m28.5803\u001b[0m       27.3969  0.0589\n",
      "     37       \u001b[36m28.5680\u001b[0m       27.4036  0.0871\n",
      "     38       \u001b[36m28.5568\u001b[0m       27.4088  0.1010\n",
      "     39       \u001b[36m28.5465\u001b[0m       27.4135  0.0774\n",
      "     40       \u001b[36m28.5369\u001b[0m       27.4186  0.0578\n",
      "     41       \u001b[36m28.5283\u001b[0m       27.4238  0.0516\n",
      "     42       \u001b[36m28.5203\u001b[0m       27.4292  0.0605\n",
      "     43       \u001b[36m28.5128\u001b[0m       27.4353  0.0491\n",
      "     44       \u001b[36m28.5061\u001b[0m       27.4391  0.1019\n",
      "     45       \u001b[36m28.5000\u001b[0m       27.4403  0.0498\n",
      "     46       \u001b[36m28.4944\u001b[0m       27.4412  0.0658\n",
      "     47       \u001b[36m28.4893\u001b[0m       27.4431  0.0589\n",
      "     48       \u001b[36m28.4846\u001b[0m       27.4447  0.0519\n",
      "     49       \u001b[36m28.4800\u001b[0m       27.4430  0.0581\n",
      "     50       \u001b[36m28.4757\u001b[0m       27.4410  0.0648\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m42.6486\u001b[0m       \u001b[32m44.4678\u001b[0m  0.0498\n",
      "      2       \u001b[36m42.2753\u001b[0m       \u001b[32m44.0093\u001b[0m  0.0717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      3       \u001b[36m41.9122\u001b[0m       \u001b[32m43.5615\u001b[0m  0.0891\n",
      "      4       \u001b[36m41.5580\u001b[0m       \u001b[32m43.1232\u001b[0m  0.0709\n",
      "      5       \u001b[36m41.2119\u001b[0m       \u001b[32m42.6928\u001b[0m  0.0417\n",
      "      6       \u001b[36m40.8727\u001b[0m       \u001b[32m42.2698\u001b[0m  0.0586\n",
      "      7       \u001b[36m40.5399\u001b[0m       \u001b[32m41.8529\u001b[0m  0.0464\n",
      "      8       \u001b[36m40.2129\u001b[0m       \u001b[32m41.4415\u001b[0m  0.0545\n",
      "      9       \u001b[36m39.8914\u001b[0m       \u001b[32m41.0352\u001b[0m  0.0464\n",
      "     10       \u001b[36m39.5751\u001b[0m       \u001b[32m40.6338\u001b[0m  0.0543\n",
      "     11       \u001b[36m39.2638\u001b[0m       \u001b[32m40.2375\u001b[0m  0.0473\n",
      "     12       \u001b[36m38.9573\u001b[0m       \u001b[32m39.8459\u001b[0m  0.0550\n",
      "     13       \u001b[36m38.6555\u001b[0m       \u001b[32m39.4587\u001b[0m  0.0436\n",
      "     14       \u001b[36m38.3582\u001b[0m       \u001b[32m39.0757\u001b[0m  0.0573\n",
      "     15       \u001b[36m38.0655\u001b[0m       \u001b[32m38.6971\u001b[0m  0.0464\n",
      "     16       \u001b[36m37.7774\u001b[0m       \u001b[32m38.3229\u001b[0m  0.0570\n",
      "     17       \u001b[36m37.4941\u001b[0m       \u001b[32m37.9530\u001b[0m  0.0534\n",
      "     18       \u001b[36m37.2159\u001b[0m       \u001b[32m37.5876\u001b[0m  0.0575\n",
      "     19       \u001b[36m36.9431\u001b[0m       \u001b[32m37.2268\u001b[0m  0.0593\n",
      "     20       \u001b[36m36.6757\u001b[0m       \u001b[32m36.8713\u001b[0m  0.0537\n",
      "     21       \u001b[36m36.4142\u001b[0m       \u001b[32m36.5211\u001b[0m  0.0502\n",
      "     22       \u001b[36m36.1587\u001b[0m       \u001b[32m36.1771\u001b[0m  0.0574\n",
      "     23       \u001b[36m35.9099\u001b[0m       \u001b[32m35.8396\u001b[0m  0.0461\n",
      "     24       \u001b[36m35.6683\u001b[0m       \u001b[32m35.5089\u001b[0m  0.0552\n",
      "     25       \u001b[36m35.4341\u001b[0m       \u001b[32m35.1854\u001b[0m  0.0504\n",
      "     26       \u001b[36m35.2078\u001b[0m       \u001b[32m34.8694\u001b[0m  0.0562\n",
      "     27       \u001b[36m34.9899\u001b[0m       \u001b[32m34.5620\u001b[0m  0.0424\n",
      "     28       \u001b[36m34.7810\u001b[0m       \u001b[32m34.2637\u001b[0m  0.0499\n",
      "     29       \u001b[36m34.5814\u001b[0m       \u001b[32m33.9753\u001b[0m  0.0523\n",
      "     30       \u001b[36m34.3914\u001b[0m       \u001b[32m33.6971\u001b[0m  0.0572\n",
      "     31       \u001b[36m34.2116\u001b[0m       \u001b[32m33.4299\u001b[0m  0.0492\n",
      "     32       \u001b[36m34.0420\u001b[0m       \u001b[32m33.1741\u001b[0m  0.0505\n",
      "     33       \u001b[36m33.8832\u001b[0m       \u001b[32m32.9305\u001b[0m  0.0502\n",
      "     34       \u001b[36m33.7351\u001b[0m       \u001b[32m32.6996\u001b[0m  0.0490\n",
      "     35       \u001b[36m33.5979\u001b[0m       \u001b[32m32.4814\u001b[0m  0.0522\n",
      "     36       \u001b[36m33.4714\u001b[0m       \u001b[32m32.2761\u001b[0m  0.0711\n",
      "     37       \u001b[36m33.3556\u001b[0m       \u001b[32m32.0836\u001b[0m  0.0533\n",
      "     38       \u001b[36m33.2501\u001b[0m       \u001b[32m31.9040\u001b[0m  0.0539\n",
      "     39       \u001b[36m33.1546\u001b[0m       \u001b[32m31.7374\u001b[0m  0.0503\n",
      "     40       \u001b[36m33.0689\u001b[0m       \u001b[32m31.5838\u001b[0m  0.0562\n",
      "     41       \u001b[36m32.9923\u001b[0m       \u001b[32m31.4425\u001b[0m  0.0458\n",
      "     42       \u001b[36m32.9243\u001b[0m       \u001b[32m31.3130\u001b[0m  0.0539\n",
      "     43       \u001b[36m32.8645\u001b[0m       \u001b[32m31.1946\u001b[0m  0.0475\n",
      "     44       \u001b[36m32.8118\u001b[0m       \u001b[32m31.0868\u001b[0m  0.0490\n",
      "     45       \u001b[36m32.7658\u001b[0m       \u001b[32m30.9888\u001b[0m  0.0476\n",
      "     46       \u001b[36m32.7256\u001b[0m       \u001b[32m30.9000\u001b[0m  0.0480\n",
      "     47       \u001b[36m32.6906\u001b[0m       \u001b[32m30.8196\u001b[0m  0.0488\n",
      "     48       \u001b[36m32.6603\u001b[0m       \u001b[32m30.7471\u001b[0m  0.0484\n",
      "     49       \u001b[36m32.6341\u001b[0m       \u001b[32m30.6816\u001b[0m  0.0487\n",
      "     50       \u001b[36m32.6113\u001b[0m       \u001b[32m30.6225\u001b[0m  0.0595\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m34.5280\u001b[0m       \u001b[32m32.9521\u001b[0m  0.0333\n",
      "      2       \u001b[36m34.0820\u001b[0m       \u001b[32m32.6057\u001b[0m  0.0510\n",
      "      3       \u001b[36m33.6534\u001b[0m       \u001b[32m32.2740\u001b[0m  0.0576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      4       \u001b[36m33.2396\u001b[0m       \u001b[32m31.9545\u001b[0m  0.0655\n",
      "      5       \u001b[36m32.8382\u001b[0m       \u001b[32m31.6458\u001b[0m  0.0811\n",
      "      6       \u001b[36m32.4473\u001b[0m       \u001b[32m31.3462\u001b[0m  0.0528\n",
      "      7       \u001b[36m32.0653\u001b[0m       \u001b[32m31.0551\u001b[0m  0.0589\n",
      "      8       \u001b[36m31.6913\u001b[0m       \u001b[32m30.7711\u001b[0m  0.0494\n",
      "      9       \u001b[36m31.3238\u001b[0m       \u001b[32m30.4939\u001b[0m  0.0556\n",
      "     10       \u001b[36m30.9624\u001b[0m       \u001b[32m30.2229\u001b[0m  0.0476\n",
      "     11       \u001b[36m30.6062\u001b[0m       \u001b[32m29.9577\u001b[0m  0.0572\n",
      "     12       \u001b[36m30.2546\u001b[0m       \u001b[32m29.6980\u001b[0m  0.0480\n",
      "     13       \u001b[36m29.9075\u001b[0m       \u001b[32m29.4438\u001b[0m  0.0559\n",
      "     14       \u001b[36m29.5647\u001b[0m       \u001b[32m29.1953\u001b[0m  0.0488\n",
      "     15       \u001b[36m29.2263\u001b[0m       \u001b[32m28.9529\u001b[0m  0.0585\n",
      "     16       \u001b[36m28.8924\u001b[0m       \u001b[32m28.7169\u001b[0m  0.0422\n",
      "     17       \u001b[36m28.5633\u001b[0m       \u001b[32m28.4877\u001b[0m  0.0488\n",
      "     18       \u001b[36m28.2394\u001b[0m       \u001b[32m28.2658\u001b[0m  0.0473\n",
      "     19       \u001b[36m27.9214\u001b[0m       \u001b[32m28.0521\u001b[0m  0.0485\n",
      "     20       \u001b[36m27.6099\u001b[0m       \u001b[32m27.8472\u001b[0m  0.0496\n",
      "     21       \u001b[36m27.3059\u001b[0m       \u001b[32m27.6519\u001b[0m  0.0483\n",
      "     22       \u001b[36m27.0101\u001b[0m       \u001b[32m27.4671\u001b[0m  0.0561\n",
      "     23       \u001b[36m26.7239\u001b[0m       \u001b[32m27.2937\u001b[0m  0.0614\n",
      "     24       \u001b[36m26.4481\u001b[0m       \u001b[32m27.1326\u001b[0m  0.0484\n",
      "     25       \u001b[36m26.1841\u001b[0m       \u001b[32m26.9847\u001b[0m  0.0478\n",
      "     26       \u001b[36m25.9326\u001b[0m       \u001b[32m26.8506\u001b[0m  0.0597\n",
      "     27       \u001b[36m25.6947\u001b[0m       \u001b[32m26.7306\u001b[0m  0.0549\n",
      "     28       \u001b[36m25.4709\u001b[0m       \u001b[32m26.6249\u001b[0m  0.0436\n",
      "     29       \u001b[36m25.2617\u001b[0m       \u001b[32m26.5334\u001b[0m  0.0484\n",
      "     30       \u001b[36m25.0675\u001b[0m       \u001b[32m26.4562\u001b[0m  0.0490\n",
      "     31       \u001b[36m24.8885\u001b[0m       \u001b[32m26.3925\u001b[0m  0.0485\n",
      "     32       \u001b[36m24.7248\u001b[0m       \u001b[32m26.3419\u001b[0m  0.0492\n",
      "     33       \u001b[36m24.5761\u001b[0m       \u001b[32m26.3035\u001b[0m  0.0484\n",
      "     34       \u001b[36m24.4420\u001b[0m       \u001b[32m26.2761\u001b[0m  0.0480\n",
      "     35       \u001b[36m24.3218\u001b[0m       \u001b[32m26.2587\u001b[0m  0.0485\n",
      "     36       \u001b[36m24.2148\u001b[0m       \u001b[32m26.2500\u001b[0m  0.0469\n",
      "     37       \u001b[36m24.1203\u001b[0m       \u001b[32m26.2486\u001b[0m  0.0499\n",
      "     38       \u001b[36m24.0370\u001b[0m       26.2533  0.0480\n",
      "     39       \u001b[36m23.9638\u001b[0m       26.2630  0.0594\n",
      "     40       \u001b[36m23.8999\u001b[0m       26.2764  0.0457\n",
      "     41       \u001b[36m23.8441\u001b[0m       26.2926  0.0561\n",
      "     42       \u001b[36m23.7956\u001b[0m       26.3106  0.0541\n",
      "     43       \u001b[36m23.7534\u001b[0m       26.3297  0.0659\n",
      "     44       \u001b[36m23.7165\u001b[0m       26.3495  0.0512\n",
      "     45       \u001b[36m23.6844\u001b[0m       26.3693  0.0565\n",
      "     46       \u001b[36m23.6563\u001b[0m       26.3886  0.0442\n",
      "     47       \u001b[36m23.6317\u001b[0m       26.4071  0.0503\n",
      "     48       \u001b[36m23.6099\u001b[0m       26.4245  0.0524\n",
      "     49       \u001b[36m23.5905\u001b[0m       26.4408  0.0571\n",
      "     50       \u001b[36m23.5732\u001b[0m       26.4558  0.0461\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m42.7822\u001b[0m       \u001b[32m33.6011\u001b[0m  0.0525\n",
      "      2       \u001b[36m42.0628\u001b[0m       \u001b[32m33.1178\u001b[0m  0.0486\n",
      "      3       \u001b[36m41.3939\u001b[0m       \u001b[32m32.6668\u001b[0m  0.0419\n",
      "      4       \u001b[36m40.7653\u001b[0m       \u001b[32m32.2424\u001b[0m  0.0539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      5       \u001b[36m40.1688\u001b[0m       \u001b[32m31.8398\u001b[0m  0.0516\n",
      "      6       \u001b[36m39.5987\u001b[0m       \u001b[32m31.4551\u001b[0m  0.0746\n",
      "      7       \u001b[36m39.0501\u001b[0m       \u001b[32m31.0862\u001b[0m  0.0461\n",
      "      8       \u001b[36m38.5192\u001b[0m       \u001b[32m30.7310\u001b[0m  0.0553\n",
      "      9       \u001b[36m38.0034\u001b[0m       \u001b[32m30.3884\u001b[0m  0.0581\n",
      "     10       \u001b[36m37.5007\u001b[0m       \u001b[32m30.0569\u001b[0m  0.0566\n",
      "     11       \u001b[36m37.0100\u001b[0m       \u001b[32m29.7365\u001b[0m  0.0514\n",
      "     12       \u001b[36m36.5311\u001b[0m       \u001b[32m29.4268\u001b[0m  0.0596\n",
      "     13       \u001b[36m36.0633\u001b[0m       \u001b[32m29.1279\u001b[0m  0.0616\n",
      "     14       \u001b[36m35.6065\u001b[0m       \u001b[32m28.8399\u001b[0m  0.0565\n",
      "     15       \u001b[36m35.1611\u001b[0m       \u001b[32m28.5631\u001b[0m  0.0449\n",
      "     16       \u001b[36m34.7274\u001b[0m       \u001b[32m28.2984\u001b[0m  0.0565\n",
      "     17       \u001b[36m34.3064\u001b[0m       \u001b[32m28.0465\u001b[0m  0.0471\n",
      "     18       \u001b[36m33.8986\u001b[0m       \u001b[32m27.8076\u001b[0m  0.0576\n",
      "     19       \u001b[36m33.5044\u001b[0m       \u001b[32m27.5827\u001b[0m  0.0491\n",
      "     20       \u001b[36m33.1247\u001b[0m       \u001b[32m27.3723\u001b[0m  0.0578\n",
      "     21       \u001b[36m32.7603\u001b[0m       \u001b[32m27.1771\u001b[0m  0.0437\n",
      "     22       \u001b[36m32.4116\u001b[0m       \u001b[32m26.9978\u001b[0m  0.0491\n",
      "     23       \u001b[36m32.0789\u001b[0m       \u001b[32m26.8349\u001b[0m  0.0481\n",
      "     24       \u001b[36m31.7629\u001b[0m       \u001b[32m26.6888\u001b[0m  0.0578\n",
      "     25       \u001b[36m31.4638\u001b[0m       \u001b[32m26.5596\u001b[0m  0.0475\n",
      "     26       \u001b[36m31.1813\u001b[0m       \u001b[32m26.4474\u001b[0m  0.0562\n",
      "     27       \u001b[36m30.9159\u001b[0m       \u001b[32m26.3528\u001b[0m  0.0553\n",
      "     28       \u001b[36m30.6681\u001b[0m       \u001b[32m26.2753\u001b[0m  0.0694\n",
      "     29       \u001b[36m30.4379\u001b[0m       \u001b[32m26.2154\u001b[0m  0.0539\n",
      "     30       \u001b[36m30.2260\u001b[0m       \u001b[32m26.1722\u001b[0m  0.0510\n",
      "     31       \u001b[36m30.0321\u001b[0m       \u001b[32m26.1451\u001b[0m  0.0579\n",
      "     32       \u001b[36m29.8563\u001b[0m       \u001b[32m26.1329\u001b[0m  0.0524\n",
      "     33       \u001b[36m29.6991\u001b[0m       26.1342  0.0461\n",
      "     34       \u001b[36m29.5590\u001b[0m       26.1477  0.0596\n",
      "     35       \u001b[36m29.4354\u001b[0m       26.1717  0.0465\n",
      "     36       \u001b[36m29.3272\u001b[0m       26.2046  0.0554\n",
      "     37       \u001b[36m29.2333\u001b[0m       26.2445  0.0482\n",
      "     38       \u001b[36m29.1524\u001b[0m       26.2899  0.0524\n",
      "     39       \u001b[36m29.0832\u001b[0m       26.3390  0.0473\n",
      "     40       \u001b[36m29.0244\u001b[0m       26.3904  0.0586\n",
      "     41       \u001b[36m28.9748\u001b[0m       26.4425  0.0464\n",
      "     42       \u001b[36m28.9335\u001b[0m       26.4943  0.0554\n",
      "     43       \u001b[36m28.8990\u001b[0m       26.5449  0.0500\n",
      "     44       \u001b[36m28.8700\u001b[0m       26.5935  0.0552\n",
      "     45       \u001b[36m28.8456\u001b[0m       26.6396  0.0450\n",
      "     46       \u001b[36m28.8250\u001b[0m       26.6831  0.0648\n",
      "     47       \u001b[36m28.8075\u001b[0m       26.7237  0.0550\n",
      "     48       \u001b[36m28.7925\u001b[0m       26.7613  0.0654\n",
      "     49       \u001b[36m28.7796\u001b[0m       26.7958  0.0674\n",
      "     50       \u001b[36m28.7683\u001b[0m       26.8273  0.0608\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m43.0521\u001b[0m       \u001b[32m44.7371\u001b[0m  0.0322\n",
      "      2       \u001b[36m42.4292\u001b[0m       \u001b[32m43.9808\u001b[0m  0.0608\n",
      "      3       \u001b[36m41.8687\u001b[0m       \u001b[32m43.2562\u001b[0m  0.0492\n",
      "      4       \u001b[36m41.3334\u001b[0m       \u001b[32m42.5472\u001b[0m  0.0582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      5       \u001b[36m40.7993\u001b[0m       \u001b[32m41.8236\u001b[0m  0.0665\n",
      "      6       \u001b[36m40.2288\u001b[0m       \u001b[32m40.9974\u001b[0m  0.0480\n",
      "      7       \u001b[36m39.5431\u001b[0m       \u001b[32m39.9502\u001b[0m  0.0499\n",
      "      8       \u001b[36m38.6818\u001b[0m       \u001b[32m38.6700\u001b[0m  0.0528\n",
      "      9       \u001b[36m37.6763\u001b[0m       \u001b[32m37.2256\u001b[0m  0.0570\n",
      "     10       \u001b[36m36.6052\u001b[0m       \u001b[32m35.7110\u001b[0m  0.0503\n",
      "     11       \u001b[36m35.5737\u001b[0m       \u001b[32m34.2547\u001b[0m  0.0640\n",
      "     12       \u001b[36m34.7163\u001b[0m       \u001b[32m33.0345\u001b[0m  0.0551\n",
      "     13       \u001b[36m34.1512\u001b[0m       \u001b[32m32.1823\u001b[0m  0.0663\n",
      "     14       \u001b[36m33.8886\u001b[0m       \u001b[32m31.7013\u001b[0m  0.0561\n",
      "     15       \u001b[36m33.7914\u001b[0m       \u001b[32m31.4660\u001b[0m  0.0655\n",
      "     16       \u001b[36m33.6990\u001b[0m       \u001b[32m31.3587\u001b[0m  0.0513\n",
      "     17       \u001b[36m33.5690\u001b[0m       \u001b[32m31.3218\u001b[0m  0.0552\n",
      "     18       \u001b[36m33.4361\u001b[0m       \u001b[32m31.3183\u001b[0m  0.0492\n",
      "     19       \u001b[36m33.3287\u001b[0m       \u001b[32m31.3022\u001b[0m  0.0548\n",
      "     20       \u001b[36m33.2410\u001b[0m       \u001b[32m31.2474\u001b[0m  0.0508\n",
      "     21       \u001b[36m33.1621\u001b[0m       \u001b[32m31.1617\u001b[0m  0.0627\n",
      "     22       \u001b[36m33.0873\u001b[0m       \u001b[32m31.0597\u001b[0m  0.0566\n",
      "     23       \u001b[36m33.0181\u001b[0m       \u001b[32m30.9572\u001b[0m  0.0634\n",
      "     24       \u001b[36m32.9557\u001b[0m       \u001b[32m30.8646\u001b[0m  0.0488\n",
      "     25       \u001b[36m32.8997\u001b[0m       \u001b[32m30.7880\u001b[0m  0.0587\n",
      "     26       \u001b[36m32.8491\u001b[0m       \u001b[32m30.7270\u001b[0m  0.0497\n",
      "     27       \u001b[36m32.8031\u001b[0m       \u001b[32m30.6793\u001b[0m  0.0614\n",
      "     28       \u001b[36m32.7613\u001b[0m       \u001b[32m30.6411\u001b[0m  0.0473\n",
      "     29       \u001b[36m32.7233\u001b[0m       \u001b[32m30.6076\u001b[0m  0.0588\n",
      "     30       \u001b[36m32.6884\u001b[0m       \u001b[32m30.5783\u001b[0m  0.0599\n",
      "     31       \u001b[36m32.6562\u001b[0m       \u001b[32m30.5486\u001b[0m  0.0696\n",
      "     32       \u001b[36m32.6263\u001b[0m       \u001b[32m30.5213\u001b[0m  0.0540\n",
      "     33       \u001b[36m32.5984\u001b[0m       \u001b[32m30.4972\u001b[0m  0.0614\n",
      "     34       \u001b[36m32.5723\u001b[0m       \u001b[32m30.4749\u001b[0m  0.0490\n",
      "     35       \u001b[36m32.5478\u001b[0m       \u001b[32m30.4558\u001b[0m  0.0574\n",
      "     36       \u001b[36m32.5249\u001b[0m       \u001b[32m30.4390\u001b[0m  0.0446\n",
      "     37       \u001b[36m32.5036\u001b[0m       \u001b[32m30.4246\u001b[0m  0.0519\n",
      "     38       \u001b[36m32.4836\u001b[0m       \u001b[32m30.4130\u001b[0m  0.0543\n",
      "     39       \u001b[36m32.4650\u001b[0m       \u001b[32m30.4023\u001b[0m  0.0579\n",
      "     40       \u001b[36m32.4474\u001b[0m       \u001b[32m30.3898\u001b[0m  0.0457\n",
      "     41       \u001b[36m32.4305\u001b[0m       \u001b[32m30.3783\u001b[0m  0.0517\n",
      "     42       \u001b[36m32.4150\u001b[0m       \u001b[32m30.3685\u001b[0m  0.0546\n",
      "     43       \u001b[36m32.4003\u001b[0m       \u001b[32m30.3554\u001b[0m  0.0555\n",
      "     44       \u001b[36m32.3866\u001b[0m       \u001b[32m30.3426\u001b[0m  0.0504\n",
      "     45       \u001b[36m32.3741\u001b[0m       \u001b[32m30.3324\u001b[0m  0.0499\n",
      "     46       \u001b[36m32.3624\u001b[0m       \u001b[32m30.3226\u001b[0m  0.0556\n",
      "     47       \u001b[36m32.3516\u001b[0m       \u001b[32m30.3154\u001b[0m  0.0574\n",
      "     48       \u001b[36m32.3415\u001b[0m       \u001b[32m30.3086\u001b[0m  0.0582\n",
      "     49       \u001b[36m32.3321\u001b[0m       \u001b[32m30.3021\u001b[0m  0.0651\n",
      "     50       \u001b[36m32.3233\u001b[0m       \u001b[32m30.2950\u001b[0m  0.0560\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m33.9307\u001b[0m       \u001b[32m32.3421\u001b[0m  0.0435\n",
      "      2       \u001b[36m33.2119\u001b[0m       \u001b[32m31.9145\u001b[0m  0.0467\n",
      "      3       \u001b[36m32.5779\u001b[0m       \u001b[32m31.5096\u001b[0m  0.0517\n",
      "      4       \u001b[36m31.9752\u001b[0m       \u001b[32m31.1005\u001b[0m  0.0537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      5       \u001b[36m31.3862\u001b[0m       \u001b[32m30.6544\u001b[0m  0.0744\n",
      "      6       \u001b[36m30.7563\u001b[0m       \u001b[32m30.0966\u001b[0m  0.0614\n",
      "      7       \u001b[36m29.9881\u001b[0m       \u001b[32m29.3831\u001b[0m  0.0458\n",
      "      8       \u001b[36m29.0106\u001b[0m       \u001b[32m28.5653\u001b[0m  0.0538\n",
      "      9       \u001b[36m27.8738\u001b[0m       \u001b[32m27.7632\u001b[0m  0.0502\n",
      "     10       \u001b[36m26.7099\u001b[0m       \u001b[32m27.1472\u001b[0m  0.0554\n",
      "     11       \u001b[36m25.6764\u001b[0m       \u001b[32m26.9004\u001b[0m  0.0499\n",
      "     12       \u001b[36m24.9583\u001b[0m       27.0903  0.0559\n",
      "     13       \u001b[36m24.6347\u001b[0m       27.4663  0.0479\n",
      "     14       \u001b[36m24.5472\u001b[0m       27.6012  0.0602\n",
      "     15       \u001b[36m24.4502\u001b[0m       27.4045  0.0569\n",
      "     16       \u001b[36m24.2965\u001b[0m       27.1018  0.0629\n",
      "     17       \u001b[36m24.1601\u001b[0m       \u001b[32m26.8681\u001b[0m  0.0488\n",
      "     18       \u001b[36m24.0647\u001b[0m       \u001b[32m26.7376\u001b[0m  0.0628\n",
      "     19       \u001b[36m23.9870\u001b[0m       \u001b[32m26.6871\u001b[0m  0.0484\n",
      "     20       \u001b[36m23.9113\u001b[0m       26.6923  0.0582\n",
      "     21       \u001b[36m23.8381\u001b[0m       26.7313  0.0418\n",
      "     22       \u001b[36m23.7703\u001b[0m       26.7809  0.0520\n",
      "     23       \u001b[36m23.7099\u001b[0m       26.8228  0.0469\n",
      "     24       \u001b[36m23.6562\u001b[0m       26.8484  0.0497\n",
      "     25       \u001b[36m23.6085\u001b[0m       26.8573  0.0528\n",
      "     26       \u001b[36m23.5662\u001b[0m       26.8555  0.0566\n",
      "     27       \u001b[36m23.5286\u001b[0m       26.8498  0.0474\n",
      "     28       \u001b[36m23.4950\u001b[0m       26.8430  0.0580\n",
      "     29       \u001b[36m23.4642\u001b[0m       26.8380  0.0434\n",
      "     30       \u001b[36m23.4357\u001b[0m       26.8347  0.0583\n",
      "     31       \u001b[36m23.4095\u001b[0m       26.8313  0.0498\n",
      "     32       \u001b[36m23.3852\u001b[0m       26.8275  0.0390\n",
      "     33       \u001b[36m23.3626\u001b[0m       26.8216  0.0736\n",
      "     34       \u001b[36m23.3419\u001b[0m       26.8134  0.0745\n",
      "     35       \u001b[36m23.3227\u001b[0m       26.8052  0.0550\n",
      "     36       \u001b[36m23.3051\u001b[0m       26.7969  0.0673\n",
      "     37       \u001b[36m23.2887\u001b[0m       26.7885  0.0492\n",
      "     38       \u001b[36m23.2734\u001b[0m       26.7806  0.0574\n",
      "     39       \u001b[36m23.2591\u001b[0m       26.7733  0.0510\n",
      "     40       \u001b[36m23.2459\u001b[0m       26.7672  0.0600\n",
      "     41       \u001b[36m23.2335\u001b[0m       26.7617  0.0486\n",
      "     42       \u001b[36m23.2220\u001b[0m       26.7553  0.0572\n",
      "     43       \u001b[36m23.2112\u001b[0m       26.7486  0.0501\n",
      "     44       \u001b[36m23.2013\u001b[0m       26.7412  0.0581\n",
      "     45       \u001b[36m23.1920\u001b[0m       26.7347  0.0482\n",
      "     46       \u001b[36m23.1834\u001b[0m       26.7293  0.0568\n",
      "     47       \u001b[36m23.1754\u001b[0m       26.7237  0.0458\n",
      "     48       \u001b[36m23.1678\u001b[0m       26.7177  0.0484\n",
      "     49       \u001b[36m23.1607\u001b[0m       26.7110  0.0522\n",
      "     50       \u001b[36m23.1541\u001b[0m       26.7062  0.0592\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m40.7837\u001b[0m       \u001b[32m32.3734\u001b[0m  0.0381\n",
      "      2       \u001b[36m40.3394\u001b[0m       \u001b[32m32.1168\u001b[0m  0.0663\n",
      "      3       \u001b[36m39.9069\u001b[0m       \u001b[32m31.8395\u001b[0m  0.0492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      4       \u001b[36m39.4355\u001b[0m       \u001b[32m31.5077\u001b[0m  0.0777\n",
      "      5       \u001b[36m38.8939\u001b[0m       \u001b[32m31.1218\u001b[0m  0.0542\n",
      "      6       \u001b[36m38.3165\u001b[0m       \u001b[32m30.6962\u001b[0m  0.0444\n",
      "      7       \u001b[36m37.7052\u001b[0m       \u001b[32m30.2019\u001b[0m  0.0530\n",
      "      8       \u001b[36m36.9856\u001b[0m       \u001b[32m29.6034\u001b[0m  0.0496\n",
      "      9       \u001b[36m36.1063\u001b[0m       \u001b[32m28.9207\u001b[0m  0.0597\n",
      "     10       \u001b[36m35.0815\u001b[0m       \u001b[32m28.2005\u001b[0m  0.0466\n",
      "     11       \u001b[36m33.9510\u001b[0m       \u001b[32m27.5305\u001b[0m  0.0558\n",
      "     12       \u001b[36m32.7870\u001b[0m       \u001b[32m27.0312\u001b[0m  0.0507\n",
      "     13       \u001b[36m31.7063\u001b[0m       \u001b[32m26.8455\u001b[0m  0.0585\n",
      "     14       \u001b[36m30.8623\u001b[0m       27.0672  0.0493\n",
      "     15       \u001b[36m30.3796\u001b[0m       27.5948  0.0582\n",
      "     16       \u001b[36m30.2243\u001b[0m       28.0754  0.0477\n",
      "     17       \u001b[36m30.1775\u001b[0m       28.1881  0.0602\n",
      "     18       \u001b[36m30.0631\u001b[0m       27.9692  0.0630\n",
      "     19       \u001b[36m29.8875\u001b[0m       27.6518  0.0626\n",
      "     20       \u001b[36m29.7238\u001b[0m       27.3958  0.0509\n",
      "     21       \u001b[36m29.5985\u001b[0m       27.2498  0.0627\n",
      "     22       \u001b[36m29.5001\u001b[0m       27.2015  0.0462\n",
      "     23       \u001b[36m29.4165\u001b[0m       27.2228  0.0549\n",
      "     24       \u001b[36m29.3441\u001b[0m       27.2825  0.0476\n",
      "     25       \u001b[36m29.2807\u001b[0m       27.3509  0.0603\n",
      "     26       \u001b[36m29.2244\u001b[0m       27.4092  0.0486\n",
      "     27       \u001b[36m29.1721\u001b[0m       27.4471  0.0576\n",
      "     28       \u001b[36m29.1215\u001b[0m       27.4618  0.0490\n",
      "     29       \u001b[36m29.0728\u001b[0m       27.4614  0.0494\n",
      "     30       \u001b[36m29.0261\u001b[0m       27.4543  0.0551\n",
      "     31       \u001b[36m28.9824\u001b[0m       27.4477  0.0555\n",
      "     32       \u001b[36m28.9420\u001b[0m       27.4469  0.0488\n",
      "     33       \u001b[36m28.9051\u001b[0m       27.4525  0.0573\n",
      "     34       \u001b[36m28.8716\u001b[0m       27.4642  0.0462\n",
      "     35       \u001b[36m28.8410\u001b[0m       27.4757  0.0574\n",
      "     36       \u001b[36m28.8123\u001b[0m       27.4824  0.0561\n",
      "     37       \u001b[36m28.7851\u001b[0m       27.4863  0.0682\n",
      "     38       \u001b[36m28.7598\u001b[0m       27.4889  0.0542\n",
      "     39       \u001b[36m28.7365\u001b[0m       27.4913  0.0666\n",
      "     40       \u001b[36m28.7151\u001b[0m       27.4954  0.0457\n",
      "     41       \u001b[36m28.6955\u001b[0m       27.5002  0.0525\n",
      "     42       \u001b[36m28.6778\u001b[0m       27.5041  0.0542\n",
      "     43       \u001b[36m28.6615\u001b[0m       27.5057  0.0576\n",
      "     44       \u001b[36m28.6462\u001b[0m       27.5039  0.0472\n",
      "     45       \u001b[36m28.6321\u001b[0m       27.5006  0.0524\n",
      "     46       \u001b[36m28.6192\u001b[0m       27.4978  0.0546\n",
      "     47       \u001b[36m28.6075\u001b[0m       27.4966  0.0563\n",
      "     48       \u001b[36m28.5968\u001b[0m       27.4962  0.0479\n",
      "     49       \u001b[36m28.5870\u001b[0m       27.4939  0.0500\n",
      "     50       \u001b[36m28.5778\u001b[0m       27.4908  0.0559\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m43.9301\u001b[0m       \u001b[32m46.0941\u001b[0m  0.0423\n",
      "      2       \u001b[36m43.5680\u001b[0m       \u001b[32m45.6645\u001b[0m  0.0506\n",
      "      3       \u001b[36m43.2282\u001b[0m       \u001b[32m45.2594\u001b[0m  0.0490\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      4       \u001b[36m42.9080\u001b[0m       \u001b[32m44.8747\u001b[0m  0.0674\n",
      "      5       \u001b[36m42.6037\u001b[0m       \u001b[32m44.5070\u001b[0m  0.0710\n",
      "      6       \u001b[36m42.3138\u001b[0m       \u001b[32m44.1558\u001b[0m  0.0498\n",
      "      7       \u001b[36m42.0369\u001b[0m       \u001b[32m43.8207\u001b[0m  0.0527\n",
      "      8       \u001b[36m41.7722\u001b[0m       \u001b[32m43.4991\u001b[0m  0.0593\n",
      "      9       \u001b[36m41.5161\u001b[0m       \u001b[32m43.1871\u001b[0m  0.0456\n",
      "     10       \u001b[36m41.2677\u001b[0m       \u001b[32m42.8826\u001b[0m  0.0563\n",
      "     11       \u001b[36m41.0259\u001b[0m       \u001b[32m42.5842\u001b[0m  0.0483\n",
      "     12       \u001b[36m40.7893\u001b[0m       \u001b[32m42.2908\u001b[0m  0.0590\n",
      "     13       \u001b[36m40.5568\u001b[0m       \u001b[32m42.0016\u001b[0m  0.0433\n",
      "     14       \u001b[36m40.3280\u001b[0m       \u001b[32m41.7156\u001b[0m  0.0536\n",
      "     15       \u001b[36m40.1020\u001b[0m       \u001b[32m41.4318\u001b[0m  0.0441\n",
      "     16       \u001b[36m39.8778\u001b[0m       \u001b[32m41.1492\u001b[0m  0.0487\n",
      "     17       \u001b[36m39.6543\u001b[0m       \u001b[32m40.8671\u001b[0m  0.0473\n",
      "     18       \u001b[36m39.4319\u001b[0m       \u001b[32m40.5850\u001b[0m  0.0475\n",
      "     19       \u001b[36m39.2105\u001b[0m       \u001b[32m40.3035\u001b[0m  0.0501\n",
      "     20       \u001b[36m38.9905\u001b[0m       \u001b[32m40.0225\u001b[0m  0.0496\n",
      "     21       \u001b[36m38.7718\u001b[0m       \u001b[32m39.7424\u001b[0m  0.0475\n",
      "     22       \u001b[36m38.5548\u001b[0m       \u001b[32m39.4629\u001b[0m  0.0600\n",
      "     23       \u001b[36m38.3393\u001b[0m       \u001b[32m39.1840\u001b[0m  0.0559\n",
      "     24       \u001b[36m38.1255\u001b[0m       \u001b[32m38.9058\u001b[0m  0.0631\n",
      "     25       \u001b[36m37.9132\u001b[0m       \u001b[32m38.6283\u001b[0m  0.0561\n",
      "     26       \u001b[36m37.7025\u001b[0m       \u001b[32m38.3517\u001b[0m  0.0566\n",
      "     27       \u001b[36m37.4936\u001b[0m       \u001b[32m38.0758\u001b[0m  0.0439\n",
      "     28       \u001b[36m37.2864\u001b[0m       \u001b[32m37.8011\u001b[0m  0.0503\n",
      "     29       \u001b[36m37.0812\u001b[0m       \u001b[32m37.5273\u001b[0m  0.0503\n",
      "     30       \u001b[36m36.8784\u001b[0m       \u001b[32m37.2545\u001b[0m  0.0489\n",
      "     31       \u001b[36m36.6780\u001b[0m       \u001b[32m36.9832\u001b[0m  0.0486\n",
      "     32       \u001b[36m36.4801\u001b[0m       \u001b[32m36.7139\u001b[0m  0.0595\n",
      "     33       \u001b[36m36.2851\u001b[0m       \u001b[32m36.4472\u001b[0m  0.0489\n",
      "     34       \u001b[36m36.0935\u001b[0m       \u001b[32m36.1832\u001b[0m  0.0539\n",
      "     35       \u001b[36m35.9055\u001b[0m       \u001b[32m35.9224\u001b[0m  0.0435\n",
      "     36       \u001b[36m35.7213\u001b[0m       \u001b[32m35.6650\u001b[0m  0.0524\n",
      "     37       \u001b[36m35.5411\u001b[0m       \u001b[32m35.4114\u001b[0m  0.0543\n",
      "     38       \u001b[36m35.3653\u001b[0m       \u001b[32m35.1619\u001b[0m  0.0548\n",
      "     39       \u001b[36m35.1940\u001b[0m       \u001b[32m34.9168\u001b[0m  0.0477\n",
      "     40       \u001b[36m35.0275\u001b[0m       \u001b[32m34.6763\u001b[0m  0.0534\n",
      "     41       \u001b[36m34.8659\u001b[0m       \u001b[32m34.4407\u001b[0m  0.0582\n",
      "     42       \u001b[36m34.7097\u001b[0m       \u001b[32m34.2106\u001b[0m  0.0647\n",
      "     43       \u001b[36m34.5589\u001b[0m       \u001b[32m33.9862\u001b[0m  0.0517\n",
      "     44       \u001b[36m34.4136\u001b[0m       \u001b[32m33.7677\u001b[0m  0.0612\n",
      "     45       \u001b[36m34.2741\u001b[0m       \u001b[32m33.5555\u001b[0m  0.0504\n",
      "     46       \u001b[36m34.1407\u001b[0m       \u001b[32m33.3499\u001b[0m  0.0516\n",
      "     47       \u001b[36m34.0133\u001b[0m       \u001b[32m33.1510\u001b[0m  0.0511\n",
      "     48       \u001b[36m33.8922\u001b[0m       \u001b[32m32.9593\u001b[0m  0.0583\n",
      "     49       \u001b[36m33.7774\u001b[0m       \u001b[32m32.7748\u001b[0m  0.0480\n",
      "     50       \u001b[36m33.6690\u001b[0m       \u001b[32m32.5978\u001b[0m  0.0526\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m33.9416\u001b[0m       \u001b[32m32.5261\u001b[0m  0.0291\n",
      "      2       \u001b[36m33.6589\u001b[0m       \u001b[32m32.3145\u001b[0m  0.0569\n",
      "      3       \u001b[36m33.3881\u001b[0m       \u001b[32m32.1117\u001b[0m  0.0585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      4       \u001b[36m33.1275\u001b[0m       \u001b[32m31.9170\u001b[0m  0.0563\n",
      "      5       \u001b[36m32.8758\u001b[0m       \u001b[32m31.7287\u001b[0m  0.0522\n",
      "      6       \u001b[36m32.6314\u001b[0m       \u001b[32m31.5467\u001b[0m  0.0562\n",
      "      7       \u001b[36m32.3937\u001b[0m       \u001b[32m31.3697\u001b[0m  0.0457\n",
      "      8       \u001b[36m32.1615\u001b[0m       \u001b[32m31.1973\u001b[0m  0.0675\n",
      "      9       \u001b[36m31.9337\u001b[0m       \u001b[32m31.0285\u001b[0m  0.0483\n",
      "     10       \u001b[36m31.7094\u001b[0m       \u001b[32m30.8628\u001b[0m  0.0629\n",
      "     11       \u001b[36m31.4885\u001b[0m       \u001b[32m30.7003\u001b[0m  0.0498\n",
      "     12       \u001b[36m31.2707\u001b[0m       \u001b[32m30.5402\u001b[0m  0.0551\n",
      "     13       \u001b[36m31.0550\u001b[0m       \u001b[32m30.3820\u001b[0m  0.0436\n",
      "     14       \u001b[36m30.8410\u001b[0m       \u001b[32m30.2250\u001b[0m  0.0531\n",
      "     15       \u001b[36m30.6286\u001b[0m       \u001b[32m30.0699\u001b[0m  0.0510\n",
      "     16       \u001b[36m30.4171\u001b[0m       \u001b[32m29.9165\u001b[0m  0.0497\n",
      "     17       \u001b[36m30.2070\u001b[0m       \u001b[32m29.7649\u001b[0m  0.0477\n",
      "     18       \u001b[36m29.9977\u001b[0m       \u001b[32m29.6143\u001b[0m  0.0474\n",
      "     19       \u001b[36m29.7892\u001b[0m       \u001b[32m29.4655\u001b[0m  0.0519\n",
      "     20       \u001b[36m29.5822\u001b[0m       \u001b[32m29.3185\u001b[0m  0.0485\n",
      "     21       \u001b[36m29.3765\u001b[0m       \u001b[32m29.1736\u001b[0m  0.0483\n",
      "     22       \u001b[36m29.1724\u001b[0m       \u001b[32m29.0307\u001b[0m  0.0560\n",
      "     23       \u001b[36m28.9694\u001b[0m       \u001b[32m28.8902\u001b[0m  0.0396\n",
      "     24       \u001b[36m28.7681\u001b[0m       \u001b[32m28.7522\u001b[0m  0.0474\n",
      "     25       \u001b[36m28.5689\u001b[0m       \u001b[32m28.6168\u001b[0m  0.0750\n",
      "     26       \u001b[36m28.3720\u001b[0m       \u001b[32m28.4838\u001b[0m  0.0655\n",
      "     27       \u001b[36m28.1773\u001b[0m       \u001b[32m28.3535\u001b[0m  0.0537\n",
      "     28       \u001b[36m27.9848\u001b[0m       \u001b[32m28.2259\u001b[0m  0.0630\n",
      "     29       \u001b[36m27.7947\u001b[0m       \u001b[32m28.1013\u001b[0m  0.0511\n",
      "     30       \u001b[36m27.6070\u001b[0m       \u001b[32m27.9798\u001b[0m  0.0592\n",
      "     31       \u001b[36m27.4222\u001b[0m       \u001b[32m27.8617\u001b[0m  0.0456\n",
      "     32       \u001b[36m27.2405\u001b[0m       \u001b[32m27.7471\u001b[0m  0.0542\n",
      "     33       \u001b[36m27.0620\u001b[0m       \u001b[32m27.6361\u001b[0m  0.0473\n",
      "     34       \u001b[36m26.8870\u001b[0m       \u001b[32m27.5288\u001b[0m  0.0584\n",
      "     35       \u001b[36m26.7159\u001b[0m       \u001b[32m27.4256\u001b[0m  0.0417\n",
      "     36       \u001b[36m26.5491\u001b[0m       \u001b[32m27.3264\u001b[0m  0.0485\n",
      "     37       \u001b[36m26.3866\u001b[0m       \u001b[32m27.2314\u001b[0m  0.0482\n",
      "     38       \u001b[36m26.2285\u001b[0m       \u001b[32m27.1406\u001b[0m  0.0477\n",
      "     39       \u001b[36m26.0751\u001b[0m       \u001b[32m27.0541\u001b[0m  0.0504\n",
      "     40       \u001b[36m25.9264\u001b[0m       \u001b[32m26.9720\u001b[0m  0.0496\n",
      "     41       \u001b[36m25.7825\u001b[0m       \u001b[32m26.8942\u001b[0m  0.0468\n",
      "     42       \u001b[36m25.6433\u001b[0m       \u001b[32m26.8208\u001b[0m  0.0489\n",
      "     43       \u001b[36m25.5091\u001b[0m       \u001b[32m26.7517\u001b[0m  0.0490\n",
      "     44       \u001b[36m25.3798\u001b[0m       \u001b[32m26.6870\u001b[0m  0.0482\n",
      "     45       \u001b[36m25.2554\u001b[0m       \u001b[32m26.6266\u001b[0m  0.0547\n",
      "     46       \u001b[36m25.1358\u001b[0m       \u001b[32m26.5704\u001b[0m  0.0671\n",
      "     47       \u001b[36m25.0211\u001b[0m       \u001b[32m26.5185\u001b[0m  0.0543\n",
      "     48       \u001b[36m24.9112\u001b[0m       \u001b[32m26.4706\u001b[0m  0.0663\n",
      "     49       \u001b[36m24.8060\u001b[0m       \u001b[32m26.4268\u001b[0m  0.0461\n",
      "     50       \u001b[36m24.7055\u001b[0m       \u001b[32m26.3869\u001b[0m  0.0531\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m42.8669\u001b[0m       \u001b[32m33.8794\u001b[0m  0.0303\n",
      "      2       \u001b[36m42.4061\u001b[0m       \u001b[32m33.5475\u001b[0m  0.0553\n",
      "      3       \u001b[36m41.9570\u001b[0m       \u001b[32m33.2239\u001b[0m  0.0486\n",
      "      4       \u001b[36m41.5161\u001b[0m       \u001b[32m32.9070\u001b[0m  0.0567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      5       \u001b[36m41.0814\u001b[0m       \u001b[32m32.5941\u001b[0m  0.0696\n",
      "      6       \u001b[36m40.6498\u001b[0m       \u001b[32m32.2850\u001b[0m  0.0463\n",
      "      7       \u001b[36m40.2191\u001b[0m       \u001b[32m31.9784\u001b[0m  0.0555\n",
      "      8       \u001b[36m39.7882\u001b[0m       \u001b[32m31.6740\u001b[0m  0.0432\n",
      "      9       \u001b[36m39.3565\u001b[0m       \u001b[32m31.3703\u001b[0m  0.0472\n",
      "     10       \u001b[36m38.9231\u001b[0m       \u001b[32m31.0681\u001b[0m  0.0504\n",
      "     11       \u001b[36m38.4887\u001b[0m       \u001b[32m30.7669\u001b[0m  0.0494\n",
      "     12       \u001b[36m38.0516\u001b[0m       \u001b[32m30.4669\u001b[0m  0.0482\n",
      "     13       \u001b[36m37.6115\u001b[0m       \u001b[32m30.1683\u001b[0m  0.0572\n",
      "     14       \u001b[36m37.1689\u001b[0m       \u001b[32m29.8717\u001b[0m  0.0630\n",
      "     15       \u001b[36m36.7241\u001b[0m       \u001b[32m29.5769\u001b[0m  0.0696\n",
      "     16       \u001b[36m36.2778\u001b[0m       \u001b[32m29.2850\u001b[0m  0.0519\n",
      "     17       \u001b[36m35.8298\u001b[0m       \u001b[32m28.9972\u001b[0m  0.0617\n",
      "     18       \u001b[36m35.3829\u001b[0m       \u001b[32m28.7136\u001b[0m  0.0533\n",
      "     19       \u001b[36m34.9382\u001b[0m       \u001b[32m28.4367\u001b[0m  0.0580\n",
      "     20       \u001b[36m34.4989\u001b[0m       \u001b[32m28.1688\u001b[0m  0.0445\n",
      "     21       \u001b[36m34.0671\u001b[0m       \u001b[32m27.9121\u001b[0m  0.0582\n",
      "     22       \u001b[36m33.6451\u001b[0m       \u001b[32m27.6684\u001b[0m  0.0471\n",
      "     23       \u001b[36m33.2349\u001b[0m       \u001b[32m27.4395\u001b[0m  0.0501\n",
      "     24       \u001b[36m32.8386\u001b[0m       \u001b[32m27.2272\u001b[0m  0.0534\n",
      "     25       \u001b[36m32.4584\u001b[0m       \u001b[32m27.0331\u001b[0m  0.0574\n",
      "     26       \u001b[36m32.0959\u001b[0m       \u001b[32m26.8586\u001b[0m  0.0500\n",
      "     27       \u001b[36m31.7525\u001b[0m       \u001b[32m26.7049\u001b[0m  0.0576\n",
      "     28       \u001b[36m31.4299\u001b[0m       \u001b[32m26.5730\u001b[0m  0.0486\n",
      "     29       \u001b[36m31.1289\u001b[0m       \u001b[32m26.4631\u001b[0m  0.0571\n",
      "     30       \u001b[36m30.8505\u001b[0m       \u001b[32m26.3753\u001b[0m  0.0471\n",
      "     31       \u001b[36m30.5948\u001b[0m       \u001b[32m26.3093\u001b[0m  0.0585\n",
      "     32       \u001b[36m30.3624\u001b[0m       \u001b[32m26.2640\u001b[0m  0.0540\n",
      "     33       \u001b[36m30.1527\u001b[0m       \u001b[32m26.2381\u001b[0m  0.0625\n",
      "     34       \u001b[36m29.9652\u001b[0m       \u001b[32m26.2299\u001b[0m  0.0550\n",
      "     35       \u001b[36m29.7992\u001b[0m       26.2378  0.0549\n",
      "     36       \u001b[36m29.6534\u001b[0m       26.2590  0.0448\n",
      "     37       \u001b[36m29.5264\u001b[0m       26.2912  0.0557\n",
      "     38       \u001b[36m29.4166\u001b[0m       26.3320  0.0477\n",
      "     39       \u001b[36m29.3224\u001b[0m       26.3791  0.0541\n",
      "     40       \u001b[36m29.2420\u001b[0m       26.4304  0.0458\n",
      "     41       \u001b[36m29.1737\u001b[0m       26.4839  0.0498\n",
      "     42       \u001b[36m29.1160\u001b[0m       26.5380  0.0497\n",
      "     43       \u001b[36m29.0672\u001b[0m       26.5916  0.0479\n",
      "     44       \u001b[36m29.0261\u001b[0m       26.6432  0.0497\n",
      "     45       \u001b[36m28.9913\u001b[0m       26.6921  0.0484\n",
      "     46       \u001b[36m28.9617\u001b[0m       26.7378  0.0502\n",
      "     47       \u001b[36m28.9362\u001b[0m       26.7800  0.0486\n",
      "     48       \u001b[36m28.9141\u001b[0m       26.8185  0.0464\n",
      "     49       \u001b[36m28.8948\u001b[0m       26.8532  0.0475\n",
      "     50       \u001b[36m28.8777\u001b[0m       26.8844  0.0587\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m42.4755\u001b[0m       \u001b[32m43.8194\u001b[0m  0.0620\n",
      "      2       \u001b[36m41.5490\u001b[0m       \u001b[32m42.7038\u001b[0m  0.0518\n",
      "      3       \u001b[36m40.6651\u001b[0m       \u001b[32m41.4722\u001b[0m  0.0633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      4       \u001b[36m39.6772\u001b[0m       \u001b[32m40.0489\u001b[0m  0.0461\n",
      "      5       \u001b[36m38.5587\u001b[0m       \u001b[32m38.5018\u001b[0m  0.0531\n",
      "      6       \u001b[36m37.3654\u001b[0m       \u001b[32m36.8943\u001b[0m  0.0356\n",
      "      7       \u001b[36m36.1741\u001b[0m       \u001b[32m35.2117\u001b[0m  0.0315\n",
      "      8       \u001b[36m35.0561\u001b[0m       \u001b[32m33.5618\u001b[0m  0.0303\n",
      "      9       \u001b[36m34.2093\u001b[0m       \u001b[32m32.3044\u001b[0m  0.0306\n",
      "     10       \u001b[36m33.8224\u001b[0m       \u001b[32m31.6438\u001b[0m  0.0300\n",
      "     11       \u001b[36m33.7442\u001b[0m       \u001b[32m31.3803\u001b[0m  0.0306\n",
      "     12       \u001b[36m33.6335\u001b[0m       \u001b[32m31.3006\u001b[0m  0.0299\n",
      "     13       \u001b[36m33.4384\u001b[0m       31.3378  0.0306\n",
      "     14       \u001b[36m33.2751\u001b[0m       31.3947  0.0302\n",
      "     15       \u001b[36m33.1670\u001b[0m       31.3626  0.0300\n",
      "     16       \u001b[36m33.0745\u001b[0m       \u001b[32m31.2354\u001b[0m  0.0296\n",
      "     17       \u001b[36m32.9837\u001b[0m       \u001b[32m31.0721\u001b[0m  0.0301\n",
      "     18       \u001b[36m32.9022\u001b[0m       \u001b[32m30.9249\u001b[0m  0.0301\n",
      "     19       \u001b[36m32.8321\u001b[0m       \u001b[32m30.8143\u001b[0m  0.0294\n",
      "     20       \u001b[36m32.7726\u001b[0m       \u001b[32m30.7350\u001b[0m  0.0300\n",
      "     21       \u001b[36m32.7200\u001b[0m       \u001b[32m30.6766\u001b[0m  0.0301\n",
      "     22       \u001b[36m32.6717\u001b[0m       \u001b[32m30.6350\u001b[0m  0.0306\n",
      "     23       \u001b[36m32.6276\u001b[0m       \u001b[32m30.6066\u001b[0m  0.0301\n",
      "     24       \u001b[36m32.5880\u001b[0m       \u001b[32m30.5830\u001b[0m  0.0295\n",
      "     25       \u001b[36m32.5521\u001b[0m       \u001b[32m30.5541\u001b[0m  0.0298\n",
      "     26       \u001b[36m32.5191\u001b[0m       \u001b[32m30.5209\u001b[0m  0.0301\n",
      "     27       \u001b[36m32.4892\u001b[0m       \u001b[32m30.4897\u001b[0m  0.0452\n",
      "     28       \u001b[36m32.4624\u001b[0m       \u001b[32m30.4654\u001b[0m  0.0377\n",
      "     29       \u001b[36m32.4382\u001b[0m       \u001b[32m30.4475\u001b[0m  0.0393\n",
      "     30       \u001b[36m32.4162\u001b[0m       \u001b[32m30.4325\u001b[0m  0.0416\n",
      "     31       \u001b[36m32.3966\u001b[0m       \u001b[32m30.4192\u001b[0m  0.0315\n",
      "     32       \u001b[36m32.3787\u001b[0m       \u001b[32m30.4079\u001b[0m  0.0314\n",
      "     33       \u001b[36m32.3625\u001b[0m       \u001b[32m30.3956\u001b[0m  0.0299\n",
      "     34       \u001b[36m32.3478\u001b[0m       \u001b[32m30.3847\u001b[0m  0.0297\n",
      "     35       \u001b[36m32.3344\u001b[0m       \u001b[32m30.3754\u001b[0m  0.0307\n",
      "     36       \u001b[36m32.3222\u001b[0m       \u001b[32m30.3659\u001b[0m  0.0295\n",
      "     37       \u001b[36m32.3112\u001b[0m       \u001b[32m30.3564\u001b[0m  0.0300\n",
      "     38       \u001b[36m32.3011\u001b[0m       \u001b[32m30.3491\u001b[0m  0.0307\n",
      "     39       \u001b[36m32.2920\u001b[0m       \u001b[32m30.3421\u001b[0m  0.0301\n",
      "     40       \u001b[36m32.2835\u001b[0m       \u001b[32m30.3362\u001b[0m  0.0298\n",
      "     41       \u001b[36m32.2758\u001b[0m       \u001b[32m30.3315\u001b[0m  0.0296\n",
      "     42       \u001b[36m32.2687\u001b[0m       \u001b[32m30.3270\u001b[0m  0.0293\n",
      "     43       \u001b[36m32.2621\u001b[0m       \u001b[32m30.3213\u001b[0m  0.0294\n",
      "     44       \u001b[36m32.2561\u001b[0m       \u001b[32m30.3151\u001b[0m  0.0295\n",
      "     45       \u001b[36m32.2504\u001b[0m       \u001b[32m30.3101\u001b[0m  0.0314\n",
      "     46       \u001b[36m32.2452\u001b[0m       \u001b[32m30.3059\u001b[0m  0.0288\n",
      "     47       \u001b[36m32.2403\u001b[0m       \u001b[32m30.3018\u001b[0m  0.0293\n",
      "     48       \u001b[36m32.2357\u001b[0m       \u001b[32m30.2978\u001b[0m  0.0305\n",
      "     49       \u001b[36m32.2314\u001b[0m       \u001b[32m30.2937\u001b[0m  0.0294\n",
      "     50       \u001b[36m32.2273\u001b[0m       \u001b[32m30.2893\u001b[0m  0.0306\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m34.4883\u001b[0m       \u001b[32m32.6447\u001b[0m  0.0282\n",
      "      2       \u001b[36m33.6142\u001b[0m       \u001b[32m31.9879\u001b[0m  0.0295\n",
      "      3       \u001b[36m32.7092\u001b[0m       \u001b[32m31.2572\u001b[0m  0.0293\n",
      "      4       \u001b[36m31.7206\u001b[0m       \u001b[32m30.4735\u001b[0m  0.0303\n",
      "      5       \u001b[36m30.6886\u001b[0m       \u001b[32m29.6612\u001b[0m  0.0310\n",
      "      6       \u001b[36m29.5849\u001b[0m       \u001b[32m28.7868\u001b[0m  0.0320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      7       \u001b[36m28.3223\u001b[0m       \u001b[32m27.9023\u001b[0m  0.0544\n",
      "      8       \u001b[36m26.9672\u001b[0m       \u001b[32m27.2341\u001b[0m  0.0351\n",
      "      9       \u001b[36m25.7529\u001b[0m       \u001b[32m27.0591\u001b[0m  0.0403\n",
      "     10       \u001b[36m24.9628\u001b[0m       27.4335  0.0426\n",
      "     11       \u001b[36m24.6798\u001b[0m       27.8799  0.0298\n",
      "     12       \u001b[36m24.5974\u001b[0m       27.8424  0.0296\n",
      "     13       \u001b[36m24.4231\u001b[0m       27.4500  0.0291\n",
      "     14       \u001b[36m24.2202\u001b[0m       27.0845  0.0283\n",
      "     15       \u001b[36m24.0836\u001b[0m       \u001b[32m26.8806\u001b[0m  0.0282\n",
      "     16       \u001b[36m23.9875\u001b[0m       \u001b[32m26.8145\u001b[0m  0.0292\n",
      "     17       \u001b[36m23.8973\u001b[0m       26.8446  0.0296\n",
      "     18       \u001b[36m23.8081\u001b[0m       26.9323  0.0284\n",
      "     19       \u001b[36m23.7277\u001b[0m       27.0359  0.0287\n",
      "     20       \u001b[36m23.6602\u001b[0m       27.1166  0.0288\n",
      "     21       \u001b[36m23.6030\u001b[0m       27.1568  0.0285\n",
      "     22       \u001b[36m23.5539\u001b[0m       27.1615  0.0288\n",
      "     23       \u001b[36m23.5117\u001b[0m       27.1463  0.0302\n",
      "     24       \u001b[36m23.4755\u001b[0m       27.1252  0.0285\n",
      "     25       \u001b[36m23.4437\u001b[0m       27.1061  0.0297\n",
      "     26       \u001b[36m23.4144\u001b[0m       27.0906  0.0282\n",
      "     27       \u001b[36m23.3869\u001b[0m       27.0784  0.0285\n",
      "     28       \u001b[36m23.3613\u001b[0m       27.0664  0.0292\n",
      "     29       \u001b[36m23.3377\u001b[0m       27.0529  0.0283\n",
      "     30       \u001b[36m23.3160\u001b[0m       27.0366  0.0279\n",
      "     31       \u001b[36m23.2961\u001b[0m       27.0188  0.0286\n",
      "     32       \u001b[36m23.2779\u001b[0m       27.0011  0.0292\n",
      "     33       \u001b[36m23.2613\u001b[0m       26.9844  0.0283\n",
      "     34       \u001b[36m23.2459\u001b[0m       26.9705  0.0279\n",
      "     35       \u001b[36m23.2317\u001b[0m       26.9576  0.0280\n",
      "     36       \u001b[36m23.2186\u001b[0m       26.9463  0.0288\n",
      "     37       \u001b[36m23.2066\u001b[0m       26.9355  0.0290\n",
      "     38       \u001b[36m23.1955\u001b[0m       26.9257  0.0483\n",
      "     39       \u001b[36m23.1853\u001b[0m       26.9161  0.0518\n",
      "     40       \u001b[36m23.1758\u001b[0m       26.9070  0.0450\n",
      "     41       \u001b[36m23.1671\u001b[0m       26.8982  0.0453\n",
      "     42       \u001b[36m23.1590\u001b[0m       26.8884  0.0352\n",
      "     43       \u001b[36m23.1516\u001b[0m       26.8793  0.0320\n",
      "     44       \u001b[36m23.1446\u001b[0m       26.8714  0.0293\n",
      "     45       \u001b[36m23.1381\u001b[0m       26.8632  0.0296\n",
      "     46       \u001b[36m23.1320\u001b[0m       26.8550  0.0291\n",
      "     47       \u001b[36m23.1263\u001b[0m       26.8483  0.0305\n",
      "     48       \u001b[36m23.1208\u001b[0m       26.8422  0.0297\n",
      "     49       \u001b[36m23.1157\u001b[0m       26.8352  0.0305\n",
      "     50       \u001b[36m23.1109\u001b[0m       26.8278  0.0306\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m40.0286\u001b[0m       \u001b[32m31.7395\u001b[0m  0.0290\n",
      "      2       \u001b[36m39.1686\u001b[0m       \u001b[32m31.1953\u001b[0m  0.0295\n",
      "      3       \u001b[36m38.3254\u001b[0m       \u001b[32m30.6035\u001b[0m  0.0296\n",
      "      4       \u001b[36m37.4180\u001b[0m       \u001b[32m29.9253\u001b[0m  0.0315\n",
      "      5       \u001b[36m36.4222\u001b[0m       \u001b[32m29.1677\u001b[0m  0.0300\n",
      "      6       \u001b[36m35.3110\u001b[0m       \u001b[32m28.3358\u001b[0m  0.0307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      7       \u001b[36m34.0205\u001b[0m       \u001b[32m27.4936\u001b[0m  0.0333\n",
      "      8       \u001b[36m32.6145\u001b[0m       \u001b[32m26.8522\u001b[0m  0.0308\n",
      "      9       \u001b[36m31.3412\u001b[0m       \u001b[32m26.7169\u001b[0m  0.0304\n",
      "     10       \u001b[36m30.5003\u001b[0m       27.2580  0.0303\n",
      "     11       \u001b[36m30.2492\u001b[0m       28.0578  0.0303\n",
      "     12       30.2825       28.3003  0.0303\n",
      "     13       \u001b[36m30.1537\u001b[0m       27.9218  0.0306\n",
      "     14       \u001b[36m29.8924\u001b[0m       27.4367  0.0299\n",
      "     15       \u001b[36m29.6821\u001b[0m       27.1341  0.0301\n",
      "     16       \u001b[36m29.5364\u001b[0m       27.0301  0.0328\n",
      "     17       \u001b[36m29.4173\u001b[0m       27.0713  0.0473\n",
      "     18       \u001b[36m29.3163\u001b[0m       27.1924  0.0376\n",
      "     19       \u001b[36m29.2352\u001b[0m       27.3242  0.0420\n",
      "     20       \u001b[36m29.1688\u001b[0m       27.4055  0.0428\n",
      "     21       \u001b[36m29.1043\u001b[0m       27.4186  0.0299\n",
      "     22       \u001b[36m29.0368\u001b[0m       27.3872  0.0305\n",
      "     23       \u001b[36m28.9710\u001b[0m       27.3442  0.0293\n",
      "     24       \u001b[36m28.9122\u001b[0m       27.3186  0.0293\n",
      "     25       \u001b[36m28.8604\u001b[0m       27.3192  0.0311\n",
      "     26       \u001b[36m28.8159\u001b[0m       27.3312  0.0326\n",
      "     27       \u001b[36m28.7767\u001b[0m       27.3425  0.0326\n",
      "     28       \u001b[36m28.7414\u001b[0m       27.3511  0.0303\n",
      "     29       \u001b[36m28.7094\u001b[0m       27.3519  0.0293\n",
      "     30       \u001b[36m28.6803\u001b[0m       27.3470  0.0293\n",
      "     31       \u001b[36m28.6542\u001b[0m       27.3411  0.0553\n",
      "     32       \u001b[36m28.6310\u001b[0m       27.3411  0.0308\n",
      "     33       \u001b[36m28.6110\u001b[0m       27.3412  0.0301\n",
      "     34       \u001b[36m28.5935\u001b[0m       27.3425  0.0288\n",
      "     35       \u001b[36m28.5781\u001b[0m       27.3457  0.0292\n",
      "     36       \u001b[36m28.5646\u001b[0m       27.3477  0.0283\n",
      "     37       \u001b[36m28.5527\u001b[0m       27.3461  0.0290\n",
      "     38       \u001b[36m28.5419\u001b[0m       27.3419  0.0296\n",
      "     39       \u001b[36m28.5321\u001b[0m       27.3384  0.0410\n",
      "     40       \u001b[36m28.5236\u001b[0m       27.3366  0.0466\n",
      "     41       \u001b[36m28.5158\u001b[0m       27.3346  0.0304\n",
      "     42       \u001b[36m28.5088\u001b[0m       27.3347  0.0150\n",
      "     43       \u001b[36m28.5025\u001b[0m       27.3338  0.0152\n",
      "     44       \u001b[36m28.4967\u001b[0m       27.3309  0.0134\n",
      "     45       \u001b[36m28.4912\u001b[0m       27.3288  0.0127\n",
      "     46       \u001b[36m28.4864\u001b[0m       27.3269  0.0147\n",
      "     47       \u001b[36m28.4817\u001b[0m       27.3266  0.0125\n",
      "     48       \u001b[36m28.4776\u001b[0m       27.3264  0.0134\n",
      "     49       \u001b[36m28.4735\u001b[0m       27.3288  0.0164\n",
      "     50       \u001b[36m28.4701\u001b[0m       27.3283  0.0132\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m43.1771\u001b[0m       \u001b[32m45.0970\u001b[0m  0.0116\n",
      "      2       \u001b[36m42.8038\u001b[0m       \u001b[32m44.6435\u001b[0m  0.0119\n",
      "      3       \u001b[36m42.4437\u001b[0m       \u001b[32m44.2040\u001b[0m  0.0119\n",
      "      4       \u001b[36m42.0948\u001b[0m       \u001b[32m43.7757\u001b[0m  0.0164\n",
      "      5       \u001b[36m41.7552\u001b[0m       \u001b[32m43.3562\u001b[0m  0.0119\n",
      "      6       \u001b[36m41.4235\u001b[0m       \u001b[32m42.9441\u001b[0m  0.0130\n",
      "      7       \u001b[36m41.0978\u001b[0m       \u001b[32m42.5377\u001b[0m  0.0128\n",
      "      8       \u001b[36m40.7773\u001b[0m       \u001b[32m42.1356\u001b[0m  0.0117\n",
      "      9       \u001b[36m40.4613\u001b[0m       \u001b[32m41.7371\u001b[0m  0.0110\n",
      "     10       \u001b[36m40.1487\u001b[0m       \u001b[32m41.3419\u001b[0m  0.0110\n",
      "     11       \u001b[36m39.8390\u001b[0m       \u001b[32m40.9491\u001b[0m  0.0112\n",
      "     12       \u001b[36m39.5321\u001b[0m       \u001b[32m40.5578\u001b[0m  0.0111\n",
      "     13       \u001b[36m39.2277\u001b[0m       \u001b[32m40.1682\u001b[0m  0.0111\n",
      "     14       \u001b[36m38.9260\u001b[0m       \u001b[32m39.7804\u001b[0m  0.0109\n",
      "     15       \u001b[36m38.6271\u001b[0m       \u001b[32m39.3948\u001b[0m  0.0110\n",
      "     16       \u001b[36m38.3314\u001b[0m       \u001b[32m39.0116\u001b[0m  0.0112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m38.0391\u001b[0m       \u001b[32m38.6312\u001b[0m  0.0115\n",
      "     18       \u001b[36m37.7505\u001b[0m       \u001b[32m38.2537\u001b[0m  0.0110\n",
      "     19       \u001b[36m37.4661\u001b[0m       \u001b[32m37.8796\u001b[0m  0.0111\n",
      "     20       \u001b[36m37.1865\u001b[0m       \u001b[32m37.5096\u001b[0m  0.0107\n",
      "     21       \u001b[36m36.9119\u001b[0m       \u001b[32m37.1441\u001b[0m  0.0106\n",
      "     22       \u001b[36m36.6430\u001b[0m       \u001b[32m36.7837\u001b[0m  0.0111\n",
      "     23       \u001b[36m36.3804\u001b[0m       \u001b[32m36.4289\u001b[0m  0.0110\n",
      "     24       \u001b[36m36.1247\u001b[0m       \u001b[32m36.0806\u001b[0m  0.0111\n",
      "     25       \u001b[36m35.8766\u001b[0m       \u001b[32m35.7395\u001b[0m  0.0109\n",
      "     26       \u001b[36m35.6367\u001b[0m       \u001b[32m35.4061\u001b[0m  0.0104\n",
      "     27       \u001b[36m35.4058\u001b[0m       \u001b[32m35.0810\u001b[0m  0.0112\n",
      "     28       \u001b[36m35.1841\u001b[0m       \u001b[32m34.7656\u001b[0m  0.0108\n",
      "     29       \u001b[36m34.9727\u001b[0m       \u001b[32m34.4604\u001b[0m  0.0106\n",
      "     30       \u001b[36m34.7718\u001b[0m       \u001b[32m34.1658\u001b[0m  0.0107\n",
      "     31       \u001b[36m34.5817\u001b[0m       \u001b[32m33.8828\u001b[0m  0.0108\n",
      "     32       \u001b[36m34.4027\u001b[0m       \u001b[32m33.6118\u001b[0m  0.0106\n",
      "     33       \u001b[36m34.2351\u001b[0m       \u001b[32m33.3534\u001b[0m  0.0111\n",
      "     34       \u001b[36m34.0788\u001b[0m       \u001b[32m33.1083\u001b[0m  0.0109\n",
      "     35       \u001b[36m33.9339\u001b[0m       \u001b[32m32.8766\u001b[0m  0.0105\n",
      "     36       \u001b[36m33.8004\u001b[0m       \u001b[32m32.6584\u001b[0m  0.0107\n",
      "     37       \u001b[36m33.6779\u001b[0m       \u001b[32m32.4538\u001b[0m  0.0109\n",
      "     38       \u001b[36m33.5663\u001b[0m       \u001b[32m32.2629\u001b[0m  0.0111\n",
      "     39       \u001b[36m33.4652\u001b[0m       \u001b[32m32.0856\u001b[0m  0.0109\n",
      "     40       \u001b[36m33.3742\u001b[0m       \u001b[32m31.9216\u001b[0m  0.0107\n",
      "     41       \u001b[36m33.2924\u001b[0m       \u001b[32m31.7704\u001b[0m  0.0106\n",
      "     42       \u001b[36m33.2193\u001b[0m       \u001b[32m31.6315\u001b[0m  0.0111\n",
      "     43       \u001b[36m33.1542\u001b[0m       \u001b[32m31.5045\u001b[0m  0.0111\n",
      "     44       \u001b[36m33.0963\u001b[0m       \u001b[32m31.3887\u001b[0m  0.0110\n",
      "     45       \u001b[36m33.0450\u001b[0m       \u001b[32m31.2835\u001b[0m  0.0106\n",
      "     46       \u001b[36m32.9998\u001b[0m       \u001b[32m31.1882\u001b[0m  0.0109\n",
      "     47       \u001b[36m32.9599\u001b[0m       \u001b[32m31.1018\u001b[0m  0.0109\n",
      "     48       \u001b[36m32.9247\u001b[0m       \u001b[32m31.0236\u001b[0m  0.0106\n",
      "     49       \u001b[36m32.8934\u001b[0m       \u001b[32m30.9528\u001b[0m  0.0112\n",
      "     50       \u001b[36m32.8656\u001b[0m       \u001b[32m30.8886\u001b[0m  0.0107\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m34.6396\u001b[0m       \u001b[32m33.1314\u001b[0m  0.0107\n",
      "      2       \u001b[36m34.3152\u001b[0m       \u001b[32m32.8800\u001b[0m  0.0115\n",
      "      3       \u001b[36m33.9983\u001b[0m       \u001b[32m32.6345\u001b[0m  0.0118\n",
      "      4       \u001b[36m33.6875\u001b[0m       \u001b[32m32.3941\u001b[0m  0.0108\n",
      "      5       \u001b[36m33.3815\u001b[0m       \u001b[32m32.1576\u001b[0m  0.0106\n",
      "      6       \u001b[36m33.0793\u001b[0m       \u001b[32m31.9247\u001b[0m  0.0103\n",
      "      7       \u001b[36m32.7801\u001b[0m       \u001b[32m31.6946\u001b[0m  0.0112\n",
      "      8       \u001b[36m32.4832\u001b[0m       \u001b[32m31.4668\u001b[0m  0.0111\n",
      "      9       \u001b[36m32.1876\u001b[0m       \u001b[32m31.2405\u001b[0m  0.0108\n",
      "     10       \u001b[36m31.8926\u001b[0m       \u001b[32m31.0150\u001b[0m  0.0108\n",
      "     11       \u001b[36m31.5979\u001b[0m       \u001b[32m30.7909\u001b[0m  0.0107\n",
      "     12       \u001b[36m31.3037\u001b[0m       \u001b[32m30.5684\u001b[0m  0.0109\n",
      "     13       \u001b[36m31.0099\u001b[0m       \u001b[32m30.3481\u001b[0m  0.0107\n",
      "     14       \u001b[36m30.7172\u001b[0m       \u001b[32m30.1302\u001b[0m  0.0113\n",
      "     15       \u001b[36m30.4258\u001b[0m       \u001b[32m29.9148\u001b[0m  0.0108\n",
      "     16       \u001b[36m30.1362\u001b[0m       \u001b[32m29.7022\u001b[0m  0.0108\n",
      "     17       \u001b[36m29.8483\u001b[0m       \u001b[32m29.4925\u001b[0m  0.0118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     18       \u001b[36m29.5622\u001b[0m       \u001b[32m29.2860\u001b[0m  0.0113\n",
      "     19       \u001b[36m29.2785\u001b[0m       \u001b[32m29.0833\u001b[0m  0.0109\n",
      "     20       \u001b[36m28.9979\u001b[0m       \u001b[32m28.8847\u001b[0m  0.0105\n",
      "     21       \u001b[36m28.7207\u001b[0m       \u001b[32m28.6907\u001b[0m  0.0107\n",
      "     22       \u001b[36m28.4475\u001b[0m       \u001b[32m28.5018\u001b[0m  0.0104\n",
      "     23       \u001b[36m28.1787\u001b[0m       \u001b[32m28.3186\u001b[0m  0.0110\n",
      "     24       \u001b[36m27.9149\u001b[0m       \u001b[32m28.1413\u001b[0m  0.0111\n",
      "     25       \u001b[36m27.6567\u001b[0m       \u001b[32m27.9705\u001b[0m  0.0112\n",
      "     26       \u001b[36m27.4045\u001b[0m       \u001b[32m27.8066\u001b[0m  0.0106\n",
      "     27       \u001b[36m27.1591\u001b[0m       \u001b[32m27.6502\u001b[0m  0.0105\n",
      "     28       \u001b[36m26.9208\u001b[0m       \u001b[32m27.5014\u001b[0m  0.0112\n",
      "     29       \u001b[36m26.6900\u001b[0m       \u001b[32m27.3606\u001b[0m  0.0112\n",
      "     30       \u001b[36m26.4671\u001b[0m       \u001b[32m27.2281\u001b[0m  0.0116\n",
      "     31       \u001b[36m26.2523\u001b[0m       \u001b[32m27.1040\u001b[0m  0.0110\n",
      "     32       \u001b[36m26.0461\u001b[0m       \u001b[32m26.9886\u001b[0m  0.0130\n",
      "     33       \u001b[36m25.8486\u001b[0m       \u001b[32m26.8820\u001b[0m  0.0161\n",
      "     34       \u001b[36m25.6598\u001b[0m       \u001b[32m26.7842\u001b[0m  0.0126\n",
      "     35       \u001b[36m25.4801\u001b[0m       \u001b[32m26.6953\u001b[0m  0.0114\n",
      "     36       \u001b[36m25.3097\u001b[0m       \u001b[32m26.6151\u001b[0m  0.0117\n",
      "     37       \u001b[36m25.1487\u001b[0m       \u001b[32m26.5436\u001b[0m  0.0116\n",
      "     38       \u001b[36m24.9970\u001b[0m       \u001b[32m26.4808\u001b[0m  0.0141\n",
      "     39       \u001b[36m24.8548\u001b[0m       \u001b[32m26.4262\u001b[0m  0.0120\n",
      "     40       \u001b[36m24.7220\u001b[0m       \u001b[32m26.3795\u001b[0m  0.0117\n",
      "     41       \u001b[36m24.5984\u001b[0m       \u001b[32m26.3405\u001b[0m  0.0114\n",
      "     42       \u001b[36m24.4841\u001b[0m       \u001b[32m26.3088\u001b[0m  0.0111\n",
      "     43       \u001b[36m24.3789\u001b[0m       \u001b[32m26.2837\u001b[0m  0.0113\n",
      "     44       \u001b[36m24.2822\u001b[0m       \u001b[32m26.2648\u001b[0m  0.0110\n",
      "     45       \u001b[36m24.1937\u001b[0m       \u001b[32m26.2516\u001b[0m  0.0111\n",
      "     46       \u001b[36m24.1130\u001b[0m       \u001b[32m26.2436\u001b[0m  0.0107\n",
      "     47       \u001b[36m24.0396\u001b[0m       \u001b[32m26.2399\u001b[0m  0.0108\n",
      "     48       \u001b[36m23.9730\u001b[0m       26.2401  0.0112\n",
      "     49       \u001b[36m23.9129\u001b[0m       26.2437  0.0109\n",
      "     50       \u001b[36m23.8586\u001b[0m       26.2500  0.0110\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m40.4758\u001b[0m       \u001b[32m32.1057\u001b[0m  0.0108\n",
      "      2       \u001b[36m40.0800\u001b[0m       \u001b[32m31.8324\u001b[0m  0.0105\n",
      "      3       \u001b[36m39.6968\u001b[0m       \u001b[32m31.5678\u001b[0m  0.0111\n",
      "      4       \u001b[36m39.3232\u001b[0m       \u001b[32m31.3112\u001b[0m  0.0105\n",
      "      5       \u001b[36m38.9585\u001b[0m       \u001b[32m31.0616\u001b[0m  0.0107\n",
      "      6       \u001b[36m38.6008\u001b[0m       \u001b[32m30.8183\u001b[0m  0.0106\n",
      "      7       \u001b[36m38.2495\u001b[0m       \u001b[32m30.5804\u001b[0m  0.0106\n",
      "      8       \u001b[36m37.9029\u001b[0m       \u001b[32m30.3473\u001b[0m  0.0111\n",
      "      9       \u001b[36m37.5607\u001b[0m       \u001b[32m30.1183\u001b[0m  0.0110\n",
      "     10       \u001b[36m37.2223\u001b[0m       \u001b[32m29.8934\u001b[0m  0.0105\n",
      "     11       \u001b[36m36.8870\u001b[0m       \u001b[32m29.6720\u001b[0m  0.0110\n",
      "     12       \u001b[36m36.5540\u001b[0m       \u001b[32m29.4541\u001b[0m  0.0107\n",
      "     13       \u001b[36m36.2232\u001b[0m       \u001b[32m29.2395\u001b[0m  0.0108\n",
      "     14       \u001b[36m35.8943\u001b[0m       \u001b[32m29.0285\u001b[0m  0.0110\n",
      "     15       \u001b[36m35.5673\u001b[0m       \u001b[32m28.8211\u001b[0m  0.0112\n",
      "     16       \u001b[36m35.2422\u001b[0m       \u001b[32m28.6173\u001b[0m  0.0103\n",
      "     17       \u001b[36m34.9191\u001b[0m       \u001b[32m28.4179\u001b[0m  0.0107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     18       \u001b[36m34.5986\u001b[0m       \u001b[32m28.2231\u001b[0m  0.0118\n",
      "     19       \u001b[36m34.2811\u001b[0m       \u001b[32m28.0336\u001b[0m  0.0110\n",
      "     20       \u001b[36m33.9672\u001b[0m       \u001b[32m27.8497\u001b[0m  0.0108\n",
      "     21       \u001b[36m33.6573\u001b[0m       \u001b[32m27.6724\u001b[0m  0.0105\n",
      "     22       \u001b[36m33.3523\u001b[0m       \u001b[32m27.5023\u001b[0m  0.0104\n",
      "     23       \u001b[36m33.0530\u001b[0m       \u001b[32m27.3406\u001b[0m  0.0107\n",
      "     24       \u001b[36m32.7604\u001b[0m       \u001b[32m27.1879\u001b[0m  0.0109\n",
      "     25       \u001b[36m32.4751\u001b[0m       \u001b[32m27.0449\u001b[0m  0.0109\n",
      "     26       \u001b[36m32.1980\u001b[0m       \u001b[32m26.9122\u001b[0m  0.0105\n",
      "     27       \u001b[36m31.9298\u001b[0m       \u001b[32m26.7902\u001b[0m  0.0110\n",
      "     28       \u001b[36m31.6709\u001b[0m       \u001b[32m26.6797\u001b[0m  0.0112\n",
      "     29       \u001b[36m31.4223\u001b[0m       \u001b[32m26.5812\u001b[0m  0.0109\n",
      "     30       \u001b[36m31.1849\u001b[0m       \u001b[32m26.4951\u001b[0m  0.0107\n",
      "     31       \u001b[36m30.9591\u001b[0m       \u001b[32m26.4216\u001b[0m  0.0110\n",
      "     32       \u001b[36m30.7454\u001b[0m       \u001b[32m26.3609\u001b[0m  0.0106\n",
      "     33       \u001b[36m30.5442\u001b[0m       \u001b[32m26.3130\u001b[0m  0.0107\n",
      "     34       \u001b[36m30.3560\u001b[0m       \u001b[32m26.2778\u001b[0m  0.0109\n",
      "     35       \u001b[36m30.1809\u001b[0m       \u001b[32m26.2547\u001b[0m  0.0112\n",
      "     36       \u001b[36m30.0190\u001b[0m       \u001b[32m26.2433\u001b[0m  0.0105\n",
      "     37       \u001b[36m29.8703\u001b[0m       \u001b[32m26.2428\u001b[0m  0.0106\n",
      "     38       \u001b[36m29.7346\u001b[0m       26.2524  0.0111\n",
      "     39       \u001b[36m29.6116\u001b[0m       26.2711  0.0106\n",
      "     40       \u001b[36m29.5009\u001b[0m       26.2981  0.0110\n",
      "     41       \u001b[36m29.4017\u001b[0m       26.3319  0.0105\n",
      "     42       \u001b[36m29.3136\u001b[0m       26.3714  0.0106\n",
      "     43       \u001b[36m29.2359\u001b[0m       26.4154  0.0110\n",
      "     44       \u001b[36m29.1676\u001b[0m       26.4626  0.0111\n",
      "     45       \u001b[36m29.1080\u001b[0m       26.5119  0.0106\n",
      "     46       \u001b[36m29.0561\u001b[0m       26.5625  0.0103\n",
      "     47       \u001b[36m29.0111\u001b[0m       26.6133  0.0105\n",
      "     48       \u001b[36m28.9722\u001b[0m       26.6636  0.0110\n",
      "     49       \u001b[36m28.9388\u001b[0m       26.7129  0.0108\n",
      "     50       \u001b[36m28.9101\u001b[0m       26.7604  0.0109\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m42.2474\u001b[0m       \u001b[32m43.3067\u001b[0m  0.0117\n",
      "      2       \u001b[36m41.1240\u001b[0m       \u001b[32m41.8297\u001b[0m  0.0113\n",
      "      3       \u001b[36m39.9538\u001b[0m       \u001b[32m40.2171\u001b[0m  0.0125\n",
      "      4       \u001b[36m38.6782\u001b[0m       \u001b[32m38.4058\u001b[0m  0.0113\n",
      "      5       \u001b[36m37.2381\u001b[0m       \u001b[32m36.2352\u001b[0m  0.0113\n",
      "      6       \u001b[36m35.6530\u001b[0m       \u001b[32m33.9140\u001b[0m  0.0113\n",
      "      7       \u001b[36m34.3254\u001b[0m       \u001b[32m32.1299\u001b[0m  0.0122\n",
      "      8       \u001b[36m33.7566\u001b[0m       \u001b[32m31.3342\u001b[0m  0.0120\n",
      "      9       \u001b[36m33.7330\u001b[0m       \u001b[32m31.0966\u001b[0m  0.0122\n",
      "     10       \u001b[36m33.5600\u001b[0m       \u001b[32m31.0778\u001b[0m  0.0119\n",
      "     11       \u001b[36m33.2902\u001b[0m       31.2282  0.0113\n",
      "     12       \u001b[36m33.1493\u001b[0m       31.3146  0.0124\n",
      "     13       \u001b[36m33.0643\u001b[0m       31.2164  0.0118\n",
      "     14       \u001b[36m32.9675\u001b[0m       \u001b[32m31.0159\u001b[0m  0.0202\n",
      "     15       \u001b[36m32.8770\u001b[0m       \u001b[32m30.8236\u001b[0m  0.0178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     16       \u001b[36m32.8069\u001b[0m       \u001b[32m30.6935\u001b[0m  0.0164\n",
      "     17       \u001b[36m32.7510\u001b[0m       \u001b[32m30.6325\u001b[0m  0.0121\n",
      "     18       \u001b[36m32.7021\u001b[0m       \u001b[32m30.6107\u001b[0m  0.0121\n",
      "     19       \u001b[36m32.6563\u001b[0m       \u001b[32m30.5939\u001b[0m  0.0120\n",
      "     20       \u001b[36m32.6144\u001b[0m       \u001b[32m30.5698\u001b[0m  0.0143\n",
      "     21       \u001b[36m32.5771\u001b[0m       \u001b[32m30.5377\u001b[0m  0.0119\n",
      "     22       \u001b[36m32.5430\u001b[0m       \u001b[32m30.5051\u001b[0m  0.0118\n",
      "     23       \u001b[36m32.5122\u001b[0m       \u001b[32m30.4743\u001b[0m  0.0116\n",
      "     24       \u001b[36m32.4845\u001b[0m       \u001b[32m30.4466\u001b[0m  0.0117\n",
      "     25       \u001b[36m32.4598\u001b[0m       \u001b[32m30.4257\u001b[0m  0.0116\n",
      "     26       \u001b[36m32.4374\u001b[0m       \u001b[32m30.4117\u001b[0m  0.0121\n",
      "     27       \u001b[36m32.4172\u001b[0m       \u001b[32m30.4024\u001b[0m  0.0118\n",
      "     28       \u001b[36m32.3989\u001b[0m       \u001b[32m30.3929\u001b[0m  0.0117\n",
      "     29       \u001b[36m32.3821\u001b[0m       \u001b[32m30.3806\u001b[0m  0.0118\n",
      "     30       \u001b[36m32.3669\u001b[0m       \u001b[32m30.3689\u001b[0m  0.0123\n",
      "     31       \u001b[36m32.3529\u001b[0m       \u001b[32m30.3586\u001b[0m  0.0120\n",
      "     32       \u001b[36m32.3403\u001b[0m       \u001b[32m30.3503\u001b[0m  0.0120\n",
      "     33       \u001b[36m32.3288\u001b[0m       \u001b[32m30.3435\u001b[0m  0.0113\n",
      "     34       \u001b[36m32.3184\u001b[0m       \u001b[32m30.3368\u001b[0m  0.0115\n",
      "     35       \u001b[36m32.3087\u001b[0m       \u001b[32m30.3299\u001b[0m  0.0112\n",
      "     36       \u001b[36m32.2997\u001b[0m       \u001b[32m30.3224\u001b[0m  0.0115\n",
      "     37       \u001b[36m32.2912\u001b[0m       \u001b[32m30.3150\u001b[0m  0.0115\n",
      "     38       \u001b[36m32.2834\u001b[0m       \u001b[32m30.3085\u001b[0m  0.0116\n",
      "     39       \u001b[36m32.2760\u001b[0m       \u001b[32m30.3018\u001b[0m  0.0114\n",
      "     40       \u001b[36m32.2691\u001b[0m       \u001b[32m30.2953\u001b[0m  0.0112\n",
      "     41       \u001b[36m32.2628\u001b[0m       \u001b[32m30.2882\u001b[0m  0.0116\n",
      "     42       \u001b[36m32.2567\u001b[0m       \u001b[32m30.2810\u001b[0m  0.0112\n",
      "     43       \u001b[36m32.2510\u001b[0m       \u001b[32m30.2743\u001b[0m  0.0111\n",
      "     44       \u001b[36m32.2457\u001b[0m       \u001b[32m30.2683\u001b[0m  0.0112\n",
      "     45       \u001b[36m32.2408\u001b[0m       \u001b[32m30.2626\u001b[0m  0.0114\n",
      "     46       \u001b[36m32.2361\u001b[0m       \u001b[32m30.2570\u001b[0m  0.0118\n",
      "     47       \u001b[36m32.2315\u001b[0m       \u001b[32m30.2511\u001b[0m  0.0112\n",
      "     48       \u001b[36m32.2272\u001b[0m       \u001b[32m30.2465\u001b[0m  0.0111\n",
      "     49       \u001b[36m32.2232\u001b[0m       \u001b[32m30.2422\u001b[0m  0.0112\n",
      "     50       \u001b[36m32.2193\u001b[0m       \u001b[32m30.2377\u001b[0m  0.0116\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m33.3644\u001b[0m       \u001b[32m31.7515\u001b[0m  0.0116\n",
      "      2       \u001b[36m32.2957\u001b[0m       \u001b[32m31.0298\u001b[0m  0.0113\n",
      "      3       \u001b[36m31.2907\u001b[0m       \u001b[32m30.2903\u001b[0m  0.0115\n",
      "      4       \u001b[36m30.2467\u001b[0m       \u001b[32m29.5403\u001b[0m  0.0114\n",
      "      5       \u001b[36m29.1859\u001b[0m       \u001b[32m28.7151\u001b[0m  0.0113\n",
      "      6       \u001b[36m27.9818\u001b[0m       \u001b[32m27.7419\u001b[0m  0.0115\n",
      "      7       \u001b[36m26.5775\u001b[0m       \u001b[32m26.8916\u001b[0m  0.0114\n",
      "      8       \u001b[36m25.2810\u001b[0m       \u001b[32m26.6762\u001b[0m  0.0113\n",
      "      9       \u001b[36m24.5747\u001b[0m       27.2275  0.0112\n",
      "     10       \u001b[36m24.4864\u001b[0m       27.6075  0.0113\n",
      "     11       \u001b[36m24.3976\u001b[0m       27.2842  0.0113\n",
      "     12       \u001b[36m24.1543\u001b[0m       26.8408  0.0117\n",
      "     13       \u001b[36m23.9918\u001b[0m       \u001b[32m26.5939\u001b[0m  0.0113\n",
      "     14       \u001b[36m23.9092\u001b[0m       \u001b[32m26.5127\u001b[0m  0.0113\n",
      "     15       \u001b[36m23.8245\u001b[0m       26.5390  0.0115\n",
      "     16       \u001b[36m23.7315\u001b[0m       26.6357  0.0111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m23.6544\u001b[0m       26.7427  0.0116\n",
      "     18       \u001b[36m23.5947\u001b[0m       26.8029  0.0116\n",
      "     19       \u001b[36m23.5426\u001b[0m       26.8067  0.0114\n",
      "     20       \u001b[36m23.4978\u001b[0m       26.7819  0.0112\n",
      "     21       \u001b[36m23.4597\u001b[0m       26.7617  0.0113\n",
      "     22       \u001b[36m23.4272\u001b[0m       26.7576  0.0113\n",
      "     23       \u001b[36m23.3977\u001b[0m       26.7671  0.0116\n",
      "     24       \u001b[36m23.3693\u001b[0m       26.7826  0.0113\n",
      "     25       \u001b[36m23.3426\u001b[0m       26.7945  0.0115\n",
      "     26       \u001b[36m23.3188\u001b[0m       26.7972  0.0116\n",
      "     27       \u001b[36m23.2974\u001b[0m       26.7929  0.0113\n",
      "     28       \u001b[36m23.2783\u001b[0m       26.7882  0.0115\n",
      "     29       \u001b[36m23.2606\u001b[0m       26.7868  0.0115\n",
      "     30       \u001b[36m23.2440\u001b[0m       26.7871  0.0116\n",
      "     31       \u001b[36m23.2288\u001b[0m       26.7864  0.0114\n",
      "     32       \u001b[36m23.2148\u001b[0m       26.7824  0.0112\n",
      "     33       \u001b[36m23.2022\u001b[0m       26.7755  0.0115\n",
      "     34       \u001b[36m23.1908\u001b[0m       26.7677  0.0114\n",
      "     35       \u001b[36m23.1803\u001b[0m       26.7640  0.0116\n",
      "     36       \u001b[36m23.1706\u001b[0m       26.7621  0.0112\n",
      "     37       \u001b[36m23.1617\u001b[0m       26.7600  0.0113\n",
      "     38       \u001b[36m23.1533\u001b[0m       26.7583  0.0116\n",
      "     39       \u001b[36m23.1457\u001b[0m       26.7557  0.0111\n",
      "     40       \u001b[36m23.1388\u001b[0m       26.7532  0.0113\n",
      "     41       \u001b[36m23.1323\u001b[0m       26.7499  0.0120\n",
      "     42       \u001b[36m23.1263\u001b[0m       26.7471  0.0115\n",
      "     43       \u001b[36m23.1207\u001b[0m       26.7452  0.0116\n",
      "     44       \u001b[36m23.1154\u001b[0m       26.7446  0.0115\n",
      "     45       \u001b[36m23.1105\u001b[0m       26.7441  0.0172\n",
      "     46       \u001b[36m23.1059\u001b[0m       26.7433  0.0228\n",
      "     47       \u001b[36m23.1015\u001b[0m       26.7429  0.0130\n",
      "     48       \u001b[36m23.0974\u001b[0m       26.7408  0.0135\n",
      "     49       \u001b[36m23.0936\u001b[0m       26.7383  0.0156\n",
      "     50       \u001b[36m23.0899\u001b[0m       26.7368  0.0256\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m40.7366\u001b[0m       \u001b[32m32.0527\u001b[0m  0.0120\n",
      "      2       \u001b[36m39.4185\u001b[0m       \u001b[32m31.1446\u001b[0m  0.0118\n",
      "      3       \u001b[36m38.0791\u001b[0m       \u001b[32m30.1399\u001b[0m  0.0116\n",
      "      4       \u001b[36m36.6181\u001b[0m       \u001b[32m29.0093\u001b[0m  0.0118\n",
      "      5       \u001b[36m34.9698\u001b[0m       \u001b[32m27.8330\u001b[0m  0.0116\n",
      "      6       \u001b[36m33.1337\u001b[0m       \u001b[32m26.9069\u001b[0m  0.0115\n",
      "      7       \u001b[36m31.4169\u001b[0m       \u001b[32m26.7611\u001b[0m  0.0118\n",
      "      8       \u001b[36m30.4082\u001b[0m       27.6939  0.0117\n",
      "      9       \u001b[36m30.3380\u001b[0m       28.6426  0.0116\n",
      "     10       30.3950       28.4208  0.0119\n",
      "     11       \u001b[36m30.0753\u001b[0m       27.6944  0.0115\n",
      "     12       \u001b[36m29.7668\u001b[0m       27.2058  0.0114\n",
      "     13       \u001b[36m29.6084\u001b[0m       27.0423  0.0114\n",
      "     14       \u001b[36m29.4855\u001b[0m       27.1024  0.0115\n",
      "     15       \u001b[36m29.3700\u001b[0m       27.2972  0.0114\n",
      "     16       \u001b[36m29.2842\u001b[0m       27.5068  0.0114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m29.2193\u001b[0m       27.6203  0.0116\n",
      "     18       \u001b[36m29.1536\u001b[0m       27.6210  0.0115\n",
      "     19       \u001b[36m29.0813\u001b[0m       27.5655  0.0116\n",
      "     20       \u001b[36m29.0098\u001b[0m       27.5164  0.0116\n",
      "     21       \u001b[36m28.9488\u001b[0m       27.4956  0.0115\n",
      "     22       \u001b[36m28.8979\u001b[0m       27.5035  0.0113\n",
      "     23       \u001b[36m28.8547\u001b[0m       27.5294  0.0116\n",
      "     24       \u001b[36m28.8171\u001b[0m       27.5506  0.0130\n",
      "     25       \u001b[36m28.7832\u001b[0m       27.5549  0.0117\n",
      "     26       \u001b[36m28.7520\u001b[0m       27.5423  0.0115\n",
      "     27       \u001b[36m28.7221\u001b[0m       27.5247  0.0113\n",
      "     28       \u001b[36m28.6947\u001b[0m       27.5110  0.0116\n",
      "     29       \u001b[36m28.6703\u001b[0m       27.5081  0.0114\n",
      "     30       \u001b[36m28.6489\u001b[0m       27.5120  0.0113\n",
      "     31       \u001b[36m28.6298\u001b[0m       27.5170  0.0113\n",
      "     32       \u001b[36m28.6129\u001b[0m       27.5198  0.0110\n",
      "     33       \u001b[36m28.5974\u001b[0m       27.5224  0.0117\n",
      "     34       \u001b[36m28.5833\u001b[0m       27.5231  0.0114\n",
      "     35       \u001b[36m28.5704\u001b[0m       27.5236  0.0115\n",
      "     36       \u001b[36m28.5589\u001b[0m       27.5232  0.0116\n",
      "     37       \u001b[36m28.5484\u001b[0m       27.5222  0.0111\n",
      "     38       \u001b[36m28.5388\u001b[0m       27.5219  0.0118\n",
      "     39       \u001b[36m28.5301\u001b[0m       27.5231  0.0117\n",
      "     40       \u001b[36m28.5224\u001b[0m       27.5268  0.0121\n",
      "     41       \u001b[36m28.5153\u001b[0m       27.5297  0.0114\n",
      "     42       \u001b[36m28.5087\u001b[0m       27.5314  0.0112\n",
      "     43       \u001b[36m28.5025\u001b[0m       27.5322  0.0118\n",
      "     44       \u001b[36m28.4968\u001b[0m       27.5324  0.0113\n",
      "     45       \u001b[36m28.4914\u001b[0m       27.5327  0.0114\n",
      "     46       \u001b[36m28.4862\u001b[0m       27.5353  0.0114\n",
      "     47       \u001b[36m28.4815\u001b[0m       27.5393  0.0113\n",
      "     48       \u001b[36m28.4770\u001b[0m       27.5410  0.0118\n",
      "     49       \u001b[36m28.4726\u001b[0m       27.5429  0.0116\n",
      "     50       \u001b[36m28.4685\u001b[0m       27.5467  0.0116\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m42.7621\u001b[0m       \u001b[32m44.5873\u001b[0m  0.0105\n",
      "      2       \u001b[36m42.3373\u001b[0m       \u001b[32m44.0648\u001b[0m  0.0105\n",
      "      3       \u001b[36m41.9263\u001b[0m       \u001b[32m43.5560\u001b[0m  0.0110\n",
      "      4       \u001b[36m41.5270\u001b[0m       \u001b[32m43.0590\u001b[0m  0.0110\n",
      "      5       \u001b[36m41.1377\u001b[0m       \u001b[32m42.5717\u001b[0m  0.0106\n",
      "      6       \u001b[36m40.7565\u001b[0m       \u001b[32m42.0923\u001b[0m  0.0104\n",
      "      7       \u001b[36m40.3824\u001b[0m       \u001b[32m41.6195\u001b[0m  0.0107\n",
      "      8       \u001b[36m40.0148\u001b[0m       \u001b[32m41.1527\u001b[0m  0.0107\n",
      "      9       \u001b[36m39.6531\u001b[0m       \u001b[32m40.6910\u001b[0m  0.0107\n",
      "     10       \u001b[36m39.2968\u001b[0m       \u001b[32m40.2334\u001b[0m  0.0107\n",
      "     11       \u001b[36m38.9454\u001b[0m       \u001b[32m39.7804\u001b[0m  0.0106\n",
      "     12       \u001b[36m38.5994\u001b[0m       \u001b[32m39.3318\u001b[0m  0.0106\n",
      "     13       \u001b[36m38.2590\u001b[0m       \u001b[32m38.8884\u001b[0m  0.0105\n",
      "     14       \u001b[36m37.9246\u001b[0m       \u001b[32m38.4507\u001b[0m  0.0107\n",
      "     15       \u001b[36m37.5969\u001b[0m       \u001b[32m38.0189\u001b[0m  0.0112\n",
      "     16       \u001b[36m37.2764\u001b[0m       \u001b[32m37.5942\u001b[0m  0.0103\n",
      "     17       \u001b[36m36.9641\u001b[0m       \u001b[32m37.1774\u001b[0m  0.0104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     18       \u001b[36m36.6606\u001b[0m       \u001b[32m36.7692\u001b[0m  0.0113\n",
      "     19       \u001b[36m36.3664\u001b[0m       \u001b[32m36.3705\u001b[0m  0.0109\n",
      "     20       \u001b[36m36.0824\u001b[0m       \u001b[32m35.9823\u001b[0m  0.0107\n",
      "     21       \u001b[36m35.8093\u001b[0m       \u001b[32m35.6052\u001b[0m  0.0103\n",
      "     22       \u001b[36m35.5479\u001b[0m       \u001b[32m35.2405\u001b[0m  0.0102\n",
      "     23       \u001b[36m35.2987\u001b[0m       \u001b[32m34.8888\u001b[0m  0.0109\n",
      "     24       \u001b[36m35.0621\u001b[0m       \u001b[32m34.5508\u001b[0m  0.0137\n",
      "     25       \u001b[36m34.8385\u001b[0m       \u001b[32m34.2274\u001b[0m  0.0148\n",
      "     26       \u001b[36m34.6284\u001b[0m       \u001b[32m33.9191\u001b[0m  0.0115\n",
      "     27       \u001b[36m34.4320\u001b[0m       \u001b[32m33.6263\u001b[0m  0.0112\n",
      "     28       \u001b[36m34.2494\u001b[0m       \u001b[32m33.3496\u001b[0m  0.0119\n",
      "     29       \u001b[36m34.0807\u001b[0m       \u001b[32m33.0891\u001b[0m  0.0122\n",
      "     30       \u001b[36m33.9255\u001b[0m       \u001b[32m32.8448\u001b[0m  0.0131\n",
      "     31       \u001b[36m33.7837\u001b[0m       \u001b[32m32.6168\u001b[0m  0.0127\n",
      "     32       \u001b[36m33.6548\u001b[0m       \u001b[32m32.4049\u001b[0m  0.0133\n",
      "     33       \u001b[36m33.5383\u001b[0m       \u001b[32m32.2087\u001b[0m  0.0124\n",
      "     34       \u001b[36m33.4334\u001b[0m       \u001b[32m32.0280\u001b[0m  0.0114\n",
      "     35       \u001b[36m33.3395\u001b[0m       \u001b[32m31.8621\u001b[0m  0.0110\n",
      "     36       \u001b[36m33.2558\u001b[0m       \u001b[32m31.7104\u001b[0m  0.0106\n",
      "     37       \u001b[36m33.1814\u001b[0m       \u001b[32m31.5721\u001b[0m  0.0117\n",
      "     38       \u001b[36m33.1155\u001b[0m       \u001b[32m31.4464\u001b[0m  0.0113\n",
      "     39       \u001b[36m33.0572\u001b[0m       \u001b[32m31.3324\u001b[0m  0.0113\n",
      "     40       \u001b[36m33.0058\u001b[0m       \u001b[32m31.2292\u001b[0m  0.0114\n",
      "     41       \u001b[36m32.9604\u001b[0m       \u001b[32m31.1359\u001b[0m  0.0112\n",
      "     42       \u001b[36m32.9204\u001b[0m       \u001b[32m31.0517\u001b[0m  0.0116\n",
      "     43       \u001b[36m32.8851\u001b[0m       \u001b[32m30.9759\u001b[0m  0.0109\n",
      "     44       \u001b[36m32.8540\u001b[0m       \u001b[32m30.9075\u001b[0m  0.0114\n",
      "     45       \u001b[36m32.8263\u001b[0m       \u001b[32m30.8459\u001b[0m  0.0105\n",
      "     46       \u001b[36m32.8017\u001b[0m       \u001b[32m30.7904\u001b[0m  0.0108\n",
      "     47       \u001b[36m32.7797\u001b[0m       \u001b[32m30.7402\u001b[0m  0.0111\n",
      "     48       \u001b[36m32.7598\u001b[0m       \u001b[32m30.6949\u001b[0m  0.0110\n",
      "     49       \u001b[36m32.7419\u001b[0m       \u001b[32m30.6538\u001b[0m  0.0113\n",
      "     50       \u001b[36m32.7256\u001b[0m       \u001b[32m30.6167\u001b[0m  0.0104\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m33.7632\u001b[0m       \u001b[32m32.3350\u001b[0m  0.0107\n",
      "      2       \u001b[36m33.3724\u001b[0m       \u001b[32m32.0379\u001b[0m  0.0103\n",
      "      3       \u001b[36m32.9911\u001b[0m       \u001b[32m31.7489\u001b[0m  0.0107\n",
      "      4       \u001b[36m32.6179\u001b[0m       \u001b[32m31.4669\u001b[0m  0.0103\n",
      "      5       \u001b[36m32.2512\u001b[0m       \u001b[32m31.1911\u001b[0m  0.0103\n",
      "      6       \u001b[36m31.8901\u001b[0m       \u001b[32m30.9211\u001b[0m  0.0106\n",
      "      7       \u001b[36m31.5338\u001b[0m       \u001b[32m30.6562\u001b[0m  0.0113\n",
      "      8       \u001b[36m31.1816\u001b[0m       \u001b[32m30.3961\u001b[0m  0.0103\n",
      "      9       \u001b[36m30.8331\u001b[0m       \u001b[32m30.1409\u001b[0m  0.0116\n",
      "     10       \u001b[36m30.4884\u001b[0m       \u001b[32m29.8905\u001b[0m  0.0105\n",
      "     11       \u001b[36m30.1471\u001b[0m       \u001b[32m29.6450\u001b[0m  0.0102\n",
      "     12       \u001b[36m29.8092\u001b[0m       \u001b[32m29.4048\u001b[0m  0.0102\n",
      "     13       \u001b[36m29.4751\u001b[0m       \u001b[32m29.1701\u001b[0m  0.0107\n",
      "     14       \u001b[36m29.1453\u001b[0m       \u001b[32m28.9413\u001b[0m  0.0112\n",
      "     15       \u001b[36m28.8201\u001b[0m       \u001b[32m28.7192\u001b[0m  0.0109\n",
      "     16       \u001b[36m28.5002\u001b[0m       \u001b[32m28.5045\u001b[0m  0.0102\n",
      "     17       \u001b[36m28.1861\u001b[0m       \u001b[32m28.2975\u001b[0m  0.0109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     18       \u001b[36m27.8786\u001b[0m       \u001b[32m28.0991\u001b[0m  0.0116\n",
      "     19       \u001b[36m27.5784\u001b[0m       \u001b[32m27.9096\u001b[0m  0.0110\n",
      "     20       \u001b[36m27.2865\u001b[0m       \u001b[32m27.7299\u001b[0m  0.0111\n",
      "     21       \u001b[36m27.0032\u001b[0m       \u001b[32m27.5602\u001b[0m  0.0108\n",
      "     22       \u001b[36m26.7296\u001b[0m       \u001b[32m27.4013\u001b[0m  0.0107\n",
      "     23       \u001b[36m26.4663\u001b[0m       \u001b[32m27.2535\u001b[0m  0.0103\n",
      "     24       \u001b[36m26.2142\u001b[0m       \u001b[32m27.1171\u001b[0m  0.0115\n",
      "     25       \u001b[36m25.9736\u001b[0m       \u001b[32m26.9924\u001b[0m  0.0112\n",
      "     26       \u001b[36m25.7453\u001b[0m       \u001b[32m26.8795\u001b[0m  0.0104\n",
      "     27       \u001b[36m25.5294\u001b[0m       \u001b[32m26.7785\u001b[0m  0.0103\n",
      "     28       \u001b[36m25.3264\u001b[0m       \u001b[32m26.6892\u001b[0m  0.0105\n",
      "     29       \u001b[36m25.1364\u001b[0m       \u001b[32m26.6115\u001b[0m  0.0113\n",
      "     30       \u001b[36m24.9597\u001b[0m       \u001b[32m26.5449\u001b[0m  0.0108\n",
      "     31       \u001b[36m24.7960\u001b[0m       \u001b[32m26.4891\u001b[0m  0.0102\n",
      "     32       \u001b[36m24.6454\u001b[0m       \u001b[32m26.4433\u001b[0m  0.0107\n",
      "     33       \u001b[36m24.5074\u001b[0m       \u001b[32m26.4071\u001b[0m  0.0110\n",
      "     34       \u001b[36m24.3819\u001b[0m       \u001b[32m26.3798\u001b[0m  0.0106\n",
      "     35       \u001b[36m24.2682\u001b[0m       \u001b[32m26.3603\u001b[0m  0.0110\n",
      "     36       \u001b[36m24.1657\u001b[0m       \u001b[32m26.3478\u001b[0m  0.0106\n",
      "     37       \u001b[36m24.0738\u001b[0m       \u001b[32m26.3414\u001b[0m  0.0109\n",
      "     38       \u001b[36m23.9918\u001b[0m       \u001b[32m26.3401\u001b[0m  0.0112\n",
      "     39       \u001b[36m23.9188\u001b[0m       26.3433  0.0113\n",
      "     40       \u001b[36m23.8541\u001b[0m       26.3502  0.0114\n",
      "     41       \u001b[36m23.7970\u001b[0m       26.3598  0.0112\n",
      "     42       \u001b[36m23.7467\u001b[0m       26.3717  0.0106\n",
      "     43       \u001b[36m23.7023\u001b[0m       26.3850  0.0105\n",
      "     44       \u001b[36m23.6632\u001b[0m       26.3993  0.0114\n",
      "     45       \u001b[36m23.6287\u001b[0m       26.4141  0.0112\n",
      "     46       \u001b[36m23.5982\u001b[0m       26.4290  0.0111\n",
      "     47       \u001b[36m23.5713\u001b[0m       26.4437  0.0108\n",
      "     48       \u001b[36m23.5473\u001b[0m       26.4580  0.0108\n",
      "     49       \u001b[36m23.5260\u001b[0m       26.4717  0.0103\n",
      "     50       \u001b[36m23.5070\u001b[0m       26.4848  0.0107\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m41.9389\u001b[0m       \u001b[32m33.0614\u001b[0m  0.0110\n",
      "      2       \u001b[36m41.3327\u001b[0m       \u001b[32m32.6504\u001b[0m  0.0111\n",
      "      3       \u001b[36m40.7616\u001b[0m       \u001b[32m32.2622\u001b[0m  0.0106\n",
      "      4       \u001b[36m40.2180\u001b[0m       \u001b[32m31.8931\u001b[0m  0.0115\n",
      "      5       \u001b[36m39.6961\u001b[0m       \u001b[32m31.5395\u001b[0m  0.0111\n",
      "      6       \u001b[36m39.1915\u001b[0m       \u001b[32m31.1988\u001b[0m  0.0110\n",
      "      7       \u001b[36m38.7008\u001b[0m       \u001b[32m30.8685\u001b[0m  0.0102\n",
      "      8       \u001b[36m38.2209\u001b[0m       \u001b[32m30.5479\u001b[0m  0.0137\n",
      "      9       \u001b[36m37.7506\u001b[0m       \u001b[32m30.2355\u001b[0m  0.0138\n",
      "     10       \u001b[36m37.2883\u001b[0m       \u001b[32m29.9308\u001b[0m  0.0138\n",
      "     11       \u001b[36m36.8332\u001b[0m       \u001b[32m29.6333\u001b[0m  0.0118\n",
      "     12       \u001b[36m36.3843\u001b[0m       \u001b[32m29.3431\u001b[0m  0.0114\n",
      "     13       \u001b[36m35.9418\u001b[0m       \u001b[32m29.0604\u001b[0m  0.0117\n",
      "     14       \u001b[36m35.5057\u001b[0m       \u001b[32m28.7856\u001b[0m  0.0129\n",
      "     15       \u001b[36m35.0764\u001b[0m       \u001b[32m28.5192\u001b[0m  0.0115\n",
      "     16       \u001b[36m34.6544\u001b[0m       \u001b[32m28.2620\u001b[0m  0.0114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m34.2407\u001b[0m       \u001b[32m28.0152\u001b[0m  0.0127\n",
      "     18       \u001b[36m33.8363\u001b[0m       \u001b[32m27.7797\u001b[0m  0.0109\n",
      "     19       \u001b[36m33.4423\u001b[0m       \u001b[32m27.5569\u001b[0m  0.0108\n",
      "     20       \u001b[36m33.0598\u001b[0m       \u001b[32m27.3476\u001b[0m  0.0112\n",
      "     21       \u001b[36m32.6898\u001b[0m       \u001b[32m27.1531\u001b[0m  0.0107\n",
      "     22       \u001b[36m32.3334\u001b[0m       \u001b[32m26.9742\u001b[0m  0.0112\n",
      "     23       \u001b[36m31.9915\u001b[0m       \u001b[32m26.8124\u001b[0m  0.0115\n",
      "     24       \u001b[36m31.6656\u001b[0m       \u001b[32m26.6684\u001b[0m  0.0112\n",
      "     25       \u001b[36m31.3567\u001b[0m       \u001b[32m26.5431\u001b[0m  0.0117\n",
      "     26       \u001b[36m31.0658\u001b[0m       \u001b[32m26.4370\u001b[0m  0.0111\n",
      "     27       \u001b[36m30.7940\u001b[0m       \u001b[32m26.3505\u001b[0m  0.0110\n",
      "     28       \u001b[36m30.5425\u001b[0m       \u001b[32m26.2836\u001b[0m  0.0106\n",
      "     29       \u001b[36m30.3117\u001b[0m       \u001b[32m26.2358\u001b[0m  0.0117\n",
      "     30       \u001b[36m30.1020\u001b[0m       \u001b[32m26.2064\u001b[0m  0.0111\n",
      "     31       \u001b[36m29.9133\u001b[0m       \u001b[32m26.1943\u001b[0m  0.0106\n",
      "     32       \u001b[36m29.7455\u001b[0m       26.1979  0.0113\n",
      "     33       \u001b[36m29.5978\u001b[0m       26.2154  0.0103\n",
      "     34       \u001b[36m29.4692\u001b[0m       26.2449  0.0108\n",
      "     35       \u001b[36m29.3582\u001b[0m       26.2843  0.0106\n",
      "     36       \u001b[36m29.2637\u001b[0m       26.3315  0.0110\n",
      "     37       \u001b[36m29.1839\u001b[0m       26.3841  0.0104\n",
      "     38       \u001b[36m29.1172\u001b[0m       26.4403  0.0103\n",
      "     39       \u001b[36m29.0620\u001b[0m       26.4984  0.0107\n",
      "     40       \u001b[36m29.0165\u001b[0m       26.5565  0.0114\n",
      "     41       \u001b[36m28.9794\u001b[0m       26.6136  0.0110\n",
      "     42       \u001b[36m28.9492\u001b[0m       26.6684  0.0106\n",
      "     43       \u001b[36m28.9246\u001b[0m       26.7203  0.0101\n",
      "     44       \u001b[36m28.9044\u001b[0m       26.7689  0.0110\n",
      "     45       \u001b[36m28.8878\u001b[0m       26.8139  0.0107\n",
      "     46       \u001b[36m28.8739\u001b[0m       26.8552  0.0110\n",
      "     47       \u001b[36m28.8622\u001b[0m       26.8928  0.0112\n",
      "     48       \u001b[36m28.8522\u001b[0m       26.9266  0.0109\n",
      "     49       \u001b[36m28.8435\u001b[0m       26.9569  0.0108\n",
      "     50       \u001b[36m28.8359\u001b[0m       26.9839  0.0111\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m45.0883\u001b[0m       \u001b[32m46.8031\u001b[0m  0.0117\n",
      "      2       \u001b[36m43.8960\u001b[0m       \u001b[32m45.3560\u001b[0m  0.0117\n",
      "      3       \u001b[36m42.7336\u001b[0m       \u001b[32m43.6727\u001b[0m  0.0113\n",
      "      4       \u001b[36m41.3613\u001b[0m       \u001b[32m41.7336\u001b[0m  0.0119\n",
      "      5       \u001b[36m39.8273\u001b[0m       \u001b[32m39.6415\u001b[0m  0.0115\n",
      "      6       \u001b[36m38.2090\u001b[0m       \u001b[32m37.3923\u001b[0m  0.0118\n",
      "      7       \u001b[36m36.5671\u001b[0m       \u001b[32m35.1046\u001b[0m  0.0115\n",
      "      8       \u001b[36m35.1292\u001b[0m       \u001b[32m33.1943\u001b[0m  0.0119\n",
      "      9       \u001b[36m34.2630\u001b[0m       \u001b[32m32.0428\u001b[0m  0.0116\n",
      "     10       \u001b[36m34.0274\u001b[0m       \u001b[32m31.5720\u001b[0m  0.0116\n",
      "     11       \u001b[36m33.9765\u001b[0m       \u001b[32m31.3952\u001b[0m  0.0115\n",
      "     12       \u001b[36m33.7777\u001b[0m       \u001b[32m31.3785\u001b[0m  0.0126\n",
      "     13       \u001b[36m33.5490\u001b[0m       31.4567  0.0117\n",
      "     14       \u001b[36m33.4007\u001b[0m       31.4810  0.0117\n",
      "     15       \u001b[36m33.2944\u001b[0m       31.4013  0.0118\n",
      "     16       \u001b[36m33.1952\u001b[0m       \u001b[32m31.2580\u001b[0m  0.0115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m33.1013\u001b[0m       \u001b[32m31.1019\u001b[0m  0.0119\n",
      "     18       \u001b[36m33.0186\u001b[0m       \u001b[32m30.9734\u001b[0m  0.0120\n",
      "     19       \u001b[36m32.9512\u001b[0m       \u001b[32m30.8818\u001b[0m  0.0116\n",
      "     20       \u001b[36m32.8932\u001b[0m       \u001b[32m30.8198\u001b[0m  0.0116\n",
      "     21       \u001b[36m32.8380\u001b[0m       \u001b[32m30.7827\u001b[0m  0.0115\n",
      "     22       \u001b[36m32.7867\u001b[0m       \u001b[32m30.7631\u001b[0m  0.0115\n",
      "     23       \u001b[36m32.7401\u001b[0m       \u001b[32m30.7461\u001b[0m  0.0121\n",
      "     24       \u001b[36m32.6975\u001b[0m       \u001b[32m30.7208\u001b[0m  0.0120\n",
      "     25       \u001b[36m32.6584\u001b[0m       \u001b[32m30.6878\u001b[0m  0.0118\n",
      "     26       \u001b[36m32.6225\u001b[0m       \u001b[32m30.6560\u001b[0m  0.0114\n",
      "     27       \u001b[36m32.5899\u001b[0m       \u001b[32m30.6320\u001b[0m  0.0113\n",
      "     28       \u001b[36m32.5600\u001b[0m       \u001b[32m30.6140\u001b[0m  0.0119\n",
      "     29       \u001b[36m32.5325\u001b[0m       \u001b[32m30.5981\u001b[0m  0.0116\n",
      "     30       \u001b[36m32.5072\u001b[0m       \u001b[32m30.5845\u001b[0m  0.0117\n",
      "     31       \u001b[36m32.4841\u001b[0m       \u001b[32m30.5710\u001b[0m  0.0113\n",
      "     32       \u001b[36m32.4629\u001b[0m       \u001b[32m30.5569\u001b[0m  0.0115\n",
      "     33       \u001b[36m32.4435\u001b[0m       \u001b[32m30.5439\u001b[0m  0.0118\n",
      "     34       \u001b[36m32.4256\u001b[0m       \u001b[32m30.5336\u001b[0m  0.0120\n",
      "     35       \u001b[36m32.4093\u001b[0m       \u001b[32m30.5244\u001b[0m  0.0113\n",
      "     36       \u001b[36m32.3942\u001b[0m       \u001b[32m30.5169\u001b[0m  0.0116\n",
      "     37       \u001b[36m32.3806\u001b[0m       \u001b[32m30.5095\u001b[0m  0.0117\n",
      "     38       \u001b[36m32.3679\u001b[0m       \u001b[32m30.5001\u001b[0m  0.0117\n",
      "     39       \u001b[36m32.3561\u001b[0m       \u001b[32m30.4919\u001b[0m  0.0117\n",
      "     40       \u001b[36m32.3452\u001b[0m       \u001b[32m30.4852\u001b[0m  0.0164\n",
      "     41       \u001b[36m32.3351\u001b[0m       \u001b[32m30.4760\u001b[0m  0.0159\n",
      "     42       \u001b[36m32.3257\u001b[0m       \u001b[32m30.4672\u001b[0m  0.0130\n",
      "     43       \u001b[36m32.3169\u001b[0m       \u001b[32m30.4595\u001b[0m  0.0129\n",
      "     44       \u001b[36m32.3087\u001b[0m       \u001b[32m30.4513\u001b[0m  0.0125\n",
      "     45       \u001b[36m32.3010\u001b[0m       \u001b[32m30.4432\u001b[0m  0.0144\n",
      "     46       \u001b[36m32.2938\u001b[0m       \u001b[32m30.4370\u001b[0m  0.0131\n",
      "     47       \u001b[36m32.2870\u001b[0m       \u001b[32m30.4313\u001b[0m  0.0128\n",
      "     48       \u001b[36m32.2805\u001b[0m       \u001b[32m30.4258\u001b[0m  0.0120\n",
      "     49       \u001b[36m32.2745\u001b[0m       \u001b[32m30.4208\u001b[0m  0.0118\n",
      "     50       \u001b[36m32.2687\u001b[0m       \u001b[32m30.4160\u001b[0m  0.0119\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m33.5480\u001b[0m       \u001b[32m31.6962\u001b[0m  0.0123\n",
      "      2       \u001b[36m32.1886\u001b[0m       \u001b[32m30.7357\u001b[0m  0.0119\n",
      "      3       \u001b[36m30.8512\u001b[0m       \u001b[32m29.7398\u001b[0m  0.0120\n",
      "      4       \u001b[36m29.4550\u001b[0m       \u001b[32m28.6354\u001b[0m  0.0119\n",
      "      5       \u001b[36m27.8740\u001b[0m       \u001b[32m27.5331\u001b[0m  0.0118\n",
      "      6       \u001b[36m26.2148\u001b[0m       \u001b[32m26.8565\u001b[0m  0.0118\n",
      "      7       \u001b[36m24.9641\u001b[0m       27.0820  0.0122\n",
      "      8       \u001b[36m24.5325\u001b[0m       27.7531  0.0117\n",
      "      9       \u001b[36m24.5165\u001b[0m       27.7304  0.0113\n",
      "     10       \u001b[36m24.3040\u001b[0m       27.1881  0.0114\n",
      "     11       \u001b[36m24.0557\u001b[0m       \u001b[32m26.7794\u001b[0m  0.0115\n",
      "     12       \u001b[36m23.9422\u001b[0m       \u001b[32m26.6131\u001b[0m  0.0119\n",
      "     13       \u001b[36m23.8712\u001b[0m       \u001b[32m26.5902\u001b[0m  0.0118\n",
      "     14       \u001b[36m23.7806\u001b[0m       26.6671  0.0118\n",
      "     15       \u001b[36m23.6921\u001b[0m       26.8037  0.0122\n",
      "     16       \u001b[36m23.6228\u001b[0m       26.9255  0.0118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m23.5662\u001b[0m       26.9842  0.0123\n",
      "     18       \u001b[36m23.5182\u001b[0m       26.9840  0.0115\n",
      "     19       \u001b[36m23.4766\u001b[0m       26.9573  0.0118\n",
      "     20       \u001b[36m23.4408\u001b[0m       26.9342  0.0114\n",
      "     21       \u001b[36m23.4098\u001b[0m       26.9253  0.0113\n",
      "     22       \u001b[36m23.3803\u001b[0m       26.9281  0.0117\n",
      "     23       \u001b[36m23.3526\u001b[0m       26.9349  0.0114\n",
      "     24       \u001b[36m23.3274\u001b[0m       26.9378  0.0118\n",
      "     25       \u001b[36m23.3049\u001b[0m       26.9340  0.0116\n",
      "     26       \u001b[36m23.2850\u001b[0m       26.9253  0.0114\n",
      "     27       \u001b[36m23.2674\u001b[0m       26.9165  0.0118\n",
      "     28       \u001b[36m23.2515\u001b[0m       26.9094  0.0116\n",
      "     29       \u001b[36m23.2369\u001b[0m       26.9053  0.0117\n",
      "     30       \u001b[36m23.2234\u001b[0m       26.9002  0.0117\n",
      "     31       \u001b[36m23.2111\u001b[0m       26.8943  0.0116\n",
      "     32       \u001b[36m23.1997\u001b[0m       26.8872  0.0118\n",
      "     33       \u001b[36m23.1893\u001b[0m       26.8784  0.0114\n",
      "     34       \u001b[36m23.1797\u001b[0m       26.8678  0.0117\n",
      "     35       \u001b[36m23.1707\u001b[0m       26.8589  0.0121\n",
      "     36       \u001b[36m23.1623\u001b[0m       26.8503  0.0113\n",
      "     37       \u001b[36m23.1545\u001b[0m       26.8412  0.0119\n",
      "     38       \u001b[36m23.1472\u001b[0m       26.8312  0.0118\n",
      "     39       \u001b[36m23.1405\u001b[0m       26.8225  0.0118\n",
      "     40       \u001b[36m23.1342\u001b[0m       26.8143  0.0116\n",
      "     41       \u001b[36m23.1283\u001b[0m       26.8079  0.0114\n",
      "     42       \u001b[36m23.1227\u001b[0m       26.8028  0.0130\n",
      "     43       \u001b[36m23.1175\u001b[0m       26.7978  0.0131\n",
      "     44       \u001b[36m23.1125\u001b[0m       26.7928  0.0119\n",
      "     45       \u001b[36m23.1078\u001b[0m       26.7877  0.0113\n",
      "     46       \u001b[36m23.1033\u001b[0m       26.7822  0.0114\n",
      "     47       \u001b[36m23.0991\u001b[0m       26.7780  0.0118\n",
      "     48       \u001b[36m23.0952\u001b[0m       26.7729  0.0114\n",
      "     49       \u001b[36m23.0914\u001b[0m       26.7686  0.0116\n",
      "     50       \u001b[36m23.0877\u001b[0m       26.7650  0.0122\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m39.3982\u001b[0m       \u001b[32m31.0288\u001b[0m  0.0116\n",
      "      2       \u001b[36m38.2721\u001b[0m       \u001b[32m30.3665\u001b[0m  0.0119\n",
      "      3       \u001b[36m37.2716\u001b[0m       \u001b[32m29.7099\u001b[0m  0.0118\n",
      "      4       \u001b[36m36.2770\u001b[0m       \u001b[32m29.0237\u001b[0m  0.0115\n",
      "      5       \u001b[36m35.2171\u001b[0m       \u001b[32m28.2874\u001b[0m  0.0114\n",
      "      6       \u001b[36m34.0358\u001b[0m       \u001b[32m27.5506\u001b[0m  0.0115\n",
      "      7       \u001b[36m32.7784\u001b[0m       \u001b[32m26.9501\u001b[0m  0.0119\n",
      "      8       \u001b[36m31.5870\u001b[0m       \u001b[32m26.6790\u001b[0m  0.0117\n",
      "      9       \u001b[36m30.6594\u001b[0m       26.8905  0.0116\n",
      "     10       \u001b[36m30.1604\u001b[0m       27.4842  0.0114\n",
      "     11       \u001b[36m30.0450\u001b[0m       27.9994  0.0117\n",
      "     12       \u001b[36m30.0171\u001b[0m       28.0529  0.0122\n",
      "     13       \u001b[36m29.8745\u001b[0m       27.7580  0.0117\n",
      "     14       \u001b[36m29.6764\u001b[0m       27.4185  0.0122\n",
      "     15       \u001b[36m29.5159\u001b[0m       27.1958  0.0116\n",
      "     16       \u001b[36m29.4032\u001b[0m       27.1074  0.0113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m29.3145\u001b[0m       27.1214  0.0124\n",
      "     18       \u001b[36m29.2351\u001b[0m       27.1990  0.0122\n",
      "     19       \u001b[36m29.1658\u001b[0m       27.2981  0.0175\n",
      "     20       \u001b[36m29.1051\u001b[0m       27.3818  0.0125\n",
      "     21       \u001b[36m29.0481\u001b[0m       27.4285  0.0157\n",
      "     22       \u001b[36m28.9920\u001b[0m       27.4401  0.0143\n",
      "     23       \u001b[36m28.9362\u001b[0m       27.4407  0.0150\n",
      "     24       \u001b[36m28.8847\u001b[0m       27.4413  0.0129\n",
      "     25       \u001b[36m28.8387\u001b[0m       27.4391  0.0132\n",
      "     26       \u001b[36m28.7970\u001b[0m       27.4441  0.0124\n",
      "     27       \u001b[36m28.7601\u001b[0m       27.4575  0.0125\n",
      "     28       \u001b[36m28.7273\u001b[0m       27.4725  0.0119\n",
      "     29       \u001b[36m28.6979\u001b[0m       27.4886  0.0119\n",
      "     30       \u001b[36m28.6714\u001b[0m       27.4998  0.0124\n",
      "     31       \u001b[36m28.6473\u001b[0m       27.5047  0.0122\n",
      "     32       \u001b[36m28.6254\u001b[0m       27.5063  0.0120\n",
      "     33       \u001b[36m28.6057\u001b[0m       27.5061  0.0119\n",
      "     34       \u001b[36m28.5882\u001b[0m       27.5056  0.0119\n",
      "     35       \u001b[36m28.5729\u001b[0m       27.5033  0.0131\n",
      "     36       \u001b[36m28.5592\u001b[0m       27.5003  0.0123\n",
      "     37       \u001b[36m28.5469\u001b[0m       27.4958  0.0118\n",
      "     38       \u001b[36m28.5359\u001b[0m       27.4904  0.0116\n",
      "     39       \u001b[36m28.5259\u001b[0m       27.4865  0.0116\n",
      "     40       \u001b[36m28.5170\u001b[0m       27.4821  0.0119\n",
      "     41       \u001b[36m28.5089\u001b[0m       27.4777  0.0118\n",
      "     42       \u001b[36m28.5018\u001b[0m       27.4729  0.0125\n",
      "     43       \u001b[36m28.4949\u001b[0m       27.4684  0.0128\n",
      "     44       \u001b[36m28.4892\u001b[0m       27.4626  0.0115\n",
      "     45       \u001b[36m28.4833\u001b[0m       27.4588  0.0120\n",
      "     46       \u001b[36m28.4790\u001b[0m       27.4522  0.0129\n",
      "     47       \u001b[36m28.4736\u001b[0m       27.4496  0.0118\n",
      "     48       \u001b[36m28.4710\u001b[0m       27.4412  0.0117\n",
      "     49       \u001b[36m28.4661\u001b[0m       27.4418  0.0114\n",
      "     50       28.4672       27.4298  0.0129\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m44.3789\u001b[0m       \u001b[32m46.6071\u001b[0m  0.0114\n",
      "      2       \u001b[36m43.9777\u001b[0m       \u001b[32m46.1451\u001b[0m  0.0112\n",
      "      3       \u001b[36m43.6031\u001b[0m       \u001b[32m45.7141\u001b[0m  0.0118\n",
      "      4       \u001b[36m43.2530\u001b[0m       \u001b[32m45.3066\u001b[0m  0.0111\n",
      "      5       \u001b[36m42.9229\u001b[0m       \u001b[32m44.9153\u001b[0m  0.0110\n",
      "      6       \u001b[36m42.6062\u001b[0m       \u001b[32m44.5346\u001b[0m  0.0110\n",
      "      7       \u001b[36m42.2985\u001b[0m       \u001b[32m44.1597\u001b[0m  0.0111\n",
      "      8       \u001b[36m41.9969\u001b[0m       \u001b[32m43.7894\u001b[0m  0.0110\n",
      "      9       \u001b[36m41.6990\u001b[0m       \u001b[32m43.4184\u001b[0m  0.0111\n",
      "     10       \u001b[36m41.4024\u001b[0m       \u001b[32m43.0478\u001b[0m  0.0109\n",
      "     11       \u001b[36m41.1063\u001b[0m       \u001b[32m42.6753\u001b[0m  0.0108\n",
      "     12       \u001b[36m40.8098\u001b[0m       \u001b[32m42.2997\u001b[0m  0.0111\n",
      "     13       \u001b[36m40.5120\u001b[0m       \u001b[32m41.9203\u001b[0m  0.0113\n",
      "     14       \u001b[36m40.2122\u001b[0m       \u001b[32m41.5367\u001b[0m  0.0112\n",
      "     15       \u001b[36m39.9100\u001b[0m       \u001b[32m41.1480\u001b[0m  0.0109\n",
      "     16       \u001b[36m39.6054\u001b[0m       \u001b[32m40.7543\u001b[0m  0.0109\n",
      "     17       \u001b[36m39.2980\u001b[0m       \u001b[32m40.3554\u001b[0m  0.0113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     18       \u001b[36m38.9883\u001b[0m       \u001b[32m39.9515\u001b[0m  0.0115\n",
      "     19       \u001b[36m38.6767\u001b[0m       \u001b[32m39.5429\u001b[0m  0.0113\n",
      "     20       \u001b[36m38.3640\u001b[0m       \u001b[32m39.1319\u001b[0m  0.0109\n",
      "     21       \u001b[36m38.0508\u001b[0m       \u001b[32m38.7187\u001b[0m  0.0107\n",
      "     22       \u001b[36m37.7383\u001b[0m       \u001b[32m38.3052\u001b[0m  0.0107\n",
      "     23       \u001b[36m37.4279\u001b[0m       \u001b[32m37.8927\u001b[0m  0.0107\n",
      "     24       \u001b[36m37.1209\u001b[0m       \u001b[32m37.4825\u001b[0m  0.0108\n",
      "     25       \u001b[36m36.8185\u001b[0m       \u001b[32m37.0764\u001b[0m  0.0115\n",
      "     26       \u001b[36m36.5219\u001b[0m       \u001b[32m36.6764\u001b[0m  0.0107\n",
      "     27       \u001b[36m36.2326\u001b[0m       \u001b[32m36.2844\u001b[0m  0.0110\n",
      "     28       \u001b[36m35.9523\u001b[0m       \u001b[32m35.9022\u001b[0m  0.0110\n",
      "     29       \u001b[36m35.6824\u001b[0m       \u001b[32m35.5311\u001b[0m  0.0109\n",
      "     30       \u001b[36m35.4239\u001b[0m       \u001b[32m35.1724\u001b[0m  0.0111\n",
      "     31       \u001b[36m35.1776\u001b[0m       \u001b[32m34.8270\u001b[0m  0.0107\n",
      "     32       \u001b[36m34.9441\u001b[0m       \u001b[32m34.4954\u001b[0m  0.0108\n",
      "     33       \u001b[36m34.7240\u001b[0m       \u001b[32m34.1780\u001b[0m  0.0111\n",
      "     34       \u001b[36m34.5174\u001b[0m       \u001b[32m33.8755\u001b[0m  0.0111\n",
      "     35       \u001b[36m34.3245\u001b[0m       \u001b[32m33.5879\u001b[0m  0.0111\n",
      "     36       \u001b[36m34.1451\u001b[0m       \u001b[32m33.3154\u001b[0m  0.0109\n",
      "     37       \u001b[36m33.9791\u001b[0m       \u001b[32m33.0581\u001b[0m  0.0108\n",
      "     38       \u001b[36m33.8262\u001b[0m       \u001b[32m32.8158\u001b[0m  0.0109\n",
      "     39       \u001b[36m33.6861\u001b[0m       \u001b[32m32.5884\u001b[0m  0.0111\n",
      "     40       \u001b[36m33.5581\u001b[0m       \u001b[32m32.3757\u001b[0m  0.0110\n",
      "     41       \u001b[36m33.4419\u001b[0m       \u001b[32m32.1775\u001b[0m  0.0106\n",
      "     42       \u001b[36m33.3368\u001b[0m       \u001b[32m31.9934\u001b[0m  0.0106\n",
      "     43       \u001b[36m33.2421\u001b[0m       \u001b[32m31.8230\u001b[0m  0.0113\n",
      "     44       \u001b[36m33.1572\u001b[0m       \u001b[32m31.6658\u001b[0m  0.0108\n",
      "     45       \u001b[36m33.0815\u001b[0m       \u001b[32m31.5212\u001b[0m  0.0112\n",
      "     46       \u001b[36m33.0142\u001b[0m       \u001b[32m31.3889\u001b[0m  0.0109\n",
      "     47       \u001b[36m32.9548\u001b[0m       \u001b[32m31.2681\u001b[0m  0.0107\n",
      "     48       \u001b[36m32.9023\u001b[0m       \u001b[32m31.1581\u001b[0m  0.0113\n",
      "     49       \u001b[36m32.8560\u001b[0m       \u001b[32m31.0583\u001b[0m  0.0130\n",
      "     50       \u001b[36m32.8154\u001b[0m       \u001b[32m30.9679\u001b[0m  0.0203\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m32.5185\u001b[0m       \u001b[32m31.5348\u001b[0m  0.0117\n",
      "      2       \u001b[36m32.2116\u001b[0m       \u001b[32m31.3050\u001b[0m  0.0125\n",
      "      3       \u001b[36m31.9095\u001b[0m       \u001b[32m31.0802\u001b[0m  0.0126\n",
      "      4       \u001b[36m31.6113\u001b[0m       \u001b[32m30.8596\u001b[0m  0.0144\n",
      "      5       \u001b[36m31.3156\u001b[0m       \u001b[32m30.6427\u001b[0m  0.0118\n",
      "      6       \u001b[36m31.0230\u001b[0m       \u001b[32m30.4291\u001b[0m  0.0122\n",
      "      7       \u001b[36m30.7314\u001b[0m       \u001b[32m30.2173\u001b[0m  0.0128\n",
      "      8       \u001b[36m30.4400\u001b[0m       \u001b[32m30.0069\u001b[0m  0.0115\n",
      "      9       \u001b[36m30.1490\u001b[0m       \u001b[32m29.7977\u001b[0m  0.0113\n",
      "     10       \u001b[36m29.8571\u001b[0m       \u001b[32m29.5886\u001b[0m  0.0112\n",
      "     11       \u001b[36m29.5637\u001b[0m       \u001b[32m29.3791\u001b[0m  0.0110\n",
      "     12       \u001b[36m29.2702\u001b[0m       \u001b[32m29.1699\u001b[0m  0.0124\n",
      "     13       \u001b[36m28.9770\u001b[0m       \u001b[32m28.9610\u001b[0m  0.0114\n",
      "     14       \u001b[36m28.6842\u001b[0m       \u001b[32m28.7542\u001b[0m  0.0113\n",
      "     15       \u001b[36m28.3929\u001b[0m       \u001b[32m28.5498\u001b[0m  0.0110\n",
      "     16       \u001b[36m28.1036\u001b[0m       \u001b[32m28.3490\u001b[0m  0.0117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m27.8186\u001b[0m       \u001b[32m28.1529\u001b[0m  0.0119\n",
      "     18       \u001b[36m27.5389\u001b[0m       \u001b[32m27.9633\u001b[0m  0.0108\n",
      "     19       \u001b[36m27.2664\u001b[0m       \u001b[32m27.7813\u001b[0m  0.0111\n",
      "     20       \u001b[36m27.0021\u001b[0m       \u001b[32m27.6077\u001b[0m  0.0108\n",
      "     21       \u001b[36m26.7470\u001b[0m       \u001b[32m27.4428\u001b[0m  0.0106\n",
      "     22       \u001b[36m26.5016\u001b[0m       \u001b[32m27.2875\u001b[0m  0.0114\n",
      "     23       \u001b[36m26.2667\u001b[0m       \u001b[32m27.1423\u001b[0m  0.0112\n",
      "     24       \u001b[36m26.0426\u001b[0m       \u001b[32m27.0075\u001b[0m  0.0108\n",
      "     25       \u001b[36m25.8299\u001b[0m       \u001b[32m26.8834\u001b[0m  0.0106\n",
      "     26       \u001b[36m25.6288\u001b[0m       \u001b[32m26.7702\u001b[0m  0.0106\n",
      "     27       \u001b[36m25.4395\u001b[0m       \u001b[32m26.6676\u001b[0m  0.0116\n",
      "     28       \u001b[36m25.2618\u001b[0m       \u001b[32m26.5755\u001b[0m  0.0109\n",
      "     29       \u001b[36m25.0954\u001b[0m       \u001b[32m26.4937\u001b[0m  0.0112\n",
      "     30       \u001b[36m24.9402\u001b[0m       \u001b[32m26.4218\u001b[0m  0.0105\n",
      "     31       \u001b[36m24.7959\u001b[0m       \u001b[32m26.3596\u001b[0m  0.0106\n",
      "     32       \u001b[36m24.6623\u001b[0m       \u001b[32m26.3065\u001b[0m  0.0120\n",
      "     33       \u001b[36m24.5391\u001b[0m       \u001b[32m26.2621\u001b[0m  0.0114\n",
      "     34       \u001b[36m24.4258\u001b[0m       \u001b[32m26.2257\u001b[0m  0.0112\n",
      "     35       \u001b[36m24.3220\u001b[0m       \u001b[32m26.1967\u001b[0m  0.0105\n",
      "     36       \u001b[36m24.2276\u001b[0m       \u001b[32m26.1743\u001b[0m  0.0106\n",
      "     37       \u001b[36m24.1414\u001b[0m       \u001b[32m26.1578\u001b[0m  0.0115\n",
      "     38       \u001b[36m24.0632\u001b[0m       \u001b[32m26.1468\u001b[0m  0.0109\n",
      "     39       \u001b[36m23.9922\u001b[0m       \u001b[32m26.1403\u001b[0m  0.0112\n",
      "     40       \u001b[36m23.9282\u001b[0m       \u001b[32m26.1379\u001b[0m  0.0106\n",
      "     41       \u001b[36m23.8706\u001b[0m       26.1389  0.0105\n",
      "     42       \u001b[36m23.8188\u001b[0m       26.1427  0.0112\n",
      "     43       \u001b[36m23.7723\u001b[0m       26.1487  0.0112\n",
      "     44       \u001b[36m23.7306\u001b[0m       26.1566  0.0112\n",
      "     45       \u001b[36m23.6933\u001b[0m       26.1660  0.0104\n",
      "     46       \u001b[36m23.6599\u001b[0m       26.1765  0.0109\n",
      "     47       \u001b[36m23.6299\u001b[0m       26.1876  0.0114\n",
      "     48       \u001b[36m23.6030\u001b[0m       26.1992  0.0107\n",
      "     49       \u001b[36m23.5789\u001b[0m       26.2110  0.0106\n",
      "     50       \u001b[36m23.5572\u001b[0m       26.2227  0.0108\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m41.5103\u001b[0m       \u001b[32m32.8331\u001b[0m  0.0106\n",
      "      2       \u001b[36m41.0929\u001b[0m       \u001b[32m32.5510\u001b[0m  0.0109\n",
      "      3       \u001b[36m40.6889\u001b[0m       \u001b[32m32.2766\u001b[0m  0.0108\n",
      "      4       \u001b[36m40.2936\u001b[0m       \u001b[32m32.0076\u001b[0m  0.0108\n",
      "      5       \u001b[36m39.9050\u001b[0m       \u001b[32m31.7411\u001b[0m  0.0106\n",
      "      6       \u001b[36m39.5186\u001b[0m       \u001b[32m31.4759\u001b[0m  0.0106\n",
      "      7       \u001b[36m39.1327\u001b[0m       \u001b[32m31.2112\u001b[0m  0.0106\n",
      "      8       \u001b[36m38.7457\u001b[0m       \u001b[32m30.9457\u001b[0m  0.0106\n",
      "      9       \u001b[36m38.3539\u001b[0m       \u001b[32m30.6785\u001b[0m  0.0107\n",
      "     10       \u001b[36m37.9578\u001b[0m       \u001b[32m30.4100\u001b[0m  0.0105\n",
      "     11       \u001b[36m37.5560\u001b[0m       \u001b[32m30.1405\u001b[0m  0.0105\n",
      "     12       \u001b[36m37.1488\u001b[0m       \u001b[32m29.8701\u001b[0m  0.0108\n",
      "     13       \u001b[36m36.7378\u001b[0m       \u001b[32m29.6000\u001b[0m  0.0108\n",
      "     14       \u001b[36m36.3236\u001b[0m       \u001b[32m29.3309\u001b[0m  0.0107\n",
      "     15       \u001b[36m35.9089\u001b[0m       \u001b[32m29.0644\u001b[0m  0.0109\n",
      "     16       \u001b[36m35.4955\u001b[0m       \u001b[32m28.8020\u001b[0m  0.0106\n",
      "     17       \u001b[36m35.0850\u001b[0m       \u001b[32m28.5448\u001b[0m  0.0116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     18       \u001b[36m34.6791\u001b[0m       \u001b[32m28.2940\u001b[0m  0.0117\n",
      "     19       \u001b[36m34.2797\u001b[0m       \u001b[32m28.0522\u001b[0m  0.0111\n",
      "     20       \u001b[36m33.8891\u001b[0m       \u001b[32m27.8214\u001b[0m  0.0108\n",
      "     21       \u001b[36m33.5078\u001b[0m       \u001b[32m27.6027\u001b[0m  0.0107\n",
      "     22       \u001b[36m33.1370\u001b[0m       \u001b[32m27.3972\u001b[0m  0.0108\n",
      "     23       \u001b[36m32.7778\u001b[0m       \u001b[32m27.2063\u001b[0m  0.0108\n",
      "     24       \u001b[36m32.4308\u001b[0m       \u001b[32m27.0312\u001b[0m  0.0110\n",
      "     25       \u001b[36m32.0967\u001b[0m       \u001b[32m26.8740\u001b[0m  0.0110\n",
      "     26       \u001b[36m31.7776\u001b[0m       \u001b[32m26.7357\u001b[0m  0.0107\n",
      "     27       \u001b[36m31.4749\u001b[0m       \u001b[32m26.6169\u001b[0m  0.0107\n",
      "     28       \u001b[36m31.1897\u001b[0m       \u001b[32m26.5185\u001b[0m  0.0110\n",
      "     29       \u001b[36m30.9233\u001b[0m       \u001b[32m26.4406\u001b[0m  0.0109\n",
      "     30       \u001b[36m30.6762\u001b[0m       \u001b[32m26.3827\u001b[0m  0.0106\n",
      "     31       \u001b[36m30.4490\u001b[0m       \u001b[32m26.3441\u001b[0m  0.0105\n",
      "     32       \u001b[36m30.2420\u001b[0m       \u001b[32m26.3235\u001b[0m  0.0126\n",
      "     33       \u001b[36m30.0551\u001b[0m       \u001b[32m26.3195\u001b[0m  0.0149\n",
      "     34       \u001b[36m29.8880\u001b[0m       26.3302  0.0117\n",
      "     35       \u001b[36m29.7397\u001b[0m       26.3535  0.0119\n",
      "     36       \u001b[36m29.6096\u001b[0m       26.3872  0.0119\n",
      "     37       \u001b[36m29.4964\u001b[0m       26.4290  0.0123\n",
      "     38       \u001b[36m29.3987\u001b[0m       26.4769  0.0164\n",
      "     39       \u001b[36m29.3149\u001b[0m       26.5286  0.0128\n",
      "     40       \u001b[36m29.2434\u001b[0m       26.5824  0.0113\n",
      "     41       \u001b[36m29.1825\u001b[0m       26.6367  0.0115\n",
      "     42       \u001b[36m29.1310\u001b[0m       26.6902  0.0115\n",
      "     43       \u001b[36m29.0874\u001b[0m       26.7417  0.0113\n",
      "     44       \u001b[36m29.0505\u001b[0m       26.7907  0.0112\n",
      "     45       \u001b[36m29.0190\u001b[0m       26.8365  0.0112\n",
      "     46       \u001b[36m28.9921\u001b[0m       26.8787  0.0111\n",
      "     47       \u001b[36m28.9689\u001b[0m       26.9171  0.0110\n",
      "     48       \u001b[36m28.9487\u001b[0m       26.9518  0.0113\n",
      "     49       \u001b[36m28.9309\u001b[0m       26.9829  0.0112\n",
      "     50       \u001b[36m28.9151\u001b[0m       27.0104  0.0111\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m42.9853\u001b[0m       \u001b[32m43.8154\u001b[0m  0.0119\n",
      "      2       \u001b[36m41.4372\u001b[0m       \u001b[32m41.6641\u001b[0m  0.0116\n",
      "      3       \u001b[36m39.7053\u001b[0m       \u001b[32m39.3734\u001b[0m  0.0123\n",
      "      4       \u001b[36m37.9226\u001b[0m       \u001b[32m36.9426\u001b[0m  0.0121\n",
      "      5       \u001b[36m36.1305\u001b[0m       \u001b[32m34.3374\u001b[0m  0.0118\n",
      "      6       \u001b[36m34.5736\u001b[0m       \u001b[32m32.2503\u001b[0m  0.0119\n",
      "      7       \u001b[36m33.8615\u001b[0m       \u001b[32m31.3584\u001b[0m  0.0120\n",
      "      8       33.8617       \u001b[32m31.1285\u001b[0m  0.0120\n",
      "      9       \u001b[36m33.6804\u001b[0m       \u001b[32m31.0990\u001b[0m  0.0119\n",
      "     10       \u001b[36m33.3595\u001b[0m       31.2814  0.0116\n",
      "     11       \u001b[36m33.1996\u001b[0m       31.4132  0.0117\n",
      "     12       \u001b[36m33.1119\u001b[0m       31.3084  0.0117\n",
      "     13       \u001b[36m33.0011\u001b[0m       \u001b[32m31.0690\u001b[0m  0.0117\n",
      "     14       \u001b[36m32.8963\u001b[0m       \u001b[32m30.8471\u001b[0m  0.0118\n",
      "     15       \u001b[36m32.8187\u001b[0m       \u001b[32m30.7122\u001b[0m  0.0120\n",
      "     16       \u001b[36m32.7584\u001b[0m       \u001b[32m30.6572\u001b[0m  0.0120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m32.7066\u001b[0m       \u001b[32m30.6321\u001b[0m  0.0118\n",
      "     18       \u001b[36m32.6558\u001b[0m       \u001b[32m30.6149\u001b[0m  0.0118\n",
      "     19       \u001b[36m32.6107\u001b[0m       \u001b[32m30.6027\u001b[0m  0.0115\n",
      "     20       \u001b[36m32.5707\u001b[0m       \u001b[32m30.5825\u001b[0m  0.0124\n",
      "     21       \u001b[36m32.5340\u001b[0m       \u001b[32m30.5491\u001b[0m  0.0116\n",
      "     22       \u001b[36m32.5012\u001b[0m       \u001b[32m30.5111\u001b[0m  0.0120\n",
      "     23       \u001b[36m32.4723\u001b[0m       \u001b[32m30.4862\u001b[0m  0.0121\n",
      "     24       \u001b[36m32.4469\u001b[0m       \u001b[32m30.4789\u001b[0m  0.0120\n",
      "     25       \u001b[36m32.4239\u001b[0m       \u001b[32m30.4745\u001b[0m  0.0120\n",
      "     26       \u001b[36m32.4030\u001b[0m       \u001b[32m30.4668\u001b[0m  0.0121\n",
      "     27       \u001b[36m32.3839\u001b[0m       \u001b[32m30.4568\u001b[0m  0.0121\n",
      "     28       \u001b[36m32.3668\u001b[0m       \u001b[32m30.4458\u001b[0m  0.0119\n",
      "     29       \u001b[36m32.3512\u001b[0m       \u001b[32m30.4354\u001b[0m  0.0117\n",
      "     30       \u001b[36m32.3371\u001b[0m       \u001b[32m30.4260\u001b[0m  0.0122\n",
      "     31       \u001b[36m32.3244\u001b[0m       \u001b[32m30.4192\u001b[0m  0.0121\n",
      "     32       \u001b[36m32.3130\u001b[0m       \u001b[32m30.4142\u001b[0m  0.0122\n",
      "     33       \u001b[36m32.3025\u001b[0m       \u001b[32m30.4075\u001b[0m  0.0121\n",
      "     34       \u001b[36m32.2930\u001b[0m       \u001b[32m30.4023\u001b[0m  0.0116\n",
      "     35       \u001b[36m32.2842\u001b[0m       \u001b[32m30.3968\u001b[0m  0.0120\n",
      "     36       \u001b[36m32.2761\u001b[0m       \u001b[32m30.3910\u001b[0m  0.0118\n",
      "     37       \u001b[36m32.2686\u001b[0m       \u001b[32m30.3896\u001b[0m  0.0121\n",
      "     38       \u001b[36m32.2618\u001b[0m       \u001b[32m30.3849\u001b[0m  0.0116\n",
      "     39       \u001b[36m32.2554\u001b[0m       \u001b[32m30.3828\u001b[0m  0.0115\n",
      "     40       \u001b[36m32.2494\u001b[0m       \u001b[32m30.3786\u001b[0m  0.0120\n",
      "     41       \u001b[36m32.2437\u001b[0m       \u001b[32m30.3753\u001b[0m  0.0115\n",
      "     42       \u001b[36m32.2384\u001b[0m       \u001b[32m30.3738\u001b[0m  0.0116\n",
      "     43       \u001b[36m32.2333\u001b[0m       \u001b[32m30.3711\u001b[0m  0.0116\n",
      "     44       \u001b[36m32.2286\u001b[0m       \u001b[32m30.3705\u001b[0m  0.0118\n",
      "     45       \u001b[36m32.2241\u001b[0m       \u001b[32m30.3675\u001b[0m  0.0122\n",
      "     46       \u001b[36m32.2198\u001b[0m       \u001b[32m30.3664\u001b[0m  0.0117\n",
      "     47       \u001b[36m32.2157\u001b[0m       \u001b[32m30.3639\u001b[0m  0.0114\n",
      "     48       \u001b[36m32.2118\u001b[0m       \u001b[32m30.3625\u001b[0m  0.0118\n",
      "     49       \u001b[36m32.2081\u001b[0m       \u001b[32m30.3598\u001b[0m  0.0118\n",
      "     50       \u001b[36m32.2046\u001b[0m       \u001b[32m30.3593\u001b[0m  0.0127\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m33.8709\u001b[0m       \u001b[32m32.0392\u001b[0m  0.0127\n",
      "      2       \u001b[36m32.6421\u001b[0m       \u001b[32m31.2164\u001b[0m  0.0128\n",
      "      3       \u001b[36m31.4615\u001b[0m       \u001b[32m30.3159\u001b[0m  0.0137\n",
      "      4       \u001b[36m30.1707\u001b[0m       \u001b[32m29.3290\u001b[0m  0.0126\n",
      "      5       \u001b[36m28.7667\u001b[0m       \u001b[32m28.2502\u001b[0m  0.0127\n",
      "      6       \u001b[36m27.2150\u001b[0m       \u001b[32m27.2683\u001b[0m  0.0120\n",
      "      7       \u001b[36m25.7362\u001b[0m       \u001b[32m26.8486\u001b[0m  0.0121\n",
      "      8       \u001b[36m24.8029\u001b[0m       27.2682  0.0122\n",
      "      9       \u001b[36m24.5988\u001b[0m       27.8012  0.0122\n",
      "     10       \u001b[36m24.5553\u001b[0m       27.6213  0.0125\n",
      "     11       \u001b[36m24.3139\u001b[0m       27.1394  0.0152\n",
      "     12       \u001b[36m24.1073\u001b[0m       \u001b[32m26.8291\u001b[0m  0.0126\n",
      "     13       \u001b[36m24.0134\u001b[0m       \u001b[32m26.7109\u001b[0m  0.0126\n",
      "     14       \u001b[36m23.9353\u001b[0m       \u001b[32m26.7099\u001b[0m  0.0128\n",
      "     15       \u001b[36m23.8388\u001b[0m       26.7960  0.0121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     16       \u001b[36m23.7510\u001b[0m       26.9226  0.0139\n",
      "     17       \u001b[36m23.6827\u001b[0m       27.0199  0.0124\n",
      "     18       \u001b[36m23.6239\u001b[0m       27.0554  0.0136\n",
      "     19       \u001b[36m23.5717\u001b[0m       27.0453  0.0158\n",
      "     20       \u001b[36m23.5268\u001b[0m       27.0201  0.0123\n",
      "     21       \u001b[36m23.4875\u001b[0m       27.0026  0.0119\n",
      "     22       \u001b[36m23.4528\u001b[0m       27.0019  0.0123\n",
      "     23       \u001b[36m23.4201\u001b[0m       27.0119  0.0122\n",
      "     24       \u001b[36m23.3894\u001b[0m       27.0207  0.0122\n",
      "     25       \u001b[36m23.3615\u001b[0m       27.0199  0.0118\n",
      "     26       \u001b[36m23.3363\u001b[0m       27.0100  0.0120\n",
      "     27       \u001b[36m23.3138\u001b[0m       26.9958  0.0123\n",
      "     28       \u001b[36m23.2936\u001b[0m       26.9816  0.0119\n",
      "     29       \u001b[36m23.2752\u001b[0m       26.9695  0.0116\n",
      "     30       \u001b[36m23.2583\u001b[0m       26.9601  0.0119\n",
      "     31       \u001b[36m23.2428\u001b[0m       26.9495  0.0121\n",
      "     32       \u001b[36m23.2285\u001b[0m       26.9386  0.0117\n",
      "     33       \u001b[36m23.2154\u001b[0m       26.9279  0.0117\n",
      "     34       \u001b[36m23.2035\u001b[0m       26.9166  0.0118\n",
      "     35       \u001b[36m23.1926\u001b[0m       26.9063  0.0118\n",
      "     36       \u001b[36m23.1824\u001b[0m       26.8977  0.0128\n",
      "     37       \u001b[36m23.1731\u001b[0m       26.8890  0.0121\n",
      "     38       \u001b[36m23.1644\u001b[0m       26.8808  0.0122\n",
      "     39       \u001b[36m23.1562\u001b[0m       26.8732  0.0118\n",
      "     40       \u001b[36m23.1486\u001b[0m       26.8673  0.0117\n",
      "     41       \u001b[36m23.1414\u001b[0m       26.8617  0.0120\n",
      "     42       \u001b[36m23.1348\u001b[0m       26.8548  0.0118\n",
      "     43       \u001b[36m23.1286\u001b[0m       26.8487  0.0114\n",
      "     44       \u001b[36m23.1228\u001b[0m       26.8443  0.0118\n",
      "     45       \u001b[36m23.1173\u001b[0m       26.8390  0.0117\n",
      "     46       \u001b[36m23.1122\u001b[0m       26.8343  0.0123\n",
      "     47       \u001b[36m23.1073\u001b[0m       26.8299  0.0122\n",
      "     48       \u001b[36m23.1027\u001b[0m       26.8261  0.0117\n",
      "     49       \u001b[36m23.0983\u001b[0m       26.8203  0.0119\n",
      "     50       \u001b[36m23.0942\u001b[0m       26.8148  0.0117\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m39.6554\u001b[0m       \u001b[32m31.0637\u001b[0m  0.0124\n",
      "      2       \u001b[36m38.0308\u001b[0m       \u001b[32m30.0205\u001b[0m  0.0121\n",
      "      3       \u001b[36m36.4735\u001b[0m       \u001b[32m28.9784\u001b[0m  0.0118\n",
      "      4       \u001b[36m34.8871\u001b[0m       \u001b[32m27.9152\u001b[0m  0.0124\n",
      "      5       \u001b[36m33.1373\u001b[0m       \u001b[32m26.9742\u001b[0m  0.0120\n",
      "      6       \u001b[36m31.4057\u001b[0m       \u001b[32m26.6313\u001b[0m  0.0118\n",
      "      7       \u001b[36m30.2348\u001b[0m       27.3335  0.0123\n",
      "      8       \u001b[36m29.9905\u001b[0m       28.4110  0.0123\n",
      "      9       30.0936       28.4936  0.0122\n",
      "     10       \u001b[36m29.8531\u001b[0m       27.8415  0.0121\n",
      "     11       \u001b[36m29.5301\u001b[0m       27.3043  0.0119\n",
      "     12       \u001b[36m29.3430\u001b[0m       27.0784  0.0120\n",
      "     13       \u001b[36m29.2257\u001b[0m       27.0925  0.0117\n",
      "     14       \u001b[36m29.1331\u001b[0m       27.2586  0.0117\n",
      "     15       \u001b[36m29.0641\u001b[0m       27.4679  0.0121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     16       \u001b[36m29.0159\u001b[0m       27.6052  0.0120\n",
      "     17       \u001b[36m28.9665\u001b[0m       27.6207  0.0118\n",
      "     18       \u001b[36m28.9097\u001b[0m       27.5519  0.0121\n",
      "     19       \u001b[36m28.8531\u001b[0m       27.4746  0.0122\n",
      "     20       \u001b[36m28.8041\u001b[0m       27.4375  0.0115\n",
      "     21       \u001b[36m28.7643\u001b[0m       27.4415  0.0115\n",
      "     22       \u001b[36m28.7323\u001b[0m       27.4620  0.0115\n",
      "     23       \u001b[36m28.7045\u001b[0m       27.4792  0.0116\n",
      "     24       \u001b[36m28.6790\u001b[0m       27.4839  0.0121\n",
      "     25       \u001b[36m28.6551\u001b[0m       27.4758  0.0122\n",
      "     26       \u001b[36m28.6326\u001b[0m       27.4643  0.0115\n",
      "     27       \u001b[36m28.6124\u001b[0m       27.4587  0.0119\n",
      "     28       \u001b[36m28.5947\u001b[0m       27.4577  0.0117\n",
      "     29       \u001b[36m28.5792\u001b[0m       27.4591  0.0122\n",
      "     30       \u001b[36m28.5652\u001b[0m       27.4591  0.0122\n",
      "     31       \u001b[36m28.5526\u001b[0m       27.4571  0.0118\n",
      "     32       \u001b[36m28.5411\u001b[0m       27.4528  0.0121\n",
      "     33       \u001b[36m28.5307\u001b[0m       27.4494  0.0124\n",
      "     34       \u001b[36m28.5213\u001b[0m       27.4464  0.0123\n",
      "     35       \u001b[36m28.5130\u001b[0m       27.4439  0.0118\n",
      "     36       \u001b[36m28.5055\u001b[0m       27.4427  0.0118\n",
      "     37       \u001b[36m28.4986\u001b[0m       27.4414  0.0124\n",
      "     38       \u001b[36m28.4923\u001b[0m       27.4404  0.0148\n",
      "     39       \u001b[36m28.4865\u001b[0m       27.4387  0.0155\n",
      "     40       \u001b[36m28.4811\u001b[0m       27.4355  0.0121\n",
      "     41       \u001b[36m28.4760\u001b[0m       27.4325  0.0141\n",
      "     42       \u001b[36m28.4713\u001b[0m       27.4310  0.0124\n",
      "     43       \u001b[36m28.4669\u001b[0m       27.4306  0.0153\n",
      "     44       \u001b[36m28.4628\u001b[0m       27.4302  0.0119\n",
      "     45       \u001b[36m28.4589\u001b[0m       27.4287  0.0131\n",
      "     46       \u001b[36m28.4551\u001b[0m       27.4264  0.0122\n",
      "     47       \u001b[36m28.4515\u001b[0m       27.4260  0.0120\n",
      "     48       \u001b[36m28.4483\u001b[0m       27.4256  0.0117\n",
      "     49       \u001b[36m28.4452\u001b[0m       27.4250  0.0120\n",
      "     50       \u001b[36m28.4422\u001b[0m       27.4224  0.0121\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m42.6522\u001b[0m       \u001b[32m44.5814\u001b[0m  0.0113\n",
      "      2       \u001b[36m42.2556\u001b[0m       \u001b[32m44.0990\u001b[0m  0.0112\n",
      "      3       \u001b[36m41.8744\u001b[0m       \u001b[32m43.6310\u001b[0m  0.0112\n",
      "      4       \u001b[36m41.5058\u001b[0m       \u001b[32m43.1754\u001b[0m  0.0112\n",
      "      5       \u001b[36m41.1474\u001b[0m       \u001b[32m42.7273\u001b[0m  0.0112\n",
      "      6       \u001b[36m40.7953\u001b[0m       \u001b[32m42.2813\u001b[0m  0.0107\n",
      "      7       \u001b[36m40.4466\u001b[0m       \u001b[32m41.8357\u001b[0m  0.0109\n",
      "      8       \u001b[36m40.1003\u001b[0m       \u001b[32m41.3900\u001b[0m  0.0108\n",
      "      9       \u001b[36m39.7559\u001b[0m       \u001b[32m40.9441\u001b[0m  0.0109\n",
      "     10       \u001b[36m39.4131\u001b[0m       \u001b[32m40.4982\u001b[0m  0.0107\n",
      "     11       \u001b[36m39.0713\u001b[0m       \u001b[32m40.0510\u001b[0m  0.0107\n",
      "     12       \u001b[36m38.7306\u001b[0m       \u001b[32m39.6031\u001b[0m  0.0111\n",
      "     13       \u001b[36m38.3908\u001b[0m       \u001b[32m39.1538\u001b[0m  0.0110\n",
      "     14       \u001b[36m38.0524\u001b[0m       \u001b[32m38.7048\u001b[0m  0.0109\n",
      "     15       \u001b[36m37.7166\u001b[0m       \u001b[32m38.2573\u001b[0m  0.0115\n",
      "     16       \u001b[36m37.3851\u001b[0m       \u001b[32m37.8138\u001b[0m  0.0108\n",
      "     17       \u001b[36m37.0595\u001b[0m       \u001b[32m37.3763\u001b[0m  0.0114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     18       \u001b[36m36.7412\u001b[0m       \u001b[32m36.9457\u001b[0m  0.0111\n",
      "     19       \u001b[36m36.4314\u001b[0m       \u001b[32m36.5241\u001b[0m  0.0110\n",
      "     20       \u001b[36m36.1314\u001b[0m       \u001b[32m36.1128\u001b[0m  0.0112\n",
      "     21       \u001b[36m35.8430\u001b[0m       \u001b[32m35.7140\u001b[0m  0.0107\n",
      "     22       \u001b[36m35.5671\u001b[0m       \u001b[32m35.3284\u001b[0m  0.0108\n",
      "     23       \u001b[36m35.3041\u001b[0m       \u001b[32m34.9569\u001b[0m  0.0116\n",
      "     24       \u001b[36m35.0548\u001b[0m       \u001b[32m34.6005\u001b[0m  0.0116\n",
      "     25       \u001b[36m34.8196\u001b[0m       \u001b[32m34.2599\u001b[0m  0.0109\n",
      "     26       \u001b[36m34.5988\u001b[0m       \u001b[32m33.9356\u001b[0m  0.0109\n",
      "     27       \u001b[36m34.3927\u001b[0m       \u001b[32m33.6280\u001b[0m  0.0108\n",
      "     28       \u001b[36m34.2012\u001b[0m       \u001b[32m33.3372\u001b[0m  0.0108\n",
      "     29       \u001b[36m34.0241\u001b[0m       \u001b[32m33.0638\u001b[0m  0.0109\n",
      "     30       \u001b[36m33.8613\u001b[0m       \u001b[32m32.8075\u001b[0m  0.0108\n",
      "     31       \u001b[36m33.7125\u001b[0m       \u001b[32m32.5683\u001b[0m  0.0108\n",
      "     32       \u001b[36m33.5772\u001b[0m       \u001b[32m32.3459\u001b[0m  0.0109\n",
      "     33       \u001b[36m33.4547\u001b[0m       \u001b[32m32.1400\u001b[0m  0.0109\n",
      "     34       \u001b[36m33.3446\u001b[0m       \u001b[32m31.9503\u001b[0m  0.0107\n",
      "     35       \u001b[36m33.2461\u001b[0m       \u001b[32m31.7760\u001b[0m  0.0110\n",
      "     36       \u001b[36m33.1585\u001b[0m       \u001b[32m31.6168\u001b[0m  0.0108\n",
      "     37       \u001b[36m33.0810\u001b[0m       \u001b[32m31.4716\u001b[0m  0.0108\n",
      "     38       \u001b[36m33.0127\u001b[0m       \u001b[32m31.3399\u001b[0m  0.0110\n",
      "     39       \u001b[36m32.9526\u001b[0m       \u001b[32m31.2207\u001b[0m  0.0111\n",
      "     40       \u001b[36m32.8999\u001b[0m       \u001b[32m31.1131\u001b[0m  0.0109\n",
      "     41       \u001b[36m32.8539\u001b[0m       \u001b[32m31.0161\u001b[0m  0.0107\n",
      "     42       \u001b[36m32.8137\u001b[0m       \u001b[32m30.9290\u001b[0m  0.0107\n",
      "     43       \u001b[36m32.7786\u001b[0m       \u001b[32m30.8507\u001b[0m  0.0110\n",
      "     44       \u001b[36m32.7480\u001b[0m       \u001b[32m30.7804\u001b[0m  0.0108\n",
      "     45       \u001b[36m32.7214\u001b[0m       \u001b[32m30.7174\u001b[0m  0.0108\n",
      "     46       \u001b[36m32.6982\u001b[0m       \u001b[32m30.6610\u001b[0m  0.0107\n",
      "     47       \u001b[36m32.6779\u001b[0m       \u001b[32m30.6104\u001b[0m  0.0109\n",
      "     48       \u001b[36m32.6601\u001b[0m       \u001b[32m30.5652\u001b[0m  0.0111\n",
      "     49       \u001b[36m32.6443\u001b[0m       \u001b[32m30.5248\u001b[0m  0.0112\n",
      "     50       \u001b[36m32.6303\u001b[0m       \u001b[32m30.4885\u001b[0m  0.0107\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m34.4053\u001b[0m       \u001b[32m32.8524\u001b[0m  0.0106\n",
      "      2       \u001b[36m33.9732\u001b[0m       \u001b[32m32.5278\u001b[0m  0.0108\n",
      "      3       \u001b[36m33.5571\u001b[0m       \u001b[32m32.2145\u001b[0m  0.0108\n",
      "      4       \u001b[36m33.1532\u001b[0m       \u001b[32m31.9102\u001b[0m  0.0105\n",
      "      5       \u001b[36m32.7586\u001b[0m       \u001b[32m31.6133\u001b[0m  0.0109\n",
      "      6       \u001b[36m32.3704\u001b[0m       \u001b[32m31.3214\u001b[0m  0.0108\n",
      "      7       \u001b[36m31.9873\u001b[0m       \u001b[32m31.0341\u001b[0m  0.0106\n",
      "      8       \u001b[36m31.6086\u001b[0m       \u001b[32m30.7505\u001b[0m  0.0113\n",
      "      9       \u001b[36m31.2339\u001b[0m       \u001b[32m30.4713\u001b[0m  0.0113\n",
      "     10       \u001b[36m30.8634\u001b[0m       \u001b[32m30.1966\u001b[0m  0.0112\n",
      "     11       \u001b[36m30.4964\u001b[0m       \u001b[32m29.9257\u001b[0m  0.0107\n",
      "     12       \u001b[36m30.1321\u001b[0m       \u001b[32m29.6581\u001b[0m  0.0105\n",
      "     13       \u001b[36m29.7706\u001b[0m       \u001b[32m29.3951\u001b[0m  0.0111\n",
      "     14       \u001b[36m29.4136\u001b[0m       \u001b[32m29.1373\u001b[0m  0.0109\n",
      "     15       \u001b[36m29.0619\u001b[0m       \u001b[32m28.8861\u001b[0m  0.0107\n",
      "     16       \u001b[36m28.7163\u001b[0m       \u001b[32m28.6426\u001b[0m  0.0109\n",
      "     17       \u001b[36m28.3790\u001b[0m       \u001b[32m28.4082\u001b[0m  0.0106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     18       \u001b[36m28.0509\u001b[0m       \u001b[32m28.1838\u001b[0m  0.0114\n",
      "     19       \u001b[36m27.7331\u001b[0m       \u001b[32m27.9703\u001b[0m  0.0111\n",
      "     20       \u001b[36m27.4264\u001b[0m       \u001b[32m27.7682\u001b[0m  0.0113\n",
      "     21       \u001b[36m27.1315\u001b[0m       \u001b[32m27.5785\u001b[0m  0.0171\n",
      "     22       \u001b[36m26.8491\u001b[0m       \u001b[32m27.4016\u001b[0m  0.0135\n",
      "     23       \u001b[36m26.5792\u001b[0m       \u001b[32m27.2380\u001b[0m  0.0119\n",
      "     24       \u001b[36m26.3222\u001b[0m       \u001b[32m27.0883\u001b[0m  0.0115\n",
      "     25       \u001b[36m26.0785\u001b[0m       \u001b[32m26.9524\u001b[0m  0.0122\n",
      "     26       \u001b[36m25.8483\u001b[0m       \u001b[32m26.8305\u001b[0m  0.0136\n",
      "     27       \u001b[36m25.6318\u001b[0m       \u001b[32m26.7228\u001b[0m  0.0120\n",
      "     28       \u001b[36m25.4295\u001b[0m       \u001b[32m26.6289\u001b[0m  0.0124\n",
      "     29       \u001b[36m25.2413\u001b[0m       \u001b[32m26.5485\u001b[0m  0.0113\n",
      "     30       \u001b[36m25.0669\u001b[0m       \u001b[32m26.4809\u001b[0m  0.0112\n",
      "     31       \u001b[36m24.9060\u001b[0m       \u001b[32m26.4254\u001b[0m  0.0112\n",
      "     32       \u001b[36m24.7583\u001b[0m       \u001b[32m26.3811\u001b[0m  0.0114\n",
      "     33       \u001b[36m24.6232\u001b[0m       \u001b[32m26.3471\u001b[0m  0.0113\n",
      "     34       \u001b[36m24.5001\u001b[0m       \u001b[32m26.3222\u001b[0m  0.0112\n",
      "     35       \u001b[36m24.3886\u001b[0m       \u001b[32m26.3057\u001b[0m  0.0112\n",
      "     36       \u001b[36m24.2880\u001b[0m       \u001b[32m26.2964\u001b[0m  0.0110\n",
      "     37       \u001b[36m24.1978\u001b[0m       \u001b[32m26.2934\u001b[0m  0.0112\n",
      "     38       \u001b[36m24.1170\u001b[0m       26.2955  0.0112\n",
      "     39       \u001b[36m24.0451\u001b[0m       26.3019  0.0112\n",
      "     40       \u001b[36m23.9813\u001b[0m       26.3116  0.0109\n",
      "     41       \u001b[36m23.9246\u001b[0m       26.3238  0.0105\n",
      "     42       \u001b[36m23.8743\u001b[0m       26.3377  0.0104\n",
      "     43       \u001b[36m23.8298\u001b[0m       26.3528  0.0107\n",
      "     44       \u001b[36m23.7905\u001b[0m       26.3684  0.0106\n",
      "     45       \u001b[36m23.7556\u001b[0m       26.3844  0.0108\n",
      "     46       \u001b[36m23.7246\u001b[0m       26.4002  0.0110\n",
      "     47       \u001b[36m23.6970\u001b[0m       26.4156  0.0113\n",
      "     48       \u001b[36m23.6723\u001b[0m       26.4303  0.0117\n",
      "     49       \u001b[36m23.6501\u001b[0m       26.4442  0.0119\n",
      "     50       \u001b[36m23.6302\u001b[0m       26.4573  0.0109\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m41.5594\u001b[0m       \u001b[32m32.9483\u001b[0m  0.0109\n",
      "      2       \u001b[36m41.0408\u001b[0m       \u001b[32m32.5947\u001b[0m  0.0107\n",
      "      3       \u001b[36m40.5373\u001b[0m       \u001b[32m32.2503\u001b[0m  0.0111\n",
      "      4       \u001b[36m40.0426\u001b[0m       \u001b[32m31.9109\u001b[0m  0.0113\n",
      "      5       \u001b[36m39.5534\u001b[0m       \u001b[32m31.5737\u001b[0m  0.0106\n",
      "      6       \u001b[36m39.0650\u001b[0m       \u001b[32m31.2380\u001b[0m  0.0106\n",
      "      7       \u001b[36m38.5762\u001b[0m       \u001b[32m30.9037\u001b[0m  0.0110\n",
      "      8       \u001b[36m38.0864\u001b[0m       \u001b[32m30.5710\u001b[0m  0.0113\n",
      "      9       \u001b[36m37.5955\u001b[0m       \u001b[32m30.2399\u001b[0m  0.0111\n",
      "     10       \u001b[36m37.1044\u001b[0m       \u001b[32m29.9127\u001b[0m  0.0109\n",
      "     11       \u001b[36m36.6170\u001b[0m       \u001b[32m29.5909\u001b[0m  0.0109\n",
      "     12       \u001b[36m36.1353\u001b[0m       \u001b[32m29.2756\u001b[0m  0.0108\n",
      "     13       \u001b[36m35.6607\u001b[0m       \u001b[32m28.9682\u001b[0m  0.0111\n",
      "     14       \u001b[36m35.1943\u001b[0m       \u001b[32m28.6702\u001b[0m  0.0110\n",
      "     15       \u001b[36m34.7371\u001b[0m       \u001b[32m28.3834\u001b[0m  0.0107\n",
      "     16       \u001b[36m34.2907\u001b[0m       \u001b[32m28.1092\u001b[0m  0.0109\n",
      "     17       \u001b[36m33.8560\u001b[0m       \u001b[32m27.8487\u001b[0m  0.0108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     18       \u001b[36m33.4342\u001b[0m       \u001b[32m27.6036\u001b[0m  0.0116\n",
      "     19       \u001b[36m33.0266\u001b[0m       \u001b[32m27.3755\u001b[0m  0.0110\n",
      "     20       \u001b[36m32.6341\u001b[0m       \u001b[32m27.1660\u001b[0m  0.0111\n",
      "     21       \u001b[36m32.2581\u001b[0m       \u001b[32m26.9770\u001b[0m  0.0111\n",
      "     22       \u001b[36m31.9009\u001b[0m       \u001b[32m26.8095\u001b[0m  0.0107\n",
      "     23       \u001b[36m31.5632\u001b[0m       \u001b[32m26.6646\u001b[0m  0.0110\n",
      "     24       \u001b[36m31.2462\u001b[0m       \u001b[32m26.5428\u001b[0m  0.0110\n",
      "     25       \u001b[36m30.9517\u001b[0m       \u001b[32m26.4443\u001b[0m  0.0108\n",
      "     26       \u001b[36m30.6813\u001b[0m       \u001b[32m26.3694\u001b[0m  0.0108\n",
      "     27       \u001b[36m30.4357\u001b[0m       \u001b[32m26.3168\u001b[0m  0.0110\n",
      "     28       \u001b[36m30.2147\u001b[0m       \u001b[32m26.2855\u001b[0m  0.0111\n",
      "     29       \u001b[36m30.0184\u001b[0m       \u001b[32m26.2734\u001b[0m  0.0110\n",
      "     30       \u001b[36m29.8461\u001b[0m       26.2783  0.0108\n",
      "     31       \u001b[36m29.6964\u001b[0m       26.2975  0.0109\n",
      "     32       \u001b[36m29.5674\u001b[0m       26.3286  0.0109\n",
      "     33       \u001b[36m29.4576\u001b[0m       26.3687  0.0111\n",
      "     34       \u001b[36m29.3651\u001b[0m       26.4154  0.0110\n",
      "     35       \u001b[36m29.2876\u001b[0m       26.4664  0.0110\n",
      "     36       \u001b[36m29.2229\u001b[0m       26.5195  0.0107\n",
      "     37       \u001b[36m29.1691\u001b[0m       26.5729  0.0106\n",
      "     38       \u001b[36m29.1246\u001b[0m       26.6253  0.0110\n",
      "     39       \u001b[36m29.0877\u001b[0m       26.6757  0.0110\n",
      "     40       \u001b[36m29.0570\u001b[0m       26.7232  0.0114\n",
      "     41       \u001b[36m29.0311\u001b[0m       26.7676  0.0107\n",
      "     42       \u001b[36m29.0091\u001b[0m       26.8083  0.0108\n",
      "     43       \u001b[36m28.9902\u001b[0m       26.8455  0.0111\n",
      "     44       \u001b[36m28.9738\u001b[0m       26.8791  0.0110\n",
      "     45       \u001b[36m28.9593\u001b[0m       26.9093  0.0109\n",
      "     46       \u001b[36m28.9463\u001b[0m       26.9362  0.0109\n",
      "     47       \u001b[36m28.9344\u001b[0m       26.9600  0.0108\n",
      "     48       \u001b[36m28.9235\u001b[0m       26.9810  0.0112\n",
      "     49       \u001b[36m28.9132\u001b[0m       26.9993  0.0109\n",
      "     50       \u001b[36m28.9035\u001b[0m       27.0155  0.0116\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m42.1325\u001b[0m       \u001b[32m42.4837\u001b[0m  0.0118\n",
      "      2       \u001b[36m40.3117\u001b[0m       \u001b[32m40.2003\u001b[0m  0.0117\n",
      "      3       \u001b[36m38.5667\u001b[0m       \u001b[32m37.6621\u001b[0m  0.0120\n",
      "      4       \u001b[36m36.6195\u001b[0m       \u001b[32m34.6870\u001b[0m  0.0199\n",
      "      5       \u001b[36m34.7392\u001b[0m       \u001b[32m32.1470\u001b[0m  0.0229\n",
      "      6       \u001b[36m33.8363\u001b[0m       \u001b[32m31.0895\u001b[0m  0.0138\n",
      "      7       33.8583       \u001b[32m30.8612\u001b[0m  0.0169\n",
      "      8       \u001b[36m33.6050\u001b[0m       30.8882  0.0168\n",
      "      9       \u001b[36m33.2609\u001b[0m       31.1397  0.0179\n",
      "     10       \u001b[36m33.1392\u001b[0m       31.2114  0.0119\n",
      "     11       \u001b[36m33.0450\u001b[0m       31.0167  0.0118\n",
      "     12       \u001b[36m32.9244\u001b[0m       \u001b[32m30.7488\u001b[0m  0.0120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     13       \u001b[36m32.8301\u001b[0m       \u001b[32m30.5502\u001b[0m  0.0127\n",
      "     14       \u001b[36m32.7629\u001b[0m       \u001b[32m30.4616\u001b[0m  0.0123\n",
      "     15       \u001b[36m32.7089\u001b[0m       \u001b[32m30.4449\u001b[0m  0.0123\n",
      "     16       \u001b[36m32.6602\u001b[0m       \u001b[32m30.4323\u001b[0m  0.0123\n",
      "     17       \u001b[36m32.6133\u001b[0m       \u001b[32m30.4175\u001b[0m  0.0120\n",
      "     18       \u001b[36m32.5734\u001b[0m       \u001b[32m30.3970\u001b[0m  0.0121\n",
      "     19       \u001b[36m32.5355\u001b[0m       \u001b[32m30.3687\u001b[0m  0.0121\n",
      "     20       \u001b[36m32.5012\u001b[0m       \u001b[32m30.3308\u001b[0m  0.0118\n",
      "     21       \u001b[36m32.4709\u001b[0m       \u001b[32m30.2998\u001b[0m  0.0116\n",
      "     22       \u001b[36m32.4444\u001b[0m       \u001b[32m30.2919\u001b[0m  0.0114\n",
      "     23       \u001b[36m32.4208\u001b[0m       30.2935  0.0117\n",
      "     24       \u001b[36m32.3993\u001b[0m       \u001b[32m30.2862\u001b[0m  0.0116\n",
      "     25       \u001b[36m32.3798\u001b[0m       \u001b[32m30.2789\u001b[0m  0.0118\n",
      "     26       \u001b[36m32.3622\u001b[0m       \u001b[32m30.2735\u001b[0m  0.0120\n",
      "     27       \u001b[36m32.3462\u001b[0m       \u001b[32m30.2647\u001b[0m  0.0121\n",
      "     28       \u001b[36m32.3319\u001b[0m       \u001b[32m30.2597\u001b[0m  0.0125\n",
      "     29       \u001b[36m32.3191\u001b[0m       \u001b[32m30.2595\u001b[0m  0.0119\n",
      "     30       \u001b[36m32.3077\u001b[0m       \u001b[32m30.2569\u001b[0m  0.0119\n",
      "     31       \u001b[36m32.2972\u001b[0m       \u001b[32m30.2554\u001b[0m  0.0119\n",
      "     32       \u001b[36m32.2878\u001b[0m       \u001b[32m30.2529\u001b[0m  0.0118\n",
      "     33       \u001b[36m32.2791\u001b[0m       \u001b[32m30.2495\u001b[0m  0.0114\n",
      "     34       \u001b[36m32.2711\u001b[0m       \u001b[32m30.2484\u001b[0m  0.0119\n",
      "     35       \u001b[36m32.2639\u001b[0m       \u001b[32m30.2454\u001b[0m  0.0119\n",
      "     36       \u001b[36m32.2570\u001b[0m       \u001b[32m30.2433\u001b[0m  0.0119\n",
      "     37       \u001b[36m32.2507\u001b[0m       \u001b[32m30.2395\u001b[0m  0.0116\n",
      "     38       \u001b[36m32.2447\u001b[0m       \u001b[32m30.2377\u001b[0m  0.0118\n",
      "     39       \u001b[36m32.2392\u001b[0m       \u001b[32m30.2360\u001b[0m  0.0115\n",
      "     40       \u001b[36m32.2340\u001b[0m       \u001b[32m30.2354\u001b[0m  0.0117\n",
      "     41       \u001b[36m32.2291\u001b[0m       \u001b[32m30.2343\u001b[0m  0.0119\n",
      "     42       \u001b[36m32.2244\u001b[0m       \u001b[32m30.2330\u001b[0m  0.0114\n",
      "     43       \u001b[36m32.2199\u001b[0m       \u001b[32m30.2319\u001b[0m  0.0115\n",
      "     44       \u001b[36m32.2157\u001b[0m       \u001b[32m30.2309\u001b[0m  0.0118\n",
      "     45       \u001b[36m32.2117\u001b[0m       \u001b[32m30.2301\u001b[0m  0.0115\n",
      "     46       \u001b[36m32.2078\u001b[0m       30.2305  0.0124\n",
      "     47       \u001b[36m32.2041\u001b[0m       \u001b[32m30.2289\u001b[0m  0.0118\n",
      "     48       \u001b[36m32.2005\u001b[0m       30.2292  0.0117\n",
      "     49       \u001b[36m32.1970\u001b[0m       \u001b[32m30.2268\u001b[0m  0.0121\n",
      "     50       \u001b[36m32.1937\u001b[0m       \u001b[32m30.2266\u001b[0m  0.0115\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m33.9013\u001b[0m       \u001b[32m31.9432\u001b[0m  0.0118\n",
      "      2       \u001b[36m32.2584\u001b[0m       \u001b[32m30.7444\u001b[0m  0.0116\n",
      "      3       \u001b[36m30.6188\u001b[0m       \u001b[32m29.4786\u001b[0m  0.0114\n",
      "      4       \u001b[36m28.8801\u001b[0m       \u001b[32m28.0536\u001b[0m  0.0122\n",
      "      5       \u001b[36m26.8064\u001b[0m       \u001b[32m26.9144\u001b[0m  0.0118\n",
      "      6       \u001b[36m25.0231\u001b[0m       27.2207  0.0117\n",
      "      7       \u001b[36m24.5573\u001b[0m       28.3349  0.0116\n",
      "      8       24.6280       27.9667  0.0120\n",
      "      9       \u001b[36m24.2361\u001b[0m       27.1560  0.0118\n",
      "     10       \u001b[36m23.9970\u001b[0m       \u001b[32m26.8209\u001b[0m  0.0120\n",
      "     11       \u001b[36m23.9488\u001b[0m       \u001b[32m26.7981\u001b[0m  0.0116\n",
      "     12       \u001b[36m23.8515\u001b[0m       26.9517  0.0118\n",
      "     13       \u001b[36m23.7262\u001b[0m       27.2169  0.0116\n",
      "     14       \u001b[36m23.6505\u001b[0m       27.4081  0.0116\n",
      "     15       \u001b[36m23.5959\u001b[0m       27.4036  0.0120\n",
      "     16       \u001b[36m23.5415\u001b[0m       27.3017  0.0116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m23.5001\u001b[0m       27.2245  0.0119\n",
      "     18       \u001b[36m23.4634\u001b[0m       27.2064  0.0113\n",
      "     19       \u001b[36m23.4277\u001b[0m       27.2267  0.0112\n",
      "     20       \u001b[36m23.3946\u001b[0m       27.2462  0.0116\n",
      "     21       \u001b[36m23.3642\u001b[0m       27.2472  0.0118\n",
      "     22       \u001b[36m23.3374\u001b[0m       27.2344  0.0118\n",
      "     23       \u001b[36m23.3135\u001b[0m       27.2159  0.0113\n",
      "     24       \u001b[36m23.2923\u001b[0m       27.1982  0.0115\n",
      "     25       \u001b[36m23.2732\u001b[0m       27.1863  0.0118\n",
      "     26       \u001b[36m23.2554\u001b[0m       27.1789  0.0120\n",
      "     27       \u001b[36m23.2386\u001b[0m       27.1714  0.0121\n",
      "     28       \u001b[36m23.2234\u001b[0m       27.1596  0.0114\n",
      "     29       \u001b[36m23.2097\u001b[0m       27.1433  0.0115\n",
      "     30       \u001b[36m23.1974\u001b[0m       27.1270  0.0118\n",
      "     31       \u001b[36m23.1861\u001b[0m       27.1101  0.0182\n",
      "     32       \u001b[36m23.1757\u001b[0m       27.0948  0.0147\n",
      "     33       \u001b[36m23.1660\u001b[0m       27.0806  0.0128\n",
      "     34       \u001b[36m23.1570\u001b[0m       27.0684  0.0124\n",
      "     35       \u001b[36m23.1487\u001b[0m       27.0537  0.0126\n",
      "     36       \u001b[36m23.1410\u001b[0m       27.0387  0.0154\n",
      "     37       \u001b[36m23.1340\u001b[0m       27.0260  0.0131\n",
      "     38       \u001b[36m23.1275\u001b[0m       27.0136  0.0124\n",
      "     39       \u001b[36m23.1214\u001b[0m       27.0014  0.0123\n",
      "     40       \u001b[36m23.1157\u001b[0m       26.9892  0.0120\n",
      "     41       \u001b[36m23.1104\u001b[0m       26.9775  0.0120\n",
      "     42       \u001b[36m23.1055\u001b[0m       26.9675  0.0118\n",
      "     43       \u001b[36m23.1006\u001b[0m       26.9571  0.0122\n",
      "     44       \u001b[36m23.0960\u001b[0m       26.9471  0.0120\n",
      "     45       \u001b[36m23.0916\u001b[0m       26.9347  0.0120\n",
      "     46       \u001b[36m23.0875\u001b[0m       26.9251  0.0121\n",
      "     47       \u001b[36m23.0836\u001b[0m       26.9191  0.0119\n",
      "     48       \u001b[36m23.0799\u001b[0m       26.9132  0.0119\n",
      "     49       \u001b[36m23.0764\u001b[0m       26.9044  0.0113\n",
      "     50       \u001b[36m23.0731\u001b[0m       26.8965  0.0121\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m39.3698\u001b[0m       \u001b[32m30.8451\u001b[0m  0.0117\n",
      "      2       \u001b[36m37.5395\u001b[0m       \u001b[32m29.6209\u001b[0m  0.0124\n",
      "      3       \u001b[36m35.6900\u001b[0m       \u001b[32m28.2114\u001b[0m  0.0125\n",
      "      4       \u001b[36m33.4307\u001b[0m       \u001b[32m26.8836\u001b[0m  0.0122\n",
      "      5       \u001b[36m31.1399\u001b[0m       \u001b[32m26.7646\u001b[0m  0.0122\n",
      "      6       \u001b[36m30.0244\u001b[0m       28.4263  0.0121\n",
      "      7       30.2762       28.9336  0.0126\n",
      "      8       \u001b[36m30.0160\u001b[0m       27.7711  0.0125\n",
      "      9       \u001b[36m29.5007\u001b[0m       26.9695  0.0123\n",
      "     10       \u001b[36m29.3074\u001b[0m       \u001b[32m26.7526\u001b[0m  0.0127\n",
      "     11       \u001b[36m29.1749\u001b[0m       26.8786  0.0123\n",
      "     12       \u001b[36m29.0488\u001b[0m       27.2208  0.0120\n",
      "     13       \u001b[36m28.9907\u001b[0m       27.5206  0.0124\n",
      "     14       \u001b[36m28.9558\u001b[0m       27.5708  0.0123\n",
      "     15       \u001b[36m28.8941\u001b[0m       27.4599  0.0121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     16       \u001b[36m28.8217\u001b[0m       27.3542  0.0120\n",
      "     17       \u001b[36m28.7683\u001b[0m       27.3135  0.0118\n",
      "     18       \u001b[36m28.7290\u001b[0m       27.3283  0.0120\n",
      "     19       \u001b[36m28.6988\u001b[0m       27.3673  0.0119\n",
      "     20       \u001b[36m28.6734\u001b[0m       27.3878  0.0120\n",
      "     21       \u001b[36m28.6482\u001b[0m       27.3639  0.0118\n",
      "     22       \u001b[36m28.6218\u001b[0m       27.3277  0.0116\n",
      "     23       \u001b[36m28.5980\u001b[0m       27.3182  0.0123\n",
      "     24       \u001b[36m28.5793\u001b[0m       27.3284  0.0122\n",
      "     25       \u001b[36m28.5643\u001b[0m       27.3373  0.0126\n",
      "     26       \u001b[36m28.5509\u001b[0m       27.3370  0.0120\n",
      "     27       \u001b[36m28.5383\u001b[0m       27.3282  0.0121\n",
      "     28       \u001b[36m28.5266\u001b[0m       27.3195  0.0127\n",
      "     29       \u001b[36m28.5163\u001b[0m       27.3174  0.0126\n",
      "     30       \u001b[36m28.5075\u001b[0m       27.3187  0.0120\n",
      "     31       \u001b[36m28.4998\u001b[0m       27.3188  0.0118\n",
      "     32       \u001b[36m28.4927\u001b[0m       27.3169  0.0119\n",
      "     33       \u001b[36m28.4862\u001b[0m       27.3139  0.0122\n",
      "     34       \u001b[36m28.4800\u001b[0m       27.3101  0.0119\n",
      "     35       \u001b[36m28.4743\u001b[0m       27.3070  0.0118\n",
      "     36       \u001b[36m28.4692\u001b[0m       27.3052  0.0119\n",
      "     37       \u001b[36m28.4645\u001b[0m       27.3038  0.0117\n",
      "     38       \u001b[36m28.4601\u001b[0m       27.3029  0.0125\n",
      "     39       \u001b[36m28.4559\u001b[0m       27.3013  0.0118\n",
      "     40       \u001b[36m28.4520\u001b[0m       27.2995  0.0118\n",
      "     41       \u001b[36m28.4482\u001b[0m       27.2981  0.0119\n",
      "     42       \u001b[36m28.4447\u001b[0m       27.2974  0.0117\n",
      "     43       \u001b[36m28.4414\u001b[0m       27.2976  0.0124\n",
      "     44       \u001b[36m28.4384\u001b[0m       27.2973  0.0124\n",
      "     45       \u001b[36m28.4355\u001b[0m       27.2957  0.0124\n",
      "     46       \u001b[36m28.4327\u001b[0m       27.2932  0.0123\n",
      "     47       \u001b[36m28.4301\u001b[0m       27.2920  0.0122\n",
      "     48       \u001b[36m28.4276\u001b[0m       27.2904  0.0126\n",
      "     49       \u001b[36m28.4252\u001b[0m       27.2891  0.0123\n",
      "     50       \u001b[36m28.4230\u001b[0m       27.2878  0.0124\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m44.0035\u001b[0m       \u001b[32m46.0991\u001b[0m  0.0112\n",
      "      2       \u001b[36m43.5289\u001b[0m       \u001b[32m45.5154\u001b[0m  0.0114\n",
      "      3       \u001b[36m43.0742\u001b[0m       \u001b[32m44.9521\u001b[0m  0.0114\n",
      "      4       \u001b[36m42.6347\u001b[0m       \u001b[32m44.4027\u001b[0m  0.0116\n",
      "      5       \u001b[36m42.2050\u001b[0m       \u001b[32m43.8623\u001b[0m  0.0115\n",
      "      6       \u001b[36m41.7827\u001b[0m       \u001b[32m43.3291\u001b[0m  0.0113\n",
      "      7       \u001b[36m41.3651\u001b[0m       \u001b[32m42.7987\u001b[0m  0.0129\n",
      "      8       \u001b[36m40.9503\u001b[0m       \u001b[32m42.2697\u001b[0m  0.0155\n",
      "      9       \u001b[36m40.5374\u001b[0m       \u001b[32m41.7422\u001b[0m  0.0191\n",
      "     10       \u001b[36m40.1262\u001b[0m       \u001b[32m41.2156\u001b[0m  0.0194\n",
      "     11       \u001b[36m39.7164\u001b[0m       \u001b[32m40.6888\u001b[0m  0.0129\n",
      "     12       \u001b[36m39.3082\u001b[0m       \u001b[32m40.1628\u001b[0m  0.0113\n",
      "     13       \u001b[36m38.9022\u001b[0m       \u001b[32m39.6396\u001b[0m  0.0138\n",
      "     14       \u001b[36m38.4997\u001b[0m       \u001b[32m39.1202\u001b[0m  0.0122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     15       \u001b[36m38.1018\u001b[0m       \u001b[32m38.6063\u001b[0m  0.0113\n",
      "     16       \u001b[36m37.7102\u001b[0m       \u001b[32m38.0991\u001b[0m  0.0122\n",
      "     17       \u001b[36m37.3268\u001b[0m       \u001b[32m37.6013\u001b[0m  0.0112\n",
      "     18       \u001b[36m36.9536\u001b[0m       \u001b[32m37.1146\u001b[0m  0.0112\n",
      "     19       \u001b[36m36.5925\u001b[0m       \u001b[32m36.6407\u001b[0m  0.0110\n",
      "     20       \u001b[36m36.2450\u001b[0m       \u001b[32m36.1820\u001b[0m  0.0111\n",
      "     21       \u001b[36m35.9129\u001b[0m       \u001b[32m35.7395\u001b[0m  0.0114\n",
      "     22       \u001b[36m35.5974\u001b[0m       \u001b[32m35.3141\u001b[0m  0.0114\n",
      "     23       \u001b[36m35.2993\u001b[0m       \u001b[32m34.9067\u001b[0m  0.0113\n",
      "     24       \u001b[36m35.0196\u001b[0m       \u001b[32m34.5185\u001b[0m  0.0110\n",
      "     25       \u001b[36m34.7589\u001b[0m       \u001b[32m34.1500\u001b[0m  0.0112\n",
      "     26       \u001b[36m34.5175\u001b[0m       \u001b[32m33.8021\u001b[0m  0.0111\n",
      "     27       \u001b[36m34.2958\u001b[0m       \u001b[32m33.4754\u001b[0m  0.0108\n",
      "     28       \u001b[36m34.0938\u001b[0m       \u001b[32m33.1707\u001b[0m  0.0108\n",
      "     29       \u001b[36m33.9112\u001b[0m       \u001b[32m32.8883\u001b[0m  0.0110\n",
      "     30       \u001b[36m33.7476\u001b[0m       \u001b[32m32.6278\u001b[0m  0.0109\n",
      "     31       \u001b[36m33.6019\u001b[0m       \u001b[32m32.3887\u001b[0m  0.0113\n",
      "     32       \u001b[36m33.4731\u001b[0m       \u001b[32m32.1707\u001b[0m  0.0109\n",
      "     33       \u001b[36m33.3602\u001b[0m       \u001b[32m31.9731\u001b[0m  0.0108\n",
      "     34       \u001b[36m33.2618\u001b[0m       \u001b[32m31.7947\u001b[0m  0.0107\n",
      "     35       \u001b[36m33.1766\u001b[0m       \u001b[32m31.6343\u001b[0m  0.0107\n",
      "     36       \u001b[36m33.1031\u001b[0m       \u001b[32m31.4905\u001b[0m  0.0107\n",
      "     37       \u001b[36m33.0399\u001b[0m       \u001b[32m31.3620\u001b[0m  0.0109\n",
      "     38       \u001b[36m32.9857\u001b[0m       \u001b[32m31.2479\u001b[0m  0.0109\n",
      "     39       \u001b[36m32.9393\u001b[0m       \u001b[32m31.1463\u001b[0m  0.0110\n",
      "     40       \u001b[36m32.8996\u001b[0m       \u001b[32m31.0563\u001b[0m  0.0108\n",
      "     41       \u001b[36m32.8657\u001b[0m       \u001b[32m30.9765\u001b[0m  0.0109\n",
      "     42       \u001b[36m32.8365\u001b[0m       \u001b[32m30.9057\u001b[0m  0.0109\n",
      "     43       \u001b[36m32.8112\u001b[0m       \u001b[32m30.8430\u001b[0m  0.0110\n",
      "     44       \u001b[36m32.7893\u001b[0m       \u001b[32m30.7875\u001b[0m  0.0108\n",
      "     45       \u001b[36m32.7702\u001b[0m       \u001b[32m30.7383\u001b[0m  0.0108\n",
      "     46       \u001b[36m32.7534\u001b[0m       \u001b[32m30.6945\u001b[0m  0.0110\n",
      "     47       \u001b[36m32.7385\u001b[0m       \u001b[32m30.6554\u001b[0m  0.0109\n",
      "     48       \u001b[36m32.7251\u001b[0m       \u001b[32m30.6204\u001b[0m  0.0106\n",
      "     49       \u001b[36m32.7130\u001b[0m       \u001b[32m30.5891\u001b[0m  0.0106\n",
      "     50       \u001b[36m32.7019\u001b[0m       \u001b[32m30.5611\u001b[0m  0.0107\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m33.7305\u001b[0m       \u001b[32m32.3521\u001b[0m  0.0109\n",
      "      2       \u001b[36m33.2941\u001b[0m       \u001b[32m32.0255\u001b[0m  0.0107\n",
      "      3       \u001b[36m32.8712\u001b[0m       \u001b[32m31.7084\u001b[0m  0.0105\n",
      "      4       \u001b[36m32.4582\u001b[0m       \u001b[32m31.3982\u001b[0m  0.0106\n",
      "      5       \u001b[36m32.0517\u001b[0m       \u001b[32m31.0937\u001b[0m  0.0111\n",
      "      6       \u001b[36m31.6498\u001b[0m       \u001b[32m30.7936\u001b[0m  0.0107\n",
      "      7       \u001b[36m31.2506\u001b[0m       \u001b[32m30.4967\u001b[0m  0.0107\n",
      "      8       \u001b[36m30.8534\u001b[0m       \u001b[32m30.2026\u001b[0m  0.0109\n",
      "      9       \u001b[36m30.4578\u001b[0m       \u001b[32m29.9116\u001b[0m  0.0107\n",
      "     10       \u001b[36m30.0634\u001b[0m       \u001b[32m29.6238\u001b[0m  0.0108\n",
      "     11       \u001b[36m29.6707\u001b[0m       \u001b[32m29.3398\u001b[0m  0.0105\n",
      "     12       \u001b[36m29.2808\u001b[0m       \u001b[32m29.0606\u001b[0m  0.0108\n",
      "     13       \u001b[36m28.8946\u001b[0m       \u001b[32m28.7878\u001b[0m  0.0108\n",
      "     14       \u001b[36m28.5140\u001b[0m       \u001b[32m28.5228\u001b[0m  0.0108\n",
      "     15       \u001b[36m28.1412\u001b[0m       \u001b[32m28.2672\u001b[0m  0.0105\n",
      "     16       \u001b[36m27.7778\u001b[0m       \u001b[32m28.0223\u001b[0m  0.0106\n",
      "     17       \u001b[36m27.4255\u001b[0m       \u001b[32m27.7903\u001b[0m  0.0108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     18       \u001b[36m27.0865\u001b[0m       \u001b[32m27.5726\u001b[0m  0.0114\n",
      "     19       \u001b[36m26.7629\u001b[0m       \u001b[32m27.3707\u001b[0m  0.0106\n",
      "     20       \u001b[36m26.4562\u001b[0m       \u001b[32m27.1857\u001b[0m  0.0108\n",
      "     21       \u001b[36m26.1678\u001b[0m       \u001b[32m27.0185\u001b[0m  0.0105\n",
      "     22       \u001b[36m25.8986\u001b[0m       \u001b[32m26.8696\u001b[0m  0.0108\n",
      "     23       \u001b[36m25.6492\u001b[0m       \u001b[32m26.7390\u001b[0m  0.0109\n",
      "     24       \u001b[36m25.4199\u001b[0m       \u001b[32m26.6264\u001b[0m  0.0111\n",
      "     25       \u001b[36m25.2107\u001b[0m       \u001b[32m26.5311\u001b[0m  0.0107\n",
      "     26       \u001b[36m25.0209\u001b[0m       \u001b[32m26.4522\u001b[0m  0.0105\n",
      "     27       \u001b[36m24.8499\u001b[0m       \u001b[32m26.3885\u001b[0m  0.0105\n",
      "     28       \u001b[36m24.6965\u001b[0m       \u001b[32m26.3387\u001b[0m  0.0109\n",
      "     29       \u001b[36m24.5596\u001b[0m       \u001b[32m26.3014\u001b[0m  0.0106\n",
      "     30       \u001b[36m24.4382\u001b[0m       \u001b[32m26.2749\u001b[0m  0.0109\n",
      "     31       \u001b[36m24.3309\u001b[0m       \u001b[32m26.2578\u001b[0m  0.0109\n",
      "     32       \u001b[36m24.2363\u001b[0m       \u001b[32m26.2485\u001b[0m  0.0106\n",
      "     33       \u001b[36m24.1531\u001b[0m       \u001b[32m26.2459\u001b[0m  0.0110\n",
      "     34       \u001b[36m24.0803\u001b[0m       26.2484  0.0109\n",
      "     35       \u001b[36m24.0164\u001b[0m       26.2551  0.0111\n",
      "     36       \u001b[36m23.9605\u001b[0m       26.2648  0.0115\n",
      "     37       \u001b[36m23.9114\u001b[0m       26.2768  0.0107\n",
      "     38       \u001b[36m23.8687\u001b[0m       26.2903  0.0104\n",
      "     39       \u001b[36m23.8310\u001b[0m       26.3046  0.0111\n",
      "     40       \u001b[36m23.7978\u001b[0m       26.3190  0.0113\n",
      "     41       \u001b[36m23.7682\u001b[0m       26.3334  0.0140\n",
      "     42       \u001b[36m23.7417\u001b[0m       26.3473  0.0139\n",
      "     43       \u001b[36m23.7180\u001b[0m       26.3607  0.0113\n",
      "     44       \u001b[36m23.6967\u001b[0m       26.3732  0.0121\n",
      "     45       \u001b[36m23.6774\u001b[0m       26.3850  0.0120\n",
      "     46       \u001b[36m23.6597\u001b[0m       26.3958  0.0118\n",
      "     47       \u001b[36m23.6435\u001b[0m       26.4058  0.0118\n",
      "     48       \u001b[36m23.6285\u001b[0m       26.4149  0.0112\n",
      "     49       \u001b[36m23.6146\u001b[0m       26.4232  0.0118\n",
      "     50       \u001b[36m23.6017\u001b[0m       26.4307  0.0115\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m40.4053\u001b[0m       \u001b[32m32.0980\u001b[0m  0.0112\n",
      "      2       \u001b[36m39.8892\u001b[0m       \u001b[32m31.7439\u001b[0m  0.0113\n",
      "      3       \u001b[36m39.3829\u001b[0m       \u001b[32m31.3979\u001b[0m  0.0112\n",
      "      4       \u001b[36m38.8838\u001b[0m       \u001b[32m31.0591\u001b[0m  0.0107\n",
      "      5       \u001b[36m38.3911\u001b[0m       \u001b[32m30.7271\u001b[0m  0.0107\n",
      "      6       \u001b[36m37.9048\u001b[0m       \u001b[32m30.4021\u001b[0m  0.0107\n",
      "      7       \u001b[36m37.4252\u001b[0m       \u001b[32m30.0838\u001b[0m  0.0111\n",
      "      8       \u001b[36m36.9526\u001b[0m       \u001b[32m29.7734\u001b[0m  0.0108\n",
      "      9       \u001b[36m36.4879\u001b[0m       \u001b[32m29.4718\u001b[0m  0.0111\n",
      "     10       \u001b[36m36.0325\u001b[0m       \u001b[32m29.1800\u001b[0m  0.0108\n",
      "     11       \u001b[36m35.5880\u001b[0m       \u001b[32m28.8994\u001b[0m  0.0108\n",
      "     12       \u001b[36m35.1555\u001b[0m       \u001b[32m28.6304\u001b[0m  0.0111\n",
      "     13       \u001b[36m34.7354\u001b[0m       \u001b[32m28.3738\u001b[0m  0.0110\n",
      "     14       \u001b[36m34.3286\u001b[0m       \u001b[32m28.1306\u001b[0m  0.0111\n",
      "     15       \u001b[36m33.9356\u001b[0m       \u001b[32m27.9013\u001b[0m  0.0105\n",
      "     16       \u001b[36m33.5562\u001b[0m       \u001b[32m27.6862\u001b[0m  0.0112\n",
      "     17       \u001b[36m33.1900\u001b[0m       \u001b[32m27.4861\u001b[0m  0.0112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     18       \u001b[36m32.8370\u001b[0m       \u001b[32m27.3016\u001b[0m  0.0117\n",
      "     19       \u001b[36m32.4975\u001b[0m       \u001b[32m27.1331\u001b[0m  0.0108\n",
      "     20       \u001b[36m32.1716\u001b[0m       \u001b[32m26.9811\u001b[0m  0.0110\n",
      "     21       \u001b[36m31.8602\u001b[0m       \u001b[32m26.8461\u001b[0m  0.0113\n",
      "     22       \u001b[36m31.5640\u001b[0m       \u001b[32m26.7286\u001b[0m  0.0106\n",
      "     23       \u001b[36m31.2836\u001b[0m       \u001b[32m26.6287\u001b[0m  0.0109\n",
      "     24       \u001b[36m31.0200\u001b[0m       \u001b[32m26.5468\u001b[0m  0.0110\n",
      "     25       \u001b[36m30.7740\u001b[0m       \u001b[32m26.4826\u001b[0m  0.0107\n",
      "     26       \u001b[36m30.5457\u001b[0m       \u001b[32m26.4354\u001b[0m  0.0107\n",
      "     27       \u001b[36m30.3359\u001b[0m       \u001b[32m26.4044\u001b[0m  0.0108\n",
      "     28       \u001b[36m30.1449\u001b[0m       \u001b[32m26.3885\u001b[0m  0.0111\n",
      "     29       \u001b[36m29.9724\u001b[0m       \u001b[32m26.3867\u001b[0m  0.0109\n",
      "     30       \u001b[36m29.8179\u001b[0m       26.3972  0.0111\n",
      "     31       \u001b[36m29.6812\u001b[0m       26.4182  0.0116\n",
      "     32       \u001b[36m29.5613\u001b[0m       26.4479  0.0114\n",
      "     33       \u001b[36m29.4567\u001b[0m       26.4844  0.0113\n",
      "     34       \u001b[36m29.3660\u001b[0m       26.5258  0.0111\n",
      "     35       \u001b[36m29.2878\u001b[0m       26.5705  0.0113\n",
      "     36       \u001b[36m29.2208\u001b[0m       26.6170  0.0110\n",
      "     37       \u001b[36m29.1634\u001b[0m       26.6639  0.0110\n",
      "     38       \u001b[36m29.1145\u001b[0m       26.7100  0.0110\n",
      "     39       \u001b[36m29.0726\u001b[0m       26.7550  0.0114\n",
      "     40       \u001b[36m29.0368\u001b[0m       26.7980  0.0110\n",
      "     41       \u001b[36m29.0061\u001b[0m       26.8384  0.0114\n",
      "     42       \u001b[36m28.9797\u001b[0m       26.8764  0.0110\n",
      "     43       \u001b[36m28.9568\u001b[0m       26.9115  0.0110\n",
      "     44       \u001b[36m28.9368\u001b[0m       26.9435  0.0111\n",
      "     45       \u001b[36m28.9191\u001b[0m       26.9724  0.0114\n",
      "     46       \u001b[36m28.9033\u001b[0m       26.9985  0.0112\n",
      "     47       \u001b[36m28.8891\u001b[0m       27.0217  0.0107\n",
      "     48       \u001b[36m28.8761\u001b[0m       27.0426  0.0113\n",
      "     49       \u001b[36m28.8643\u001b[0m       27.0611  0.0107\n",
      "     50       \u001b[36m28.8535\u001b[0m       27.0775  0.0110\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m43.8819\u001b[0m       \u001b[32m46.0181\u001b[0m  0.0118\n",
      "      2       \u001b[36m43.5847\u001b[0m       \u001b[32m45.6092\u001b[0m  0.0114\n",
      "      3       \u001b[36m43.2725\u001b[0m       \u001b[32m45.1409\u001b[0m  0.0123\n",
      "      4       \u001b[36m42.9010\u001b[0m       \u001b[32m44.5849\u001b[0m  0.0119\n",
      "      5       \u001b[36m42.4595\u001b[0m       \u001b[32m43.9283\u001b[0m  0.0117\n",
      "      6       \u001b[36m41.9261\u001b[0m       \u001b[32m43.1829\u001b[0m  0.0115\n",
      "      7       \u001b[36m41.3140\u001b[0m       \u001b[32m42.3555\u001b[0m  0.0123\n",
      "      8       \u001b[36m40.6231\u001b[0m       \u001b[32m41.4075\u001b[0m  0.0115\n",
      "      9       \u001b[36m39.8330\u001b[0m       \u001b[32m40.3007\u001b[0m  0.0114\n",
      "     10       \u001b[36m38.9330\u001b[0m       \u001b[32m39.0269\u001b[0m  0.0114\n",
      "     11       \u001b[36m37.9422\u001b[0m       \u001b[32m37.6414\u001b[0m  0.0116\n",
      "     12       \u001b[36m36.9118\u001b[0m       \u001b[32m36.2145\u001b[0m  0.0119\n",
      "     13       \u001b[36m35.9203\u001b[0m       \u001b[32m34.8336\u001b[0m  0.0117\n",
      "     14       \u001b[36m35.0629\u001b[0m       \u001b[32m33.6025\u001b[0m  0.0114\n",
      "     15       \u001b[36m34.4234\u001b[0m       \u001b[32m32.6457\u001b[0m  0.0116\n",
      "     16       \u001b[36m34.0607\u001b[0m       \u001b[32m32.0351\u001b[0m  0.0117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m33.9025\u001b[0m       \u001b[32m31.7010\u001b[0m  0.0123\n",
      "     18       \u001b[36m33.8019\u001b[0m       \u001b[32m31.5327\u001b[0m  0.0118\n",
      "     19       \u001b[36m33.6772\u001b[0m       \u001b[32m31.4570\u001b[0m  0.0116\n",
      "     20       \u001b[36m33.5397\u001b[0m       \u001b[32m31.4325\u001b[0m  0.0116\n",
      "     21       \u001b[36m33.4227\u001b[0m       \u001b[32m31.4141\u001b[0m  0.0112\n",
      "     22       \u001b[36m33.3284\u001b[0m       \u001b[32m31.3679\u001b[0m  0.0116\n",
      "     23       \u001b[36m33.2470\u001b[0m       \u001b[32m31.2897\u001b[0m  0.0130\n",
      "     24       \u001b[36m33.1728\u001b[0m       \u001b[32m31.2012\u001b[0m  0.0181\n",
      "     25       \u001b[36m33.1056\u001b[0m       \u001b[32m31.1175\u001b[0m  0.0123\n",
      "     26       \u001b[36m33.0451\u001b[0m       \u001b[32m31.0425\u001b[0m  0.0137\n",
      "     27       \u001b[36m32.9911\u001b[0m       \u001b[32m30.9772\u001b[0m  0.0134\n",
      "     28       \u001b[36m32.9421\u001b[0m       \u001b[32m30.9246\u001b[0m  0.0143\n",
      "     29       \u001b[36m32.8976\u001b[0m       \u001b[32m30.8828\u001b[0m  0.0131\n",
      "     30       \u001b[36m32.8558\u001b[0m       \u001b[32m30.8523\u001b[0m  0.0143\n",
      "     31       \u001b[36m32.8175\u001b[0m       \u001b[32m30.8307\u001b[0m  0.0123\n",
      "     32       \u001b[36m32.7821\u001b[0m       \u001b[32m30.8121\u001b[0m  0.0122\n",
      "     33       \u001b[36m32.7496\u001b[0m       \u001b[32m30.7931\u001b[0m  0.0125\n",
      "     34       \u001b[36m32.7190\u001b[0m       \u001b[32m30.7748\u001b[0m  0.0121\n",
      "     35       \u001b[36m32.6901\u001b[0m       \u001b[32m30.7595\u001b[0m  0.0126\n",
      "     36       \u001b[36m32.6632\u001b[0m       \u001b[32m30.7470\u001b[0m  0.0126\n",
      "     37       \u001b[36m32.6376\u001b[0m       \u001b[32m30.7351\u001b[0m  0.0121\n",
      "     38       \u001b[36m32.6137\u001b[0m       \u001b[32m30.7228\u001b[0m  0.0122\n",
      "     39       \u001b[36m32.5912\u001b[0m       \u001b[32m30.7125\u001b[0m  0.0120\n",
      "     40       \u001b[36m32.5700\u001b[0m       \u001b[32m30.7065\u001b[0m  0.0125\n",
      "     41       \u001b[36m32.5500\u001b[0m       \u001b[32m30.7015\u001b[0m  0.0118\n",
      "     42       \u001b[36m32.5310\u001b[0m       \u001b[32m30.6937\u001b[0m  0.0120\n",
      "     43       \u001b[36m32.5130\u001b[0m       \u001b[32m30.6841\u001b[0m  0.0117\n",
      "     44       \u001b[36m32.4960\u001b[0m       \u001b[32m30.6756\u001b[0m  0.0121\n",
      "     45       \u001b[36m32.4801\u001b[0m       \u001b[32m30.6683\u001b[0m  0.0137\n",
      "     46       \u001b[36m32.4651\u001b[0m       \u001b[32m30.6604\u001b[0m  0.0128\n",
      "     47       \u001b[36m32.4507\u001b[0m       \u001b[32m30.6518\u001b[0m  0.0124\n",
      "     48       \u001b[36m32.4373\u001b[0m       \u001b[32m30.6442\u001b[0m  0.0124\n",
      "     49       \u001b[36m32.4245\u001b[0m       \u001b[32m30.6374\u001b[0m  0.0121\n",
      "     50       \u001b[36m32.4125\u001b[0m       \u001b[32m30.6311\u001b[0m  0.0124\n",
      "     51       \u001b[36m32.4012\u001b[0m       \u001b[32m30.6233\u001b[0m  0.0126\n",
      "     52       \u001b[36m32.3905\u001b[0m       \u001b[32m30.6147\u001b[0m  0.0127\n",
      "     53       \u001b[36m32.3806\u001b[0m       \u001b[32m30.6065\u001b[0m  0.0124\n",
      "     54       \u001b[36m32.3712\u001b[0m       \u001b[32m30.5931\u001b[0m  0.0121\n",
      "     55       \u001b[36m32.3621\u001b[0m       \u001b[32m30.5793\u001b[0m  0.0127\n",
      "     56       \u001b[36m32.3536\u001b[0m       \u001b[32m30.5693\u001b[0m  0.0127\n",
      "     57       \u001b[36m32.3455\u001b[0m       \u001b[32m30.5576\u001b[0m  0.0122\n",
      "     58       \u001b[36m32.3378\u001b[0m       \u001b[32m30.5456\u001b[0m  0.0115\n",
      "     59       \u001b[36m32.3306\u001b[0m       \u001b[32m30.5353\u001b[0m  0.0115\n",
      "     60       \u001b[36m32.3239\u001b[0m       \u001b[32m30.5243\u001b[0m  0.0120\n",
      "     61       \u001b[36m32.3175\u001b[0m       \u001b[32m30.5131\u001b[0m  0.0118\n",
      "     62       \u001b[36m32.3116\u001b[0m       \u001b[32m30.5033\u001b[0m  0.0116\n",
      "     63       \u001b[36m32.3059\u001b[0m       \u001b[32m30.4935\u001b[0m  0.0116\n",
      "     64       \u001b[36m32.3004\u001b[0m       \u001b[32m30.4839\u001b[0m  0.0114\n",
      "     65       \u001b[36m32.2952\u001b[0m       \u001b[32m30.4741\u001b[0m  0.0120\n",
      "     66       \u001b[36m32.2902\u001b[0m       \u001b[32m30.4647\u001b[0m  0.0119\n",
      "     67       \u001b[36m32.2854\u001b[0m       \u001b[32m30.4557\u001b[0m  0.0121\n",
      "     68       \u001b[36m32.2808\u001b[0m       \u001b[32m30.4478\u001b[0m  0.0118\n",
      "     69       \u001b[36m32.2765\u001b[0m       \u001b[32m30.4405\u001b[0m  0.0111\n",
      "     70       \u001b[36m32.2723\u001b[0m       \u001b[32m30.4333\u001b[0m  0.0118\n",
      "     71       \u001b[36m32.2683\u001b[0m       \u001b[32m30.4272\u001b[0m  0.0120\n",
      "     72       \u001b[36m32.2645\u001b[0m       \u001b[32m30.4211\u001b[0m  0.0118\n",
      "     73       \u001b[36m32.2608\u001b[0m       \u001b[32m30.4152\u001b[0m  0.0115\n",
      "     74       \u001b[36m32.2572\u001b[0m       \u001b[32m30.4102\u001b[0m  0.0116\n",
      "     75       \u001b[36m32.2538\u001b[0m       \u001b[32m30.4053\u001b[0m  0.0121\n",
      "     76       \u001b[36m32.2505\u001b[0m       \u001b[32m30.4009\u001b[0m  0.0117\n",
      "     77       \u001b[36m32.2474\u001b[0m       \u001b[32m30.3963\u001b[0m  0.0119\n",
      "     78       \u001b[36m32.2443\u001b[0m       \u001b[32m30.3923\u001b[0m  0.0116\n",
      "     79       \u001b[36m32.2413\u001b[0m       \u001b[32m30.3889\u001b[0m  0.0116\n",
      "     80       \u001b[36m32.2383\u001b[0m       \u001b[32m30.3853\u001b[0m  0.0120\n",
      "     81       \u001b[36m32.2355\u001b[0m       \u001b[32m30.3819\u001b[0m  0.0117\n",
      "     82       \u001b[36m32.2328\u001b[0m       \u001b[32m30.3774\u001b[0m  0.0116\n",
      "     83       \u001b[36m32.2300\u001b[0m       \u001b[32m30.3723\u001b[0m  0.0114\n",
      "     84       \u001b[36m32.2274\u001b[0m       \u001b[32m30.3677\u001b[0m  0.0115\n",
      "     85       \u001b[36m32.2249\u001b[0m       \u001b[32m30.3642\u001b[0m  0.0118\n",
      "     86       \u001b[36m32.2224\u001b[0m       \u001b[32m30.3613\u001b[0m  0.0117\n",
      "     87       \u001b[36m32.2201\u001b[0m       \u001b[32m30.3584\u001b[0m  0.0116\n",
      "     88       \u001b[36m32.2177\u001b[0m       \u001b[32m30.3554\u001b[0m  0.0116\n",
      "     89       \u001b[36m32.2155\u001b[0m       \u001b[32m30.3518\u001b[0m  0.0116\n",
      "     90       \u001b[36m32.2133\u001b[0m       \u001b[32m30.3483\u001b[0m  0.0122\n",
      "     91       \u001b[36m32.2112\u001b[0m       \u001b[32m30.3448\u001b[0m  0.0117\n",
      "     92       \u001b[36m32.2091\u001b[0m       \u001b[32m30.3413\u001b[0m  0.0119\n",
      "     93       \u001b[36m32.2071\u001b[0m       \u001b[32m30.3382\u001b[0m  0.0114\n",
      "     94       \u001b[36m32.2052\u001b[0m       \u001b[32m30.3355\u001b[0m  0.0113\n",
      "     95       \u001b[36m32.2033\u001b[0m       \u001b[32m30.3327\u001b[0m  0.0120\n",
      "     96       \u001b[36m32.2015\u001b[0m       \u001b[32m30.3300\u001b[0m  0.0120\n",
      "     97       \u001b[36m32.1997\u001b[0m       \u001b[32m30.3273\u001b[0m  0.0119\n",
      "     98       \u001b[36m32.1979\u001b[0m       \u001b[32m30.3244\u001b[0m  0.0116\n",
      "     99       \u001b[36m32.1962\u001b[0m       \u001b[32m30.3213\u001b[0m  0.0114\n",
      "    100       \u001b[36m32.1945\u001b[0m       \u001b[32m30.3177\u001b[0m  0.0121\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m33.8644\u001b[0m       \u001b[32m32.5240\u001b[0m  0.0174\n",
      "      2       \u001b[36m33.4961\u001b[0m       \u001b[32m32.2503\u001b[0m  0.0188\n",
      "      3       \u001b[36m33.0897\u001b[0m       \u001b[32m31.9511\u001b[0m  0.0130\n",
      "      4       \u001b[36m32.6428\u001b[0m       \u001b[32m31.6289\u001b[0m  0.0136\n",
      "      5       \u001b[36m32.1509\u001b[0m       \u001b[32m31.2828\u001b[0m  0.0140\n",
      "      6       \u001b[36m31.6251\u001b[0m       \u001b[32m30.9012\u001b[0m  0.0197\n",
      "      7       \u001b[36m31.0673\u001b[0m       \u001b[32m30.4639\u001b[0m  0.0160\n",
      "      8       \u001b[36m30.4560\u001b[0m       \u001b[32m29.9428\u001b[0m  0.0129\n",
      "      9       \u001b[36m29.7493\u001b[0m       \u001b[32m29.3175\u001b[0m  0.0120\n",
      "     10       \u001b[36m28.9073\u001b[0m       \u001b[32m28.5969\u001b[0m  0.0127\n",
      "     11       \u001b[36m27.9194\u001b[0m       \u001b[32m27.8406\u001b[0m  0.0120\n",
      "     12       \u001b[36m26.8575\u001b[0m       \u001b[32m27.1790\u001b[0m  0.0150\n",
      "     13       \u001b[36m25.8565\u001b[0m       \u001b[32m26.7791\u001b[0m  0.0122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     14       \u001b[36m25.0695\u001b[0m       \u001b[32m26.7619\u001b[0m  0.0131\n",
      "     15       \u001b[36m24.6182\u001b[0m       27.0633  0.0121\n",
      "     16       \u001b[36m24.4653\u001b[0m       27.3736  0.0119\n",
      "     17       \u001b[36m24.4125\u001b[0m       27.4240  0.0127\n",
      "     18       \u001b[36m24.3158\u001b[0m       27.2562  0.0132\n",
      "     19       \u001b[36m24.1938\u001b[0m       27.0438  0.0119\n",
      "     20       \u001b[36m24.0944\u001b[0m       26.8850  0.0133\n",
      "     21       \u001b[36m24.0223\u001b[0m       26.7976  0.0123\n",
      "     22       \u001b[36m23.9623\u001b[0m       26.7700  0.0122\n",
      "     23       \u001b[36m23.9039\u001b[0m       26.7842  0.0122\n",
      "     24       \u001b[36m23.8449\u001b[0m       26.8231  0.0132\n",
      "     25       \u001b[36m23.7883\u001b[0m       26.8697  0.0121\n",
      "     26       \u001b[36m23.7372\u001b[0m       26.9085  0.0137\n",
      "     27       \u001b[36m23.6912\u001b[0m       26.9319  0.0133\n",
      "     28       \u001b[36m23.6497\u001b[0m       26.9406  0.0134\n",
      "     29       \u001b[36m23.6123\u001b[0m       26.9390  0.0140\n",
      "     30       \u001b[36m23.5784\u001b[0m       26.9330  0.0142\n",
      "     31       \u001b[36m23.5474\u001b[0m       26.9263  0.0133\n",
      "     32       \u001b[36m23.5188\u001b[0m       26.9217  0.0143\n",
      "     33       \u001b[36m23.4923\u001b[0m       26.9195  0.0126\n",
      "     34       \u001b[36m23.4676\u001b[0m       26.9170  0.0130\n",
      "     35       \u001b[36m23.4445\u001b[0m       26.9129  0.0132\n",
      "     36       \u001b[36m23.4230\u001b[0m       26.9069  0.0125\n",
      "     37       \u001b[36m23.4030\u001b[0m       26.8992  0.0127\n",
      "     38       \u001b[36m23.3843\u001b[0m       26.8905  0.0133\n",
      "     39       \u001b[36m23.3668\u001b[0m       26.8806  0.0125\n",
      "     40       \u001b[36m23.3504\u001b[0m       26.8703  0.0134\n",
      "     41       \u001b[36m23.3350\u001b[0m       26.8589  0.0134\n",
      "     42       \u001b[36m23.3203\u001b[0m       26.8480  0.0123\n",
      "     43       \u001b[36m23.3064\u001b[0m       26.8376  0.0128\n",
      "     44       \u001b[36m23.2932\u001b[0m       26.8275  0.0124\n",
      "     45       \u001b[36m23.2806\u001b[0m       26.8167  0.0130\n",
      "     46       \u001b[36m23.2687\u001b[0m       26.8061  0.0126\n",
      "     47       \u001b[36m23.2573\u001b[0m       26.7969  0.0150\n",
      "     48       \u001b[36m23.2465\u001b[0m       26.7867  0.0126\n",
      "     49       \u001b[36m23.2364\u001b[0m       26.7757  0.0150\n",
      "     50       \u001b[36m23.2268\u001b[0m       26.7645  0.0150\n",
      "     51       \u001b[36m23.2178\u001b[0m       \u001b[32m26.7545\u001b[0m  0.0149\n",
      "     52       \u001b[36m23.2092\u001b[0m       \u001b[32m26.7459\u001b[0m  0.0136\n",
      "     53       \u001b[36m23.2011\u001b[0m       \u001b[32m26.7371\u001b[0m  0.0123\n",
      "     54       \u001b[36m23.1935\u001b[0m       \u001b[32m26.7291\u001b[0m  0.0131\n",
      "     55       \u001b[36m23.1864\u001b[0m       \u001b[32m26.7222\u001b[0m  0.0134\n",
      "     56       \u001b[36m23.1796\u001b[0m       \u001b[32m26.7145\u001b[0m  0.0134\n",
      "     57       \u001b[36m23.1732\u001b[0m       \u001b[32m26.7066\u001b[0m  0.0126\n",
      "     58       \u001b[36m23.1673\u001b[0m       \u001b[32m26.6997\u001b[0m  0.0123\n",
      "     59       \u001b[36m23.1616\u001b[0m       \u001b[32m26.6934\u001b[0m  0.0129\n",
      "     60       \u001b[36m23.1562\u001b[0m       \u001b[32m26.6877\u001b[0m  0.0136\n",
      "     61       \u001b[36m23.1510\u001b[0m       \u001b[32m26.6824\u001b[0m  0.0128\n",
      "     62       \u001b[36m23.1462\u001b[0m       \u001b[32m26.6776\u001b[0m  0.0140\n",
      "     63       \u001b[36m23.1416\u001b[0m       \u001b[32m26.6731\u001b[0m  0.0134\n",
      "     64       \u001b[36m23.1372\u001b[0m       \u001b[32m26.6685\u001b[0m  0.0131\n",
      "     65       \u001b[36m23.1330\u001b[0m       \u001b[32m26.6639\u001b[0m  0.0127\n",
      "     66       \u001b[36m23.1289\u001b[0m       \u001b[32m26.6592\u001b[0m  0.0224\n",
      "     67       \u001b[36m23.1251\u001b[0m       \u001b[32m26.6552\u001b[0m  0.0146\n",
      "     68       \u001b[36m23.1214\u001b[0m       \u001b[32m26.6524\u001b[0m  0.0126\n",
      "     69       \u001b[36m23.1179\u001b[0m       \u001b[32m26.6497\u001b[0m  0.0127\n",
      "     70       \u001b[36m23.1146\u001b[0m       \u001b[32m26.6468\u001b[0m  0.0125\n",
      "     71       \u001b[36m23.1113\u001b[0m       \u001b[32m26.6431\u001b[0m  0.0148\n",
      "     72       \u001b[36m23.1082\u001b[0m       \u001b[32m26.6406\u001b[0m  0.0194\n",
      "     73       \u001b[36m23.1052\u001b[0m       \u001b[32m26.6382\u001b[0m  0.0211\n",
      "     74       \u001b[36m23.1023\u001b[0m       \u001b[32m26.6358\u001b[0m  0.0218\n",
      "     75       \u001b[36m23.0995\u001b[0m       \u001b[32m26.6336\u001b[0m  0.0158\n",
      "     76       \u001b[36m23.0968\u001b[0m       \u001b[32m26.6320\u001b[0m  0.0150\n",
      "     77       \u001b[36m23.0941\u001b[0m       \u001b[32m26.6306\u001b[0m  0.0163\n",
      "     78       \u001b[36m23.0916\u001b[0m       \u001b[32m26.6290\u001b[0m  0.0117\n",
      "     79       \u001b[36m23.0892\u001b[0m       \u001b[32m26.6275\u001b[0m  0.0119\n",
      "     80       \u001b[36m23.0868\u001b[0m       \u001b[32m26.6255\u001b[0m  0.0116\n",
      "     81       \u001b[36m23.0846\u001b[0m       \u001b[32m26.6235\u001b[0m  0.0121\n",
      "     82       \u001b[36m23.0824\u001b[0m       \u001b[32m26.6212\u001b[0m  0.0117\n",
      "     83       \u001b[36m23.0803\u001b[0m       \u001b[32m26.6192\u001b[0m  0.0120\n",
      "     84       \u001b[36m23.0782\u001b[0m       \u001b[32m26.6178\u001b[0m  0.0117\n",
      "     85       \u001b[36m23.0762\u001b[0m       \u001b[32m26.6169\u001b[0m  0.0124\n",
      "     86       \u001b[36m23.0742\u001b[0m       \u001b[32m26.6157\u001b[0m  0.0119\n",
      "     87       \u001b[36m23.0723\u001b[0m       \u001b[32m26.6146\u001b[0m  0.0119\n",
      "     88       \u001b[36m23.0705\u001b[0m       \u001b[32m26.6134\u001b[0m  0.0115\n",
      "     89       \u001b[36m23.0687\u001b[0m       \u001b[32m26.6121\u001b[0m  0.0117\n",
      "     90       \u001b[36m23.0670\u001b[0m       \u001b[32m26.6104\u001b[0m  0.0142\n",
      "     91       \u001b[36m23.0653\u001b[0m       \u001b[32m26.6084\u001b[0m  0.0122\n",
      "     92       \u001b[36m23.0637\u001b[0m       \u001b[32m26.6064\u001b[0m  0.0117\n",
      "     93       \u001b[36m23.0621\u001b[0m       \u001b[32m26.6054\u001b[0m  0.0121\n",
      "     94       \u001b[36m23.0606\u001b[0m       \u001b[32m26.6053\u001b[0m  0.0121\n",
      "     95       \u001b[36m23.0591\u001b[0m       26.6054  0.0123\n",
      "     96       \u001b[36m23.0577\u001b[0m       \u001b[32m26.6046\u001b[0m  0.0124\n",
      "     97       \u001b[36m23.0563\u001b[0m       \u001b[32m26.6039\u001b[0m  0.0119\n",
      "     98       \u001b[36m23.0549\u001b[0m       \u001b[32m26.6028\u001b[0m  0.0120\n",
      "     99       \u001b[36m23.0536\u001b[0m       \u001b[32m26.6021\u001b[0m  0.0123\n",
      "    100       \u001b[36m23.0523\u001b[0m       26.6030  0.0119\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m40.4323\u001b[0m       \u001b[32m32.1203\u001b[0m  0.0124\n",
      "      2       \u001b[36m39.9145\u001b[0m       \u001b[32m31.8082\u001b[0m  0.0122\n",
      "      3       \u001b[36m39.4036\u001b[0m       \u001b[32m31.4614\u001b[0m  0.0123\n",
      "      4       \u001b[36m38.8469\u001b[0m       \u001b[32m31.0499\u001b[0m  0.0122\n",
      "      5       \u001b[36m38.2129\u001b[0m       \u001b[32m30.5832\u001b[0m  0.0117\n",
      "      6       \u001b[36m37.5126\u001b[0m       \u001b[32m30.0561\u001b[0m  0.0121\n",
      "      7       \u001b[36m36.7336\u001b[0m       \u001b[32m29.4557\u001b[0m  0.0123\n",
      "      8       \u001b[36m35.8378\u001b[0m       \u001b[32m28.7813\u001b[0m  0.0119\n",
      "      9       \u001b[36m34.8004\u001b[0m       \u001b[32m28.0625\u001b[0m  0.0121\n",
      "     10       \u001b[36m33.6595\u001b[0m       \u001b[32m27.3981\u001b[0m  0.0122\n",
      "     11       \u001b[36m32.5087\u001b[0m       \u001b[32m26.9284\u001b[0m  0.0118\n",
      "     12       \u001b[36m31.4943\u001b[0m       \u001b[32m26.8059\u001b[0m  0.0121\n",
      "     13       \u001b[36m30.7753\u001b[0m       27.0856  0.0123\n",
      "     14       \u001b[36m30.4220\u001b[0m       27.5981  0.0123\n",
      "     15       \u001b[36m30.3299\u001b[0m       27.9853  0.0123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     16       \u001b[36m30.2803\u001b[0m       28.0197  0.0125\n",
      "     17       \u001b[36m30.1506\u001b[0m       27.7905  0.0123\n",
      "     18       \u001b[36m29.9745\u001b[0m       27.5046  0.0125\n",
      "     19       \u001b[36m29.8113\u001b[0m       27.2885  0.0125\n",
      "     20       \u001b[36m29.6786\u001b[0m       27.1744  0.0120\n",
      "     21       \u001b[36m29.5676\u001b[0m       27.1500  0.0120\n",
      "     22       \u001b[36m29.4697\u001b[0m       27.1875  0.0119\n",
      "     23       \u001b[36m29.3824\u001b[0m       27.2561  0.0123\n",
      "     24       \u001b[36m29.3053\u001b[0m       27.3316  0.0122\n",
      "     25       \u001b[36m29.2369\u001b[0m       27.3928  0.0123\n",
      "     26       \u001b[36m29.1739\u001b[0m       27.4303  0.0120\n",
      "     27       \u001b[36m29.1148\u001b[0m       27.4483  0.0120\n",
      "     28       \u001b[36m29.0595\u001b[0m       27.4572  0.0124\n",
      "     29       \u001b[36m29.0100\u001b[0m       27.4621  0.0119\n",
      "     30       \u001b[36m28.9653\u001b[0m       27.4656  0.0119\n",
      "     31       \u001b[36m28.9250\u001b[0m       27.4725  0.0118\n",
      "     32       \u001b[36m28.8888\u001b[0m       27.4828  0.0119\n",
      "     33       \u001b[36m28.8566\u001b[0m       27.4933  0.0118\n",
      "     34       \u001b[36m28.8275\u001b[0m       27.5004  0.0115\n",
      "     35       \u001b[36m28.8005\u001b[0m       27.5065  0.0114\n",
      "     36       \u001b[36m28.7753\u001b[0m       27.5143  0.0117\n",
      "     37       \u001b[36m28.7521\u001b[0m       27.5206  0.0122\n",
      "     38       \u001b[36m28.7305\u001b[0m       27.5261  0.0129\n",
      "     39       \u001b[36m28.7104\u001b[0m       27.5313  0.0170\n",
      "     40       \u001b[36m28.6918\u001b[0m       27.5334  0.0162\n",
      "     41       \u001b[36m28.6748\u001b[0m       27.5376  0.0140\n",
      "     42       \u001b[36m28.6592\u001b[0m       27.5443  0.0133\n",
      "     43       \u001b[36m28.6449\u001b[0m       27.5468  0.0120\n",
      "     44       \u001b[36m28.6312\u001b[0m       27.5481  0.0122\n",
      "     45       \u001b[36m28.6185\u001b[0m       27.5522  0.0160\n",
      "     46       \u001b[36m28.6070\u001b[0m       27.5558  0.0170\n",
      "     47       \u001b[36m28.5959\u001b[0m       27.5586  0.0132\n",
      "     48       \u001b[36m28.5857\u001b[0m       27.5646  0.0134\n",
      "     49       \u001b[36m28.5765\u001b[0m       27.5684  0.0123\n",
      "     50       \u001b[36m28.5682\u001b[0m       27.5691  0.0134\n",
      "     51       \u001b[36m28.5602\u001b[0m       27.5687  0.0137\n",
      "     52       \u001b[36m28.5527\u001b[0m       27.5683  0.0138\n",
      "     53       \u001b[36m28.5455\u001b[0m       27.5689  0.0120\n",
      "     54       \u001b[36m28.5387\u001b[0m       27.5699  0.0117\n",
      "     55       \u001b[36m28.5324\u001b[0m       27.5701  0.0119\n",
      "     56       \u001b[36m28.5265\u001b[0m       27.5694  0.0122\n",
      "     57       \u001b[36m28.5210\u001b[0m       27.5686  0.0117\n",
      "     58       \u001b[36m28.5158\u001b[0m       27.5669  0.0118\n",
      "     59       \u001b[36m28.5109\u001b[0m       27.5658  0.0119\n",
      "     60       \u001b[36m28.5062\u001b[0m       27.5659  0.0121\n",
      "     61       \u001b[36m28.5018\u001b[0m       27.5653  0.0121\n",
      "     62       \u001b[36m28.4975\u001b[0m       27.5636  0.0120\n",
      "     63       \u001b[36m28.4935\u001b[0m       27.5606  0.0115\n",
      "     64       \u001b[36m28.4897\u001b[0m       27.5580  0.0115\n",
      "     65       \u001b[36m28.4861\u001b[0m       27.5562  0.0119\n",
      "     66       \u001b[36m28.4827\u001b[0m       27.5547  0.0117\n",
      "     67       \u001b[36m28.4795\u001b[0m       27.5539  0.0115\n",
      "     68       \u001b[36m28.4766\u001b[0m       27.5541  0.0119\n",
      "     69       \u001b[36m28.4738\u001b[0m       27.5539  0.0114\n",
      "     70       \u001b[36m28.4710\u001b[0m       27.5508  0.0119\n",
      "     71       \u001b[36m28.4682\u001b[0m       27.5471  0.0117\n",
      "     72       \u001b[36m28.4657\u001b[0m       27.5442  0.0116\n",
      "     73       \u001b[36m28.4633\u001b[0m       27.5431  0.0114\n",
      "     74       \u001b[36m28.4611\u001b[0m       27.5423  0.0113\n",
      "     75       \u001b[36m28.4590\u001b[0m       27.5405  0.0118\n",
      "     76       \u001b[36m28.4569\u001b[0m       27.5376  0.0119\n",
      "     77       \u001b[36m28.4548\u001b[0m       27.5345  0.0118\n",
      "     78       \u001b[36m28.4528\u001b[0m       27.5320  0.0115\n",
      "     79       \u001b[36m28.4509\u001b[0m       27.5303  0.0113\n",
      "     80       \u001b[36m28.4492\u001b[0m       27.5297  0.0122\n",
      "     81       \u001b[36m28.4475\u001b[0m       27.5292  0.0119\n",
      "     82       \u001b[36m28.4460\u001b[0m       27.5282  0.0113\n",
      "     83       \u001b[36m28.4445\u001b[0m       27.5261  0.0114\n",
      "     84       \u001b[36m28.4429\u001b[0m       27.5234  0.0118\n",
      "     85       \u001b[36m28.4414\u001b[0m       27.5211  0.0140\n",
      "     86       \u001b[36m28.4399\u001b[0m       27.5185  0.0120\n",
      "     87       \u001b[36m28.4385\u001b[0m       27.5153  0.0115\n",
      "     88       \u001b[36m28.4372\u001b[0m       27.5139  0.0117\n",
      "     89       \u001b[36m28.4359\u001b[0m       27.5129  0.0115\n",
      "     90       \u001b[36m28.4346\u001b[0m       27.5113  0.0118\n",
      "     91       \u001b[36m28.4334\u001b[0m       27.5090  0.0114\n",
      "     92       \u001b[36m28.4322\u001b[0m       27.5066  0.0115\n",
      "     93       \u001b[36m28.4310\u001b[0m       27.5044  0.0115\n",
      "     94       \u001b[36m28.4299\u001b[0m       27.5019  0.0117\n",
      "     95       \u001b[36m28.4288\u001b[0m       27.4998  0.0116\n",
      "     96       \u001b[36m28.4277\u001b[0m       27.4978  0.0113\n",
      "     97       \u001b[36m28.4267\u001b[0m       27.4950  0.0117\n",
      "     98       \u001b[36m28.4257\u001b[0m       27.4914  0.0116\n",
      "     99       \u001b[36m28.4248\u001b[0m       27.4891  0.0116\n",
      "    100       \u001b[36m28.4239\u001b[0m       27.4879  0.0120\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m42.9148\u001b[0m       \u001b[32m44.8866\u001b[0m  0.0108\n",
      "      2       \u001b[36m42.6228\u001b[0m       \u001b[32m44.5348\u001b[0m  0.0111\n",
      "      3       \u001b[36m42.3391\u001b[0m       \u001b[32m44.1920\u001b[0m  0.0108\n",
      "      4       \u001b[36m42.0632\u001b[0m       \u001b[32m43.8573\u001b[0m  0.0110\n",
      "      5       \u001b[36m41.7941\u001b[0m       \u001b[32m43.5297\u001b[0m  0.0106\n",
      "      6       \u001b[36m41.5310\u001b[0m       \u001b[32m43.2079\u001b[0m  0.0106\n",
      "      7       \u001b[36m41.2735\u001b[0m       \u001b[32m42.8917\u001b[0m  0.0112\n",
      "      8       \u001b[36m41.0211\u001b[0m       \u001b[32m42.5806\u001b[0m  0.0111\n",
      "      9       \u001b[36m40.7734\u001b[0m       \u001b[32m42.2735\u001b[0m  0.0108\n",
      "     10       \u001b[36m40.5301\u001b[0m       \u001b[32m41.9702\u001b[0m  0.0107\n",
      "     11       \u001b[36m40.2906\u001b[0m       \u001b[32m41.6697\u001b[0m  0.0110\n",
      "     12       \u001b[36m40.0542\u001b[0m       \u001b[32m41.3721\u001b[0m  0.0110\n",
      "     13       \u001b[36m39.8206\u001b[0m       \u001b[32m41.0767\u001b[0m  0.0110\n",
      "     14       \u001b[36m39.5891\u001b[0m       \u001b[32m40.7836\u001b[0m  0.0110\n",
      "     15       \u001b[36m39.3601\u001b[0m       \u001b[32m40.4914\u001b[0m  0.0108\n",
      "     16       \u001b[36m39.1334\u001b[0m       \u001b[32m40.1997\u001b[0m  0.0106\n",
      "     17       \u001b[36m38.9086\u001b[0m       \u001b[32m39.9089\u001b[0m  0.0110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     18       \u001b[36m38.6854\u001b[0m       \u001b[32m39.6190\u001b[0m  0.0110\n",
      "     19       \u001b[36m38.4637\u001b[0m       \u001b[32m39.3294\u001b[0m  0.0112\n",
      "     20       \u001b[36m38.2434\u001b[0m       \u001b[32m39.0399\u001b[0m  0.0111\n",
      "     21       \u001b[36m38.0241\u001b[0m       \u001b[32m38.7510\u001b[0m  0.0109\n",
      "     22       \u001b[36m37.8060\u001b[0m       \u001b[32m38.4631\u001b[0m  0.0108\n",
      "     23       \u001b[36m37.5894\u001b[0m       \u001b[32m38.1767\u001b[0m  0.0111\n",
      "     24       \u001b[36m37.3750\u001b[0m       \u001b[32m37.8919\u001b[0m  0.0113\n",
      "     25       \u001b[36m37.1630\u001b[0m       \u001b[32m37.6096\u001b[0m  0.0119\n",
      "     26       \u001b[36m36.9539\u001b[0m       \u001b[32m37.3300\u001b[0m  0.0152\n",
      "     27       \u001b[36m36.7478\u001b[0m       \u001b[32m37.0530\u001b[0m  0.0134\n",
      "     28       \u001b[36m36.5449\u001b[0m       \u001b[32m36.7791\u001b[0m  0.0123\n",
      "     29       \u001b[36m36.3455\u001b[0m       \u001b[32m36.5086\u001b[0m  0.0148\n",
      "     30       \u001b[36m36.1499\u001b[0m       \u001b[32m36.2417\u001b[0m  0.0209\n",
      "     31       \u001b[36m35.9581\u001b[0m       \u001b[32m35.9790\u001b[0m  0.0148\n",
      "     32       \u001b[36m35.7706\u001b[0m       \u001b[32m35.7209\u001b[0m  0.0158\n",
      "     33       \u001b[36m35.5877\u001b[0m       \u001b[32m35.4676\u001b[0m  0.0113\n",
      "     34       \u001b[36m35.4096\u001b[0m       \u001b[32m35.2193\u001b[0m  0.0111\n",
      "     35       \u001b[36m35.2364\u001b[0m       \u001b[32m34.9764\u001b[0m  0.0109\n",
      "     36       \u001b[36m35.0682\u001b[0m       \u001b[32m34.7390\u001b[0m  0.0110\n",
      "     37       \u001b[36m34.9054\u001b[0m       \u001b[32m34.5072\u001b[0m  0.0112\n",
      "     38       \u001b[36m34.7483\u001b[0m       \u001b[32m34.2816\u001b[0m  0.0112\n",
      "     39       \u001b[36m34.5971\u001b[0m       \u001b[32m34.0624\u001b[0m  0.0112\n",
      "     40       \u001b[36m34.4516\u001b[0m       \u001b[32m33.8494\u001b[0m  0.0111\n",
      "     41       \u001b[36m34.3121\u001b[0m       \u001b[32m33.6429\u001b[0m  0.0112\n",
      "     42       \u001b[36m34.1787\u001b[0m       \u001b[32m33.4432\u001b[0m  0.0121\n",
      "     43       \u001b[36m34.0514\u001b[0m       \u001b[32m33.2503\u001b[0m  0.0112\n",
      "     44       \u001b[36m33.9302\u001b[0m       \u001b[32m33.0642\u001b[0m  0.0109\n",
      "     45       \u001b[36m33.8152\u001b[0m       \u001b[32m32.8851\u001b[0m  0.0113\n",
      "     46       \u001b[36m33.7064\u001b[0m       \u001b[32m32.7132\u001b[0m  0.0118\n",
      "     47       \u001b[36m33.6037\u001b[0m       \u001b[32m32.5483\u001b[0m  0.0115\n",
      "     48       \u001b[36m33.5070\u001b[0m       \u001b[32m32.3905\u001b[0m  0.0111\n",
      "     49       \u001b[36m33.4162\u001b[0m       \u001b[32m32.2398\u001b[0m  0.0107\n",
      "     50       \u001b[36m33.3313\u001b[0m       \u001b[32m32.0961\u001b[0m  0.0109\n",
      "     51       \u001b[36m33.2520\u001b[0m       \u001b[32m31.9595\u001b[0m  0.0114\n",
      "     52       \u001b[36m33.1782\u001b[0m       \u001b[32m31.8303\u001b[0m  0.0116\n",
      "     53       \u001b[36m33.1097\u001b[0m       \u001b[32m31.7079\u001b[0m  0.0115\n",
      "     54       \u001b[36m33.0464\u001b[0m       \u001b[32m31.5922\u001b[0m  0.0110\n",
      "     55       \u001b[36m32.9879\u001b[0m       \u001b[32m31.4831\u001b[0m  0.0114\n",
      "     56       \u001b[36m32.9340\u001b[0m       \u001b[32m31.3804\u001b[0m  0.0116\n",
      "     57       \u001b[36m32.8846\u001b[0m       \u001b[32m31.2838\u001b[0m  0.0114\n",
      "     58       \u001b[36m32.8394\u001b[0m       \u001b[32m31.1933\u001b[0m  0.0118\n",
      "     59       \u001b[36m32.7982\u001b[0m       \u001b[32m31.1084\u001b[0m  0.0119\n",
      "     60       \u001b[36m32.7607\u001b[0m       \u001b[32m31.0290\u001b[0m  0.0115\n",
      "     61       \u001b[36m32.7266\u001b[0m       \u001b[32m30.9548\u001b[0m  0.0113\n",
      "     62       \u001b[36m32.6956\u001b[0m       \u001b[32m30.8857\u001b[0m  0.0112\n",
      "     63       \u001b[36m32.6675\u001b[0m       \u001b[32m30.8214\u001b[0m  0.0112\n",
      "     64       \u001b[36m32.6420\u001b[0m       \u001b[32m30.7614\u001b[0m  0.0111\n",
      "     65       \u001b[36m32.6190\u001b[0m       \u001b[32m30.7057\u001b[0m  0.0112\n",
      "     66       \u001b[36m32.5982\u001b[0m       \u001b[32m30.6540\u001b[0m  0.0113\n",
      "     67       \u001b[36m32.5794\u001b[0m       \u001b[32m30.6061\u001b[0m  0.0130\n",
      "     68       \u001b[36m32.5624\u001b[0m       \u001b[32m30.5616\u001b[0m  0.0120\n",
      "     69       \u001b[36m32.5471\u001b[0m       \u001b[32m30.5203\u001b[0m  0.0110\n",
      "     70       \u001b[36m32.5332\u001b[0m       \u001b[32m30.4821\u001b[0m  0.0115\n",
      "     71       \u001b[36m32.5206\u001b[0m       \u001b[32m30.4467\u001b[0m  0.0121\n",
      "     72       \u001b[36m32.5092\u001b[0m       \u001b[32m30.4140\u001b[0m  0.0116\n",
      "     73       \u001b[36m32.4990\u001b[0m       \u001b[32m30.3838\u001b[0m  0.0115\n",
      "     74       \u001b[36m32.4896\u001b[0m       \u001b[32m30.3557\u001b[0m  0.0114\n",
      "     75       \u001b[36m32.4811\u001b[0m       \u001b[32m30.3298\u001b[0m  0.0113\n",
      "     76       \u001b[36m32.4733\u001b[0m       \u001b[32m30.3057\u001b[0m  0.0119\n",
      "     77       \u001b[36m32.4661\u001b[0m       \u001b[32m30.2834\u001b[0m  0.0114\n",
      "     78       \u001b[36m32.4594\u001b[0m       \u001b[32m30.2627\u001b[0m  0.0114\n",
      "     79       \u001b[36m32.4533\u001b[0m       \u001b[32m30.2435\u001b[0m  0.0112\n",
      "     80       \u001b[36m32.4476\u001b[0m       \u001b[32m30.2256\u001b[0m  0.0111\n",
      "     81       \u001b[36m32.4423\u001b[0m       \u001b[32m30.2090\u001b[0m  0.0114\n",
      "     82       \u001b[36m32.4374\u001b[0m       \u001b[32m30.1935\u001b[0m  0.0112\n",
      "     83       \u001b[36m32.4328\u001b[0m       \u001b[32m30.1791\u001b[0m  0.0113\n",
      "     84       \u001b[36m32.4285\u001b[0m       \u001b[32m30.1657\u001b[0m  0.0111\n",
      "     85       \u001b[36m32.4245\u001b[0m       \u001b[32m30.1532\u001b[0m  0.0112\n",
      "     86       \u001b[36m32.4207\u001b[0m       \u001b[32m30.1415\u001b[0m  0.0115\n",
      "     87       \u001b[36m32.4171\u001b[0m       \u001b[32m30.1306\u001b[0m  0.0116\n",
      "     88       \u001b[36m32.4137\u001b[0m       \u001b[32m30.1204\u001b[0m  0.0117\n",
      "     89       \u001b[36m32.4104\u001b[0m       \u001b[32m30.1108\u001b[0m  0.0115\n",
      "     90       \u001b[36m32.4073\u001b[0m       \u001b[32m30.1018\u001b[0m  0.0112\n",
      "     91       \u001b[36m32.4043\u001b[0m       \u001b[32m30.0933\u001b[0m  0.0114\n",
      "     92       \u001b[36m32.4014\u001b[0m       \u001b[32m30.0854\u001b[0m  0.0117\n",
      "     93       \u001b[36m32.3986\u001b[0m       \u001b[32m30.0780\u001b[0m  0.0115\n",
      "     94       \u001b[36m32.3959\u001b[0m       \u001b[32m30.0709\u001b[0m  0.0115\n",
      "     95       \u001b[36m32.3934\u001b[0m       \u001b[32m30.0643\u001b[0m  0.0113\n",
      "     96       \u001b[36m32.3909\u001b[0m       \u001b[32m30.0580\u001b[0m  0.0115\n",
      "     97       \u001b[36m32.3884\u001b[0m       \u001b[32m30.0521\u001b[0m  0.0113\n",
      "     98       \u001b[36m32.3861\u001b[0m       \u001b[32m30.0465\u001b[0m  0.0112\n",
      "     99       \u001b[36m32.3838\u001b[0m       \u001b[32m30.0411\u001b[0m  0.0124\n",
      "    100       \u001b[36m32.3815\u001b[0m       \u001b[32m30.0361\u001b[0m  0.0112\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m32.5627\u001b[0m       \u001b[32m31.4925\u001b[0m  0.0117\n",
      "      2       \u001b[36m32.3163\u001b[0m       \u001b[32m31.3093\u001b[0m  0.0113\n",
      "      3       \u001b[36m32.0745\u001b[0m       \u001b[32m31.1299\u001b[0m  0.0114\n",
      "      4       \u001b[36m31.8368\u001b[0m       \u001b[32m30.9543\u001b[0m  0.0113\n",
      "      5       \u001b[36m31.6030\u001b[0m       \u001b[32m30.7821\u001b[0m  0.0124\n",
      "      6       \u001b[36m31.3724\u001b[0m       \u001b[32m30.6129\u001b[0m  0.0174\n",
      "      7       \u001b[36m31.1445\u001b[0m       \u001b[32m30.4464\u001b[0m  0.0132\n",
      "      8       \u001b[36m30.9190\u001b[0m       \u001b[32m30.2826\u001b[0m  0.0127\n",
      "      9       \u001b[36m30.6959\u001b[0m       \u001b[32m30.1212\u001b[0m  0.0120\n",
      "     10       \u001b[36m30.4754\u001b[0m       \u001b[32m29.9621\u001b[0m  0.0115\n",
      "     11       \u001b[36m30.2573\u001b[0m       \u001b[32m29.8054\u001b[0m  0.0134\n",
      "     12       \u001b[36m30.0411\u001b[0m       \u001b[32m29.6510\u001b[0m  0.0114\n",
      "     13       \u001b[36m29.8266\u001b[0m       \u001b[32m29.4987\u001b[0m  0.0121\n",
      "     14       \u001b[36m29.6139\u001b[0m       \u001b[32m29.3485\u001b[0m  0.0111\n",
      "     15       \u001b[36m29.4029\u001b[0m       \u001b[32m29.2004\u001b[0m  0.0109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     16       \u001b[36m29.1936\u001b[0m       \u001b[32m29.0544\u001b[0m  0.0112\n",
      "     17       \u001b[36m28.9863\u001b[0m       \u001b[32m28.9107\u001b[0m  0.0111\n",
      "     18       \u001b[36m28.7809\u001b[0m       \u001b[32m28.7692\u001b[0m  0.0114\n",
      "     19       \u001b[36m28.5774\u001b[0m       \u001b[32m28.6300\u001b[0m  0.0109\n",
      "     20       \u001b[36m28.3760\u001b[0m       \u001b[32m28.4932\u001b[0m  0.0107\n",
      "     21       \u001b[36m28.1765\u001b[0m       \u001b[32m28.3588\u001b[0m  0.0109\n",
      "     22       \u001b[36m27.9791\u001b[0m       \u001b[32m28.2271\u001b[0m  0.0109\n",
      "     23       \u001b[36m27.7839\u001b[0m       \u001b[32m28.0979\u001b[0m  0.0109\n",
      "     24       \u001b[36m27.5910\u001b[0m       \u001b[32m27.9715\u001b[0m  0.0110\n",
      "     25       \u001b[36m27.4007\u001b[0m       \u001b[32m27.8481\u001b[0m  0.0108\n",
      "     26       \u001b[36m27.2133\u001b[0m       \u001b[32m27.7278\u001b[0m  0.0111\n",
      "     27       \u001b[36m27.0289\u001b[0m       \u001b[32m27.6108\u001b[0m  0.0109\n",
      "     28       \u001b[36m26.8479\u001b[0m       \u001b[32m27.4970\u001b[0m  0.0108\n",
      "     29       \u001b[36m26.6702\u001b[0m       \u001b[32m27.3868\u001b[0m  0.0106\n",
      "     30       \u001b[36m26.4963\u001b[0m       \u001b[32m27.2802\u001b[0m  0.0107\n",
      "     31       \u001b[36m26.3261\u001b[0m       \u001b[32m27.1774\u001b[0m  0.0110\n",
      "     32       \u001b[36m26.1601\u001b[0m       \u001b[32m27.0787\u001b[0m  0.0111\n",
      "     33       \u001b[36m25.9986\u001b[0m       \u001b[32m26.9844\u001b[0m  0.0110\n",
      "     34       \u001b[36m25.8417\u001b[0m       \u001b[32m26.8944\u001b[0m  0.0110\n",
      "     35       \u001b[36m25.6894\u001b[0m       \u001b[32m26.8088\u001b[0m  0.0107\n",
      "     36       \u001b[36m25.5420\u001b[0m       \u001b[32m26.7277\u001b[0m  0.0110\n",
      "     37       \u001b[36m25.3997\u001b[0m       \u001b[32m26.6513\u001b[0m  0.0112\n",
      "     38       \u001b[36m25.2625\u001b[0m       \u001b[32m26.5794\u001b[0m  0.0112\n",
      "     39       \u001b[36m25.1306\u001b[0m       \u001b[32m26.5124\u001b[0m  0.0106\n",
      "     40       \u001b[36m25.0040\u001b[0m       \u001b[32m26.4501\u001b[0m  0.0105\n",
      "     41       \u001b[36m24.8830\u001b[0m       \u001b[32m26.3928\u001b[0m  0.0111\n",
      "     42       \u001b[36m24.7674\u001b[0m       \u001b[32m26.3406\u001b[0m  0.0106\n",
      "     43       \u001b[36m24.6573\u001b[0m       \u001b[32m26.2932\u001b[0m  0.0110\n",
      "     44       \u001b[36m24.5526\u001b[0m       \u001b[32m26.2505\u001b[0m  0.0108\n",
      "     45       \u001b[36m24.4535\u001b[0m       \u001b[32m26.2124\u001b[0m  0.0107\n",
      "     46       \u001b[36m24.3599\u001b[0m       \u001b[32m26.1788\u001b[0m  0.0109\n",
      "     47       \u001b[36m24.2717\u001b[0m       \u001b[32m26.1495\u001b[0m  0.0111\n",
      "     48       \u001b[36m24.1887\u001b[0m       \u001b[32m26.1244\u001b[0m  0.0108\n",
      "     49       \u001b[36m24.1108\u001b[0m       \u001b[32m26.1032\u001b[0m  0.0106\n",
      "     50       \u001b[36m24.0380\u001b[0m       \u001b[32m26.0861\u001b[0m  0.0107\n",
      "     51       \u001b[36m23.9701\u001b[0m       \u001b[32m26.0728\u001b[0m  0.0111\n",
      "     52       \u001b[36m23.9069\u001b[0m       \u001b[32m26.0629\u001b[0m  0.0108\n",
      "     53       \u001b[36m23.8484\u001b[0m       \u001b[32m26.0562\u001b[0m  0.0110\n",
      "     54       \u001b[36m23.7942\u001b[0m       \u001b[32m26.0522\u001b[0m  0.0108\n",
      "     55       \u001b[36m23.7440\u001b[0m       \u001b[32m26.0509\u001b[0m  0.0106\n",
      "     56       \u001b[36m23.6979\u001b[0m       26.0519  0.0111\n",
      "     57       \u001b[36m23.6554\u001b[0m       26.0550  0.0112\n",
      "     58       \u001b[36m23.6164\u001b[0m       26.0599  0.0110\n",
      "     59       \u001b[36m23.5807\u001b[0m       26.0664  0.0109\n",
      "     60       \u001b[36m23.5480\u001b[0m       26.0742  0.0107\n",
      "     61       \u001b[36m23.5180\u001b[0m       26.0832  0.0110\n",
      "     62       \u001b[36m23.4907\u001b[0m       26.0931  0.0109\n",
      "     63       \u001b[36m23.4659\u001b[0m       26.1037  0.0110\n",
      "     64       \u001b[36m23.4434\u001b[0m       26.1149  0.0106\n",
      "     65       \u001b[36m23.4228\u001b[0m       26.1265  0.0106\n",
      "     66       \u001b[36m23.4042\u001b[0m       26.1383  0.0110\n",
      "     67       \u001b[36m23.3873\u001b[0m       26.1502  0.0109\n",
      "     68       \u001b[36m23.3719\u001b[0m       26.1622  0.0109\n",
      "     69       \u001b[36m23.3579\u001b[0m       26.1740  0.0108\n",
      "     70       \u001b[36m23.3451\u001b[0m       26.1856  0.0107\n",
      "     71       \u001b[36m23.3334\u001b[0m       26.1970  0.0109\n",
      "     72       \u001b[36m23.3228\u001b[0m       26.2080  0.0111\n",
      "     73       \u001b[36m23.3131\u001b[0m       26.2187  0.0108\n",
      "     74       \u001b[36m23.3042\u001b[0m       26.2290  0.0104\n",
      "     75       \u001b[36m23.2961\u001b[0m       26.2390  0.0107\n",
      "     76       \u001b[36m23.2886\u001b[0m       26.2484  0.0108\n",
      "     77       \u001b[36m23.2816\u001b[0m       26.2575  0.0113\n",
      "     78       \u001b[36m23.2752\u001b[0m       26.2661  0.0109\n",
      "     79       \u001b[36m23.2692\u001b[0m       26.2742  0.0106\n",
      "     80       \u001b[36m23.2636\u001b[0m       26.2819  0.0105\n",
      "     81       \u001b[36m23.2584\u001b[0m       26.2892  0.0109\n",
      "     82       \u001b[36m23.2536\u001b[0m       26.2961  0.0111\n",
      "     83       \u001b[36m23.2491\u001b[0m       26.3025  0.0112\n",
      "     84       \u001b[36m23.2448\u001b[0m       26.3086  0.0106\n",
      "     85       \u001b[36m23.2407\u001b[0m       26.3143  0.0112\n",
      "     86       \u001b[36m23.2369\u001b[0m       26.3197  0.0124\n",
      "     87       \u001b[36m23.2333\u001b[0m       26.3247  0.0110\n",
      "     88       \u001b[36m23.2299\u001b[0m       26.3294  0.0109\n",
      "     89       \u001b[36m23.2266\u001b[0m       26.3338  0.0106\n",
      "     90       \u001b[36m23.2235\u001b[0m       26.3379  0.0110\n",
      "     91       \u001b[36m23.2205\u001b[0m       26.3418  0.0169\n",
      "     92       \u001b[36m23.2176\u001b[0m       26.3454  0.0149\n",
      "     93       \u001b[36m23.2149\u001b[0m       26.3487  0.0126\n",
      "     94       \u001b[36m23.2123\u001b[0m       26.3518  0.0135\n",
      "     95       \u001b[36m23.2098\u001b[0m       26.3547  0.0138\n",
      "     96       \u001b[36m23.2074\u001b[0m       26.3574  0.0158\n",
      "     97       \u001b[36m23.2051\u001b[0m       26.3599  0.0133\n",
      "     98       \u001b[36m23.2029\u001b[0m       26.3622  0.0120\n",
      "     99       \u001b[36m23.2008\u001b[0m       26.3643  0.0113\n",
      "    100       \u001b[36m23.1987\u001b[0m       26.3663  0.0115\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m42.5160\u001b[0m       \u001b[32m33.7039\u001b[0m  0.0113\n",
      "      2       \u001b[36m42.0097\u001b[0m       \u001b[32m33.3447\u001b[0m  0.0115\n",
      "      3       \u001b[36m41.5215\u001b[0m       \u001b[32m32.9997\u001b[0m  0.0118\n",
      "      4       \u001b[36m41.0491\u001b[0m       \u001b[32m32.6671\u001b[0m  0.0113\n",
      "      5       \u001b[36m40.5901\u001b[0m       \u001b[32m32.3446\u001b[0m  0.0115\n",
      "      6       \u001b[36m40.1420\u001b[0m       \u001b[32m32.0316\u001b[0m  0.0115\n",
      "      7       \u001b[36m39.7035\u001b[0m       \u001b[32m31.7272\u001b[0m  0.0119\n",
      "      8       \u001b[36m39.2736\u001b[0m       \u001b[32m31.4297\u001b[0m  0.0116\n",
      "      9       \u001b[36m38.8507\u001b[0m       \u001b[32m31.1392\u001b[0m  0.0122\n",
      "     10       \u001b[36m38.4344\u001b[0m       \u001b[32m30.8547\u001b[0m  0.0114\n",
      "     11       \u001b[36m38.0243\u001b[0m       \u001b[32m30.5761\u001b[0m  0.0115\n",
      "     12       \u001b[36m37.6196\u001b[0m       \u001b[32m30.3025\u001b[0m  0.0122\n",
      "     13       \u001b[36m37.2201\u001b[0m       \u001b[32m30.0340\u001b[0m  0.0116\n",
      "     14       \u001b[36m36.8253\u001b[0m       \u001b[32m29.7702\u001b[0m  0.0120\n",
      "     15       \u001b[36m36.4352\u001b[0m       \u001b[32m29.5111\u001b[0m  0.0118\n",
      "     16       \u001b[36m36.0500\u001b[0m       \u001b[32m29.2570\u001b[0m  0.0115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m35.6697\u001b[0m       \u001b[32m29.0084\u001b[0m  0.0126\n",
      "     18       \u001b[36m35.2946\u001b[0m       \u001b[32m28.7652\u001b[0m  0.0116\n",
      "     19       \u001b[36m34.9247\u001b[0m       \u001b[32m28.5278\u001b[0m  0.0116\n",
      "     20       \u001b[36m34.5600\u001b[0m       \u001b[32m28.2968\u001b[0m  0.0118\n",
      "     21       \u001b[36m34.2004\u001b[0m       \u001b[32m28.0720\u001b[0m  0.0115\n",
      "     22       \u001b[36m33.8451\u001b[0m       \u001b[32m27.8539\u001b[0m  0.0116\n",
      "     23       \u001b[36m33.4940\u001b[0m       \u001b[32m27.6440\u001b[0m  0.0117\n",
      "     24       \u001b[36m33.1488\u001b[0m       \u001b[32m27.4429\u001b[0m  0.0120\n",
      "     25       \u001b[36m32.8102\u001b[0m       \u001b[32m27.2520\u001b[0m  0.0120\n",
      "     26       \u001b[36m32.4795\u001b[0m       \u001b[32m27.0727\u001b[0m  0.0115\n",
      "     27       \u001b[36m32.1587\u001b[0m       \u001b[32m26.9065\u001b[0m  0.0114\n",
      "     28       \u001b[36m31.8491\u001b[0m       \u001b[32m26.7543\u001b[0m  0.0113\n",
      "     29       \u001b[36m31.5515\u001b[0m       \u001b[32m26.6171\u001b[0m  0.0115\n",
      "     30       \u001b[36m31.2673\u001b[0m       \u001b[32m26.4961\u001b[0m  0.0118\n",
      "     31       \u001b[36m30.9984\u001b[0m       \u001b[32m26.3916\u001b[0m  0.0115\n",
      "     32       \u001b[36m30.7452\u001b[0m       \u001b[32m26.3041\u001b[0m  0.0115\n",
      "     33       \u001b[36m30.5084\u001b[0m       \u001b[32m26.2339\u001b[0m  0.0119\n",
      "     34       \u001b[36m30.2893\u001b[0m       \u001b[32m26.1805\u001b[0m  0.0123\n",
      "     35       \u001b[36m30.0878\u001b[0m       \u001b[32m26.1435\u001b[0m  0.0116\n",
      "     36       \u001b[36m29.9037\u001b[0m       \u001b[32m26.1222\u001b[0m  0.0118\n",
      "     37       \u001b[36m29.7371\u001b[0m       \u001b[32m26.1155\u001b[0m  0.0119\n",
      "     38       \u001b[36m29.5878\u001b[0m       26.1220  0.0117\n",
      "     39       \u001b[36m29.4553\u001b[0m       26.1403  0.0118\n",
      "     40       \u001b[36m29.3387\u001b[0m       26.1685  0.0120\n",
      "     41       \u001b[36m29.2371\u001b[0m       26.2051  0.0117\n",
      "     42       \u001b[36m29.1491\u001b[0m       26.2482  0.0119\n",
      "     43       \u001b[36m29.0736\u001b[0m       26.2961  0.0119\n",
      "     44       \u001b[36m29.0093\u001b[0m       26.3473  0.0116\n",
      "     45       \u001b[36m28.9548\u001b[0m       26.4003  0.0115\n",
      "     46       \u001b[36m28.9090\u001b[0m       26.4537  0.0115\n",
      "     47       \u001b[36m28.8705\u001b[0m       26.5066  0.0115\n",
      "     48       \u001b[36m28.8384\u001b[0m       26.5579  0.0117\n",
      "     49       \u001b[36m28.8116\u001b[0m       26.6072  0.0119\n",
      "     50       \u001b[36m28.7890\u001b[0m       26.6539  0.0117\n",
      "     51       \u001b[36m28.7701\u001b[0m       26.6978  0.0114\n",
      "     52       \u001b[36m28.7542\u001b[0m       26.7385  0.0113\n",
      "     53       \u001b[36m28.7406\u001b[0m       26.7762  0.0114\n",
      "     54       \u001b[36m28.7289\u001b[0m       26.8106  0.0118\n",
      "     55       \u001b[36m28.7187\u001b[0m       26.8419  0.0119\n",
      "     56       \u001b[36m28.7097\u001b[0m       26.8702  0.0119\n",
      "     57       \u001b[36m28.7017\u001b[0m       26.8955  0.0114\n",
      "     58       \u001b[36m28.6943\u001b[0m       26.9182  0.0117\n",
      "     59       \u001b[36m28.6876\u001b[0m       26.9384  0.0118\n",
      "     60       \u001b[36m28.6813\u001b[0m       26.9564  0.0113\n",
      "     61       \u001b[36m28.6755\u001b[0m       26.9724  0.0117\n",
      "     62       \u001b[36m28.6700\u001b[0m       26.9868  0.0117\n",
      "     63       \u001b[36m28.6647\u001b[0m       26.9993  0.0116\n",
      "     64       \u001b[36m28.6597\u001b[0m       27.0105  0.0118\n",
      "     65       \u001b[36m28.6549\u001b[0m       27.0202  0.0120\n",
      "     66       \u001b[36m28.6503\u001b[0m       27.0290  0.0116\n",
      "     67       \u001b[36m28.6458\u001b[0m       27.0364  0.0115\n",
      "     68       \u001b[36m28.6414\u001b[0m       27.0432  0.0121\n",
      "     69       \u001b[36m28.6371\u001b[0m       27.0492  0.0122\n",
      "     70       \u001b[36m28.6330\u001b[0m       27.0541  0.0160\n",
      "     71       \u001b[36m28.6289\u001b[0m       27.0587  0.0125\n",
      "     72       \u001b[36m28.6249\u001b[0m       27.0627  0.0123\n",
      "     73       \u001b[36m28.6210\u001b[0m       27.0660  0.0129\n",
      "     74       \u001b[36m28.6172\u001b[0m       27.0690  0.0126\n",
      "     75       \u001b[36m28.6135\u001b[0m       27.0717  0.0129\n",
      "     76       \u001b[36m28.6099\u001b[0m       27.0739  0.0117\n",
      "     77       \u001b[36m28.6063\u001b[0m       27.0756  0.0124\n",
      "     78       \u001b[36m28.6028\u001b[0m       27.0772  0.0117\n",
      "     79       \u001b[36m28.5993\u001b[0m       27.0787  0.0118\n",
      "     80       \u001b[36m28.5960\u001b[0m       27.0799  0.0117\n",
      "     81       \u001b[36m28.5928\u001b[0m       27.0809  0.0116\n",
      "     82       \u001b[36m28.5896\u001b[0m       27.0819  0.0119\n",
      "     83       \u001b[36m28.5865\u001b[0m       27.0828  0.0119\n",
      "     84       \u001b[36m28.5835\u001b[0m       27.0833  0.0115\n",
      "     85       \u001b[36m28.5806\u001b[0m       27.0840  0.0117\n",
      "     86       \u001b[36m28.5777\u001b[0m       27.0845  0.0114\n",
      "     87       \u001b[36m28.5749\u001b[0m       27.0849  0.0119\n",
      "     88       \u001b[36m28.5721\u001b[0m       27.0852  0.0120\n",
      "     89       \u001b[36m28.5694\u001b[0m       27.0858  0.0116\n",
      "     90       \u001b[36m28.5668\u001b[0m       27.0861  0.0119\n",
      "     91       \u001b[36m28.5643\u001b[0m       27.0862  0.0118\n",
      "     92       \u001b[36m28.5618\u001b[0m       27.0863  0.0117\n",
      "     93       \u001b[36m28.5593\u001b[0m       27.0865  0.0118\n",
      "     94       \u001b[36m28.5569\u001b[0m       27.0865  0.0117\n",
      "     95       \u001b[36m28.5546\u001b[0m       27.0867  0.0117\n",
      "     96       \u001b[36m28.5523\u001b[0m       27.0864  0.0116\n",
      "     97       \u001b[36m28.5501\u001b[0m       27.0865  0.0116\n",
      "     98       \u001b[36m28.5479\u001b[0m       27.0862  0.0117\n",
      "     99       \u001b[36m28.5456\u001b[0m       27.0861  0.0115\n",
      "    100       \u001b[36m28.5434\u001b[0m       27.0857  0.0121\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m43.5441\u001b[0m       \u001b[32m45.4783\u001b[0m  0.0127\n",
      "      2       \u001b[36m43.0304\u001b[0m       \u001b[32m44.7583\u001b[0m  0.0124\n",
      "      3       \u001b[36m42.4707\u001b[0m       \u001b[32m43.9452\u001b[0m  0.0120\n",
      "      4       \u001b[36m41.8212\u001b[0m       \u001b[32m43.0010\u001b[0m  0.0127\n",
      "      5       \u001b[36m41.0833\u001b[0m       \u001b[32m41.9789\u001b[0m  0.0123\n",
      "      6       \u001b[36m40.2820\u001b[0m       \u001b[32m40.9154\u001b[0m  0.0121\n",
      "      7       \u001b[36m39.4403\u001b[0m       \u001b[32m39.7949\u001b[0m  0.0125\n",
      "      8       \u001b[36m38.5418\u001b[0m       \u001b[32m38.5449\u001b[0m  0.0127\n",
      "      9       \u001b[36m37.5621\u001b[0m       \u001b[32m37.1560\u001b[0m  0.0124\n",
      "     10       \u001b[36m36.5401\u001b[0m       \u001b[32m35.7010\u001b[0m  0.0121\n",
      "     11       \u001b[36m35.5681\u001b[0m       \u001b[32m34.3178\u001b[0m  0.0124\n",
      "     12       \u001b[36m34.7785\u001b[0m       \u001b[32m33.1673\u001b[0m  0.0124\n",
      "     13       \u001b[36m34.2649\u001b[0m       \u001b[32m32.3593\u001b[0m  0.0125\n",
      "     14       \u001b[36m34.0246\u001b[0m       \u001b[32m31.8781\u001b[0m  0.0136\n",
      "     15       \u001b[36m33.9290\u001b[0m       \u001b[32m31.6257\u001b[0m  0.0124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     16       \u001b[36m33.8371\u001b[0m       \u001b[32m31.5041\u001b[0m  0.0132\n",
      "     17       \u001b[36m33.7040\u001b[0m       \u001b[32m31.4585\u001b[0m  0.0127\n",
      "     18       \u001b[36m33.5744\u001b[0m       \u001b[32m31.4385\u001b[0m  0.0125\n",
      "     19       \u001b[36m33.4645\u001b[0m       \u001b[32m31.3947\u001b[0m  0.0125\n",
      "     20       \u001b[36m33.3677\u001b[0m       \u001b[32m31.3129\u001b[0m  0.0125\n",
      "     21       \u001b[36m33.2779\u001b[0m       \u001b[32m31.2020\u001b[0m  0.0124\n",
      "     22       \u001b[36m33.1947\u001b[0m       \u001b[32m31.0824\u001b[0m  0.0126\n",
      "     23       \u001b[36m33.1188\u001b[0m       \u001b[32m30.9719\u001b[0m  0.0123\n",
      "     24       \u001b[36m33.0509\u001b[0m       \u001b[32m30.8801\u001b[0m  0.0123\n",
      "     25       \u001b[36m32.9904\u001b[0m       \u001b[32m30.8059\u001b[0m  0.0120\n",
      "     26       \u001b[36m32.9366\u001b[0m       \u001b[32m30.7452\u001b[0m  0.0124\n",
      "     27       \u001b[36m32.8881\u001b[0m       \u001b[32m30.6977\u001b[0m  0.0126\n",
      "     28       \u001b[36m32.8432\u001b[0m       \u001b[32m30.6584\u001b[0m  0.0125\n",
      "     29       \u001b[36m32.8020\u001b[0m       \u001b[32m30.6251\u001b[0m  0.0125\n",
      "     30       \u001b[36m32.7644\u001b[0m       \u001b[32m30.5961\u001b[0m  0.0124\n",
      "     31       \u001b[36m32.7296\u001b[0m       \u001b[32m30.5700\u001b[0m  0.0122\n",
      "     32       \u001b[36m32.6978\u001b[0m       \u001b[32m30.5420\u001b[0m  0.0125\n",
      "     33       \u001b[36m32.6685\u001b[0m       \u001b[32m30.5128\u001b[0m  0.0129\n",
      "     34       \u001b[36m32.6415\u001b[0m       \u001b[32m30.4858\u001b[0m  0.0126\n",
      "     35       \u001b[36m32.6165\u001b[0m       \u001b[32m30.4645\u001b[0m  0.0123\n",
      "     36       \u001b[36m32.5934\u001b[0m       \u001b[32m30.4455\u001b[0m  0.0122\n",
      "     37       \u001b[36m32.5719\u001b[0m       \u001b[32m30.4293\u001b[0m  0.0122\n",
      "     38       \u001b[36m32.5519\u001b[0m       \u001b[32m30.4160\u001b[0m  0.0124\n",
      "     39       \u001b[36m32.5334\u001b[0m       \u001b[32m30.4029\u001b[0m  0.0126\n",
      "     40       \u001b[36m32.5160\u001b[0m       \u001b[32m30.3886\u001b[0m  0.0123\n",
      "     41       \u001b[36m32.4996\u001b[0m       \u001b[32m30.3764\u001b[0m  0.0123\n",
      "     42       \u001b[36m32.4843\u001b[0m       \u001b[32m30.3663\u001b[0m  0.0124\n",
      "     43       \u001b[36m32.4700\u001b[0m       \u001b[32m30.3594\u001b[0m  0.0124\n",
      "     44       \u001b[36m32.4566\u001b[0m       \u001b[32m30.3519\u001b[0m  0.0127\n",
      "     45       \u001b[36m32.4442\u001b[0m       \u001b[32m30.3418\u001b[0m  0.0124\n",
      "     46       \u001b[36m32.4325\u001b[0m       \u001b[32m30.3334\u001b[0m  0.0139\n",
      "     47       \u001b[36m32.4219\u001b[0m       \u001b[32m30.3261\u001b[0m  0.0133\n",
      "     48       \u001b[36m32.4114\u001b[0m       \u001b[32m30.3197\u001b[0m  0.0129\n",
      "     49       \u001b[36m32.4016\u001b[0m       \u001b[32m30.3145\u001b[0m  0.0129\n",
      "     50       \u001b[36m32.3924\u001b[0m       \u001b[32m30.3113\u001b[0m  0.0128\n",
      "     51       \u001b[36m32.3837\u001b[0m       \u001b[32m30.3072\u001b[0m  0.0151\n",
      "     52       \u001b[36m32.3754\u001b[0m       \u001b[32m30.3025\u001b[0m  0.0148\n",
      "     53       \u001b[36m32.3675\u001b[0m       \u001b[32m30.2960\u001b[0m  0.0133\n",
      "     54       \u001b[36m32.3599\u001b[0m       \u001b[32m30.2931\u001b[0m  0.0124\n",
      "     55       \u001b[36m32.3529\u001b[0m       \u001b[32m30.2904\u001b[0m  0.0123\n",
      "     56       \u001b[36m32.3463\u001b[0m       \u001b[32m30.2844\u001b[0m  0.0124\n",
      "     57       \u001b[36m32.3397\u001b[0m       \u001b[32m30.2790\u001b[0m  0.0126\n",
      "     58       \u001b[36m32.3335\u001b[0m       \u001b[32m30.2753\u001b[0m  0.0127\n",
      "     59       \u001b[36m32.3277\u001b[0m       \u001b[32m30.2702\u001b[0m  0.0126\n",
      "     60       \u001b[36m32.3220\u001b[0m       \u001b[32m30.2653\u001b[0m  0.0126\n",
      "     61       \u001b[36m32.3166\u001b[0m       \u001b[32m30.2621\u001b[0m  0.0128\n",
      "     62       \u001b[36m32.3114\u001b[0m       \u001b[32m30.2579\u001b[0m  0.0126\n",
      "     63       \u001b[36m32.3062\u001b[0m       \u001b[32m30.2541\u001b[0m  0.0125\n",
      "     64       \u001b[36m32.3012\u001b[0m       \u001b[32m30.2516\u001b[0m  0.0125\n",
      "     65       \u001b[36m32.2964\u001b[0m       \u001b[32m30.2490\u001b[0m  0.0127\n",
      "     66       \u001b[36m32.2919\u001b[0m       \u001b[32m30.2461\u001b[0m  0.0127\n",
      "     67       \u001b[36m32.2874\u001b[0m       \u001b[32m30.2401\u001b[0m  0.0125\n",
      "     68       \u001b[36m32.2829\u001b[0m       \u001b[32m30.2366\u001b[0m  0.0123\n",
      "     69       \u001b[36m32.2788\u001b[0m       \u001b[32m30.2332\u001b[0m  0.0125\n",
      "     70       \u001b[36m32.2747\u001b[0m       \u001b[32m30.2298\u001b[0m  0.0126\n",
      "     71       \u001b[36m32.2709\u001b[0m       \u001b[32m30.2265\u001b[0m  0.0127\n",
      "     72       \u001b[36m32.2671\u001b[0m       \u001b[32m30.2223\u001b[0m  0.0124\n",
      "     73       \u001b[36m32.2634\u001b[0m       \u001b[32m30.2187\u001b[0m  0.0123\n",
      "     74       \u001b[36m32.2599\u001b[0m       \u001b[32m30.2164\u001b[0m  0.0124\n",
      "     75       \u001b[36m32.2565\u001b[0m       \u001b[32m30.2134\u001b[0m  0.0123\n",
      "     76       \u001b[36m32.2531\u001b[0m       \u001b[32m30.2100\u001b[0m  0.0126\n",
      "     77       \u001b[36m32.2499\u001b[0m       \u001b[32m30.2078\u001b[0m  0.0126\n",
      "     78       \u001b[36m32.2467\u001b[0m       \u001b[32m30.2063\u001b[0m  0.0126\n",
      "     79       \u001b[36m32.2437\u001b[0m       \u001b[32m30.2045\u001b[0m  0.0123\n",
      "     80       \u001b[36m32.2407\u001b[0m       \u001b[32m30.2020\u001b[0m  0.0122\n",
      "     81       \u001b[36m32.2378\u001b[0m       \u001b[32m30.2001\u001b[0m  0.0125\n",
      "     82       \u001b[36m32.2348\u001b[0m       \u001b[32m30.1992\u001b[0m  0.0122\n",
      "     83       \u001b[36m32.2320\u001b[0m       \u001b[32m30.1969\u001b[0m  0.0122\n",
      "     84       \u001b[36m32.2292\u001b[0m       \u001b[32m30.1949\u001b[0m  0.0122\n",
      "     85       \u001b[36m32.2266\u001b[0m       \u001b[32m30.1923\u001b[0m  0.0119\n",
      "     86       \u001b[36m32.2239\u001b[0m       \u001b[32m30.1902\u001b[0m  0.0126\n",
      "     87       \u001b[36m32.2214\u001b[0m       \u001b[32m30.1900\u001b[0m  0.0229\n",
      "     88       \u001b[36m32.2189\u001b[0m       \u001b[32m30.1890\u001b[0m  0.0128\n",
      "     89       \u001b[36m32.2165\u001b[0m       \u001b[32m30.1865\u001b[0m  0.0122\n",
      "     90       \u001b[36m32.2143\u001b[0m       \u001b[32m30.1853\u001b[0m  0.0119\n",
      "     91       \u001b[36m32.2121\u001b[0m       \u001b[32m30.1839\u001b[0m  0.0119\n",
      "     92       \u001b[36m32.2099\u001b[0m       \u001b[32m30.1807\u001b[0m  0.0139\n",
      "     93       \u001b[36m32.2077\u001b[0m       \u001b[32m30.1789\u001b[0m  0.0119\n",
      "     94       \u001b[36m32.2057\u001b[0m       \u001b[32m30.1784\u001b[0m  0.0121\n",
      "     95       \u001b[36m32.2036\u001b[0m       \u001b[32m30.1758\u001b[0m  0.0118\n",
      "     96       \u001b[36m32.2015\u001b[0m       \u001b[32m30.1734\u001b[0m  0.0128\n",
      "     97       \u001b[36m32.1996\u001b[0m       \u001b[32m30.1726\u001b[0m  0.0123\n",
      "     98       \u001b[36m32.1977\u001b[0m       \u001b[32m30.1699\u001b[0m  0.0119\n",
      "     99       \u001b[36m32.1957\u001b[0m       \u001b[32m30.1670\u001b[0m  0.0119\n",
      "    100       \u001b[36m32.1938\u001b[0m       \u001b[32m30.1651\u001b[0m  0.0125\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m34.1892\u001b[0m       \u001b[32m32.5959\u001b[0m  0.0127\n",
      "      2       \u001b[36m33.5871\u001b[0m       \u001b[32m32.1096\u001b[0m  0.0125\n",
      "      3       \u001b[36m32.9489\u001b[0m       \u001b[32m31.5816\u001b[0m  0.0123\n",
      "      4       \u001b[36m32.2346\u001b[0m       \u001b[32m30.9953\u001b[0m  0.0124\n",
      "      5       \u001b[36m31.4307\u001b[0m       \u001b[32m30.3426\u001b[0m  0.0123\n",
      "      6       \u001b[36m30.5347\u001b[0m       \u001b[32m29.6348\u001b[0m  0.0122\n",
      "      7       \u001b[36m29.5647\u001b[0m       \u001b[32m28.8952\u001b[0m  0.0124\n",
      "      8       \u001b[36m28.5442\u001b[0m       \u001b[32m28.1569\u001b[0m  0.0125\n",
      "      9       \u001b[36m27.4779\u001b[0m       \u001b[32m27.4994\u001b[0m  0.0121\n",
      "     10       \u001b[36m26.4459\u001b[0m       \u001b[32m27.0859\u001b[0m  0.0122\n",
      "     11       \u001b[36m25.6137\u001b[0m       \u001b[32m27.0485\u001b[0m  0.0118\n",
      "     12       \u001b[36m25.1202\u001b[0m       27.3219  0.0120\n",
      "     13       \u001b[36m24.9377\u001b[0m       27.5761  0.0119\n",
      "     14       \u001b[36m24.8546\u001b[0m       27.5426  0.0118\n",
      "     15       \u001b[36m24.7264\u001b[0m       27.3047  0.0119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     16       \u001b[36m24.5794\u001b[0m       27.0576  0.0118\n",
      "     17       \u001b[36m24.4588\u001b[0m       \u001b[32m26.8844\u001b[0m  0.0128\n",
      "     18       \u001b[36m24.3621\u001b[0m       \u001b[32m26.7870\u001b[0m  0.0125\n",
      "     19       \u001b[36m24.2713\u001b[0m       \u001b[32m26.7445\u001b[0m  0.0139\n",
      "     20       \u001b[36m24.1794\u001b[0m       \u001b[32m26.7381\u001b[0m  0.0128\n",
      "     21       \u001b[36m24.0894\u001b[0m       26.7497  0.0159\n",
      "     22       \u001b[36m24.0058\u001b[0m       26.7645  0.0139\n",
      "     23       \u001b[36m23.9306\u001b[0m       26.7722  0.0123\n",
      "     24       \u001b[36m23.8626\u001b[0m       26.7689  0.0124\n",
      "     25       \u001b[36m23.8012\u001b[0m       26.7595  0.0131\n",
      "     26       \u001b[36m23.7457\u001b[0m       26.7514  0.0169\n",
      "     27       \u001b[36m23.6956\u001b[0m       26.7464  0.0133\n",
      "     28       \u001b[36m23.6499\u001b[0m       26.7446  0.0123\n",
      "     29       \u001b[36m23.6078\u001b[0m       26.7457  0.0122\n",
      "     30       \u001b[36m23.5689\u001b[0m       26.7496  0.0129\n",
      "     31       \u001b[36m23.5332\u001b[0m       26.7547  0.0126\n",
      "     32       \u001b[36m23.5003\u001b[0m       26.7578  0.0126\n",
      "     33       \u001b[36m23.4698\u001b[0m       26.7603  0.0123\n",
      "     34       \u001b[36m23.4416\u001b[0m       26.7635  0.0123\n",
      "     35       \u001b[36m23.4154\u001b[0m       26.7659  0.0128\n",
      "     36       \u001b[36m23.3913\u001b[0m       26.7658  0.0125\n",
      "     37       \u001b[36m23.3693\u001b[0m       26.7644  0.0131\n",
      "     38       \u001b[36m23.3486\u001b[0m       26.7638  0.0125\n",
      "     39       \u001b[36m23.3295\u001b[0m       26.7655  0.0124\n",
      "     40       \u001b[36m23.3118\u001b[0m       26.7681  0.0127\n",
      "     41       \u001b[36m23.2954\u001b[0m       26.7685  0.0123\n",
      "     42       \u001b[36m23.2800\u001b[0m       26.7665  0.0121\n",
      "     43       \u001b[36m23.2654\u001b[0m       26.7641  0.0122\n",
      "     44       \u001b[36m23.2521\u001b[0m       26.7611  0.0126\n",
      "     45       \u001b[36m23.2396\u001b[0m       26.7581  0.0126\n",
      "     46       \u001b[36m23.2278\u001b[0m       26.7546  0.0125\n",
      "     47       \u001b[36m23.2169\u001b[0m       26.7499  0.0125\n",
      "     48       \u001b[36m23.2067\u001b[0m       26.7440  0.0121\n",
      "     49       \u001b[36m23.1972\u001b[0m       \u001b[32m26.7366\u001b[0m  0.0124\n",
      "     50       \u001b[36m23.1882\u001b[0m       \u001b[32m26.7289\u001b[0m  0.0121\n",
      "     51       \u001b[36m23.1799\u001b[0m       \u001b[32m26.7216\u001b[0m  0.0122\n",
      "     52       \u001b[36m23.1720\u001b[0m       \u001b[32m26.7142\u001b[0m  0.0124\n",
      "     53       \u001b[36m23.1647\u001b[0m       \u001b[32m26.7054\u001b[0m  0.0118\n",
      "     54       \u001b[36m23.1578\u001b[0m       \u001b[32m26.6963\u001b[0m  0.0126\n",
      "     55       \u001b[36m23.1515\u001b[0m       \u001b[32m26.6862\u001b[0m  0.0122\n",
      "     56       \u001b[36m23.1454\u001b[0m       \u001b[32m26.6756\u001b[0m  0.0117\n",
      "     57       \u001b[36m23.1398\u001b[0m       \u001b[32m26.6655\u001b[0m  0.0120\n",
      "     58       \u001b[36m23.1344\u001b[0m       \u001b[32m26.6556\u001b[0m  0.0120\n",
      "     59       \u001b[36m23.1293\u001b[0m       \u001b[32m26.6459\u001b[0m  0.0125\n",
      "     60       \u001b[36m23.1245\u001b[0m       \u001b[32m26.6379\u001b[0m  0.0126\n",
      "     61       \u001b[36m23.1200\u001b[0m       \u001b[32m26.6307\u001b[0m  0.0124\n",
      "     62       \u001b[36m23.1157\u001b[0m       \u001b[32m26.6240\u001b[0m  0.0121\n",
      "     63       \u001b[36m23.1117\u001b[0m       \u001b[32m26.6161\u001b[0m  0.0237\n",
      "     64       \u001b[36m23.1078\u001b[0m       \u001b[32m26.6083\u001b[0m  0.0119\n",
      "     65       \u001b[36m23.1041\u001b[0m       \u001b[32m26.6012\u001b[0m  0.0121\n",
      "     66       \u001b[36m23.1007\u001b[0m       \u001b[32m26.5957\u001b[0m  0.0124\n",
      "     67       \u001b[36m23.0974\u001b[0m       \u001b[32m26.5915\u001b[0m  0.0125\n",
      "     68       \u001b[36m23.0942\u001b[0m       \u001b[32m26.5862\u001b[0m  0.0130\n",
      "     69       \u001b[36m23.0913\u001b[0m       \u001b[32m26.5812\u001b[0m  0.0125\n",
      "     70       \u001b[36m23.0884\u001b[0m       \u001b[32m26.5761\u001b[0m  0.0124\n",
      "     71       \u001b[36m23.0857\u001b[0m       \u001b[32m26.5705\u001b[0m  0.0122\n",
      "     72       \u001b[36m23.0831\u001b[0m       \u001b[32m26.5658\u001b[0m  0.0121\n",
      "     73       \u001b[36m23.0806\u001b[0m       \u001b[32m26.5615\u001b[0m  0.0118\n",
      "     74       \u001b[36m23.0783\u001b[0m       \u001b[32m26.5570\u001b[0m  0.0137\n",
      "     75       \u001b[36m23.0761\u001b[0m       \u001b[32m26.5532\u001b[0m  0.0120\n",
      "     76       \u001b[36m23.0739\u001b[0m       \u001b[32m26.5503\u001b[0m  0.0122\n",
      "     77       \u001b[36m23.0718\u001b[0m       \u001b[32m26.5474\u001b[0m  0.0122\n",
      "     78       \u001b[36m23.0697\u001b[0m       \u001b[32m26.5443\u001b[0m  0.0133\n",
      "     79       \u001b[36m23.0678\u001b[0m       \u001b[32m26.5419\u001b[0m  0.0126\n",
      "     80       \u001b[36m23.0659\u001b[0m       \u001b[32m26.5395\u001b[0m  0.0121\n",
      "     81       \u001b[36m23.0641\u001b[0m       \u001b[32m26.5375\u001b[0m  0.0121\n",
      "     82       \u001b[36m23.0623\u001b[0m       \u001b[32m26.5356\u001b[0m  0.0123\n",
      "     83       \u001b[36m23.0606\u001b[0m       \u001b[32m26.5331\u001b[0m  0.0126\n",
      "     84       \u001b[36m23.0589\u001b[0m       \u001b[32m26.5308\u001b[0m  0.0133\n",
      "     85       \u001b[36m23.0573\u001b[0m       \u001b[32m26.5292\u001b[0m  0.0125\n",
      "     86       \u001b[36m23.0558\u001b[0m       \u001b[32m26.5273\u001b[0m  0.0124\n",
      "     87       \u001b[36m23.0543\u001b[0m       \u001b[32m26.5248\u001b[0m  0.0119\n",
      "     88       \u001b[36m23.0529\u001b[0m       \u001b[32m26.5216\u001b[0m  0.0124\n",
      "     89       \u001b[36m23.0515\u001b[0m       \u001b[32m26.5194\u001b[0m  0.0122\n",
      "     90       \u001b[36m23.0502\u001b[0m       \u001b[32m26.5185\u001b[0m  0.0121\n",
      "     91       \u001b[36m23.0488\u001b[0m       \u001b[32m26.5168\u001b[0m  0.0117\n",
      "     92       \u001b[36m23.0476\u001b[0m       \u001b[32m26.5144\u001b[0m  0.0120\n",
      "     93       \u001b[36m23.0464\u001b[0m       \u001b[32m26.5133\u001b[0m  0.0122\n",
      "     94       \u001b[36m23.0452\u001b[0m       \u001b[32m26.5123\u001b[0m  0.0125\n",
      "     95       \u001b[36m23.0441\u001b[0m       \u001b[32m26.5107\u001b[0m  0.0152\n",
      "     96       \u001b[36m23.0429\u001b[0m       \u001b[32m26.5096\u001b[0m  0.0145\n",
      "     97       \u001b[36m23.0418\u001b[0m       \u001b[32m26.5094\u001b[0m  0.0127\n",
      "     98       \u001b[36m23.0408\u001b[0m       \u001b[32m26.5087\u001b[0m  0.0129\n",
      "     99       \u001b[36m23.0397\u001b[0m       \u001b[32m26.5078\u001b[0m  0.0133\n",
      "    100       \u001b[36m23.0387\u001b[0m       \u001b[32m26.5066\u001b[0m  0.0131\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m40.3027\u001b[0m       \u001b[32m31.9080\u001b[0m  0.0133\n",
      "      2       \u001b[36m39.6214\u001b[0m       \u001b[32m31.4694\u001b[0m  0.0136\n",
      "      3       \u001b[36m38.9417\u001b[0m       \u001b[32m31.0187\u001b[0m  0.0130\n",
      "      4       \u001b[36m38.2151\u001b[0m       \u001b[32m30.5216\u001b[0m  0.0125\n",
      "      5       \u001b[36m37.4132\u001b[0m       \u001b[32m29.9688\u001b[0m  0.0137\n",
      "      6       \u001b[36m36.5481\u001b[0m       \u001b[32m29.3740\u001b[0m  0.0122\n",
      "      7       \u001b[36m35.6429\u001b[0m       \u001b[32m28.7464\u001b[0m  0.0124\n",
      "      8       \u001b[36m34.6772\u001b[0m       \u001b[32m28.0945\u001b[0m  0.0124\n",
      "      9       \u001b[36m33.6218\u001b[0m       \u001b[32m27.4774\u001b[0m  0.0124\n",
      "     10       \u001b[36m32.5335\u001b[0m       \u001b[32m27.0182\u001b[0m  0.0125\n",
      "     11       \u001b[36m31.5514\u001b[0m       \u001b[32m26.8731\u001b[0m  0.0124\n",
      "     12       \u001b[36m30.8393\u001b[0m       27.1298  0.0123\n",
      "     13       \u001b[36m30.4949\u001b[0m       27.6555  0.0121\n",
      "     14       \u001b[36m30.4331\u001b[0m       28.0397  0.0122\n",
      "     15       \u001b[36m30.3963\u001b[0m       27.9999  0.0123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     16       \u001b[36m30.2509\u001b[0m       27.6959  0.0125\n",
      "     17       \u001b[36m30.0638\u001b[0m       27.3790  0.0125\n",
      "     18       \u001b[36m29.9014\u001b[0m       27.1578  0.0125\n",
      "     19       \u001b[36m29.7680\u001b[0m       27.0420  0.0124\n",
      "     20       \u001b[36m29.6485\u001b[0m       27.0134  0.0124\n",
      "     21       \u001b[36m29.5380\u001b[0m       27.0425  0.0123\n",
      "     22       \u001b[36m29.4373\u001b[0m       27.0952  0.0128\n",
      "     23       \u001b[36m29.3459\u001b[0m       27.1418  0.0126\n",
      "     24       \u001b[36m29.2606\u001b[0m       27.1693  0.0126\n",
      "     25       \u001b[36m29.1810\u001b[0m       27.1818  0.0124\n",
      "     26       \u001b[36m29.1074\u001b[0m       27.1885  0.0128\n",
      "     27       \u001b[36m29.0413\u001b[0m       27.1991  0.0125\n",
      "     28       \u001b[36m28.9834\u001b[0m       27.2188  0.0131\n",
      "     29       \u001b[36m28.9343\u001b[0m       27.2444  0.0124\n",
      "     30       \u001b[36m28.8919\u001b[0m       27.2697  0.0123\n",
      "     31       \u001b[36m28.8545\u001b[0m       27.2891  0.0122\n",
      "     32       \u001b[36m28.8203\u001b[0m       27.3033  0.0127\n",
      "     33       \u001b[36m28.7888\u001b[0m       27.3129  0.0125\n",
      "     34       \u001b[36m28.7603\u001b[0m       27.3187  0.0124\n",
      "     35       \u001b[36m28.7342\u001b[0m       27.3243  0.0124\n",
      "     36       \u001b[36m28.7096\u001b[0m       27.3330  0.0124\n",
      "     37       \u001b[36m28.6873\u001b[0m       27.3426  0.0126\n",
      "     38       \u001b[36m28.6669\u001b[0m       27.3509  0.0188\n",
      "     39       \u001b[36m28.6482\u001b[0m       27.3628  0.0161\n",
      "     40       \u001b[36m28.6316\u001b[0m       27.3744  0.0124\n",
      "     41       \u001b[36m28.6169\u001b[0m       27.3818  0.0125\n",
      "     42       \u001b[36m28.6033\u001b[0m       27.3882  0.0127\n",
      "     43       \u001b[36m28.5909\u001b[0m       27.3911  0.0118\n",
      "     44       \u001b[36m28.5795\u001b[0m       27.3906  0.0121\n",
      "     45       \u001b[36m28.5691\u001b[0m       27.3904  0.0119\n",
      "     46       \u001b[36m28.5595\u001b[0m       27.3905  0.0121\n",
      "     47       \u001b[36m28.5510\u001b[0m       27.3894  0.0125\n",
      "     48       \u001b[36m28.5432\u001b[0m       27.3897  0.0118\n",
      "     49       \u001b[36m28.5360\u001b[0m       27.3906  0.0120\n",
      "     50       \u001b[36m28.5291\u001b[0m       27.3900  0.0123\n",
      "     51       \u001b[36m28.5225\u001b[0m       27.3898  0.0120\n",
      "     52       \u001b[36m28.5164\u001b[0m       27.3897  0.0122\n",
      "     53       \u001b[36m28.5108\u001b[0m       27.3877  0.0142\n",
      "     54       \u001b[36m28.5056\u001b[0m       27.3871  0.0120\n",
      "     55       \u001b[36m28.5008\u001b[0m       27.3864  0.0119\n",
      "     56       \u001b[36m28.4961\u001b[0m       27.3841  0.0125\n",
      "     57       \u001b[36m28.4916\u001b[0m       27.3826  0.0121\n",
      "     58       \u001b[36m28.4874\u001b[0m       27.3793  0.0123\n",
      "     59       \u001b[36m28.4834\u001b[0m       27.3757  0.0122\n",
      "     60       \u001b[36m28.4796\u001b[0m       27.3721  0.0124\n",
      "     61       \u001b[36m28.4760\u001b[0m       27.3676  0.0122\n",
      "     62       \u001b[36m28.4726\u001b[0m       27.3645  0.0121\n",
      "     63       \u001b[36m28.4693\u001b[0m       27.3605  0.0120\n",
      "     64       \u001b[36m28.4660\u001b[0m       27.3563  0.0118\n",
      "     65       \u001b[36m28.4630\u001b[0m       27.3530  0.0122\n",
      "     66       \u001b[36m28.4602\u001b[0m       27.3500  0.0128\n",
      "     67       \u001b[36m28.4574\u001b[0m       27.3470  0.0121\n",
      "     68       \u001b[36m28.4548\u001b[0m       27.3441  0.0123\n",
      "     69       \u001b[36m28.4522\u001b[0m       27.3407  0.0131\n",
      "     70       \u001b[36m28.4497\u001b[0m       27.3385  0.0251\n",
      "     71       \u001b[36m28.4474\u001b[0m       27.3363  0.0136\n",
      "     72       \u001b[36m28.4452\u001b[0m       27.3343  0.0145\n",
      "     73       \u001b[36m28.4431\u001b[0m       27.3330  0.0158\n",
      "     74       \u001b[36m28.4410\u001b[0m       27.3318  0.0162\n",
      "     75       \u001b[36m28.4390\u001b[0m       27.3301  0.0157\n",
      "     76       \u001b[36m28.4371\u001b[0m       27.3268  0.0141\n",
      "     77       \u001b[36m28.4353\u001b[0m       27.3234  0.0123\n",
      "     78       \u001b[36m28.4336\u001b[0m       27.3190  0.0122\n",
      "     79       \u001b[36m28.4318\u001b[0m       27.3134  0.0125\n",
      "     80       \u001b[36m28.4302\u001b[0m       27.3066  0.0125\n",
      "     81       \u001b[36m28.4286\u001b[0m       27.3014  0.0122\n",
      "     82       \u001b[36m28.4272\u001b[0m       27.2978  0.0121\n",
      "     83       \u001b[36m28.4259\u001b[0m       27.2935  0.0120\n",
      "     84       \u001b[36m28.4246\u001b[0m       27.2896  0.0124\n",
      "     85       \u001b[36m28.4234\u001b[0m       27.2855  0.0124\n",
      "     86       \u001b[36m28.4221\u001b[0m       27.2815  0.0132\n",
      "     87       \u001b[36m28.4209\u001b[0m       27.2774  0.0124\n",
      "     88       \u001b[36m28.4197\u001b[0m       27.2738  0.0115\n",
      "     89       \u001b[36m28.4187\u001b[0m       27.2704  0.0121\n",
      "     90       \u001b[36m28.4177\u001b[0m       27.2674  0.0123\n",
      "     91       \u001b[36m28.4167\u001b[0m       27.2643  0.0123\n",
      "     92       \u001b[36m28.4157\u001b[0m       27.2616  0.0129\n",
      "     93       \u001b[36m28.4148\u001b[0m       27.2588  0.0120\n",
      "     94       \u001b[36m28.4138\u001b[0m       27.2553  0.0124\n",
      "     95       \u001b[36m28.4129\u001b[0m       27.2517  0.0129\n",
      "     96       \u001b[36m28.4120\u001b[0m       27.2483  0.0130\n",
      "     97       \u001b[36m28.4110\u001b[0m       27.2459  0.0137\n",
      "     98       \u001b[36m28.4102\u001b[0m       27.2435  0.0129\n",
      "     99       \u001b[36m28.4094\u001b[0m       27.2420  0.0139\n",
      "    100       \u001b[36m28.4086\u001b[0m       27.2395  0.0140\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m43.7558\u001b[0m       \u001b[32m45.9219\u001b[0m  0.0119\n",
      "      2       \u001b[36m43.4403\u001b[0m       \u001b[32m45.5377\u001b[0m  0.0119\n",
      "      3       \u001b[36m43.1295\u001b[0m       \u001b[32m45.1571\u001b[0m  0.0118\n",
      "      4       \u001b[36m42.8225\u001b[0m       \u001b[32m44.7800\u001b[0m  0.0119\n",
      "      5       \u001b[36m42.5187\u001b[0m       \u001b[32m44.4049\u001b[0m  0.0119\n",
      "      6       \u001b[36m42.2173\u001b[0m       \u001b[32m44.0314\u001b[0m  0.0115\n",
      "      7       \u001b[36m41.9175\u001b[0m       \u001b[32m43.6586\u001b[0m  0.0132\n",
      "      8       \u001b[36m41.6190\u001b[0m       \u001b[32m43.2861\u001b[0m  0.0124\n",
      "      9       \u001b[36m41.3215\u001b[0m       \u001b[32m42.9131\u001b[0m  0.0119\n",
      "     10       \u001b[36m41.0244\u001b[0m       \u001b[32m42.5391\u001b[0m  0.0122\n",
      "     11       \u001b[36m40.7276\u001b[0m       \u001b[32m42.1636\u001b[0m  0.0126\n",
      "     12       \u001b[36m40.4305\u001b[0m       \u001b[32m41.7859\u001b[0m  0.0134\n",
      "     13       \u001b[36m40.1333\u001b[0m       \u001b[32m41.4070\u001b[0m  0.0233\n",
      "     14       \u001b[36m39.8361\u001b[0m       \u001b[32m41.0265\u001b[0m  0.0121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     15       \u001b[36m39.5389\u001b[0m       \u001b[32m40.6434\u001b[0m  0.0118\n",
      "     16       \u001b[36m39.2418\u001b[0m       \u001b[32m40.2588\u001b[0m  0.0115\n",
      "     17       \u001b[36m38.9452\u001b[0m       \u001b[32m39.8728\u001b[0m  0.0113\n",
      "     18       \u001b[36m38.6489\u001b[0m       \u001b[32m39.4859\u001b[0m  0.0116\n",
      "     19       \u001b[36m38.3537\u001b[0m       \u001b[32m39.0987\u001b[0m  0.0118\n",
      "     20       \u001b[36m38.0599\u001b[0m       \u001b[32m38.7115\u001b[0m  0.0114\n",
      "     21       \u001b[36m37.7680\u001b[0m       \u001b[32m38.3246\u001b[0m  0.0115\n",
      "     22       \u001b[36m37.4787\u001b[0m       \u001b[32m37.9392\u001b[0m  0.0113\n",
      "     23       \u001b[36m37.1930\u001b[0m       \u001b[32m37.5564\u001b[0m  0.0115\n",
      "     24       \u001b[36m36.9116\u001b[0m       \u001b[32m37.1768\u001b[0m  0.0115\n",
      "     25       \u001b[36m36.6355\u001b[0m       \u001b[32m36.8012\u001b[0m  0.0118\n",
      "     26       \u001b[36m36.3655\u001b[0m       \u001b[32m36.4309\u001b[0m  0.0116\n",
      "     27       \u001b[36m36.1026\u001b[0m       \u001b[32m36.0670\u001b[0m  0.0115\n",
      "     28       \u001b[36m35.8474\u001b[0m       \u001b[32m35.7106\u001b[0m  0.0115\n",
      "     29       \u001b[36m35.6007\u001b[0m       \u001b[32m35.3627\u001b[0m  0.0113\n",
      "     30       \u001b[36m35.3633\u001b[0m       \u001b[32m35.0244\u001b[0m  0.0111\n",
      "     31       \u001b[36m35.1359\u001b[0m       \u001b[32m34.6969\u001b[0m  0.0111\n",
      "     32       \u001b[36m34.9193\u001b[0m       \u001b[32m34.3809\u001b[0m  0.0116\n",
      "     33       \u001b[36m34.7139\u001b[0m       \u001b[32m34.0772\u001b[0m  0.0115\n",
      "     34       \u001b[36m34.5202\u001b[0m       \u001b[32m33.7865\u001b[0m  0.0112\n",
      "     35       \u001b[36m34.3386\u001b[0m       \u001b[32m33.5096\u001b[0m  0.0113\n",
      "     36       \u001b[36m34.1690\u001b[0m       \u001b[32m33.2467\u001b[0m  0.0114\n",
      "     37       \u001b[36m34.0117\u001b[0m       \u001b[32m32.9986\u001b[0m  0.0112\n",
      "     38       \u001b[36m33.8666\u001b[0m       \u001b[32m32.7654\u001b[0m  0.0114\n",
      "     39       \u001b[36m33.7335\u001b[0m       \u001b[32m32.5468\u001b[0m  0.0117\n",
      "     40       \u001b[36m33.6120\u001b[0m       \u001b[32m32.3428\u001b[0m  0.0116\n",
      "     41       \u001b[36m33.5015\u001b[0m       \u001b[32m32.1533\u001b[0m  0.0115\n",
      "     42       \u001b[36m33.4018\u001b[0m       \u001b[32m31.9779\u001b[0m  0.0138\n",
      "     43       \u001b[36m33.3123\u001b[0m       \u001b[32m31.8163\u001b[0m  0.0112\n",
      "     44       \u001b[36m33.2322\u001b[0m       \u001b[32m31.6676\u001b[0m  0.0131\n",
      "     45       \u001b[36m33.1607\u001b[0m       \u001b[32m31.5314\u001b[0m  0.0149\n",
      "     46       \u001b[36m33.0973\u001b[0m       \u001b[32m31.4070\u001b[0m  0.0117\n",
      "     47       \u001b[36m33.0412\u001b[0m       \u001b[32m31.2936\u001b[0m  0.0111\n",
      "     48       \u001b[36m32.9915\u001b[0m       \u001b[32m31.1903\u001b[0m  0.0118\n",
      "     49       \u001b[36m32.9475\u001b[0m       \u001b[32m31.0964\u001b[0m  0.0121\n",
      "     50       \u001b[36m32.9086\u001b[0m       \u001b[32m31.0112\u001b[0m  0.0135\n",
      "     51       \u001b[36m32.8742\u001b[0m       \u001b[32m30.9338\u001b[0m  0.0124\n",
      "     52       \u001b[36m32.8437\u001b[0m       \u001b[32m30.8637\u001b[0m  0.0119\n",
      "     53       \u001b[36m32.8165\u001b[0m       \u001b[32m30.8001\u001b[0m  0.0122\n",
      "     54       \u001b[36m32.7923\u001b[0m       \u001b[32m30.7424\u001b[0m  0.0115\n",
      "     55       \u001b[36m32.7706\u001b[0m       \u001b[32m30.6901\u001b[0m  0.0112\n",
      "     56       \u001b[36m32.7510\u001b[0m       \u001b[32m30.6426\u001b[0m  0.0111\n",
      "     57       \u001b[36m32.7333\u001b[0m       \u001b[32m30.5994\u001b[0m  0.0111\n",
      "     58       \u001b[36m32.7172\u001b[0m       \u001b[32m30.5601\u001b[0m  0.0117\n",
      "     59       \u001b[36m32.7024\u001b[0m       \u001b[32m30.5243\u001b[0m  0.0114\n",
      "     60       \u001b[36m32.6887\u001b[0m       \u001b[32m30.4915\u001b[0m  0.0112\n",
      "     61       \u001b[36m32.6762\u001b[0m       \u001b[32m30.4616\u001b[0m  0.0113\n",
      "     62       \u001b[36m32.6645\u001b[0m       \u001b[32m30.4341\u001b[0m  0.0113\n",
      "     63       \u001b[36m32.6536\u001b[0m       \u001b[32m30.4089\u001b[0m  0.0125\n",
      "     64       \u001b[36m32.6433\u001b[0m       \u001b[32m30.3857\u001b[0m  0.0114\n",
      "     65       \u001b[36m32.6336\u001b[0m       \u001b[32m30.3644\u001b[0m  0.0124\n",
      "     66       \u001b[36m32.6244\u001b[0m       \u001b[32m30.3446\u001b[0m  0.0111\n",
      "     67       \u001b[36m32.6157\u001b[0m       \u001b[32m30.3262\u001b[0m  0.0104\n",
      "     68       \u001b[36m32.6074\u001b[0m       \u001b[32m30.3092\u001b[0m  0.0124\n",
      "     69       \u001b[36m32.5994\u001b[0m       \u001b[32m30.2934\u001b[0m  0.0116\n",
      "     70       \u001b[36m32.5918\u001b[0m       \u001b[32m30.2787\u001b[0m  0.0116\n",
      "     71       \u001b[36m32.5845\u001b[0m       \u001b[32m30.2650\u001b[0m  0.0111\n",
      "     72       \u001b[36m32.5775\u001b[0m       \u001b[32m30.2522\u001b[0m  0.0114\n",
      "     73       \u001b[36m32.5707\u001b[0m       \u001b[32m30.2401\u001b[0m  0.0130\n",
      "     74       \u001b[36m32.5642\u001b[0m       \u001b[32m30.2289\u001b[0m  0.0116\n",
      "     75       \u001b[36m32.5578\u001b[0m       \u001b[32m30.2183\u001b[0m  0.0114\n",
      "     76       \u001b[36m32.5517\u001b[0m       \u001b[32m30.2084\u001b[0m  0.0114\n",
      "     77       \u001b[36m32.5458\u001b[0m       \u001b[32m30.1990\u001b[0m  0.0113\n",
      "     78       \u001b[36m32.5400\u001b[0m       \u001b[32m30.1901\u001b[0m  0.0132\n",
      "     79       \u001b[36m32.5344\u001b[0m       \u001b[32m30.1816\u001b[0m  0.0111\n",
      "     80       \u001b[36m32.5289\u001b[0m       \u001b[32m30.1736\u001b[0m  0.0117\n",
      "     81       \u001b[36m32.5236\u001b[0m       \u001b[32m30.1660\u001b[0m  0.0113\n",
      "     82       \u001b[36m32.5184\u001b[0m       \u001b[32m30.1588\u001b[0m  0.0119\n",
      "     83       \u001b[36m32.5133\u001b[0m       \u001b[32m30.1517\u001b[0m  0.0120\n",
      "     84       \u001b[36m32.5084\u001b[0m       \u001b[32m30.1451\u001b[0m  0.0115\n",
      "     85       \u001b[36m32.5036\u001b[0m       \u001b[32m30.1387\u001b[0m  0.0114\n",
      "     86       \u001b[36m32.4989\u001b[0m       \u001b[32m30.1326\u001b[0m  0.0108\n",
      "     87       \u001b[36m32.4943\u001b[0m       \u001b[32m30.1266\u001b[0m  0.0110\n",
      "     88       \u001b[36m32.4899\u001b[0m       \u001b[32m30.1208\u001b[0m  0.0118\n",
      "     89       \u001b[36m32.4856\u001b[0m       \u001b[32m30.1153\u001b[0m  0.0113\n",
      "     90       \u001b[36m32.4813\u001b[0m       \u001b[32m30.1100\u001b[0m  0.0114\n",
      "     91       \u001b[36m32.4772\u001b[0m       \u001b[32m30.1049\u001b[0m  0.0115\n",
      "     92       \u001b[36m32.4731\u001b[0m       \u001b[32m30.0999\u001b[0m  0.0114\n",
      "     93       \u001b[36m32.4692\u001b[0m       \u001b[32m30.0951\u001b[0m  0.0128\n",
      "     94       \u001b[36m32.4653\u001b[0m       \u001b[32m30.0904\u001b[0m  0.0113\n",
      "     95       \u001b[36m32.4615\u001b[0m       \u001b[32m30.0859\u001b[0m  0.0121\n",
      "     96       \u001b[36m32.4578\u001b[0m       \u001b[32m30.0815\u001b[0m  0.0114\n",
      "     97       \u001b[36m32.4542\u001b[0m       \u001b[32m30.0772\u001b[0m  0.0115\n",
      "     98       \u001b[36m32.4507\u001b[0m       \u001b[32m30.0731\u001b[0m  0.0119\n",
      "     99       \u001b[36m32.4472\u001b[0m       \u001b[32m30.0691\u001b[0m  0.0116\n",
      "    100       \u001b[36m32.4437\u001b[0m       \u001b[32m30.0653\u001b[0m  0.0117\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m35.2146\u001b[0m       \u001b[32m33.4589\u001b[0m  0.0111\n",
      "      2       \u001b[36m34.8090\u001b[0m       \u001b[32m33.1429\u001b[0m  0.0115\n",
      "      3       \u001b[36m34.4168\u001b[0m       \u001b[32m32.8373\u001b[0m  0.0126\n",
      "      4       \u001b[36m34.0357\u001b[0m       \u001b[32m32.5410\u001b[0m  0.0111\n",
      "      5       \u001b[36m33.6638\u001b[0m       \u001b[32m32.2529\u001b[0m  0.0117\n",
      "      6       \u001b[36m33.2996\u001b[0m       \u001b[32m31.9716\u001b[0m  0.0121\n",
      "      7       \u001b[36m32.9417\u001b[0m       \u001b[32m31.6959\u001b[0m  0.0114\n",
      "      8       \u001b[36m32.5892\u001b[0m       \u001b[32m31.4255\u001b[0m  0.0117\n",
      "      9       \u001b[36m32.2413\u001b[0m       \u001b[32m31.1598\u001b[0m  0.0113\n",
      "     10       \u001b[36m31.8973\u001b[0m       \u001b[32m30.8983\u001b[0m  0.0112\n",
      "     11       \u001b[36m31.5562\u001b[0m       \u001b[32m30.6403\u001b[0m  0.0118\n",
      "     12       \u001b[36m31.2178\u001b[0m       \u001b[32m30.3859\u001b[0m  0.0116\n",
      "     13       \u001b[36m30.8811\u001b[0m       \u001b[32m30.1345\u001b[0m  0.0118\n",
      "     14       \u001b[36m30.5456\u001b[0m       \u001b[32m29.8862\u001b[0m  0.0111\n",
      "     15       \u001b[36m30.2114\u001b[0m       \u001b[32m29.6413\u001b[0m  0.0110\n",
      "     16       \u001b[36m29.8785\u001b[0m       \u001b[32m29.3997\u001b[0m  0.0120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m29.5473\u001b[0m       \u001b[32m29.1619\u001b[0m  0.0121\n",
      "     18       \u001b[36m29.2179\u001b[0m       \u001b[32m28.9282\u001b[0m  0.0123\n",
      "     19       \u001b[36m28.8903\u001b[0m       \u001b[32m28.6994\u001b[0m  0.0124\n",
      "     20       \u001b[36m28.5657\u001b[0m       \u001b[32m28.4761\u001b[0m  0.0111\n",
      "     21       \u001b[36m28.2448\u001b[0m       \u001b[32m28.2590\u001b[0m  0.0112\n",
      "     22       \u001b[36m27.9282\u001b[0m       \u001b[32m28.0488\u001b[0m  0.0138\n",
      "     23       \u001b[36m27.6170\u001b[0m       \u001b[32m27.8468\u001b[0m  0.0126\n",
      "     24       \u001b[36m27.3121\u001b[0m       \u001b[32m27.6537\u001b[0m  0.0165\n",
      "     25       \u001b[36m27.0144\u001b[0m       \u001b[32m27.4707\u001b[0m  0.0179\n",
      "     26       \u001b[36m26.7256\u001b[0m       \u001b[32m27.2988\u001b[0m  0.0123\n",
      "     27       \u001b[36m26.4467\u001b[0m       \u001b[32m27.1389\u001b[0m  0.0122\n",
      "     28       \u001b[36m26.1786\u001b[0m       \u001b[32m26.9916\u001b[0m  0.0144\n",
      "     29       \u001b[36m25.9225\u001b[0m       \u001b[32m26.8579\u001b[0m  0.0132\n",
      "     30       \u001b[36m25.6793\u001b[0m       \u001b[32m26.7380\u001b[0m  0.0118\n",
      "     31       \u001b[36m25.4499\u001b[0m       \u001b[32m26.6324\u001b[0m  0.0163\n",
      "     32       \u001b[36m25.2353\u001b[0m       \u001b[32m26.5413\u001b[0m  0.0113\n",
      "     33       \u001b[36m25.0357\u001b[0m       \u001b[32m26.4644\u001b[0m  0.0111\n",
      "     34       \u001b[36m24.8515\u001b[0m       \u001b[32m26.4014\u001b[0m  0.0111\n",
      "     35       \u001b[36m24.6828\u001b[0m       \u001b[32m26.3517\u001b[0m  0.0122\n",
      "     36       \u001b[36m24.5295\u001b[0m       \u001b[32m26.3144\u001b[0m  0.0113\n",
      "     37       \u001b[36m24.3911\u001b[0m       \u001b[32m26.2886\u001b[0m  0.0113\n",
      "     38       \u001b[36m24.2671\u001b[0m       \u001b[32m26.2732\u001b[0m  0.0110\n",
      "     39       \u001b[36m24.1567\u001b[0m       \u001b[32m26.2668\u001b[0m  0.0111\n",
      "     40       \u001b[36m24.0591\u001b[0m       26.2683  0.0124\n",
      "     41       \u001b[36m23.9734\u001b[0m       26.2762  0.0120\n",
      "     42       \u001b[36m23.8984\u001b[0m       26.2895  0.0117\n",
      "     43       \u001b[36m23.8332\u001b[0m       26.3067  0.0105\n",
      "     44       \u001b[36m23.7766\u001b[0m       26.3270  0.0106\n",
      "     45       \u001b[36m23.7277\u001b[0m       26.3492  0.0118\n",
      "     46       \u001b[36m23.6854\u001b[0m       26.3725  0.0121\n",
      "     47       \u001b[36m23.6490\u001b[0m       26.3963  0.0122\n",
      "     48       \u001b[36m23.6176\u001b[0m       26.4200  0.0106\n",
      "     49       \u001b[36m23.5904\u001b[0m       26.4430  0.0107\n",
      "     50       \u001b[36m23.5668\u001b[0m       26.4651  0.0112\n",
      "     51       \u001b[36m23.5462\u001b[0m       26.4860  0.0112\n",
      "     52       \u001b[36m23.5283\u001b[0m       26.5055  0.0111\n",
      "     53       \u001b[36m23.5124\u001b[0m       26.5235  0.0106\n",
      "     54       \u001b[36m23.4984\u001b[0m       26.5399  0.0104\n",
      "     55       \u001b[36m23.4859\u001b[0m       26.5548  0.0117\n",
      "     56       \u001b[36m23.4747\u001b[0m       26.5682  0.0110\n",
      "     57       \u001b[36m23.4645\u001b[0m       26.5802  0.0112\n",
      "     58       \u001b[36m23.4552\u001b[0m       26.5908  0.0110\n",
      "     59       \u001b[36m23.4467\u001b[0m       26.6001  0.0105\n",
      "     60       \u001b[36m23.4388\u001b[0m       26.6081  0.0116\n",
      "     61       \u001b[36m23.4314\u001b[0m       26.6149  0.0113\n",
      "     62       \u001b[36m23.4245\u001b[0m       26.6208  0.0125\n",
      "     63       \u001b[36m23.4180\u001b[0m       26.6257  0.0105\n",
      "     64       \u001b[36m23.4118\u001b[0m       26.6298  0.0107\n",
      "     65       \u001b[36m23.4059\u001b[0m       26.6331  0.0119\n",
      "     66       \u001b[36m23.4003\u001b[0m       26.6358  0.0113\n",
      "     67       \u001b[36m23.3949\u001b[0m       26.6378  0.0110\n",
      "     68       \u001b[36m23.3898\u001b[0m       26.6392  0.0107\n",
      "     69       \u001b[36m23.3847\u001b[0m       26.6402  0.0107\n",
      "     70       \u001b[36m23.3799\u001b[0m       26.6407  0.0117\n",
      "     71       \u001b[36m23.3752\u001b[0m       26.6407  0.0113\n",
      "     72       \u001b[36m23.3706\u001b[0m       26.6405  0.0114\n",
      "     73       \u001b[36m23.3661\u001b[0m       26.6399  0.0108\n",
      "     74       \u001b[36m23.3617\u001b[0m       26.6392  0.0104\n",
      "     75       \u001b[36m23.3575\u001b[0m       26.6383  0.0117\n",
      "     76       \u001b[36m23.3534\u001b[0m       26.6373  0.0114\n",
      "     77       \u001b[36m23.3495\u001b[0m       26.6360  0.0117\n",
      "     78       \u001b[36m23.3456\u001b[0m       26.6347  0.0109\n",
      "     79       \u001b[36m23.3418\u001b[0m       26.6331  0.0107\n",
      "     80       \u001b[36m23.3382\u001b[0m       26.6315  0.0116\n",
      "     81       \u001b[36m23.3345\u001b[0m       26.6298  0.0110\n",
      "     82       \u001b[36m23.3310\u001b[0m       26.6280  0.0112\n",
      "     83       \u001b[36m23.3275\u001b[0m       26.6262  0.0103\n",
      "     84       \u001b[36m23.3242\u001b[0m       26.6246  0.0105\n",
      "     85       \u001b[36m23.3209\u001b[0m       26.6229  0.0117\n",
      "     86       \u001b[36m23.3176\u001b[0m       26.6211  0.0114\n",
      "     87       \u001b[36m23.3144\u001b[0m       26.6194  0.0113\n",
      "     88       \u001b[36m23.3113\u001b[0m       26.6176  0.0106\n",
      "     89       \u001b[36m23.3083\u001b[0m       26.6158  0.0107\n",
      "     90       \u001b[36m23.3053\u001b[0m       26.6140  0.0114\n",
      "     91       \u001b[36m23.3023\u001b[0m       26.6122  0.0111\n",
      "     92       \u001b[36m23.2994\u001b[0m       26.6105  0.0115\n",
      "     93       \u001b[36m23.2966\u001b[0m       26.6088  0.0108\n",
      "     94       \u001b[36m23.2938\u001b[0m       26.6071  0.0108\n",
      "     95       \u001b[36m23.2911\u001b[0m       26.6055  0.0118\n",
      "     96       \u001b[36m23.2884\u001b[0m       26.6039  0.0114\n",
      "     97       \u001b[36m23.2858\u001b[0m       26.6022  0.0111\n",
      "     98       \u001b[36m23.2833\u001b[0m       26.6006  0.0104\n",
      "     99       \u001b[36m23.2807\u001b[0m       26.5990  0.0104\n",
      "    100       \u001b[36m23.2782\u001b[0m       26.5974  0.0115\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m41.7506\u001b[0m       \u001b[32m33.0580\u001b[0m  0.0108\n",
      "      2       \u001b[36m41.2937\u001b[0m       \u001b[32m32.7357\u001b[0m  0.0111\n",
      "      3       \u001b[36m40.8485\u001b[0m       \u001b[32m32.4228\u001b[0m  0.0114\n",
      "      4       \u001b[36m40.4132\u001b[0m       \u001b[32m32.1182\u001b[0m  0.0110\n",
      "      5       \u001b[36m39.9859\u001b[0m       \u001b[32m31.8211\u001b[0m  0.0107\n",
      "      6       \u001b[36m39.5661\u001b[0m       \u001b[32m31.5304\u001b[0m  0.0131\n",
      "      7       \u001b[36m39.1519\u001b[0m       \u001b[32m31.2450\u001b[0m  0.0157\n",
      "      8       \u001b[36m38.7427\u001b[0m       \u001b[32m30.9648\u001b[0m  0.0123\n",
      "      9       \u001b[36m38.3381\u001b[0m       \u001b[32m30.6897\u001b[0m  0.0123\n",
      "     10       \u001b[36m37.9372\u001b[0m       \u001b[32m30.4192\u001b[0m  0.0113\n",
      "     11       \u001b[36m37.5390\u001b[0m       \u001b[32m30.1529\u001b[0m  0.0111\n",
      "     12       \u001b[36m37.1436\u001b[0m       \u001b[32m29.8908\u001b[0m  0.0140\n",
      "     13       \u001b[36m36.7507\u001b[0m       \u001b[32m29.6328\u001b[0m  0.0128\n",
      "     14       \u001b[36m36.3605\u001b[0m       \u001b[32m29.3792\u001b[0m  0.0149\n",
      "     15       \u001b[36m35.9730\u001b[0m       \u001b[32m29.1302\u001b[0m  0.0109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     16       \u001b[36m35.5883\u001b[0m       \u001b[32m28.8861\u001b[0m  0.0114\n",
      "     17       \u001b[36m35.2067\u001b[0m       \u001b[32m28.6474\u001b[0m  0.0127\n",
      "     18       \u001b[36m34.8288\u001b[0m       \u001b[32m28.4147\u001b[0m  0.0114\n",
      "     19       \u001b[36m34.4552\u001b[0m       \u001b[32m28.1886\u001b[0m  0.0115\n",
      "     20       \u001b[36m34.0865\u001b[0m       \u001b[32m27.9696\u001b[0m  0.0110\n",
      "     21       \u001b[36m33.7234\u001b[0m       \u001b[32m27.7590\u001b[0m  0.0112\n",
      "     22       \u001b[36m33.3671\u001b[0m       \u001b[32m27.5573\u001b[0m  0.0123\n",
      "     23       \u001b[36m33.0183\u001b[0m       \u001b[32m27.3658\u001b[0m  0.0116\n",
      "     24       \u001b[36m32.6778\u001b[0m       \u001b[32m27.1855\u001b[0m  0.0117\n",
      "     25       \u001b[36m32.3467\u001b[0m       \u001b[32m27.0178\u001b[0m  0.0110\n",
      "     26       \u001b[36m32.0265\u001b[0m       \u001b[32m26.8635\u001b[0m  0.0108\n",
      "     27       \u001b[36m31.7182\u001b[0m       \u001b[32m26.7236\u001b[0m  0.0114\n",
      "     28       \u001b[36m31.4231\u001b[0m       \u001b[32m26.5991\u001b[0m  0.0109\n",
      "     29       \u001b[36m31.1421\u001b[0m       \u001b[32m26.4914\u001b[0m  0.0114\n",
      "     30       \u001b[36m30.8768\u001b[0m       \u001b[32m26.4005\u001b[0m  0.0106\n",
      "     31       \u001b[36m30.6281\u001b[0m       \u001b[32m26.3266\u001b[0m  0.0106\n",
      "     32       \u001b[36m30.3967\u001b[0m       \u001b[32m26.2700\u001b[0m  0.0118\n",
      "     33       \u001b[36m30.1835\u001b[0m       \u001b[32m26.2302\u001b[0m  0.0111\n",
      "     34       \u001b[36m29.9890\u001b[0m       \u001b[32m26.2067\u001b[0m  0.0109\n",
      "     35       \u001b[36m29.8132\u001b[0m       \u001b[32m26.1984\u001b[0m  0.0111\n",
      "     36       \u001b[36m29.6560\u001b[0m       26.2038  0.0104\n",
      "     37       \u001b[36m29.5168\u001b[0m       26.2215  0.0113\n",
      "     38       \u001b[36m29.3947\u001b[0m       26.2496  0.0111\n",
      "     39       \u001b[36m29.2886\u001b[0m       26.2862  0.0113\n",
      "     40       \u001b[36m29.1972\u001b[0m       26.3293  0.0105\n",
      "     41       \u001b[36m29.1190\u001b[0m       26.3772  0.0104\n",
      "     42       \u001b[36m29.0525\u001b[0m       26.4281  0.0124\n",
      "     43       \u001b[36m28.9963\u001b[0m       26.4806  0.0110\n",
      "     44       \u001b[36m28.9491\u001b[0m       26.5334  0.0112\n",
      "     45       \u001b[36m28.9094\u001b[0m       26.5853  0.0117\n",
      "     46       \u001b[36m28.8761\u001b[0m       26.6355  0.0109\n",
      "     47       \u001b[36m28.8482\u001b[0m       26.6834  0.0115\n",
      "     48       \u001b[36m28.8246\u001b[0m       26.7285  0.0108\n",
      "     49       \u001b[36m28.8047\u001b[0m       26.7706  0.0112\n",
      "     50       \u001b[36m28.7878\u001b[0m       26.8094  0.0105\n",
      "     51       \u001b[36m28.7732\u001b[0m       26.8449  0.0105\n",
      "     52       \u001b[36m28.7606\u001b[0m       26.8771  0.0117\n",
      "     53       \u001b[36m28.7495\u001b[0m       26.9063  0.0117\n",
      "     54       \u001b[36m28.7397\u001b[0m       26.9325  0.0114\n",
      "     55       \u001b[36m28.7308\u001b[0m       26.9559  0.0107\n",
      "     56       \u001b[36m28.7227\u001b[0m       26.9767  0.0104\n",
      "     57       \u001b[36m28.7151\u001b[0m       26.9952  0.0115\n",
      "     58       \u001b[36m28.7081\u001b[0m       27.0114  0.0111\n",
      "     59       \u001b[36m28.7014\u001b[0m       27.0257  0.0113\n",
      "     60       \u001b[36m28.6950\u001b[0m       27.0384  0.0108\n",
      "     61       \u001b[36m28.6889\u001b[0m       27.0496  0.0106\n",
      "     62       \u001b[36m28.6830\u001b[0m       27.0593  0.0115\n",
      "     63       \u001b[36m28.6774\u001b[0m       27.0679  0.0113\n",
      "     64       \u001b[36m28.6719\u001b[0m       27.0753  0.0112\n",
      "     65       \u001b[36m28.6666\u001b[0m       27.0818  0.0109\n",
      "     66       \u001b[36m28.6615\u001b[0m       27.0875  0.0113\n",
      "     67       \u001b[36m28.6564\u001b[0m       27.0925  0.0121\n",
      "     68       \u001b[36m28.6515\u001b[0m       27.0968  0.0115\n",
      "     69       \u001b[36m28.6467\u001b[0m       27.1006  0.0134\n",
      "     70       \u001b[36m28.6419\u001b[0m       27.1039  0.0110\n",
      "     71       \u001b[36m28.6373\u001b[0m       27.1068  0.0110\n",
      "     72       \u001b[36m28.6328\u001b[0m       27.1092  0.0124\n",
      "     73       \u001b[36m28.6284\u001b[0m       27.1115  0.0114\n",
      "     74       \u001b[36m28.6241\u001b[0m       27.1137  0.0116\n",
      "     75       \u001b[36m28.6200\u001b[0m       27.1157  0.0110\n",
      "     76       \u001b[36m28.6159\u001b[0m       27.1174  0.0109\n",
      "     77       \u001b[36m28.6120\u001b[0m       27.1191  0.0121\n",
      "     78       \u001b[36m28.6082\u001b[0m       27.1206  0.0116\n",
      "     79       \u001b[36m28.6044\u001b[0m       27.1219  0.0120\n",
      "     80       \u001b[36m28.6008\u001b[0m       27.1234  0.0108\n",
      "     81       \u001b[36m28.5973\u001b[0m       27.1246  0.0107\n",
      "     82       \u001b[36m28.5939\u001b[0m       27.1259  0.0122\n",
      "     83       \u001b[36m28.5906\u001b[0m       27.1270  0.0121\n",
      "     84       \u001b[36m28.5874\u001b[0m       27.1280  0.0116\n",
      "     85       \u001b[36m28.5843\u001b[0m       27.1289  0.0118\n",
      "     86       \u001b[36m28.5812\u001b[0m       27.1298  0.0111\n",
      "     87       \u001b[36m28.5782\u001b[0m       27.1306  0.0122\n",
      "     88       \u001b[36m28.5754\u001b[0m       27.1316  0.0139\n",
      "     89       \u001b[36m28.5727\u001b[0m       27.1324  0.0154\n",
      "     90       \u001b[36m28.5700\u001b[0m       27.1332  0.0118\n",
      "     91       \u001b[36m28.5675\u001b[0m       27.1340  0.0118\n",
      "     92       \u001b[36m28.5650\u001b[0m       27.1346  0.0118\n",
      "     93       \u001b[36m28.5627\u001b[0m       27.1357  0.0118\n",
      "     94       \u001b[36m28.5604\u001b[0m       27.1366  0.0133\n",
      "     95       \u001b[36m28.5582\u001b[0m       27.1375  0.0120\n",
      "     96       \u001b[36m28.5560\u001b[0m       27.1383  0.0154\n",
      "     97       \u001b[36m28.5539\u001b[0m       27.1391  0.0114\n",
      "     98       \u001b[36m28.5518\u001b[0m       27.1399  0.0113\n",
      "     99       \u001b[36m28.5497\u001b[0m       27.1404  0.0114\n",
      "    100       \u001b[36m28.5478\u001b[0m       27.1412  0.0115\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m42.5293\u001b[0m       \u001b[32m43.8766\u001b[0m  0.0122\n",
      "      2       \u001b[36m41.7218\u001b[0m       \u001b[32m42.7881\u001b[0m  0.0122\n",
      "      3       \u001b[36m40.8947\u001b[0m       \u001b[32m41.6600\u001b[0m  0.0122\n",
      "      4       \u001b[36m40.0423\u001b[0m       \u001b[32m40.5429\u001b[0m  0.0121\n",
      "      5       \u001b[36m39.1648\u001b[0m       \u001b[32m39.2941\u001b[0m  0.0119\n",
      "      6       \u001b[36m38.1408\u001b[0m       \u001b[32m37.7412\u001b[0m  0.0122\n",
      "      7       \u001b[36m36.9224\u001b[0m       \u001b[32m35.9372\u001b[0m  0.0115\n",
      "      8       \u001b[36m35.6291\u001b[0m       \u001b[32m34.0674\u001b[0m  0.0119\n",
      "      9       \u001b[36m34.4961\u001b[0m       \u001b[32m32.4770\u001b[0m  0.0117\n",
      "     10       \u001b[36m33.7957\u001b[0m       \u001b[32m31.4816\u001b[0m  0.0121\n",
      "     11       \u001b[36m33.5910\u001b[0m       \u001b[32m31.0631\u001b[0m  0.0123\n",
      "     12       \u001b[36m33.5480\u001b[0m       \u001b[32m30.9281\u001b[0m  0.0127\n",
      "     13       \u001b[36m33.3986\u001b[0m       30.9360  0.0126\n",
      "     14       \u001b[36m33.2292\u001b[0m       31.0091  0.0123\n",
      "     15       \u001b[36m33.1177\u001b[0m       31.0368  0.0121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     16       \u001b[36m33.0377\u001b[0m       30.9856  0.0120\n",
      "     17       \u001b[36m32.9633\u001b[0m       \u001b[32m30.8867\u001b[0m  0.0117\n",
      "     18       \u001b[36m32.8964\u001b[0m       \u001b[32m30.7717\u001b[0m  0.0121\n",
      "     19       \u001b[36m32.8370\u001b[0m       \u001b[32m30.6713\u001b[0m  0.0119\n",
      "     20       \u001b[36m32.7874\u001b[0m       \u001b[32m30.5984\u001b[0m  0.0118\n",
      "     21       \u001b[36m32.7452\u001b[0m       \u001b[32m30.5459\u001b[0m  0.0117\n",
      "     22       \u001b[36m32.7059\u001b[0m       \u001b[32m30.5089\u001b[0m  0.0119\n",
      "     23       \u001b[36m32.6691\u001b[0m       \u001b[32m30.4808\u001b[0m  0.0119\n",
      "     24       \u001b[36m32.6348\u001b[0m       \u001b[32m30.4597\u001b[0m  0.0116\n",
      "     25       \u001b[36m32.6029\u001b[0m       \u001b[32m30.4400\u001b[0m  0.0117\n",
      "     26       \u001b[36m32.5736\u001b[0m       \u001b[32m30.4181\u001b[0m  0.0117\n",
      "     27       \u001b[36m32.5462\u001b[0m       \u001b[32m30.3923\u001b[0m  0.0114\n",
      "     28       \u001b[36m32.5208\u001b[0m       \u001b[32m30.3678\u001b[0m  0.0115\n",
      "     29       \u001b[36m32.4978\u001b[0m       \u001b[32m30.3493\u001b[0m  0.0116\n",
      "     30       \u001b[36m32.4769\u001b[0m       \u001b[32m30.3375\u001b[0m  0.0115\n",
      "     31       \u001b[36m32.4578\u001b[0m       \u001b[32m30.3282\u001b[0m  0.0115\n",
      "     32       \u001b[36m32.4403\u001b[0m       \u001b[32m30.3182\u001b[0m  0.0114\n",
      "     33       \u001b[36m32.4240\u001b[0m       \u001b[32m30.3104\u001b[0m  0.0124\n",
      "     34       \u001b[36m32.4091\u001b[0m       \u001b[32m30.3071\u001b[0m  0.0122\n",
      "     35       \u001b[36m32.3954\u001b[0m       \u001b[32m30.3005\u001b[0m  0.0123\n",
      "     36       \u001b[36m32.3825\u001b[0m       \u001b[32m30.2896\u001b[0m  0.0121\n",
      "     37       \u001b[36m32.3705\u001b[0m       \u001b[32m30.2825\u001b[0m  0.0124\n",
      "     38       \u001b[36m32.3597\u001b[0m       \u001b[32m30.2800\u001b[0m  0.0127\n",
      "     39       \u001b[36m32.3496\u001b[0m       \u001b[32m30.2763\u001b[0m  0.0120\n",
      "     40       \u001b[36m32.3402\u001b[0m       \u001b[32m30.2716\u001b[0m  0.0119\n",
      "     41       \u001b[36m32.3313\u001b[0m       \u001b[32m30.2694\u001b[0m  0.0114\n",
      "     42       \u001b[36m32.3231\u001b[0m       \u001b[32m30.2689\u001b[0m  0.0117\n",
      "     43       \u001b[36m32.3155\u001b[0m       \u001b[32m30.2658\u001b[0m  0.0122\n",
      "     44       \u001b[36m32.3082\u001b[0m       \u001b[32m30.2615\u001b[0m  0.0123\n",
      "     45       \u001b[36m32.3015\u001b[0m       \u001b[32m30.2587\u001b[0m  0.0119\n",
      "     46       \u001b[36m32.2950\u001b[0m       \u001b[32m30.2552\u001b[0m  0.0115\n",
      "     47       \u001b[36m32.2889\u001b[0m       \u001b[32m30.2516\u001b[0m  0.0112\n",
      "     48       \u001b[36m32.2832\u001b[0m       \u001b[32m30.2481\u001b[0m  0.0118\n",
      "     49       \u001b[36m32.2778\u001b[0m       \u001b[32m30.2476\u001b[0m  0.0118\n",
      "     50       \u001b[36m32.2728\u001b[0m       \u001b[32m30.2473\u001b[0m  0.0114\n",
      "     51       \u001b[36m32.2680\u001b[0m       \u001b[32m30.2430\u001b[0m  0.0112\n",
      "     52       \u001b[36m32.2635\u001b[0m       \u001b[32m30.2404\u001b[0m  0.0112\n",
      "     53       \u001b[36m32.2594\u001b[0m       30.2410  0.0119\n",
      "     54       \u001b[36m32.2555\u001b[0m       \u001b[32m30.2393\u001b[0m  0.0117\n",
      "     55       \u001b[36m32.2516\u001b[0m       \u001b[32m30.2359\u001b[0m  0.0117\n",
      "     56       \u001b[36m32.2480\u001b[0m       \u001b[32m30.2342\u001b[0m  0.0116\n",
      "     57       \u001b[36m32.2445\u001b[0m       \u001b[32m30.2340\u001b[0m  0.0114\n",
      "     58       \u001b[36m32.2412\u001b[0m       30.2342  0.0112\n",
      "     59       \u001b[36m32.2381\u001b[0m       30.2341  0.0115\n",
      "     60       \u001b[36m32.2351\u001b[0m       \u001b[32m30.2335\u001b[0m  0.0113\n",
      "     61       \u001b[36m32.2321\u001b[0m       \u001b[32m30.2329\u001b[0m  0.0118\n",
      "     62       \u001b[36m32.2293\u001b[0m       \u001b[32m30.2316\u001b[0m  0.0119\n",
      "     63       \u001b[36m32.2266\u001b[0m       \u001b[32m30.2303\u001b[0m  0.0126\n",
      "     64       \u001b[36m32.2240\u001b[0m       \u001b[32m30.2295\u001b[0m  0.0117\n",
      "     65       \u001b[36m32.2216\u001b[0m       30.2298  0.0118\n",
      "     66       \u001b[36m32.2192\u001b[0m       \u001b[32m30.2289\u001b[0m  0.0115\n",
      "     67       \u001b[36m32.2168\u001b[0m       30.2297  0.0169\n",
      "     68       \u001b[36m32.2146\u001b[0m       \u001b[32m30.2287\u001b[0m  0.0220\n",
      "     69       \u001b[36m32.2123\u001b[0m       30.2287  0.0136\n",
      "     70       \u001b[36m32.2103\u001b[0m       30.2295  0.0152\n",
      "     71       \u001b[36m32.2083\u001b[0m       30.2295  0.0154\n",
      "     72       \u001b[36m32.2063\u001b[0m       30.2297  0.0144\n",
      "     73       \u001b[36m32.2044\u001b[0m       30.2307  0.0148\n",
      "     74       \u001b[36m32.2026\u001b[0m       30.2307  0.0125\n",
      "     75       \u001b[36m32.2008\u001b[0m       30.2305  0.0119\n",
      "     76       \u001b[36m32.1991\u001b[0m       30.2306  0.0120\n",
      "     77       \u001b[36m32.1973\u001b[0m       30.2307  0.0118\n",
      "     78       \u001b[36m32.1957\u001b[0m       30.2305  0.0122\n",
      "     79       \u001b[36m32.1940\u001b[0m       30.2300  0.0126\n",
      "     80       \u001b[36m32.1924\u001b[0m       30.2300  0.0119\n",
      "     81       \u001b[36m32.1908\u001b[0m       30.2291  0.0127\n",
      "     82       \u001b[36m32.1892\u001b[0m       \u001b[32m30.2280\u001b[0m  0.0125\n",
      "     83       \u001b[36m32.1877\u001b[0m       30.2293  0.0121\n",
      "     84       \u001b[36m32.1862\u001b[0m       30.2294  0.0115\n",
      "     85       \u001b[36m32.1847\u001b[0m       30.2282  0.0116\n",
      "     86       \u001b[36m32.1833\u001b[0m       30.2280  0.0116\n",
      "     87       \u001b[36m32.1819\u001b[0m       30.2282  0.0124\n",
      "     88       \u001b[36m32.1805\u001b[0m       \u001b[32m30.2264\u001b[0m  0.0141\n",
      "     89       \u001b[36m32.1790\u001b[0m       \u001b[32m30.2247\u001b[0m  0.0123\n",
      "     90       \u001b[36m32.1777\u001b[0m       30.2247  0.0125\n",
      "     91       \u001b[36m32.1764\u001b[0m       \u001b[32m30.2242\u001b[0m  0.0121\n",
      "     92       \u001b[36m32.1750\u001b[0m       \u001b[32m30.2215\u001b[0m  0.0119\n",
      "     93       \u001b[36m32.1736\u001b[0m       \u001b[32m30.2201\u001b[0m  0.0119\n",
      "     94       \u001b[36m32.1724\u001b[0m       30.2202  0.0120\n",
      "     95       \u001b[36m32.1711\u001b[0m       \u001b[32m30.2178\u001b[0m  0.0128\n",
      "     96       \u001b[36m32.1698\u001b[0m       \u001b[32m30.2166\u001b[0m  0.0123\n",
      "     97       \u001b[36m32.1685\u001b[0m       \u001b[32m30.2160\u001b[0m  0.0122\n",
      "     98       \u001b[36m32.1673\u001b[0m       \u001b[32m30.2146\u001b[0m  0.0117\n",
      "     99       \u001b[36m32.1660\u001b[0m       \u001b[32m30.2140\u001b[0m  0.0117\n",
      "    100       \u001b[36m32.1648\u001b[0m       30.2140  0.0131\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m34.2724\u001b[0m       \u001b[32m32.5350\u001b[0m  0.0127\n",
      "      2       \u001b[36m33.4630\u001b[0m       \u001b[32m31.9449\u001b[0m  0.0125\n",
      "      3       \u001b[36m32.6557\u001b[0m       \u001b[32m31.3168\u001b[0m  0.0119\n",
      "      4       \u001b[36m31.7882\u001b[0m       \u001b[32m30.6127\u001b[0m  0.0118\n",
      "      5       \u001b[36m30.8234\u001b[0m       \u001b[32m29.8288\u001b[0m  0.0123\n",
      "      6       \u001b[36m29.7598\u001b[0m       \u001b[32m28.9858\u001b[0m  0.0126\n",
      "      7       \u001b[36m28.6004\u001b[0m       \u001b[32m28.1088\u001b[0m  0.0124\n",
      "      8       \u001b[36m27.3353\u001b[0m       \u001b[32m27.3247\u001b[0m  0.0121\n",
      "      9       \u001b[36m26.0948\u001b[0m       \u001b[32m26.9147\u001b[0m  0.0126\n",
      "     10       \u001b[36m25.1798\u001b[0m       27.1009  0.0121\n",
      "     11       \u001b[36m24.7953\u001b[0m       27.5929  0.0121\n",
      "     12       \u001b[36m24.7291\u001b[0m       27.7159  0.0120\n",
      "     13       \u001b[36m24.5987\u001b[0m       27.4043  0.0126\n",
      "     14       \u001b[36m24.3967\u001b[0m       27.0468  0.0119\n",
      "     15       \u001b[36m24.2509\u001b[0m       \u001b[32m26.8295\u001b[0m  0.0120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     16       \u001b[36m24.1553\u001b[0m       \u001b[32m26.7411\u001b[0m  0.0123\n",
      "     17       \u001b[36m24.0639\u001b[0m       \u001b[32m26.7394\u001b[0m  0.0127\n",
      "     18       \u001b[36m23.9671\u001b[0m       26.7944  0.0122\n",
      "     19       \u001b[36m23.8786\u001b[0m       26.8738  0.0120\n",
      "     20       \u001b[36m23.8051\u001b[0m       26.9404  0.0119\n",
      "     21       \u001b[36m23.7434\u001b[0m       26.9776  0.0120\n",
      "     22       \u001b[36m23.6885\u001b[0m       26.9984  0.0117\n",
      "     23       \u001b[36m23.6408\u001b[0m       27.0153  0.0125\n",
      "     24       \u001b[36m23.5987\u001b[0m       27.0403  0.0123\n",
      "     25       \u001b[36m23.5607\u001b[0m       27.0752  0.0133\n",
      "     26       \u001b[36m23.5256\u001b[0m       27.1120  0.0120\n",
      "     27       \u001b[36m23.4931\u001b[0m       27.1438  0.0118\n",
      "     28       \u001b[36m23.4629\u001b[0m       27.1664  0.0124\n",
      "     29       \u001b[36m23.4351\u001b[0m       27.1793  0.0124\n",
      "     30       \u001b[36m23.4097\u001b[0m       27.1819  0.0117\n",
      "     31       \u001b[36m23.3866\u001b[0m       27.1776  0.0120\n",
      "     32       \u001b[36m23.3654\u001b[0m       27.1724  0.0119\n",
      "     33       \u001b[36m23.3456\u001b[0m       27.1700  0.0123\n",
      "     34       \u001b[36m23.3270\u001b[0m       27.1707  0.0119\n",
      "     35       \u001b[36m23.3096\u001b[0m       27.1718  0.0118\n",
      "     36       \u001b[36m23.2933\u001b[0m       27.1723  0.0144\n",
      "     37       \u001b[36m23.2780\u001b[0m       27.1722  0.0118\n",
      "     38       \u001b[36m23.2635\u001b[0m       27.1709  0.0128\n",
      "     39       \u001b[36m23.2502\u001b[0m       27.1681  0.0125\n",
      "     40       \u001b[36m23.2376\u001b[0m       27.1637  0.0119\n",
      "     41       \u001b[36m23.2258\u001b[0m       27.1562  0.0121\n",
      "     42       \u001b[36m23.2146\u001b[0m       27.1460  0.0164\n",
      "     43       \u001b[36m23.2042\u001b[0m       27.1361  0.0155\n",
      "     44       \u001b[36m23.1944\u001b[0m       27.1270  0.0123\n",
      "     45       \u001b[36m23.1853\u001b[0m       27.1172  0.0123\n",
      "     46       \u001b[36m23.1766\u001b[0m       27.1055  0.0129\n",
      "     47       \u001b[36m23.1685\u001b[0m       27.0928  0.0147\n",
      "     48       \u001b[36m23.1609\u001b[0m       27.0794  0.0134\n",
      "     49       \u001b[36m23.1538\u001b[0m       27.0669  0.0130\n",
      "     50       \u001b[36m23.1473\u001b[0m       27.0568  0.0119\n",
      "     51       \u001b[36m23.1411\u001b[0m       27.0473  0.0120\n",
      "     52       \u001b[36m23.1353\u001b[0m       27.0373  0.0122\n",
      "     53       \u001b[36m23.1300\u001b[0m       27.0267  0.0119\n",
      "     54       \u001b[36m23.1249\u001b[0m       27.0177  0.0119\n",
      "     55       \u001b[36m23.1202\u001b[0m       27.0079  0.0120\n",
      "     56       \u001b[36m23.1158\u001b[0m       26.9976  0.0119\n",
      "     57       \u001b[36m23.1116\u001b[0m       26.9880  0.0118\n",
      "     58       \u001b[36m23.1076\u001b[0m       26.9790  0.0119\n",
      "     59       \u001b[36m23.1038\u001b[0m       26.9713  0.0120\n",
      "     60       \u001b[36m23.1003\u001b[0m       26.9648  0.0116\n",
      "     61       \u001b[36m23.0968\u001b[0m       26.9583  0.0119\n",
      "     62       \u001b[36m23.0936\u001b[0m       26.9522  0.0120\n",
      "     63       \u001b[36m23.0905\u001b[0m       26.9451  0.0117\n",
      "     64       \u001b[36m23.0875\u001b[0m       26.9376  0.0121\n",
      "     65       \u001b[36m23.0847\u001b[0m       26.9311  0.0122\n",
      "     66       \u001b[36m23.0819\u001b[0m       26.9251  0.0124\n",
      "     67       \u001b[36m23.0793\u001b[0m       26.9189  0.0124\n",
      "     68       \u001b[36m23.0767\u001b[0m       26.9131  0.0124\n",
      "     69       \u001b[36m23.0743\u001b[0m       26.9084  0.0124\n",
      "     70       \u001b[36m23.0720\u001b[0m       26.9044  0.0120\n",
      "     71       \u001b[36m23.0698\u001b[0m       26.8995  0.0122\n",
      "     72       \u001b[36m23.0676\u001b[0m       26.8944  0.0123\n",
      "     73       \u001b[36m23.0655\u001b[0m       26.8902  0.0122\n",
      "     74       \u001b[36m23.0634\u001b[0m       26.8865  0.0121\n",
      "     75       \u001b[36m23.0614\u001b[0m       26.8829  0.0119\n",
      "     76       \u001b[36m23.0595\u001b[0m       26.8789  0.0118\n",
      "     77       \u001b[36m23.0576\u001b[0m       26.8741  0.0121\n",
      "     78       \u001b[36m23.0558\u001b[0m       26.8690  0.0120\n",
      "     79       \u001b[36m23.0541\u001b[0m       26.8674  0.0118\n",
      "     80       \u001b[36m23.0524\u001b[0m       26.8652  0.0117\n",
      "     81       \u001b[36m23.0508\u001b[0m       26.8602  0.0118\n",
      "     82       \u001b[36m23.0492\u001b[0m       26.8563  0.0122\n",
      "     83       \u001b[36m23.0477\u001b[0m       26.8554  0.0119\n",
      "     84       \u001b[36m23.0463\u001b[0m       26.8534  0.0120\n",
      "     85       \u001b[36m23.0449\u001b[0m       26.8497  0.0116\n",
      "     86       \u001b[36m23.0435\u001b[0m       26.8469  0.0120\n",
      "     87       \u001b[36m23.0422\u001b[0m       26.8439  0.0118\n",
      "     88       \u001b[36m23.0409\u001b[0m       26.8421  0.0115\n",
      "     89       \u001b[36m23.0397\u001b[0m       26.8392  0.0119\n",
      "     90       \u001b[36m23.0385\u001b[0m       26.8343  0.0117\n",
      "     91       \u001b[36m23.0373\u001b[0m       26.8311  0.0115\n",
      "     92       \u001b[36m23.0361\u001b[0m       26.8279  0.0116\n",
      "     93       \u001b[36m23.0350\u001b[0m       26.8230  0.0119\n",
      "     94       \u001b[36m23.0339\u001b[0m       26.8195  0.0123\n",
      "     95       \u001b[36m23.0329\u001b[0m       26.8168  0.0120\n",
      "     96       \u001b[36m23.0318\u001b[0m       26.8151  0.0114\n",
      "     97       \u001b[36m23.0307\u001b[0m       26.8125  0.0121\n",
      "     98       \u001b[36m23.0297\u001b[0m       26.8076  0.0119\n",
      "     99       \u001b[36m23.0288\u001b[0m       26.8029  0.0119\n",
      "    100       \u001b[36m23.0279\u001b[0m       26.8002  0.0118\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m41.7657\u001b[0m       \u001b[32m32.9849\u001b[0m  0.0123\n",
      "      2       \u001b[36m41.0657\u001b[0m       \u001b[32m32.5181\u001b[0m  0.0121\n",
      "      3       \u001b[36m40.3809\u001b[0m       \u001b[32m32.0191\u001b[0m  0.0120\n",
      "      4       \u001b[36m39.5852\u001b[0m       \u001b[32m31.4157\u001b[0m  0.0118\n",
      "      5       \u001b[36m38.6319\u001b[0m       \u001b[32m30.6897\u001b[0m  0.0121\n",
      "      6       \u001b[36m37.5589\u001b[0m       \u001b[32m29.8667\u001b[0m  0.0117\n",
      "      7       \u001b[36m36.3947\u001b[0m       \u001b[32m28.9749\u001b[0m  0.0119\n",
      "      8       \u001b[36m35.0640\u001b[0m       \u001b[32m28.0500\u001b[0m  0.0120\n",
      "      9       \u001b[36m33.5623\u001b[0m       \u001b[32m27.2370\u001b[0m  0.0120\n",
      "     10       \u001b[36m32.0750\u001b[0m       \u001b[32m26.8286\u001b[0m  0.0118\n",
      "     11       \u001b[36m30.9253\u001b[0m       27.1139  0.0119\n",
      "     12       \u001b[36m30.4013\u001b[0m       27.9428  0.0121\n",
      "     13       \u001b[36m30.3824\u001b[0m       28.4893  0.0117\n",
      "     14       \u001b[36m30.3361\u001b[0m       28.2809  0.0118\n",
      "     15       \u001b[36m30.0854\u001b[0m       27.7521  0.0120\n",
      "     16       \u001b[36m29.8380\u001b[0m       27.3461  0.0118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m29.6719\u001b[0m       27.1538  0.0119\n",
      "     18       \u001b[36m29.5407\u001b[0m       27.1297  0.0119\n",
      "     19       \u001b[36m29.4271\u001b[0m       27.2147  0.0120\n",
      "     20       \u001b[36m29.3341\u001b[0m       27.3411  0.0164\n",
      "     21       \u001b[36m29.2600\u001b[0m       27.4454  0.0134\n",
      "     22       \u001b[36m29.1961\u001b[0m       27.4944  0.0127\n",
      "     23       \u001b[36m29.1325\u001b[0m       27.4997  0.0123\n",
      "     24       \u001b[36m29.0699\u001b[0m       27.4870  0.0122\n",
      "     25       \u001b[36m29.0139\u001b[0m       27.4745  0.0138\n",
      "     26       \u001b[36m28.9648\u001b[0m       27.4696  0.0125\n",
      "     27       \u001b[36m28.9224\u001b[0m       27.4755  0.0139\n",
      "     28       \u001b[36m28.8853\u001b[0m       27.4865  0.0119\n",
      "     29       \u001b[36m28.8520\u001b[0m       27.4915  0.0117\n",
      "     30       \u001b[36m28.8215\u001b[0m       27.4871  0.0117\n",
      "     31       \u001b[36m28.7931\u001b[0m       27.4749  0.0117\n",
      "     32       \u001b[36m28.7660\u001b[0m       27.4634  0.0120\n",
      "     33       \u001b[36m28.7412\u001b[0m       27.4549  0.0118\n",
      "     34       \u001b[36m28.7190\u001b[0m       27.4456  0.0118\n",
      "     35       \u001b[36m28.6985\u001b[0m       27.4376  0.0115\n",
      "     36       \u001b[36m28.6796\u001b[0m       27.4319  0.0119\n",
      "     37       \u001b[36m28.6625\u001b[0m       27.4239  0.0119\n",
      "     38       \u001b[36m28.6465\u001b[0m       27.4140  0.0118\n",
      "     39       \u001b[36m28.6315\u001b[0m       27.4026  0.0120\n",
      "     40       \u001b[36m28.6174\u001b[0m       27.3963  0.0115\n",
      "     41       \u001b[36m28.6046\u001b[0m       27.3856  0.0119\n",
      "     42       \u001b[36m28.5924\u001b[0m       27.3748  0.0117\n",
      "     43       \u001b[36m28.5811\u001b[0m       27.3716  0.0120\n",
      "     44       \u001b[36m28.5710\u001b[0m       27.3672  0.0117\n",
      "     45       \u001b[36m28.5613\u001b[0m       27.3603  0.0122\n",
      "     46       \u001b[36m28.5521\u001b[0m       27.3543  0.0123\n",
      "     47       \u001b[36m28.5436\u001b[0m       27.3485  0.0118\n",
      "     48       \u001b[36m28.5358\u001b[0m       27.3441  0.0118\n",
      "     49       \u001b[36m28.5285\u001b[0m       27.3384  0.0118\n",
      "     50       \u001b[36m28.5215\u001b[0m       27.3336  0.0118\n",
      "     51       \u001b[36m28.5151\u001b[0m       27.3324  0.0124\n",
      "     52       \u001b[36m28.5093\u001b[0m       27.3296  0.0117\n",
      "     53       \u001b[36m28.5040\u001b[0m       27.3250  0.0119\n",
      "     54       \u001b[36m28.4989\u001b[0m       27.3209  0.0116\n",
      "     55       \u001b[36m28.4943\u001b[0m       27.3192  0.0116\n",
      "     56       \u001b[36m28.4900\u001b[0m       27.3167  0.0122\n",
      "     57       \u001b[36m28.4858\u001b[0m       27.3139  0.0122\n",
      "     58       \u001b[36m28.4820\u001b[0m       27.3134  0.0116\n",
      "     59       \u001b[36m28.4785\u001b[0m       27.3112  0.0118\n",
      "     60       \u001b[36m28.4751\u001b[0m       27.3078  0.0116\n",
      "     61       \u001b[36m28.4717\u001b[0m       27.3042  0.0119\n",
      "     62       \u001b[36m28.4685\u001b[0m       27.3031  0.0118\n",
      "     63       \u001b[36m28.4656\u001b[0m       27.3015  0.0118\n",
      "     64       \u001b[36m28.4628\u001b[0m       27.2996  0.0134\n",
      "     65       \u001b[36m28.4602\u001b[0m       27.2997  0.0126\n",
      "     66       \u001b[36m28.4577\u001b[0m       27.2998  0.0119\n",
      "     67       \u001b[36m28.4554\u001b[0m       27.2991  0.0119\n",
      "     68       \u001b[36m28.4532\u001b[0m       27.2977  0.0119\n",
      "     69       \u001b[36m28.4509\u001b[0m       27.2956  0.0115\n",
      "     70       \u001b[36m28.4488\u001b[0m       27.2941  0.0119\n",
      "     71       \u001b[36m28.4467\u001b[0m       27.2942  0.0119\n",
      "     72       \u001b[36m28.4447\u001b[0m       27.2956  0.0121\n",
      "     73       \u001b[36m28.4428\u001b[0m       27.2982  0.0119\n",
      "     74       \u001b[36m28.4410\u001b[0m       27.2989  0.0122\n",
      "     75       \u001b[36m28.4392\u001b[0m       27.2978  0.0121\n",
      "     76       \u001b[36m28.4375\u001b[0m       27.2966  0.0119\n",
      "     77       \u001b[36m28.4358\u001b[0m       27.2973  0.0116\n",
      "     78       \u001b[36m28.4343\u001b[0m       27.2979  0.0122\n",
      "     79       \u001b[36m28.4327\u001b[0m       27.2980  0.0117\n",
      "     80       \u001b[36m28.4312\u001b[0m       27.2973  0.0121\n",
      "     81       \u001b[36m28.4297\u001b[0m       27.2973  0.0119\n",
      "     82       \u001b[36m28.4283\u001b[0m       27.2984  0.0117\n",
      "     83       \u001b[36m28.4270\u001b[0m       27.2994  0.0117\n",
      "     84       \u001b[36m28.4257\u001b[0m       27.2992  0.0120\n",
      "     85       \u001b[36m28.4244\u001b[0m       27.2988  0.0118\n",
      "     86       \u001b[36m28.4232\u001b[0m       27.2994  0.0118\n",
      "     87       \u001b[36m28.4221\u001b[0m       27.2998  0.0119\n",
      "     88       \u001b[36m28.4209\u001b[0m       27.2999  0.0116\n",
      "     89       \u001b[36m28.4198\u001b[0m       27.3007  0.0122\n",
      "     90       \u001b[36m28.4187\u001b[0m       27.3037  0.0118\n",
      "     91       \u001b[36m28.4179\u001b[0m       27.3054  0.0117\n",
      "     92       \u001b[36m28.4168\u001b[0m       27.3062  0.0117\n",
      "     93       \u001b[36m28.4158\u001b[0m       27.3066  0.0121\n",
      "     94       \u001b[36m28.4148\u001b[0m       27.3076  0.0119\n",
      "     95       \u001b[36m28.4139\u001b[0m       27.3079  0.0117\n",
      "     96       \u001b[36m28.4130\u001b[0m       27.3101  0.0122\n",
      "     97       \u001b[36m28.4122\u001b[0m       27.3112  0.0118\n",
      "     98       \u001b[36m28.4114\u001b[0m       27.3119  0.0156\n",
      "     99       \u001b[36m28.4105\u001b[0m       27.3128  0.0134\n",
      "    100       \u001b[36m28.4097\u001b[0m       27.3135  0.0124\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m43.1220\u001b[0m       \u001b[32m45.0756\u001b[0m  0.0119\n",
      "      2       \u001b[36m42.7558\u001b[0m       \u001b[32m44.6342\u001b[0m  0.0116\n",
      "      3       \u001b[36m42.4036\u001b[0m       \u001b[32m44.2070\u001b[0m  0.0134\n",
      "      4       \u001b[36m42.0635\u001b[0m       \u001b[32m43.7923\u001b[0m  0.0117\n",
      "      5       \u001b[36m41.7339\u001b[0m       \u001b[32m43.3885\u001b[0m  0.0125\n",
      "      6       \u001b[36m41.4137\u001b[0m       \u001b[32m42.9946\u001b[0m  0.0112\n",
      "      7       \u001b[36m41.1022\u001b[0m       \u001b[32m42.6096\u001b[0m  0.0109\n",
      "      8       \u001b[36m40.7983\u001b[0m       \u001b[32m42.2324\u001b[0m  0.0113\n",
      "      9       \u001b[36m40.5012\u001b[0m       \u001b[32m41.8623\u001b[0m  0.0118\n",
      "     10       \u001b[36m40.2102\u001b[0m       \u001b[32m41.4984\u001b[0m  0.0111\n",
      "     11       \u001b[36m39.9250\u001b[0m       \u001b[32m41.1407\u001b[0m  0.0110\n",
      "     12       \u001b[36m39.6455\u001b[0m       \u001b[32m40.7886\u001b[0m  0.0108\n",
      "     13       \u001b[36m39.3710\u001b[0m       \u001b[32m40.4417\u001b[0m  0.0118\n",
      "     14       \u001b[36m39.1016\u001b[0m       \u001b[32m40.0998\u001b[0m  0.0112\n",
      "     15       \u001b[36m38.8369\u001b[0m       \u001b[32m39.7628\u001b[0m  0.0112\n",
      "     16       \u001b[36m38.5772\u001b[0m       \u001b[32m39.4305\u001b[0m  0.0111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m38.3223\u001b[0m       \u001b[32m39.1033\u001b[0m  0.0112\n",
      "     18       \u001b[36m38.0722\u001b[0m       \u001b[32m38.7812\u001b[0m  0.0115\n",
      "     19       \u001b[36m37.8271\u001b[0m       \u001b[32m38.4642\u001b[0m  0.0111\n",
      "     20       \u001b[36m37.5871\u001b[0m       \u001b[32m38.1521\u001b[0m  0.0111\n",
      "     21       \u001b[36m37.3521\u001b[0m       \u001b[32m37.8451\u001b[0m  0.0108\n",
      "     22       \u001b[36m37.1222\u001b[0m       \u001b[32m37.5435\u001b[0m  0.0112\n",
      "     23       \u001b[36m36.8978\u001b[0m       \u001b[32m37.2473\u001b[0m  0.0112\n",
      "     24       \u001b[36m36.6787\u001b[0m       \u001b[32m36.9567\u001b[0m  0.0111\n",
      "     25       \u001b[36m36.4651\u001b[0m       \u001b[32m36.6717\u001b[0m  0.0112\n",
      "     26       \u001b[36m36.2571\u001b[0m       \u001b[32m36.3925\u001b[0m  0.0113\n",
      "     27       \u001b[36m36.0547\u001b[0m       \u001b[32m36.1191\u001b[0m  0.0115\n",
      "     28       \u001b[36m35.8580\u001b[0m       \u001b[32m35.8515\u001b[0m  0.0112\n",
      "     29       \u001b[36m35.6669\u001b[0m       \u001b[32m35.5895\u001b[0m  0.0111\n",
      "     30       \u001b[36m35.4814\u001b[0m       \u001b[32m35.3333\u001b[0m  0.0109\n",
      "     31       \u001b[36m35.3015\u001b[0m       \u001b[32m35.0828\u001b[0m  0.0108\n",
      "     32       \u001b[36m35.1273\u001b[0m       \u001b[32m34.8380\u001b[0m  0.0110\n",
      "     33       \u001b[36m34.9588\u001b[0m       \u001b[32m34.5992\u001b[0m  0.0108\n",
      "     34       \u001b[36m34.7961\u001b[0m       \u001b[32m34.3663\u001b[0m  0.0106\n",
      "     35       \u001b[36m34.6392\u001b[0m       \u001b[32m34.1394\u001b[0m  0.0110\n",
      "     36       \u001b[36m34.4881\u001b[0m       \u001b[32m33.9186\u001b[0m  0.0109\n",
      "     37       \u001b[36m34.3430\u001b[0m       \u001b[32m33.7042\u001b[0m  0.0106\n",
      "     38       \u001b[36m34.2038\u001b[0m       \u001b[32m33.4963\u001b[0m  0.0109\n",
      "     39       \u001b[36m34.0708\u001b[0m       \u001b[32m33.2953\u001b[0m  0.0108\n",
      "     40       \u001b[36m33.9441\u001b[0m       \u001b[32m33.1011\u001b[0m  0.0109\n",
      "     41       \u001b[36m33.8237\u001b[0m       \u001b[32m32.9141\u001b[0m  0.0109\n",
      "     42       \u001b[36m33.7096\u001b[0m       \u001b[32m32.7341\u001b[0m  0.0108\n",
      "     43       \u001b[36m33.6017\u001b[0m       \u001b[32m32.5615\u001b[0m  0.0114\n",
      "     44       \u001b[36m33.5003\u001b[0m       \u001b[32m32.3963\u001b[0m  0.0113\n",
      "     45       \u001b[36m33.4051\u001b[0m       \u001b[32m32.2387\u001b[0m  0.0116\n",
      "     46       \u001b[36m33.3162\u001b[0m       \u001b[32m32.0881\u001b[0m  0.0112\n",
      "     47       \u001b[36m33.2335\u001b[0m       \u001b[32m31.9453\u001b[0m  0.0113\n",
      "     48       \u001b[36m33.1568\u001b[0m       \u001b[32m31.8102\u001b[0m  0.0117\n",
      "     49       \u001b[36m33.0860\u001b[0m       \u001b[32m31.6827\u001b[0m  0.0114\n",
      "     50       \u001b[36m33.0209\u001b[0m       \u001b[32m31.5629\u001b[0m  0.0115\n",
      "     51       \u001b[36m32.9613\u001b[0m       \u001b[32m31.4506\u001b[0m  0.0112\n",
      "     52       \u001b[36m32.9069\u001b[0m       \u001b[32m31.3454\u001b[0m  0.0112\n",
      "     53       \u001b[36m32.8575\u001b[0m       \u001b[32m31.2474\u001b[0m  0.0170\n",
      "     54       \u001b[36m32.8128\u001b[0m       \u001b[32m31.1561\u001b[0m  0.0191\n",
      "     55       \u001b[36m32.7724\u001b[0m       \u001b[32m31.0712\u001b[0m  0.0113\n",
      "     56       \u001b[36m32.7358\u001b[0m       \u001b[32m30.9923\u001b[0m  0.0110\n",
      "     57       \u001b[36m32.7029\u001b[0m       \u001b[32m30.9193\u001b[0m  0.0105\n",
      "     58       \u001b[36m32.6733\u001b[0m       \u001b[32m30.8517\u001b[0m  0.0114\n",
      "     59       \u001b[36m32.6469\u001b[0m       \u001b[32m30.7894\u001b[0m  0.0121\n",
      "     60       \u001b[36m32.6232\u001b[0m       \u001b[32m30.7319\u001b[0m  0.0113\n",
      "     61       \u001b[36m32.6019\u001b[0m       \u001b[32m30.6789\u001b[0m  0.0140\n",
      "     62       \u001b[36m32.5829\u001b[0m       \u001b[32m30.6302\u001b[0m  0.0113\n",
      "     63       \u001b[36m32.5658\u001b[0m       \u001b[32m30.5853\u001b[0m  0.0124\n",
      "     64       \u001b[36m32.5506\u001b[0m       \u001b[32m30.5441\u001b[0m  0.0124\n",
      "     65       \u001b[36m32.5369\u001b[0m       \u001b[32m30.5062\u001b[0m  0.0117\n",
      "     66       \u001b[36m32.5247\u001b[0m       \u001b[32m30.4713\u001b[0m  0.0119\n",
      "     67       \u001b[36m32.5136\u001b[0m       \u001b[32m30.4392\u001b[0m  0.0114\n",
      "     68       \u001b[36m32.5037\u001b[0m       \u001b[32m30.4097\u001b[0m  0.0117\n",
      "     69       \u001b[36m32.4948\u001b[0m       \u001b[32m30.3826\u001b[0m  0.0111\n",
      "     70       \u001b[36m32.4868\u001b[0m       \u001b[32m30.3577\u001b[0m  0.0115\n",
      "     71       \u001b[36m32.4794\u001b[0m       \u001b[32m30.3349\u001b[0m  0.0115\n",
      "     72       \u001b[36m32.4727\u001b[0m       \u001b[32m30.3138\u001b[0m  0.0109\n",
      "     73       \u001b[36m32.4666\u001b[0m       \u001b[32m30.2944\u001b[0m  0.0109\n",
      "     74       \u001b[36m32.4610\u001b[0m       \u001b[32m30.2766\u001b[0m  0.0120\n",
      "     75       \u001b[36m32.4557\u001b[0m       \u001b[32m30.2601\u001b[0m  0.0111\n",
      "     76       \u001b[36m32.4509\u001b[0m       \u001b[32m30.2449\u001b[0m  0.0119\n",
      "     77       \u001b[36m32.4463\u001b[0m       \u001b[32m30.2309\u001b[0m  0.0105\n",
      "     78       \u001b[36m32.4421\u001b[0m       \u001b[32m30.2178\u001b[0m  0.0125\n",
      "     79       \u001b[36m32.4381\u001b[0m       \u001b[32m30.2057\u001b[0m  0.0222\n",
      "     80       \u001b[36m32.4343\u001b[0m       \u001b[32m30.1946\u001b[0m  0.0192\n",
      "     81       \u001b[36m32.4307\u001b[0m       \u001b[32m30.1841\u001b[0m  0.0179\n",
      "     82       \u001b[36m32.4273\u001b[0m       \u001b[32m30.1744\u001b[0m  0.0196\n",
      "     83       \u001b[36m32.4241\u001b[0m       \u001b[32m30.1653\u001b[0m  0.0134\n",
      "     84       \u001b[36m32.4210\u001b[0m       \u001b[32m30.1568\u001b[0m  0.0141\n",
      "     85       \u001b[36m32.4181\u001b[0m       \u001b[32m30.1489\u001b[0m  0.0120\n",
      "     86       \u001b[36m32.4152\u001b[0m       \u001b[32m30.1415\u001b[0m  0.0111\n",
      "     87       \u001b[36m32.4125\u001b[0m       \u001b[32m30.1346\u001b[0m  0.0110\n",
      "     88       \u001b[36m32.4098\u001b[0m       \u001b[32m30.1281\u001b[0m  0.0111\n",
      "     89       \u001b[36m32.4072\u001b[0m       \u001b[32m30.1220\u001b[0m  0.0110\n",
      "     90       \u001b[36m32.4047\u001b[0m       \u001b[32m30.1162\u001b[0m  0.0110\n",
      "     91       \u001b[36m32.4023\u001b[0m       \u001b[32m30.1108\u001b[0m  0.0111\n",
      "     92       \u001b[36m32.3999\u001b[0m       \u001b[32m30.1056\u001b[0m  0.0112\n",
      "     93       \u001b[36m32.3976\u001b[0m       \u001b[32m30.1007\u001b[0m  0.0111\n",
      "     94       \u001b[36m32.3953\u001b[0m       \u001b[32m30.0961\u001b[0m  0.0112\n",
      "     95       \u001b[36m32.3931\u001b[0m       \u001b[32m30.0917\u001b[0m  0.0110\n",
      "     96       \u001b[36m32.3910\u001b[0m       \u001b[32m30.0875\u001b[0m  0.0110\n",
      "     97       \u001b[36m32.3889\u001b[0m       \u001b[32m30.0836\u001b[0m  0.0110\n",
      "     98       \u001b[36m32.3868\u001b[0m       \u001b[32m30.0797\u001b[0m  0.0107\n",
      "     99       \u001b[36m32.3848\u001b[0m       \u001b[32m30.0761\u001b[0m  0.0107\n",
      "    100       \u001b[36m32.3828\u001b[0m       \u001b[32m30.0726\u001b[0m  0.0109\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m33.9605\u001b[0m       \u001b[32m32.4777\u001b[0m  0.0110\n",
      "      2       \u001b[36m33.5747\u001b[0m       \u001b[32m32.1905\u001b[0m  0.0110\n",
      "      3       \u001b[36m33.2036\u001b[0m       \u001b[32m31.9145\u001b[0m  0.0108\n",
      "      4       \u001b[36m32.8455\u001b[0m       \u001b[32m31.6483\u001b[0m  0.0110\n",
      "      5       \u001b[36m32.4986\u001b[0m       \u001b[32m31.3908\u001b[0m  0.0112\n",
      "      6       \u001b[36m32.1615\u001b[0m       \u001b[32m31.1412\u001b[0m  0.0110\n",
      "      7       \u001b[36m31.8331\u001b[0m       \u001b[32m30.8986\u001b[0m  0.0111\n",
      "      8       \u001b[36m31.5125\u001b[0m       \u001b[32m30.6625\u001b[0m  0.0114\n",
      "      9       \u001b[36m31.1985\u001b[0m       \u001b[32m30.4323\u001b[0m  0.0111\n",
      "     10       \u001b[36m30.8907\u001b[0m       \u001b[32m30.2073\u001b[0m  0.0114\n",
      "     11       \u001b[36m30.5883\u001b[0m       \u001b[32m29.9874\u001b[0m  0.0110\n",
      "     12       \u001b[36m30.2911\u001b[0m       \u001b[32m29.7724\u001b[0m  0.0112\n",
      "     13       \u001b[36m29.9987\u001b[0m       \u001b[32m29.5621\u001b[0m  0.0113\n",
      "     14       \u001b[36m29.7106\u001b[0m       \u001b[32m29.3562\u001b[0m  0.0112\n",
      "     15       \u001b[36m29.4267\u001b[0m       \u001b[32m29.1549\u001b[0m  0.0112\n",
      "     16       \u001b[36m29.1470\u001b[0m       \u001b[32m28.9578\u001b[0m  0.0114\n",
      "     17       \u001b[36m28.8715\u001b[0m       \u001b[32m28.7652\u001b[0m  0.0115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     18       \u001b[36m28.6004\u001b[0m       \u001b[32m28.5774\u001b[0m  0.0121\n",
      "     19       \u001b[36m28.3337\u001b[0m       \u001b[32m28.3946\u001b[0m  0.0113\n",
      "     20       \u001b[36m28.0717\u001b[0m       \u001b[32m28.2169\u001b[0m  0.0114\n",
      "     21       \u001b[36m27.8145\u001b[0m       \u001b[32m28.0447\u001b[0m  0.0112\n",
      "     22       \u001b[36m27.5625\u001b[0m       \u001b[32m27.8782\u001b[0m  0.0110\n",
      "     23       \u001b[36m27.3159\u001b[0m       \u001b[32m27.7178\u001b[0m  0.0115\n",
      "     24       \u001b[36m27.0749\u001b[0m       \u001b[32m27.5637\u001b[0m  0.0112\n",
      "     25       \u001b[36m26.8398\u001b[0m       \u001b[32m27.4164\u001b[0m  0.0110\n",
      "     26       \u001b[36m26.6112\u001b[0m       \u001b[32m27.2762\u001b[0m  0.0110\n",
      "     27       \u001b[36m26.3895\u001b[0m       \u001b[32m27.1435\u001b[0m  0.0111\n",
      "     28       \u001b[36m26.1751\u001b[0m       \u001b[32m27.0187\u001b[0m  0.0112\n",
      "     29       \u001b[36m25.9683\u001b[0m       \u001b[32m26.9020\u001b[0m  0.0110\n",
      "     30       \u001b[36m25.7695\u001b[0m       \u001b[32m26.7937\u001b[0m  0.0109\n",
      "     31       \u001b[36m25.5795\u001b[0m       \u001b[32m26.6942\u001b[0m  0.0107\n",
      "     32       \u001b[36m25.3982\u001b[0m       \u001b[32m26.6035\u001b[0m  0.0110\n",
      "     33       \u001b[36m25.2263\u001b[0m       \u001b[32m26.5217\u001b[0m  0.0112\n",
      "     34       \u001b[36m25.0639\u001b[0m       \u001b[32m26.4488\u001b[0m  0.0111\n",
      "     35       \u001b[36m24.9113\u001b[0m       \u001b[32m26.3847\u001b[0m  0.0110\n",
      "     36       \u001b[36m24.7683\u001b[0m       \u001b[32m26.3293\u001b[0m  0.0112\n",
      "     37       \u001b[36m24.6350\u001b[0m       \u001b[32m26.2822\u001b[0m  0.0113\n",
      "     38       \u001b[36m24.5113\u001b[0m       \u001b[32m26.2431\u001b[0m  0.0113\n",
      "     39       \u001b[36m24.3969\u001b[0m       \u001b[32m26.2116\u001b[0m  0.0113\n",
      "     40       \u001b[36m24.2917\u001b[0m       \u001b[32m26.1873\u001b[0m  0.0115\n",
      "     41       \u001b[36m24.1953\u001b[0m       \u001b[32m26.1695\u001b[0m  0.0115\n",
      "     42       \u001b[36m24.1074\u001b[0m       \u001b[32m26.1577\u001b[0m  0.0113\n",
      "     43       \u001b[36m24.0277\u001b[0m       \u001b[32m26.1513\u001b[0m  0.0113\n",
      "     44       \u001b[36m23.9556\u001b[0m       \u001b[32m26.1496\u001b[0m  0.0113\n",
      "     45       \u001b[36m23.8907\u001b[0m       26.1520  0.0114\n",
      "     46       \u001b[36m23.8324\u001b[0m       26.1579  0.0113\n",
      "     47       \u001b[36m23.7802\u001b[0m       26.1666  0.0111\n",
      "     48       \u001b[36m23.7336\u001b[0m       26.1776  0.0111\n",
      "     49       \u001b[36m23.6920\u001b[0m       26.1904  0.0110\n",
      "     50       \u001b[36m23.6550\u001b[0m       26.2045  0.0110\n",
      "     51       \u001b[36m23.6220\u001b[0m       26.2195  0.0110\n",
      "     52       \u001b[36m23.5926\u001b[0m       26.2350  0.0108\n",
      "     53       \u001b[36m23.5665\u001b[0m       26.2506  0.0111\n",
      "     54       \u001b[36m23.5431\u001b[0m       26.2664  0.0111\n",
      "     55       \u001b[36m23.5223\u001b[0m       26.2818  0.0109\n",
      "     56       \u001b[36m23.5037\u001b[0m       26.2968  0.0108\n",
      "     57       \u001b[36m23.4869\u001b[0m       26.3113  0.0110\n",
      "     58       \u001b[36m23.4718\u001b[0m       26.3250  0.0113\n",
      "     59       \u001b[36m23.4581\u001b[0m       26.3381  0.0118\n",
      "     60       \u001b[36m23.4458\u001b[0m       26.3505  0.0168\n",
      "     61       \u001b[36m23.4345\u001b[0m       26.3620  0.0208\n",
      "     62       \u001b[36m23.4242\u001b[0m       26.3728  0.0180\n",
      "     63       \u001b[36m23.4147\u001b[0m       26.3828  0.0158\n",
      "     64       \u001b[36m23.4059\u001b[0m       26.3920  0.0138\n",
      "     65       \u001b[36m23.3977\u001b[0m       26.4005  0.0131\n",
      "     66       \u001b[36m23.3900\u001b[0m       26.4082  0.0114\n",
      "     67       \u001b[36m23.3827\u001b[0m       26.4153  0.0114\n",
      "     68       \u001b[36m23.3759\u001b[0m       26.4216  0.0112\n",
      "     69       \u001b[36m23.3695\u001b[0m       26.4274  0.0110\n",
      "     70       \u001b[36m23.3634\u001b[0m       26.4326  0.0110\n",
      "     71       \u001b[36m23.3575\u001b[0m       26.4372  0.0110\n",
      "     72       \u001b[36m23.3519\u001b[0m       26.4414  0.0111\n",
      "     73       \u001b[36m23.3465\u001b[0m       26.4452  0.0110\n",
      "     74       \u001b[36m23.3414\u001b[0m       26.4484  0.0112\n",
      "     75       \u001b[36m23.3364\u001b[0m       26.4514  0.0112\n",
      "     76       \u001b[36m23.3316\u001b[0m       26.4540  0.0111\n",
      "     77       \u001b[36m23.3270\u001b[0m       26.4562  0.0109\n",
      "     78       \u001b[36m23.3225\u001b[0m       26.4581  0.0111\n",
      "     79       \u001b[36m23.3182\u001b[0m       26.4598  0.0109\n",
      "     80       \u001b[36m23.3139\u001b[0m       26.4613  0.0112\n",
      "     81       \u001b[36m23.3099\u001b[0m       26.4626  0.0113\n",
      "     82       \u001b[36m23.3059\u001b[0m       26.4637  0.0113\n",
      "     83       \u001b[36m23.3020\u001b[0m       26.4646  0.0109\n",
      "     84       \u001b[36m23.2982\u001b[0m       26.4653  0.0109\n",
      "     85       \u001b[36m23.2945\u001b[0m       26.4659  0.0107\n",
      "     86       \u001b[36m23.2909\u001b[0m       26.4664  0.0111\n",
      "     87       \u001b[36m23.2874\u001b[0m       26.4668  0.0116\n",
      "     88       \u001b[36m23.2840\u001b[0m       26.4672  0.0114\n",
      "     89       \u001b[36m23.2806\u001b[0m       26.4675  0.0112\n",
      "     90       \u001b[36m23.2774\u001b[0m       26.4677  0.0112\n",
      "     91       \u001b[36m23.2742\u001b[0m       26.4678  0.0112\n",
      "     92       \u001b[36m23.2711\u001b[0m       26.4679  0.0114\n",
      "     93       \u001b[36m23.2680\u001b[0m       26.4679  0.0114\n",
      "     94       \u001b[36m23.2650\u001b[0m       26.4679  0.0115\n",
      "     95       \u001b[36m23.2621\u001b[0m       26.4678  0.0111\n",
      "     96       \u001b[36m23.2592\u001b[0m       26.4676  0.0112\n",
      "     97       \u001b[36m23.2563\u001b[0m       26.4675  0.0112\n",
      "     98       \u001b[36m23.2536\u001b[0m       26.4672  0.0111\n",
      "     99       \u001b[36m23.2508\u001b[0m       26.4671  0.0111\n",
      "    100       \u001b[36m23.2481\u001b[0m       26.4667  0.0109\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m40.4554\u001b[0m       \u001b[32m32.0193\u001b[0m  0.0110\n",
      "      2       \u001b[36m39.8608\u001b[0m       \u001b[32m31.6286\u001b[0m  0.0112\n",
      "      3       \u001b[36m39.2909\u001b[0m       \u001b[32m31.2544\u001b[0m  0.0110\n",
      "      4       \u001b[36m38.7423\u001b[0m       \u001b[32m30.8951\u001b[0m  0.0110\n",
      "      5       \u001b[36m38.2127\u001b[0m       \u001b[32m30.5493\u001b[0m  0.0109\n",
      "      6       \u001b[36m37.7004\u001b[0m       \u001b[32m30.2164\u001b[0m  0.0111\n",
      "      7       \u001b[36m37.2042\u001b[0m       \u001b[32m29.8957\u001b[0m  0.0117\n",
      "      8       \u001b[36m36.7237\u001b[0m       \u001b[32m29.5870\u001b[0m  0.0119\n",
      "      9       \u001b[36m36.2579\u001b[0m       \u001b[32m29.2900\u001b[0m  0.0112\n",
      "     10       \u001b[36m35.8065\u001b[0m       \u001b[32m29.0049\u001b[0m  0.0112\n",
      "     11       \u001b[36m35.3697\u001b[0m       \u001b[32m28.7318\u001b[0m  0.0112\n",
      "     12       \u001b[36m34.9471\u001b[0m       \u001b[32m28.4706\u001b[0m  0.0115\n",
      "     13       \u001b[36m34.5386\u001b[0m       \u001b[32m28.2215\u001b[0m  0.0113\n",
      "     14       \u001b[36m34.1438\u001b[0m       \u001b[32m27.9848\u001b[0m  0.0111\n",
      "     15       \u001b[36m33.7626\u001b[0m       \u001b[32m27.7605\u001b[0m  0.0112\n",
      "     16       \u001b[36m33.3946\u001b[0m       \u001b[32m27.5490\u001b[0m  0.0113\n",
      "     17       \u001b[36m33.0396\u001b[0m       \u001b[32m27.3503\u001b[0m  0.0116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     18       \u001b[36m32.6972\u001b[0m       \u001b[32m27.1649\u001b[0m  0.0116\n",
      "     19       \u001b[36m32.3673\u001b[0m       \u001b[32m26.9930\u001b[0m  0.0114\n",
      "     20       \u001b[36m32.0499\u001b[0m       \u001b[32m26.8354\u001b[0m  0.0113\n",
      "     21       \u001b[36m31.7457\u001b[0m       \u001b[32m26.6924\u001b[0m  0.0110\n",
      "     22       \u001b[36m31.4546\u001b[0m       \u001b[32m26.5647\u001b[0m  0.0112\n",
      "     23       \u001b[36m31.1774\u001b[0m       \u001b[32m26.4527\u001b[0m  0.0109\n",
      "     24       \u001b[36m30.9144\u001b[0m       \u001b[32m26.3569\u001b[0m  0.0109\n",
      "     25       \u001b[36m30.6664\u001b[0m       \u001b[32m26.2774\u001b[0m  0.0112\n",
      "     26       \u001b[36m30.4343\u001b[0m       \u001b[32m26.2143\u001b[0m  0.0111\n",
      "     27       \u001b[36m30.2194\u001b[0m       \u001b[32m26.1677\u001b[0m  0.0110\n",
      "     28       \u001b[36m30.0217\u001b[0m       \u001b[32m26.1370\u001b[0m  0.0110\n",
      "     29       \u001b[36m29.8420\u001b[0m       \u001b[32m26.1213\u001b[0m  0.0109\n",
      "     30       \u001b[36m29.6798\u001b[0m       \u001b[32m26.1197\u001b[0m  0.0112\n",
      "     31       \u001b[36m29.5349\u001b[0m       26.1306  0.0115\n",
      "     32       \u001b[36m29.4062\u001b[0m       26.1528  0.0112\n",
      "     33       \u001b[36m29.2932\u001b[0m       26.1846  0.0114\n",
      "     34       \u001b[36m29.1949\u001b[0m       26.2240  0.0112\n",
      "     35       \u001b[36m29.1103\u001b[0m       26.2692  0.0115\n",
      "     36       \u001b[36m29.0382\u001b[0m       26.3186  0.0112\n",
      "     37       \u001b[36m28.9769\u001b[0m       26.3708  0.0111\n",
      "     38       \u001b[36m28.9253\u001b[0m       26.4243  0.0116\n",
      "     39       \u001b[36m28.8821\u001b[0m       26.4780  0.0121\n",
      "     40       \u001b[36m28.8463\u001b[0m       26.5307  0.0113\n",
      "     41       \u001b[36m28.8164\u001b[0m       26.5817  0.0140\n",
      "     42       \u001b[36m28.7916\u001b[0m       26.6305  0.0125\n",
      "     43       \u001b[36m28.7709\u001b[0m       26.6764  0.0118\n",
      "     44       \u001b[36m28.7538\u001b[0m       26.7192  0.0115\n",
      "     45       \u001b[36m28.7394\u001b[0m       26.7587  0.0118\n",
      "     46       \u001b[36m28.7273\u001b[0m       26.7949  0.0113\n",
      "     47       \u001b[36m28.7169\u001b[0m       26.8278  0.0124\n",
      "     48       \u001b[36m28.7080\u001b[0m       26.8576  0.0116\n",
      "     49       \u001b[36m28.7002\u001b[0m       26.8844  0.0129\n",
      "     50       \u001b[36m28.6933\u001b[0m       26.9084  0.0123\n",
      "     51       \u001b[36m28.6872\u001b[0m       26.9298  0.0115\n",
      "     52       \u001b[36m28.6815\u001b[0m       26.9489  0.0113\n",
      "     53       \u001b[36m28.6763\u001b[0m       26.9657  0.0112\n",
      "     54       \u001b[36m28.6715\u001b[0m       26.9807  0.0110\n",
      "     55       \u001b[36m28.6671\u001b[0m       26.9940  0.0111\n",
      "     56       \u001b[36m28.6628\u001b[0m       27.0056  0.0111\n",
      "     57       \u001b[36m28.6587\u001b[0m       27.0159  0.0113\n",
      "     58       \u001b[36m28.6547\u001b[0m       27.0248  0.0113\n",
      "     59       \u001b[36m28.6509\u001b[0m       27.0327  0.0113\n",
      "     60       \u001b[36m28.6473\u001b[0m       27.0397  0.0114\n",
      "     61       \u001b[36m28.6438\u001b[0m       27.0458  0.0135\n",
      "     62       \u001b[36m28.6404\u001b[0m       27.0509  0.0109\n",
      "     63       \u001b[36m28.6370\u001b[0m       27.0554  0.0108\n",
      "     64       \u001b[36m28.6336\u001b[0m       27.0592  0.0112\n",
      "     65       \u001b[36m28.6303\u001b[0m       27.0625  0.0136\n",
      "     66       \u001b[36m28.6271\u001b[0m       27.0653  0.0117\n",
      "     67       \u001b[36m28.6239\u001b[0m       27.0676  0.0125\n",
      "     68       \u001b[36m28.6207\u001b[0m       27.0695  0.0141\n",
      "     69       \u001b[36m28.6177\u001b[0m       27.0711  0.0114\n",
      "     70       \u001b[36m28.6147\u001b[0m       27.0725  0.0115\n",
      "     71       \u001b[36m28.6117\u001b[0m       27.0736  0.0114\n",
      "     72       \u001b[36m28.6088\u001b[0m       27.0744  0.0111\n",
      "     73       \u001b[36m28.6059\u001b[0m       27.0751  0.0114\n",
      "     74       \u001b[36m28.6031\u001b[0m       27.0756  0.0119\n",
      "     75       \u001b[36m28.6004\u001b[0m       27.0762  0.0113\n",
      "     76       \u001b[36m28.5978\u001b[0m       27.0767  0.0136\n",
      "     77       \u001b[36m28.5951\u001b[0m       27.0771  0.0123\n",
      "     78       \u001b[36m28.5926\u001b[0m       27.0773  0.0121\n",
      "     79       \u001b[36m28.5901\u001b[0m       27.0776  0.0112\n",
      "     80       \u001b[36m28.5876\u001b[0m       27.0780  0.0115\n",
      "     81       \u001b[36m28.5852\u001b[0m       27.0783  0.0125\n",
      "     82       \u001b[36m28.5829\u001b[0m       27.0785  0.0117\n",
      "     83       \u001b[36m28.5806\u001b[0m       27.0787  0.0117\n",
      "     84       \u001b[36m28.5783\u001b[0m       27.0789  0.0118\n",
      "     85       \u001b[36m28.5761\u001b[0m       27.0791  0.0120\n",
      "     86       \u001b[36m28.5739\u001b[0m       27.0792  0.0120\n",
      "     87       \u001b[36m28.5718\u001b[0m       27.0793  0.0117\n",
      "     88       \u001b[36m28.5697\u001b[0m       27.0794  0.0113\n",
      "     89       \u001b[36m28.5676\u001b[0m       27.0794  0.0113\n",
      "     90       \u001b[36m28.5656\u001b[0m       27.0795  0.0135\n",
      "     91       \u001b[36m28.5636\u001b[0m       27.0795  0.0116\n",
      "     92       \u001b[36m28.5616\u001b[0m       27.0795  0.0120\n",
      "     93       \u001b[36m28.5597\u001b[0m       27.0795  0.0115\n",
      "     94       \u001b[36m28.5578\u001b[0m       27.0795  0.0115\n",
      "     95       \u001b[36m28.5559\u001b[0m       27.0796  0.0114\n",
      "     96       \u001b[36m28.5541\u001b[0m       27.0797  0.0111\n",
      "     97       \u001b[36m28.5523\u001b[0m       27.0798  0.0119\n",
      "     98       \u001b[36m28.5506\u001b[0m       27.0799  0.0115\n",
      "     99       \u001b[36m28.5489\u001b[0m       27.0801  0.0113\n",
      "    100       \u001b[36m28.5472\u001b[0m       27.0803  0.0114\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m41.6832\u001b[0m       \u001b[32m43.1798\u001b[0m  0.0206\n",
      "      2       \u001b[36m41.1604\u001b[0m       \u001b[32m42.4379\u001b[0m  0.0150\n",
      "      3       \u001b[36m40.5715\u001b[0m       \u001b[32m41.5604\u001b[0m  0.0117\n",
      "      4       \u001b[36m39.8791\u001b[0m       \u001b[32m40.5492\u001b[0m  0.0119\n",
      "      5       \u001b[36m39.1004\u001b[0m       \u001b[32m39.4830\u001b[0m  0.0124\n",
      "      6       \u001b[36m38.2741\u001b[0m       \u001b[32m38.3824\u001b[0m  0.0123\n",
      "      7       \u001b[36m37.4042\u001b[0m       \u001b[32m37.1663\u001b[0m  0.0126\n",
      "      8       \u001b[36m36.4557\u001b[0m       \u001b[32m35.7516\u001b[0m  0.0133\n",
      "      9       \u001b[36m35.4545\u001b[0m       \u001b[32m34.2383\u001b[0m  0.0141\n",
      "     10       \u001b[36m34.5357\u001b[0m       \u001b[32m32.8760\u001b[0m  0.0127\n",
      "     11       \u001b[36m33.9207\u001b[0m       \u001b[32m31.9495\u001b[0m  0.0121\n",
      "     12       \u001b[36m33.6865\u001b[0m       \u001b[32m31.4963\u001b[0m  0.0150\n",
      "     13       \u001b[36m33.6320\u001b[0m       \u001b[32m31.3243\u001b[0m  0.0135\n",
      "     14       \u001b[36m33.5291\u001b[0m       \u001b[32m31.2862\u001b[0m  0.0133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     15       \u001b[36m33.3794\u001b[0m       31.3134  0.0130\n",
      "     16       \u001b[36m33.2508\u001b[0m       31.3380  0.0124\n",
      "     17       \u001b[36m33.1541\u001b[0m       31.3052  0.0122\n",
      "     18       \u001b[36m33.0713\u001b[0m       \u001b[32m31.2128\u001b[0m  0.0131\n",
      "     19       \u001b[36m32.9923\u001b[0m       \u001b[32m31.0874\u001b[0m  0.0199\n",
      "     20       \u001b[36m32.9184\u001b[0m       \u001b[32m30.9673\u001b[0m  0.0170\n",
      "     21       \u001b[36m32.8538\u001b[0m       \u001b[32m30.8696\u001b[0m  0.0173\n",
      "     22       \u001b[36m32.7986\u001b[0m       \u001b[32m30.7915\u001b[0m  0.0156\n",
      "     23       \u001b[36m32.7496\u001b[0m       \u001b[32m30.7288\u001b[0m  0.0157\n",
      "     24       \u001b[36m32.7049\u001b[0m       \u001b[32m30.6846\u001b[0m  0.0126\n",
      "     25       \u001b[36m32.6640\u001b[0m       \u001b[32m30.6557\u001b[0m  0.0121\n",
      "     26       \u001b[36m32.6269\u001b[0m       \u001b[32m30.6340\u001b[0m  0.0116\n",
      "     27       \u001b[36m32.5931\u001b[0m       \u001b[32m30.6115\u001b[0m  0.0118\n",
      "     28       \u001b[36m32.5619\u001b[0m       \u001b[32m30.5900\u001b[0m  0.0122\n",
      "     29       \u001b[36m32.5334\u001b[0m       \u001b[32m30.5724\u001b[0m  0.0120\n",
      "     30       \u001b[36m32.5074\u001b[0m       \u001b[32m30.5568\u001b[0m  0.0126\n",
      "     31       \u001b[36m32.4834\u001b[0m       \u001b[32m30.5416\u001b[0m  0.0117\n",
      "     32       \u001b[36m32.4612\u001b[0m       \u001b[32m30.5302\u001b[0m  0.0118\n",
      "     33       \u001b[36m32.4409\u001b[0m       \u001b[32m30.5207\u001b[0m  0.0124\n",
      "     34       \u001b[36m32.4220\u001b[0m       \u001b[32m30.5095\u001b[0m  0.0120\n",
      "     35       \u001b[36m32.4048\u001b[0m       \u001b[32m30.4960\u001b[0m  0.0118\n",
      "     36       \u001b[36m32.3892\u001b[0m       \u001b[32m30.4880\u001b[0m  0.0112\n",
      "     37       \u001b[36m32.3747\u001b[0m       \u001b[32m30.4759\u001b[0m  0.0114\n",
      "     38       \u001b[36m32.3615\u001b[0m       \u001b[32m30.4607\u001b[0m  0.0121\n",
      "     39       \u001b[36m32.3492\u001b[0m       \u001b[32m30.4505\u001b[0m  0.0117\n",
      "     40       \u001b[36m32.3380\u001b[0m       \u001b[32m30.4437\u001b[0m  0.0117\n",
      "     41       \u001b[36m32.3275\u001b[0m       \u001b[32m30.4350\u001b[0m  0.0117\n",
      "     42       \u001b[36m32.3178\u001b[0m       \u001b[32m30.4296\u001b[0m  0.0117\n",
      "     43       \u001b[36m32.3086\u001b[0m       \u001b[32m30.4229\u001b[0m  0.0121\n",
      "     44       \u001b[36m32.3001\u001b[0m       \u001b[32m30.4150\u001b[0m  0.0119\n",
      "     45       \u001b[36m32.2921\u001b[0m       \u001b[32m30.4069\u001b[0m  0.0127\n",
      "     46       \u001b[36m32.2845\u001b[0m       \u001b[32m30.3997\u001b[0m  0.0122\n",
      "     47       \u001b[36m32.2775\u001b[0m       \u001b[32m30.3920\u001b[0m  0.0114\n",
      "     48       \u001b[36m32.2709\u001b[0m       \u001b[32m30.3817\u001b[0m  0.0117\n",
      "     49       \u001b[36m32.2647\u001b[0m       \u001b[32m30.3726\u001b[0m  0.0120\n",
      "     50       \u001b[36m32.2589\u001b[0m       \u001b[32m30.3649\u001b[0m  0.0119\n",
      "     51       \u001b[36m32.2535\u001b[0m       \u001b[32m30.3551\u001b[0m  0.0119\n",
      "     52       \u001b[36m32.2482\u001b[0m       \u001b[32m30.3454\u001b[0m  0.0126\n",
      "     53       \u001b[36m32.2433\u001b[0m       \u001b[32m30.3350\u001b[0m  0.0135\n",
      "     54       \u001b[36m32.2386\u001b[0m       \u001b[32m30.3260\u001b[0m  0.0123\n",
      "     55       \u001b[36m32.2342\u001b[0m       \u001b[32m30.3202\u001b[0m  0.0123\n",
      "     56       \u001b[36m32.2299\u001b[0m       \u001b[32m30.3136\u001b[0m  0.0119\n",
      "     57       \u001b[36m32.2257\u001b[0m       \u001b[32m30.3083\u001b[0m  0.0119\n",
      "     58       \u001b[36m32.2217\u001b[0m       \u001b[32m30.3025\u001b[0m  0.0134\n",
      "     59       \u001b[36m32.2179\u001b[0m       \u001b[32m30.2958\u001b[0m  0.0124\n",
      "     60       \u001b[36m32.2142\u001b[0m       \u001b[32m30.2899\u001b[0m  0.0123\n",
      "     61       \u001b[36m32.2108\u001b[0m       \u001b[32m30.2830\u001b[0m  0.0124\n",
      "     62       \u001b[36m32.2074\u001b[0m       \u001b[32m30.2775\u001b[0m  0.0123\n",
      "     63       \u001b[36m32.2043\u001b[0m       \u001b[32m30.2712\u001b[0m  0.0143\n",
      "     64       \u001b[36m32.2012\u001b[0m       \u001b[32m30.2659\u001b[0m  0.0130\n",
      "     65       \u001b[36m32.1982\u001b[0m       \u001b[32m30.2591\u001b[0m  0.0122\n",
      "     66       \u001b[36m32.1953\u001b[0m       \u001b[32m30.2548\u001b[0m  0.0119\n",
      "     67       \u001b[36m32.1925\u001b[0m       \u001b[32m30.2496\u001b[0m  0.0119\n",
      "     68       \u001b[36m32.1898\u001b[0m       \u001b[32m30.2473\u001b[0m  0.0120\n",
      "     69       \u001b[36m32.1871\u001b[0m       \u001b[32m30.2420\u001b[0m  0.0119\n",
      "     70       \u001b[36m32.1845\u001b[0m       \u001b[32m30.2403\u001b[0m  0.0119\n",
      "     71       \u001b[36m32.1820\u001b[0m       \u001b[32m30.2340\u001b[0m  0.0120\n",
      "     72       \u001b[36m32.1795\u001b[0m       30.2349  0.0117\n",
      "     73       \u001b[36m32.1772\u001b[0m       \u001b[32m30.2245\u001b[0m  0.0122\n",
      "     74       \u001b[36m32.1748\u001b[0m       30.2300  0.0119\n",
      "     75       \u001b[36m32.1728\u001b[0m       \u001b[32m30.2120\u001b[0m  0.0120\n",
      "     76       \u001b[36m32.1706\u001b[0m       30.2291  0.0128\n",
      "     77       \u001b[36m32.1691\u001b[0m       \u001b[32m30.1975\u001b[0m  0.0219\n",
      "     78       \u001b[36m32.1675\u001b[0m       30.2402  0.0130\n",
      "     79       32.1682       \u001b[32m30.1797\u001b[0m  0.0124\n",
      "     80       32.1687       30.2483  0.0122\n",
      "     81       32.1699       \u001b[32m30.1688\u001b[0m  0.0127\n",
      "     82       32.1683       30.2052  0.0125\n",
      "     83       \u001b[36m32.1618\u001b[0m       30.1895  0.0130\n",
      "     84       \u001b[36m32.1551\u001b[0m       30.1710  0.0124\n",
      "     85       \u001b[36m32.1535\u001b[0m       30.1986  0.0125\n",
      "     86       \u001b[36m32.1527\u001b[0m       30.1737  0.0131\n",
      "     87       \u001b[36m32.1504\u001b[0m       30.1834  0.0126\n",
      "     88       \u001b[36m32.1487\u001b[0m       30.1844  0.0130\n",
      "     89       \u001b[36m32.1465\u001b[0m       30.1753  0.0123\n",
      "     90       \u001b[36m32.1451\u001b[0m       30.1824  0.0121\n",
      "     91       \u001b[36m32.1438\u001b[0m       30.1747  0.0129\n",
      "     92       \u001b[36m32.1421\u001b[0m       30.1763  0.0122\n",
      "     93       \u001b[36m32.1408\u001b[0m       30.1761  0.0164\n",
      "     94       \u001b[36m32.1393\u001b[0m       30.1732  0.0207\n",
      "     95       \u001b[36m32.1379\u001b[0m       30.1736  0.0206\n",
      "     96       \u001b[36m32.1366\u001b[0m       30.1706  0.0169\n",
      "     97       \u001b[36m32.1353\u001b[0m       30.1708  0.0127\n",
      "     98       \u001b[36m32.1341\u001b[0m       30.1690  0.0154\n",
      "     99       \u001b[36m32.1328\u001b[0m       \u001b[32m30.1677\u001b[0m  0.0124\n",
      "    100       \u001b[36m32.1315\u001b[0m       \u001b[32m30.1662\u001b[0m  0.0147\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m34.8069\u001b[0m       \u001b[32m32.9352\u001b[0m  0.0125\n",
      "      2       \u001b[36m33.9310\u001b[0m       \u001b[32m32.3009\u001b[0m  0.0123\n",
      "      3       \u001b[36m33.0900\u001b[0m       \u001b[32m31.6783\u001b[0m  0.0123\n",
      "      4       \u001b[36m32.2487\u001b[0m       \u001b[32m31.0317\u001b[0m  0.0121\n",
      "      5       \u001b[36m31.3872\u001b[0m       \u001b[32m30.3506\u001b[0m  0.0121\n",
      "      6       \u001b[36m30.4856\u001b[0m       \u001b[32m29.6152\u001b[0m  0.0123\n",
      "      7       \u001b[36m29.4979\u001b[0m       \u001b[32m28.8142\u001b[0m  0.0123\n",
      "      8       \u001b[36m28.4038\u001b[0m       \u001b[32m28.0322\u001b[0m  0.0125\n",
      "      9       \u001b[36m27.2750\u001b[0m       \u001b[32m27.3944\u001b[0m  0.0121\n",
      "     10       \u001b[36m26.2370\u001b[0m       \u001b[32m27.0334\u001b[0m  0.0122\n",
      "     11       \u001b[36m25.4264\u001b[0m       \u001b[32m27.0264\u001b[0m  0.0118\n",
      "     12       \u001b[36m24.9364\u001b[0m       27.2782  0.0127\n",
      "     13       \u001b[36m24.7216\u001b[0m       27.5126  0.0127\n",
      "     14       \u001b[36m24.6122\u001b[0m       27.5113  0.0124\n",
      "     15       \u001b[36m24.4819\u001b[0m       27.3092  0.0122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     16       \u001b[36m24.3334\u001b[0m       27.0635  0.0124\n",
      "     17       \u001b[36m24.2064\u001b[0m       \u001b[32m26.8783\u001b[0m  0.0131\n",
      "     18       \u001b[36m24.1079\u001b[0m       \u001b[32m26.7766\u001b[0m  0.0128\n",
      "     19       \u001b[36m24.0245\u001b[0m       \u001b[32m26.7429\u001b[0m  0.0121\n",
      "     20       \u001b[36m23.9470\u001b[0m       26.7550  0.0121\n",
      "     21       \u001b[36m23.8725\u001b[0m       26.7937  0.0125\n",
      "     22       \u001b[36m23.8029\u001b[0m       26.8427  0.0128\n",
      "     23       \u001b[36m23.7407\u001b[0m       26.8877  0.0125\n",
      "     24       \u001b[36m23.6856\u001b[0m       26.9193  0.0128\n",
      "     25       \u001b[36m23.6364\u001b[0m       26.9336  0.0122\n",
      "     26       \u001b[36m23.5928\u001b[0m       26.9338  0.0123\n",
      "     27       \u001b[36m23.5537\u001b[0m       26.9277  0.0125\n",
      "     28       \u001b[36m23.5183\u001b[0m       26.9236  0.0122\n",
      "     29       \u001b[36m23.4858\u001b[0m       26.9224  0.0123\n",
      "     30       \u001b[36m23.4559\u001b[0m       26.9225  0.0121\n",
      "     31       \u001b[36m23.4282\u001b[0m       26.9227  0.0121\n",
      "     32       \u001b[36m23.4026\u001b[0m       26.9217  0.0123\n",
      "     33       \u001b[36m23.3790\u001b[0m       26.9186  0.0129\n",
      "     34       \u001b[36m23.3573\u001b[0m       26.9134  0.0123\n",
      "     35       \u001b[36m23.3374\u001b[0m       26.9071  0.0119\n",
      "     36       \u001b[36m23.3190\u001b[0m       26.9017  0.0122\n",
      "     37       \u001b[36m23.3021\u001b[0m       26.8967  0.0124\n",
      "     38       \u001b[36m23.2865\u001b[0m       26.8912  0.0140\n",
      "     39       \u001b[36m23.2718\u001b[0m       26.8853  0.0129\n",
      "     40       \u001b[36m23.2582\u001b[0m       26.8796  0.0121\n",
      "     41       \u001b[36m23.2456\u001b[0m       26.8740  0.0120\n",
      "     42       \u001b[36m23.2339\u001b[0m       26.8677  0.0126\n",
      "     43       \u001b[36m23.2229\u001b[0m       26.8615  0.0124\n",
      "     44       \u001b[36m23.2128\u001b[0m       26.8559  0.0125\n",
      "     45       \u001b[36m23.2034\u001b[0m       26.8509  0.0126\n",
      "     46       \u001b[36m23.1947\u001b[0m       26.8463  0.0122\n",
      "     47       \u001b[36m23.1866\u001b[0m       26.8421  0.0128\n",
      "     48       \u001b[36m23.1789\u001b[0m       26.8378  0.0127\n",
      "     49       \u001b[36m23.1718\u001b[0m       26.8329  0.0130\n",
      "     50       \u001b[36m23.1651\u001b[0m       26.8266  0.0127\n",
      "     51       \u001b[36m23.1587\u001b[0m       26.8200  0.0208\n",
      "     52       \u001b[36m23.1526\u001b[0m       26.8134  0.0126\n",
      "     53       \u001b[36m23.1468\u001b[0m       26.8078  0.0127\n",
      "     54       \u001b[36m23.1413\u001b[0m       26.8028  0.0124\n",
      "     55       \u001b[36m23.1362\u001b[0m       26.7980  0.0119\n",
      "     56       \u001b[36m23.1313\u001b[0m       26.7931  0.0124\n",
      "     57       \u001b[36m23.1267\u001b[0m       26.7884  0.0120\n",
      "     58       \u001b[36m23.1223\u001b[0m       26.7842  0.0118\n",
      "     59       \u001b[36m23.1180\u001b[0m       26.7788  0.0120\n",
      "     60       \u001b[36m23.1140\u001b[0m       26.7739  0.0116\n",
      "     61       \u001b[36m23.1101\u001b[0m       26.7686  0.0121\n",
      "     62       \u001b[36m23.1064\u001b[0m       26.7642  0.0124\n",
      "     63       \u001b[36m23.1028\u001b[0m       26.7599  0.0129\n",
      "     64       \u001b[36m23.0995\u001b[0m       26.7558  0.0126\n",
      "     65       \u001b[36m23.0961\u001b[0m       26.7529  0.0115\n",
      "     66       \u001b[36m23.0929\u001b[0m       26.7490  0.0156\n",
      "     67       \u001b[36m23.0898\u001b[0m       26.7453  0.0150\n",
      "     68       \u001b[36m23.0868\u001b[0m       \u001b[32m26.7416\u001b[0m  0.0124\n",
      "     69       \u001b[36m23.0840\u001b[0m       \u001b[32m26.7378\u001b[0m  0.0121\n",
      "     70       \u001b[36m23.0812\u001b[0m       \u001b[32m26.7341\u001b[0m  0.0121\n",
      "     71       \u001b[36m23.0784\u001b[0m       \u001b[32m26.7299\u001b[0m  0.0142\n",
      "     72       \u001b[36m23.0758\u001b[0m       \u001b[32m26.7263\u001b[0m  0.0129\n",
      "     73       \u001b[36m23.0731\u001b[0m       \u001b[32m26.7234\u001b[0m  0.0126\n",
      "     74       \u001b[36m23.0706\u001b[0m       \u001b[32m26.7192\u001b[0m  0.0123\n",
      "     75       \u001b[36m23.0682\u001b[0m       \u001b[32m26.7161\u001b[0m  0.0120\n",
      "     76       \u001b[36m23.0658\u001b[0m       \u001b[32m26.7140\u001b[0m  0.0122\n",
      "     77       \u001b[36m23.0636\u001b[0m       \u001b[32m26.7127\u001b[0m  0.0123\n",
      "     78       \u001b[36m23.0615\u001b[0m       \u001b[32m26.7104\u001b[0m  0.0118\n",
      "     79       \u001b[36m23.0595\u001b[0m       \u001b[32m26.7091\u001b[0m  0.0117\n",
      "     80       \u001b[36m23.0575\u001b[0m       \u001b[32m26.7065\u001b[0m  0.0115\n",
      "     81       \u001b[36m23.0558\u001b[0m       \u001b[32m26.7046\u001b[0m  0.0121\n",
      "     82       \u001b[36m23.0539\u001b[0m       \u001b[32m26.7009\u001b[0m  0.0119\n",
      "     83       \u001b[36m23.0523\u001b[0m       26.7012  0.0118\n",
      "     84       \u001b[36m23.0503\u001b[0m       \u001b[32m26.6972\u001b[0m  0.0115\n",
      "     85       \u001b[36m23.0491\u001b[0m       26.6998  0.0114\n",
      "     86       \u001b[36m23.0473\u001b[0m       \u001b[32m26.6919\u001b[0m  0.0124\n",
      "     87       \u001b[36m23.0471\u001b[0m       26.7015  0.0128\n",
      "     88       \u001b[36m23.0456\u001b[0m       \u001b[32m26.6861\u001b[0m  0.0122\n",
      "     89       23.0485       26.7096  0.0125\n",
      "     90       23.0495       \u001b[32m26.6794\u001b[0m  0.0126\n",
      "     91       23.0602       26.7161  0.0129\n",
      "     92       23.0613       \u001b[32m26.6763\u001b[0m  0.0126\n",
      "     93       23.0655       26.6988  0.0122\n",
      "     94       23.0501       26.6839  0.0123\n",
      "     95       \u001b[36m23.0413\u001b[0m       26.6788  0.0119\n",
      "     96       \u001b[36m23.0343\u001b[0m       26.6909  0.0117\n",
      "     97       \u001b[36m23.0329\u001b[0m       26.6783  0.0119\n",
      "     98       23.0336       26.6877  0.0121\n",
      "     99       \u001b[36m23.0311\u001b[0m       26.6802  0.0119\n",
      "    100       \u001b[36m23.0304\u001b[0m       26.6811  0.0119\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m39.0348\u001b[0m       \u001b[32m30.9926\u001b[0m  0.0117\n",
      "      2       \u001b[36m38.2428\u001b[0m       \u001b[32m30.4365\u001b[0m  0.0112\n",
      "      3       \u001b[36m37.3589\u001b[0m       \u001b[32m29.8105\u001b[0m  0.0111\n",
      "      4       \u001b[36m36.3514\u001b[0m       \u001b[32m29.0986\u001b[0m  0.0116\n",
      "      5       \u001b[36m35.2289\u001b[0m       \u001b[32m28.3465\u001b[0m  0.0118\n",
      "      6       \u001b[36m34.0368\u001b[0m       \u001b[32m27.6186\u001b[0m  0.0120\n",
      "      7       \u001b[36m32.8073\u001b[0m       \u001b[32m27.0117\u001b[0m  0.0116\n",
      "      8       \u001b[36m31.6436\u001b[0m       \u001b[32m26.7009\u001b[0m  0.0123\n",
      "      9       \u001b[36m30.7353\u001b[0m       26.8349  0.0125\n",
      "     10       \u001b[36m30.2382\u001b[0m       27.3563  0.0127\n",
      "     11       \u001b[36m30.1161\u001b[0m       27.8770  0.0118\n",
      "     12       \u001b[36m30.1019\u001b[0m       28.0035  0.0118\n",
      "     13       \u001b[36m29.9844\u001b[0m       27.7697  0.0118\n",
      "     14       \u001b[36m29.7930\u001b[0m       27.4452  0.0124\n",
      "     15       \u001b[36m29.6210\u001b[0m       27.2030  0.0119\n",
      "     16       \u001b[36m29.4905\u001b[0m       27.0829  0.0120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m29.3872\u001b[0m       27.0711  0.0124\n",
      "     18       \u001b[36m29.2975\u001b[0m       27.1330  0.0122\n",
      "     19       \u001b[36m29.2207\u001b[0m       27.2300  0.0128\n",
      "     20       \u001b[36m29.1548\u001b[0m       27.3230  0.0123\n",
      "     21       \u001b[36m29.0959\u001b[0m       27.3832  0.0123\n",
      "     22       \u001b[36m29.0392\u001b[0m       27.4022  0.0123\n",
      "     23       \u001b[36m28.9837\u001b[0m       27.3944  0.0122\n",
      "     24       \u001b[36m28.9311\u001b[0m       27.3785  0.0126\n",
      "     25       \u001b[36m28.8830\u001b[0m       27.3693  0.0122\n",
      "     26       \u001b[36m28.8401\u001b[0m       27.3740  0.0118\n",
      "     27       \u001b[36m28.8028\u001b[0m       27.3896  0.0118\n",
      "     28       \u001b[36m28.7695\u001b[0m       27.4092  0.0117\n",
      "     29       \u001b[36m28.7397\u001b[0m       27.4233  0.0140\n",
      "     30       \u001b[36m28.7129\u001b[0m       27.4334  0.0126\n",
      "     31       \u001b[36m28.6885\u001b[0m       27.4436  0.0121\n",
      "     32       \u001b[36m28.6662\u001b[0m       27.4516  0.0117\n",
      "     33       \u001b[36m28.6457\u001b[0m       27.4557  0.0117\n",
      "     34       \u001b[36m28.6269\u001b[0m       27.4611  0.0117\n",
      "     35       \u001b[36m28.6098\u001b[0m       27.4695  0.0116\n",
      "     36       \u001b[36m28.5947\u001b[0m       27.4828  0.0118\n",
      "     37       \u001b[36m28.5816\u001b[0m       27.4938  0.0118\n",
      "     38       \u001b[36m28.5694\u001b[0m       27.5012  0.0116\n",
      "     39       \u001b[36m28.5584\u001b[0m       27.5077  0.0128\n",
      "     40       \u001b[36m28.5485\u001b[0m       27.5124  0.0134\n",
      "     41       \u001b[36m28.5394\u001b[0m       27.5160  0.0121\n",
      "     42       \u001b[36m28.5313\u001b[0m       27.5194  0.0115\n",
      "     43       \u001b[36m28.5239\u001b[0m       27.5215  0.0169\n",
      "     44       \u001b[36m28.5171\u001b[0m       27.5238  0.0239\n",
      "     45       \u001b[36m28.5109\u001b[0m       27.5253  0.0140\n",
      "     46       \u001b[36m28.5052\u001b[0m       27.5270  0.0146\n",
      "     47       \u001b[36m28.4999\u001b[0m       27.5283  0.0190\n",
      "     48       \u001b[36m28.4950\u001b[0m       27.5301  0.0142\n",
      "     49       \u001b[36m28.4903\u001b[0m       27.5306  0.0164\n",
      "     50       \u001b[36m28.4858\u001b[0m       27.5306  0.0125\n",
      "     51       \u001b[36m28.4816\u001b[0m       27.5318  0.0123\n",
      "     52       \u001b[36m28.4777\u001b[0m       27.5337  0.0123\n",
      "     53       \u001b[36m28.4740\u001b[0m       27.5345  0.0120\n",
      "     54       \u001b[36m28.4706\u001b[0m       27.5352  0.0117\n",
      "     55       \u001b[36m28.4674\u001b[0m       27.5363  0.0118\n",
      "     56       \u001b[36m28.4644\u001b[0m       27.5367  0.0119\n",
      "     57       \u001b[36m28.4616\u001b[0m       27.5357  0.0122\n",
      "     58       \u001b[36m28.4588\u001b[0m       27.5331  0.0122\n",
      "     59       \u001b[36m28.4561\u001b[0m       27.5311  0.0119\n",
      "     60       \u001b[36m28.4536\u001b[0m       27.5300  0.0115\n",
      "     61       \u001b[36m28.4512\u001b[0m       27.5294  0.0119\n",
      "     62       \u001b[36m28.4489\u001b[0m       27.5282  0.0117\n",
      "     63       \u001b[36m28.4466\u001b[0m       27.5259  0.0116\n",
      "     64       \u001b[36m28.4444\u001b[0m       27.5231  0.0114\n",
      "     65       \u001b[36m28.4423\u001b[0m       27.5198  0.0114\n",
      "     66       \u001b[36m28.4403\u001b[0m       27.5174  0.0117\n",
      "     67       \u001b[36m28.4384\u001b[0m       27.5157  0.0113\n",
      "     68       \u001b[36m28.4365\u001b[0m       27.5151  0.0113\n",
      "     69       \u001b[36m28.4347\u001b[0m       27.5136  0.0113\n",
      "     70       \u001b[36m28.4330\u001b[0m       27.5119  0.0111\n",
      "     71       \u001b[36m28.4313\u001b[0m       27.5102  0.0118\n",
      "     72       \u001b[36m28.4296\u001b[0m       27.5088  0.0114\n",
      "     73       \u001b[36m28.4281\u001b[0m       27.5079  0.0117\n",
      "     74       \u001b[36m28.4266\u001b[0m       27.5062  0.0117\n",
      "     75       \u001b[36m28.4251\u001b[0m       27.5040  0.0114\n",
      "     76       \u001b[36m28.4237\u001b[0m       27.5018  0.0118\n",
      "     77       \u001b[36m28.4224\u001b[0m       27.4996  0.0116\n",
      "     78       \u001b[36m28.4211\u001b[0m       27.4972  0.0116\n",
      "     79       \u001b[36m28.4198\u001b[0m       27.4956  0.0116\n",
      "     80       \u001b[36m28.4185\u001b[0m       27.4940  0.0112\n",
      "     81       \u001b[36m28.4173\u001b[0m       27.4909  0.0117\n",
      "     82       \u001b[36m28.4161\u001b[0m       27.4878  0.0117\n",
      "     83       \u001b[36m28.4149\u001b[0m       27.4848  0.0116\n",
      "     84       \u001b[36m28.4138\u001b[0m       27.4829  0.0114\n",
      "     85       \u001b[36m28.4127\u001b[0m       27.4811  0.0112\n",
      "     86       \u001b[36m28.4117\u001b[0m       27.4790  0.0119\n",
      "     87       \u001b[36m28.4107\u001b[0m       27.4759  0.0117\n",
      "     88       \u001b[36m28.4097\u001b[0m       27.4725  0.0119\n",
      "     89       \u001b[36m28.4087\u001b[0m       27.4699  0.0114\n",
      "     90       \u001b[36m28.4077\u001b[0m       27.4682  0.0113\n",
      "     91       \u001b[36m28.4069\u001b[0m       27.4664  0.0118\n",
      "     92       \u001b[36m28.4060\u001b[0m       27.4641  0.0116\n",
      "     93       \u001b[36m28.4051\u001b[0m       27.4615  0.0115\n",
      "     94       \u001b[36m28.4043\u001b[0m       27.4587  0.0114\n",
      "     95       \u001b[36m28.4035\u001b[0m       27.4563  0.0114\n",
      "     96       \u001b[36m28.4027\u001b[0m       27.4540  0.0116\n",
      "     97       \u001b[36m28.4020\u001b[0m       27.4517  0.0119\n",
      "     98       \u001b[36m28.4012\u001b[0m       27.4495  0.0117\n",
      "     99       \u001b[36m28.4005\u001b[0m       27.4475  0.0115\n",
      "    100       \u001b[36m28.3998\u001b[0m       27.4448  0.0111\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m43.7933\u001b[0m       \u001b[32m45.8178\u001b[0m  0.0114\n",
      "      2       \u001b[36m43.4375\u001b[0m       \u001b[32m45.3870\u001b[0m  0.0116\n",
      "      3       \u001b[36m43.0839\u001b[0m       \u001b[32m44.9565\u001b[0m  0.0110\n",
      "      4       \u001b[36m42.7324\u001b[0m       \u001b[32m44.5272\u001b[0m  0.0112\n",
      "      5       \u001b[36m42.3828\u001b[0m       \u001b[32m44.0955\u001b[0m  0.0106\n",
      "      6       \u001b[36m42.0339\u001b[0m       \u001b[32m43.6619\u001b[0m  0.0108\n",
      "      7       \u001b[36m41.6843\u001b[0m       \u001b[32m43.2254\u001b[0m  0.0117\n",
      "      8       \u001b[36m41.3330\u001b[0m       \u001b[32m42.7849\u001b[0m  0.0124\n",
      "      9       \u001b[36m40.9799\u001b[0m       \u001b[32m42.3404\u001b[0m  0.0127\n",
      "     10       \u001b[36m40.6256\u001b[0m       \u001b[32m41.8907\u001b[0m  0.0108\n",
      "     11       \u001b[36m40.2694\u001b[0m       \u001b[32m41.4368\u001b[0m  0.0106\n",
      "     12       \u001b[36m39.9120\u001b[0m       \u001b[32m40.9799\u001b[0m  0.0119\n",
      "     13       \u001b[36m39.5538\u001b[0m       \u001b[32m40.5202\u001b[0m  0.0112\n",
      "     14       \u001b[36m39.1954\u001b[0m       \u001b[32m40.0589\u001b[0m  0.0113\n",
      "     15       \u001b[36m38.8374\u001b[0m       \u001b[32m39.5964\u001b[0m  0.0110\n",
      "     16       \u001b[36m38.4804\u001b[0m       \u001b[32m39.1330\u001b[0m  0.0108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m38.1254\u001b[0m       \u001b[32m38.6703\u001b[0m  0.0128\n",
      "     18       \u001b[36m37.7737\u001b[0m       \u001b[32m38.2092\u001b[0m  0.0115\n",
      "     19       \u001b[36m37.4263\u001b[0m       \u001b[32m37.7513\u001b[0m  0.0113\n",
      "     20       \u001b[36m37.0842\u001b[0m       \u001b[32m37.2978\u001b[0m  0.0110\n",
      "     21       \u001b[36m36.7487\u001b[0m       \u001b[32m36.8498\u001b[0m  0.0119\n",
      "     22       \u001b[36m36.4206\u001b[0m       \u001b[32m36.4079\u001b[0m  0.0156\n",
      "     23       \u001b[36m36.1011\u001b[0m       \u001b[32m35.9738\u001b[0m  0.0133\n",
      "     24       \u001b[36m35.7919\u001b[0m       \u001b[32m35.5500\u001b[0m  0.0123\n",
      "     25       \u001b[36m35.4944\u001b[0m       \u001b[32m35.1383\u001b[0m  0.0115\n",
      "     26       \u001b[36m35.2099\u001b[0m       \u001b[32m34.7407\u001b[0m  0.0117\n",
      "     27       \u001b[36m34.9396\u001b[0m       \u001b[32m34.3583\u001b[0m  0.0146\n",
      "     28       \u001b[36m34.6850\u001b[0m       \u001b[32m33.9927\u001b[0m  0.0118\n",
      "     29       \u001b[36m34.4468\u001b[0m       \u001b[32m33.6453\u001b[0m  0.0119\n",
      "     30       \u001b[36m34.2257\u001b[0m       \u001b[32m33.3171\u001b[0m  0.0112\n",
      "     31       \u001b[36m34.0219\u001b[0m       \u001b[32m33.0087\u001b[0m  0.0111\n",
      "     32       \u001b[36m33.8356\u001b[0m       \u001b[32m32.7207\u001b[0m  0.0121\n",
      "     33       \u001b[36m33.6667\u001b[0m       \u001b[32m32.4535\u001b[0m  0.0114\n",
      "     34       \u001b[36m33.5152\u001b[0m       \u001b[32m32.2075\u001b[0m  0.0116\n",
      "     35       \u001b[36m33.3802\u001b[0m       \u001b[32m31.9820\u001b[0m  0.0110\n",
      "     36       \u001b[36m33.2610\u001b[0m       \u001b[32m31.7768\u001b[0m  0.0109\n",
      "     37       \u001b[36m33.1567\u001b[0m       \u001b[32m31.5913\u001b[0m  0.0118\n",
      "     38       \u001b[36m33.0661\u001b[0m       \u001b[32m31.4243\u001b[0m  0.0113\n",
      "     39       \u001b[36m32.9878\u001b[0m       \u001b[32m31.2746\u001b[0m  0.0116\n",
      "     40       \u001b[36m32.9208\u001b[0m       \u001b[32m31.1410\u001b[0m  0.0110\n",
      "     41       \u001b[36m32.8636\u001b[0m       \u001b[32m31.0220\u001b[0m  0.0110\n",
      "     42       \u001b[36m32.8149\u001b[0m       \u001b[32m30.9164\u001b[0m  0.0121\n",
      "     43       \u001b[36m32.7735\u001b[0m       \u001b[32m30.8228\u001b[0m  0.0113\n",
      "     44       \u001b[36m32.7385\u001b[0m       \u001b[32m30.7402\u001b[0m  0.0112\n",
      "     45       \u001b[36m32.7089\u001b[0m       \u001b[32m30.6675\u001b[0m  0.0110\n",
      "     46       \u001b[36m32.6838\u001b[0m       \u001b[32m30.6034\u001b[0m  0.0109\n",
      "     47       \u001b[36m32.6625\u001b[0m       \u001b[32m30.5469\u001b[0m  0.0119\n",
      "     48       \u001b[36m32.6442\u001b[0m       \u001b[32m30.4973\u001b[0m  0.0114\n",
      "     49       \u001b[36m32.6287\u001b[0m       \u001b[32m30.4536\u001b[0m  0.0112\n",
      "     50       \u001b[36m32.6153\u001b[0m       \u001b[32m30.4152\u001b[0m  0.0111\n",
      "     51       \u001b[36m32.6036\u001b[0m       \u001b[32m30.3812\u001b[0m  0.0110\n",
      "     52       \u001b[36m32.5933\u001b[0m       \u001b[32m30.3513\u001b[0m  0.0114\n",
      "     53       \u001b[36m32.5842\u001b[0m       \u001b[32m30.3249\u001b[0m  0.0110\n",
      "     54       \u001b[36m32.5759\u001b[0m       \u001b[32m30.3014\u001b[0m  0.0109\n",
      "     55       \u001b[36m32.5684\u001b[0m       \u001b[32m30.2805\u001b[0m  0.0108\n",
      "     56       \u001b[36m32.5616\u001b[0m       \u001b[32m30.2619\u001b[0m  0.0112\n",
      "     57       \u001b[36m32.5553\u001b[0m       \u001b[32m30.2451\u001b[0m  0.0119\n",
      "     58       \u001b[36m32.5495\u001b[0m       \u001b[32m30.2302\u001b[0m  0.0112\n",
      "     59       \u001b[36m32.5439\u001b[0m       \u001b[32m30.2167\u001b[0m  0.0113\n",
      "     60       \u001b[36m32.5387\u001b[0m       \u001b[32m30.2046\u001b[0m  0.0106\n",
      "     61       \u001b[36m32.5338\u001b[0m       \u001b[32m30.1935\u001b[0m  0.0108\n",
      "     62       \u001b[36m32.5291\u001b[0m       \u001b[32m30.1835\u001b[0m  0.0115\n",
      "     63       \u001b[36m32.5246\u001b[0m       \u001b[32m30.1744\u001b[0m  0.0116\n",
      "     64       \u001b[36m32.5203\u001b[0m       \u001b[32m30.1660\u001b[0m  0.0115\n",
      "     65       \u001b[36m32.5162\u001b[0m       \u001b[32m30.1582\u001b[0m  0.0105\n",
      "     66       \u001b[36m32.5122\u001b[0m       \u001b[32m30.1512\u001b[0m  0.0106\n",
      "     67       \u001b[36m32.5083\u001b[0m       \u001b[32m30.1446\u001b[0m  0.0117\n",
      "     68       \u001b[36m32.5045\u001b[0m       \u001b[32m30.1384\u001b[0m  0.0109\n",
      "     69       \u001b[36m32.5008\u001b[0m       \u001b[32m30.1328\u001b[0m  0.0110\n",
      "     70       \u001b[36m32.4973\u001b[0m       \u001b[32m30.1274\u001b[0m  0.0105\n",
      "     71       \u001b[36m32.4938\u001b[0m       \u001b[32m30.1224\u001b[0m  0.0106\n",
      "     72       \u001b[36m32.4903\u001b[0m       \u001b[32m30.1176\u001b[0m  0.0105\n",
      "     73       \u001b[36m32.4870\u001b[0m       \u001b[32m30.1132\u001b[0m  0.0108\n",
      "     74       \u001b[36m32.4837\u001b[0m       \u001b[32m30.1090\u001b[0m  0.0105\n",
      "     75       \u001b[36m32.4805\u001b[0m       \u001b[32m30.1048\u001b[0m  0.0109\n",
      "     76       \u001b[36m32.4774\u001b[0m       \u001b[32m30.1009\u001b[0m  0.0107\n",
      "     77       \u001b[36m32.4743\u001b[0m       \u001b[32m30.0971\u001b[0m  0.0110\n",
      "     78       \u001b[36m32.4714\u001b[0m       \u001b[32m30.0935\u001b[0m  0.0110\n",
      "     79       \u001b[36m32.4685\u001b[0m       \u001b[32m30.0902\u001b[0m  0.0106\n",
      "     80       \u001b[36m32.4656\u001b[0m       \u001b[32m30.0868\u001b[0m  0.0107\n",
      "     81       \u001b[36m32.4628\u001b[0m       \u001b[32m30.0836\u001b[0m  0.0104\n",
      "     82       \u001b[36m32.4600\u001b[0m       \u001b[32m30.0804\u001b[0m  0.0113\n",
      "     83       \u001b[36m32.4573\u001b[0m       \u001b[32m30.0774\u001b[0m  0.0107\n",
      "     84       \u001b[36m32.4547\u001b[0m       \u001b[32m30.0745\u001b[0m  0.0109\n",
      "     85       \u001b[36m32.4520\u001b[0m       \u001b[32m30.0716\u001b[0m  0.0109\n",
      "     86       \u001b[36m32.4495\u001b[0m       \u001b[32m30.0690\u001b[0m  0.0110\n",
      "     87       \u001b[36m32.4470\u001b[0m       \u001b[32m30.0662\u001b[0m  0.0107\n",
      "     88       \u001b[36m32.4445\u001b[0m       \u001b[32m30.0635\u001b[0m  0.0112\n",
      "     89       \u001b[36m32.4420\u001b[0m       \u001b[32m30.0609\u001b[0m  0.0110\n",
      "     90       \u001b[36m32.4396\u001b[0m       \u001b[32m30.0583\u001b[0m  0.0110\n",
      "     91       \u001b[36m32.4373\u001b[0m       \u001b[32m30.0559\u001b[0m  0.0105\n",
      "     92       \u001b[36m32.4350\u001b[0m       \u001b[32m30.0534\u001b[0m  0.0110\n",
      "     93       \u001b[36m32.4327\u001b[0m       \u001b[32m30.0511\u001b[0m  0.0110\n",
      "     94       \u001b[36m32.4304\u001b[0m       \u001b[32m30.0487\u001b[0m  0.0110\n",
      "     95       \u001b[36m32.4282\u001b[0m       \u001b[32m30.0464\u001b[0m  0.0109\n",
      "     96       \u001b[36m32.4260\u001b[0m       \u001b[32m30.0441\u001b[0m  0.0109\n",
      "     97       \u001b[36m32.4239\u001b[0m       \u001b[32m30.0418\u001b[0m  0.0105\n",
      "     98       \u001b[36m32.4217\u001b[0m       \u001b[32m30.0395\u001b[0m  0.0112\n",
      "     99       \u001b[36m32.4197\u001b[0m       \u001b[32m30.0373\u001b[0m  0.0108\n",
      "    100       \u001b[36m32.4176\u001b[0m       \u001b[32m30.0353\u001b[0m  0.0106\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m35.6200\u001b[0m       \u001b[32m33.7472\u001b[0m  0.0106\n",
      "      2       \u001b[36m35.2030\u001b[0m       \u001b[32m33.4334\u001b[0m  0.0114\n",
      "      3       \u001b[36m34.8109\u001b[0m       \u001b[32m33.1379\u001b[0m  0.0114\n",
      "      4       \u001b[36m34.4396\u001b[0m       \u001b[32m32.8579\u001b[0m  0.0108\n",
      "      5       \u001b[36m34.0862\u001b[0m       \u001b[32m32.5910\u001b[0m  0.0138\n",
      "      6       \u001b[36m33.7473\u001b[0m       \u001b[32m32.3335\u001b[0m  0.0149\n",
      "      7       \u001b[36m33.4186\u001b[0m       \u001b[32m32.0836\u001b[0m  0.0131\n",
      "      8       \u001b[36m33.0982\u001b[0m       \u001b[32m31.8406\u001b[0m  0.0117\n",
      "      9       \u001b[36m32.7831\u001b[0m       \u001b[32m31.6029\u001b[0m  0.0114\n",
      "     10       \u001b[36m32.4723\u001b[0m       \u001b[32m31.3672\u001b[0m  0.0115\n",
      "     11       \u001b[36m32.1629\u001b[0m       \u001b[32m31.1341\u001b[0m  0.0153\n",
      "     12       \u001b[36m31.8551\u001b[0m       \u001b[32m30.9049\u001b[0m  0.0121\n",
      "     13       \u001b[36m31.5505\u001b[0m       \u001b[32m30.6795\u001b[0m  0.0117\n",
      "     14       \u001b[36m31.2483\u001b[0m       \u001b[32m30.4578\u001b[0m  0.0122\n",
      "     15       \u001b[36m30.9490\u001b[0m       \u001b[32m30.2400\u001b[0m  0.0122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     16       \u001b[36m30.6518\u001b[0m       \u001b[32m30.0260\u001b[0m  0.0119\n",
      "     17       \u001b[36m30.3568\u001b[0m       \u001b[32m29.8153\u001b[0m  0.0111\n",
      "     18       \u001b[36m30.0647\u001b[0m       \u001b[32m29.6087\u001b[0m  0.0109\n",
      "     19       \u001b[36m29.7758\u001b[0m       \u001b[32m29.4063\u001b[0m  0.0110\n",
      "     20       \u001b[36m29.4903\u001b[0m       \u001b[32m29.2082\u001b[0m  0.0108\n",
      "     21       \u001b[36m29.2083\u001b[0m       \u001b[32m29.0149\u001b[0m  0.0113\n",
      "     22       \u001b[36m28.9303\u001b[0m       \u001b[32m28.8263\u001b[0m  0.0112\n",
      "     23       \u001b[36m28.6565\u001b[0m       \u001b[32m28.6430\u001b[0m  0.0112\n",
      "     24       \u001b[36m28.3873\u001b[0m       \u001b[32m28.4654\u001b[0m  0.0116\n",
      "     25       \u001b[36m28.1236\u001b[0m       \u001b[32m28.2939\u001b[0m  0.0110\n",
      "     26       \u001b[36m27.8658\u001b[0m       \u001b[32m28.1289\u001b[0m  0.0115\n",
      "     27       \u001b[36m27.6144\u001b[0m       \u001b[32m27.9706\u001b[0m  0.0112\n",
      "     28       \u001b[36m27.3699\u001b[0m       \u001b[32m27.8195\u001b[0m  0.0111\n",
      "     29       \u001b[36m27.1328\u001b[0m       \u001b[32m27.6759\u001b[0m  0.0108\n",
      "     30       \u001b[36m26.9034\u001b[0m       \u001b[32m27.5399\u001b[0m  0.0108\n",
      "     31       \u001b[36m26.6819\u001b[0m       \u001b[32m27.4116\u001b[0m  0.0112\n",
      "     32       \u001b[36m26.4685\u001b[0m       \u001b[32m27.2912\u001b[0m  0.0110\n",
      "     33       \u001b[36m26.2634\u001b[0m       \u001b[32m27.1785\u001b[0m  0.0110\n",
      "     34       \u001b[36m26.0667\u001b[0m       \u001b[32m27.0738\u001b[0m  0.0105\n",
      "     35       \u001b[36m25.8785\u001b[0m       \u001b[32m26.9768\u001b[0m  0.0108\n",
      "     36       \u001b[36m25.6988\u001b[0m       \u001b[32m26.8879\u001b[0m  0.0108\n",
      "     37       \u001b[36m25.5277\u001b[0m       \u001b[32m26.8069\u001b[0m  0.0113\n",
      "     38       \u001b[36m25.3650\u001b[0m       \u001b[32m26.7333\u001b[0m  0.0111\n",
      "     39       \u001b[36m25.2107\u001b[0m       \u001b[32m26.6670\u001b[0m  0.0107\n",
      "     40       \u001b[36m25.0646\u001b[0m       \u001b[32m26.6077\u001b[0m  0.0107\n",
      "     41       \u001b[36m24.9266\u001b[0m       \u001b[32m26.5552\u001b[0m  0.0113\n",
      "     42       \u001b[36m24.7964\u001b[0m       \u001b[32m26.5092\u001b[0m  0.0112\n",
      "     43       \u001b[36m24.6740\u001b[0m       \u001b[32m26.4695\u001b[0m  0.0110\n",
      "     44       \u001b[36m24.5595\u001b[0m       \u001b[32m26.4358\u001b[0m  0.0106\n",
      "     45       \u001b[36m24.4524\u001b[0m       \u001b[32m26.4076\u001b[0m  0.0107\n",
      "     46       \u001b[36m24.3526\u001b[0m       \u001b[32m26.3846\u001b[0m  0.0115\n",
      "     47       \u001b[36m24.2598\u001b[0m       \u001b[32m26.3664\u001b[0m  0.0113\n",
      "     48       \u001b[36m24.1736\u001b[0m       \u001b[32m26.3527\u001b[0m  0.0113\n",
      "     49       \u001b[36m24.0940\u001b[0m       \u001b[32m26.3432\u001b[0m  0.0106\n",
      "     50       \u001b[36m24.0206\u001b[0m       \u001b[32m26.3377\u001b[0m  0.0105\n",
      "     51       \u001b[36m23.9531\u001b[0m       \u001b[32m26.3356\u001b[0m  0.0120\n",
      "     52       \u001b[36m23.8911\u001b[0m       26.3365  0.0110\n",
      "     53       \u001b[36m23.8345\u001b[0m       26.3398  0.0118\n",
      "     54       \u001b[36m23.7827\u001b[0m       26.3453  0.0112\n",
      "     55       \u001b[36m23.7356\u001b[0m       26.3526  0.0107\n",
      "     56       \u001b[36m23.6928\u001b[0m       26.3614  0.0106\n",
      "     57       \u001b[36m23.6539\u001b[0m       26.3712  0.0122\n",
      "     58       \u001b[36m23.6187\u001b[0m       26.3819  0.0113\n",
      "     59       \u001b[36m23.5868\u001b[0m       26.3931  0.0109\n",
      "     60       \u001b[36m23.5580\u001b[0m       26.4047  0.0107\n",
      "     61       \u001b[36m23.5320\u001b[0m       26.4164  0.0107\n",
      "     62       \u001b[36m23.5084\u001b[0m       26.4280  0.0118\n",
      "     63       \u001b[36m23.4870\u001b[0m       26.4397  0.0119\n",
      "     64       \u001b[36m23.4676\u001b[0m       26.4512  0.0118\n",
      "     65       \u001b[36m23.4500\u001b[0m       26.4623  0.0109\n",
      "     66       \u001b[36m23.4340\u001b[0m       26.4730  0.0111\n",
      "     67       \u001b[36m23.4193\u001b[0m       26.4832  0.0125\n",
      "     68       \u001b[36m23.4060\u001b[0m       26.4929  0.0113\n",
      "     69       \u001b[36m23.3937\u001b[0m       26.5020  0.0114\n",
      "     70       \u001b[36m23.3825\u001b[0m       26.5106  0.0109\n",
      "     71       \u001b[36m23.3721\u001b[0m       26.5186  0.0111\n",
      "     72       \u001b[36m23.3626\u001b[0m       26.5260  0.0122\n",
      "     73       \u001b[36m23.3538\u001b[0m       26.5328  0.0114\n",
      "     74       \u001b[36m23.3456\u001b[0m       26.5391  0.0112\n",
      "     75       \u001b[36m23.3380\u001b[0m       26.5448  0.0109\n",
      "     76       \u001b[36m23.3310\u001b[0m       26.5500  0.0110\n",
      "     77       \u001b[36m23.3243\u001b[0m       26.5548  0.0122\n",
      "     78       \u001b[36m23.3181\u001b[0m       26.5591  0.0113\n",
      "     79       \u001b[36m23.3122\u001b[0m       26.5629  0.0116\n",
      "     80       \u001b[36m23.3067\u001b[0m       26.5664  0.0109\n",
      "     81       \u001b[36m23.3015\u001b[0m       26.5695  0.0114\n",
      "     82       \u001b[36m23.2965\u001b[0m       26.5722  0.0130\n",
      "     83       \u001b[36m23.2918\u001b[0m       26.5746  0.0120\n",
      "     84       \u001b[36m23.2873\u001b[0m       26.5767  0.0120\n",
      "     85       \u001b[36m23.2831\u001b[0m       26.5785  0.0108\n",
      "     86       \u001b[36m23.2791\u001b[0m       26.5800  0.0108\n",
      "     87       \u001b[36m23.2752\u001b[0m       26.5813  0.0124\n",
      "     88       \u001b[36m23.2715\u001b[0m       26.5824  0.0155\n",
      "     89       \u001b[36m23.2679\u001b[0m       26.5832  0.0129\n",
      "     90       \u001b[36m23.2644\u001b[0m       26.5838  0.0113\n",
      "     91       \u001b[36m23.2611\u001b[0m       26.5844  0.0110\n",
      "     92       \u001b[36m23.2579\u001b[0m       26.5847  0.0117\n",
      "     93       \u001b[36m23.2548\u001b[0m       26.5850  0.0127\n",
      "     94       \u001b[36m23.2518\u001b[0m       26.5851  0.0117\n",
      "     95       \u001b[36m23.2489\u001b[0m       26.5851  0.0124\n",
      "     96       \u001b[36m23.2461\u001b[0m       26.5849  0.0112\n",
      "     97       \u001b[36m23.2434\u001b[0m       26.5847  0.0112\n",
      "     98       \u001b[36m23.2407\u001b[0m       26.5844  0.0111\n",
      "     99       \u001b[36m23.2381\u001b[0m       26.5840  0.0110\n",
      "    100       \u001b[36m23.2356\u001b[0m       26.5836  0.0110\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m39.8864\u001b[0m       \u001b[32m31.7698\u001b[0m  0.0110\n",
      "      2       \u001b[36m39.6011\u001b[0m       \u001b[32m31.5732\u001b[0m  0.0127\n",
      "      3       \u001b[36m39.3210\u001b[0m       \u001b[32m31.3809\u001b[0m  0.0127\n",
      "      4       \u001b[36m39.0458\u001b[0m       \u001b[32m31.1928\u001b[0m  0.0113\n",
      "      5       \u001b[36m38.7751\u001b[0m       \u001b[32m31.0080\u001b[0m  0.0111\n",
      "      6       \u001b[36m38.5080\u001b[0m       \u001b[32m30.8270\u001b[0m  0.0109\n",
      "      7       \u001b[36m38.2442\u001b[0m       \u001b[32m30.6490\u001b[0m  0.0107\n",
      "      8       \u001b[36m37.9831\u001b[0m       \u001b[32m30.4739\u001b[0m  0.0109\n",
      "      9       \u001b[36m37.7243\u001b[0m       \u001b[32m30.3014\u001b[0m  0.0111\n",
      "     10       \u001b[36m37.4669\u001b[0m       \u001b[32m30.1312\u001b[0m  0.0112\n",
      "     11       \u001b[36m37.2113\u001b[0m       \u001b[32m29.9634\u001b[0m  0.0112\n",
      "     12       \u001b[36m36.9571\u001b[0m       \u001b[32m29.7976\u001b[0m  0.0120\n",
      "     13       \u001b[36m36.7037\u001b[0m       \u001b[32m29.6332\u001b[0m  0.0112\n",
      "     14       \u001b[36m36.4508\u001b[0m       \u001b[32m29.4701\u001b[0m  0.0116\n",
      "     15       \u001b[36m36.1983\u001b[0m       \u001b[32m29.3083\u001b[0m  0.0112\n",
      "     16       \u001b[36m35.9457\u001b[0m       \u001b[32m29.1473\u001b[0m  0.0115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m35.6924\u001b[0m       \u001b[32m28.9873\u001b[0m  0.0115\n",
      "     18       \u001b[36m35.4389\u001b[0m       \u001b[32m28.8287\u001b[0m  0.0112\n",
      "     19       \u001b[36m35.1858\u001b[0m       \u001b[32m28.6719\u001b[0m  0.0112\n",
      "     20       \u001b[36m34.9334\u001b[0m       \u001b[32m28.5170\u001b[0m  0.0116\n",
      "     21       \u001b[36m34.6821\u001b[0m       \u001b[32m28.3644\u001b[0m  0.0111\n",
      "     22       \u001b[36m34.4322\u001b[0m       \u001b[32m28.2142\u001b[0m  0.0115\n",
      "     23       \u001b[36m34.1839\u001b[0m       \u001b[32m28.0666\u001b[0m  0.0112\n",
      "     24       \u001b[36m33.9374\u001b[0m       \u001b[32m27.9219\u001b[0m  0.0109\n",
      "     25       \u001b[36m33.6931\u001b[0m       \u001b[32m27.7803\u001b[0m  0.0110\n",
      "     26       \u001b[36m33.4512\u001b[0m       \u001b[32m27.6421\u001b[0m  0.0109\n",
      "     27       \u001b[36m33.2120\u001b[0m       \u001b[32m27.5077\u001b[0m  0.0109\n",
      "     28       \u001b[36m32.9761\u001b[0m       \u001b[32m27.3773\u001b[0m  0.0111\n",
      "     29       \u001b[36m32.7438\u001b[0m       \u001b[32m27.2514\u001b[0m  0.0109\n",
      "     30       \u001b[36m32.5154\u001b[0m       \u001b[32m27.1302\u001b[0m  0.0107\n",
      "     31       \u001b[36m32.2914\u001b[0m       \u001b[32m27.0143\u001b[0m  0.0108\n",
      "     32       \u001b[36m32.0721\u001b[0m       \u001b[32m26.9038\u001b[0m  0.0111\n",
      "     33       \u001b[36m31.8577\u001b[0m       \u001b[32m26.7993\u001b[0m  0.0114\n",
      "     34       \u001b[36m31.6485\u001b[0m       \u001b[32m26.7010\u001b[0m  0.0117\n",
      "     35       \u001b[36m31.4450\u001b[0m       \u001b[32m26.6093\u001b[0m  0.0119\n",
      "     36       \u001b[36m31.2476\u001b[0m       \u001b[32m26.5246\u001b[0m  0.0115\n",
      "     37       \u001b[36m31.0565\u001b[0m       \u001b[32m26.4474\u001b[0m  0.0119\n",
      "     38       \u001b[36m30.8720\u001b[0m       \u001b[32m26.3778\u001b[0m  0.0119\n",
      "     39       \u001b[36m30.6946\u001b[0m       \u001b[32m26.3161\u001b[0m  0.0111\n",
      "     40       \u001b[36m30.5245\u001b[0m       \u001b[32m26.2625\u001b[0m  0.0111\n",
      "     41       \u001b[36m30.3620\u001b[0m       \u001b[32m26.2172\u001b[0m  0.0113\n",
      "     42       \u001b[36m30.2077\u001b[0m       \u001b[32m26.1802\u001b[0m  0.0110\n",
      "     43       \u001b[36m30.0619\u001b[0m       \u001b[32m26.1516\u001b[0m  0.0112\n",
      "     44       \u001b[36m29.9247\u001b[0m       \u001b[32m26.1312\u001b[0m  0.0111\n",
      "     45       \u001b[36m29.7965\u001b[0m       \u001b[32m26.1189\u001b[0m  0.0111\n",
      "     46       \u001b[36m29.6773\u001b[0m       \u001b[32m26.1144\u001b[0m  0.0111\n",
      "     47       \u001b[36m29.5671\u001b[0m       26.1174  0.0112\n",
      "     48       \u001b[36m29.4660\u001b[0m       26.1272  0.0115\n",
      "     49       \u001b[36m29.3738\u001b[0m       26.1434  0.0110\n",
      "     50       \u001b[36m29.2902\u001b[0m       26.1652  0.0110\n",
      "     51       \u001b[36m29.2149\u001b[0m       26.1919  0.0108\n",
      "     52       \u001b[36m29.1475\u001b[0m       26.2228  0.0111\n",
      "     53       \u001b[36m29.0874\u001b[0m       26.2571  0.0111\n",
      "     54       \u001b[36m29.0343\u001b[0m       26.2942  0.0110\n",
      "     55       \u001b[36m28.9876\u001b[0m       26.3333  0.0107\n",
      "     56       \u001b[36m28.9468\u001b[0m       26.3738  0.0108\n",
      "     57       \u001b[36m28.9114\u001b[0m       26.4149  0.0111\n",
      "     58       \u001b[36m28.8807\u001b[0m       26.4560  0.0113\n",
      "     59       \u001b[36m28.8540\u001b[0m       26.4965  0.0117\n",
      "     60       \u001b[36m28.8310\u001b[0m       26.5362  0.0116\n",
      "     61       \u001b[36m28.8110\u001b[0m       26.5746  0.0112\n",
      "     62       \u001b[36m28.7938\u001b[0m       26.6115  0.0117\n",
      "     63       \u001b[36m28.7789\u001b[0m       26.6466  0.0121\n",
      "     64       \u001b[36m28.7659\u001b[0m       26.6800  0.0118\n",
      "     65       \u001b[36m28.7547\u001b[0m       26.7113  0.0116\n",
      "     66       \u001b[36m28.7448\u001b[0m       26.7407  0.0110\n",
      "     67       \u001b[36m28.7362\u001b[0m       26.7681  0.0113\n",
      "     68       \u001b[36m28.7285\u001b[0m       26.7936  0.0109\n",
      "     69       \u001b[36m28.7216\u001b[0m       26.8171  0.0111\n",
      "     70       \u001b[36m28.7154\u001b[0m       26.8388  0.0130\n",
      "     71       \u001b[36m28.7098\u001b[0m       26.8588  0.0136\n",
      "     72       \u001b[36m28.7046\u001b[0m       26.8771  0.0121\n",
      "     73       \u001b[36m28.6998\u001b[0m       26.8939  0.0113\n",
      "     74       \u001b[36m28.6953\u001b[0m       26.9092  0.0127\n",
      "     75       \u001b[36m28.6911\u001b[0m       26.9231  0.0116\n",
      "     76       \u001b[36m28.6871\u001b[0m       26.9358  0.0124\n",
      "     77       \u001b[36m28.6833\u001b[0m       26.9473  0.0115\n",
      "     78       \u001b[36m28.6796\u001b[0m       26.9578  0.0142\n",
      "     79       \u001b[36m28.6761\u001b[0m       26.9673  0.0113\n",
      "     80       \u001b[36m28.6726\u001b[0m       26.9759  0.0117\n",
      "     81       \u001b[36m28.6693\u001b[0m       26.9836  0.0136\n",
      "     82       \u001b[36m28.6660\u001b[0m       26.9907  0.0113\n",
      "     83       \u001b[36m28.6628\u001b[0m       26.9970  0.0112\n",
      "     84       \u001b[36m28.6597\u001b[0m       27.0027  0.0112\n",
      "     85       \u001b[36m28.6566\u001b[0m       27.0079  0.0110\n",
      "     86       \u001b[36m28.6536\u001b[0m       27.0125  0.0111\n",
      "     87       \u001b[36m28.6507\u001b[0m       27.0167  0.0114\n",
      "     88       \u001b[36m28.6477\u001b[0m       27.0205  0.0111\n",
      "     89       \u001b[36m28.6449\u001b[0m       27.0239  0.0112\n",
      "     90       \u001b[36m28.6421\u001b[0m       27.0270  0.0108\n",
      "     91       \u001b[36m28.6393\u001b[0m       27.0297  0.0108\n",
      "     92       \u001b[36m28.6365\u001b[0m       27.0322  0.0108\n",
      "     93       \u001b[36m28.6338\u001b[0m       27.0343  0.0108\n",
      "     94       \u001b[36m28.6312\u001b[0m       27.0363  0.0110\n",
      "     95       \u001b[36m28.6285\u001b[0m       27.0381  0.0108\n",
      "     96       \u001b[36m28.6259\u001b[0m       27.0396  0.0109\n",
      "     97       \u001b[36m28.6234\u001b[0m       27.0411  0.0110\n",
      "     98       \u001b[36m28.6209\u001b[0m       27.0424  0.0107\n",
      "     99       \u001b[36m28.6184\u001b[0m       27.0435  0.0116\n",
      "    100       \u001b[36m28.6160\u001b[0m       27.0445  0.0113\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m43.1968\u001b[0m       \u001b[32m44.5719\u001b[0m  0.0116\n",
      "      2       \u001b[36m42.2305\u001b[0m       \u001b[32m43.2821\u001b[0m  0.0131\n",
      "      3       \u001b[36m41.2072\u001b[0m       \u001b[32m41.8817\u001b[0m  0.0122\n",
      "      4       \u001b[36m40.1055\u001b[0m       \u001b[32m40.3964\u001b[0m  0.0124\n",
      "      5       \u001b[36m38.9365\u001b[0m       \u001b[32m38.7658\u001b[0m  0.0118\n",
      "      6       \u001b[36m37.6662\u001b[0m       \u001b[32m36.8992\u001b[0m  0.0119\n",
      "      7       \u001b[36m36.2982\u001b[0m       \u001b[32m34.8986\u001b[0m  0.0123\n",
      "      8       \u001b[36m35.0505\u001b[0m       \u001b[32m33.1515\u001b[0m  0.0120\n",
      "      9       \u001b[36m34.2671\u001b[0m       \u001b[32m32.0493\u001b[0m  0.0116\n",
      "     10       \u001b[36m34.0327\u001b[0m       \u001b[32m31.5830\u001b[0m  0.0119\n",
      "     11       \u001b[36m33.9721\u001b[0m       \u001b[32m31.4141\u001b[0m  0.0116\n",
      "     12       \u001b[36m33.7749\u001b[0m       \u001b[32m31.4013\u001b[0m  0.0119\n",
      "     13       \u001b[36m33.5526\u001b[0m       31.4583  0.0120\n",
      "     14       \u001b[36m33.3990\u001b[0m       31.4536  0.0119\n",
      "     15       \u001b[36m33.2798\u001b[0m       \u001b[32m31.3484\u001b[0m  0.0120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     16       \u001b[36m33.1690\u001b[0m       \u001b[32m31.1828\u001b[0m  0.0118\n",
      "     17       \u001b[36m33.0667\u001b[0m       \u001b[32m31.0124\u001b[0m  0.0118\n",
      "     18       \u001b[36m32.9796\u001b[0m       \u001b[32m30.8749\u001b[0m  0.0117\n",
      "     19       \u001b[36m32.9096\u001b[0m       \u001b[32m30.7759\u001b[0m  0.0118\n",
      "     20       \u001b[36m32.8489\u001b[0m       \u001b[32m30.7064\u001b[0m  0.0118\n",
      "     21       \u001b[36m32.7921\u001b[0m       \u001b[32m30.6650\u001b[0m  0.0117\n",
      "     22       \u001b[36m32.7412\u001b[0m       \u001b[32m30.6414\u001b[0m  0.0116\n",
      "     23       \u001b[36m32.6952\u001b[0m       \u001b[32m30.6213\u001b[0m  0.0121\n",
      "     24       \u001b[36m32.6527\u001b[0m       \u001b[32m30.5931\u001b[0m  0.0117\n",
      "     25       \u001b[36m32.6130\u001b[0m       \u001b[32m30.5581\u001b[0m  0.0116\n",
      "     26       \u001b[36m32.5766\u001b[0m       \u001b[32m30.5298\u001b[0m  0.0116\n",
      "     27       \u001b[36m32.5434\u001b[0m       \u001b[32m30.5139\u001b[0m  0.0115\n",
      "     28       \u001b[36m32.5127\u001b[0m       \u001b[32m30.5013\u001b[0m  0.0117\n",
      "     29       \u001b[36m32.4848\u001b[0m       \u001b[32m30.4877\u001b[0m  0.0119\n",
      "     30       \u001b[36m32.4597\u001b[0m       \u001b[32m30.4760\u001b[0m  0.0120\n",
      "     31       \u001b[36m32.4367\u001b[0m       \u001b[32m30.4653\u001b[0m  0.0121\n",
      "     32       \u001b[36m32.4157\u001b[0m       \u001b[32m30.4531\u001b[0m  0.0123\n",
      "     33       \u001b[36m32.3966\u001b[0m       \u001b[32m30.4412\u001b[0m  0.0124\n",
      "     34       \u001b[36m32.3793\u001b[0m       \u001b[32m30.4309\u001b[0m  0.0123\n",
      "     35       \u001b[36m32.3638\u001b[0m       \u001b[32m30.4209\u001b[0m  0.0121\n",
      "     36       \u001b[36m32.3496\u001b[0m       \u001b[32m30.4118\u001b[0m  0.0122\n",
      "     37       \u001b[36m32.3365\u001b[0m       \u001b[32m30.4024\u001b[0m  0.0119\n",
      "     38       \u001b[36m32.3245\u001b[0m       \u001b[32m30.3969\u001b[0m  0.0120\n",
      "     39       \u001b[36m32.3136\u001b[0m       \u001b[32m30.3928\u001b[0m  0.0123\n",
      "     40       \u001b[36m32.3035\u001b[0m       \u001b[32m30.3876\u001b[0m  0.0118\n",
      "     41       \u001b[36m32.2944\u001b[0m       \u001b[32m30.3826\u001b[0m  0.0123\n",
      "     42       \u001b[36m32.2859\u001b[0m       \u001b[32m30.3755\u001b[0m  0.0120\n",
      "     43       \u001b[36m32.2781\u001b[0m       \u001b[32m30.3685\u001b[0m  0.0121\n",
      "     44       \u001b[36m32.2709\u001b[0m       \u001b[32m30.3612\u001b[0m  0.0119\n",
      "     45       \u001b[36m32.2643\u001b[0m       \u001b[32m30.3560\u001b[0m  0.0119\n",
      "     46       \u001b[36m32.2581\u001b[0m       \u001b[32m30.3513\u001b[0m  0.0115\n",
      "     47       \u001b[36m32.2522\u001b[0m       \u001b[32m30.3464\u001b[0m  0.0116\n",
      "     48       \u001b[36m32.2467\u001b[0m       \u001b[32m30.3421\u001b[0m  0.0117\n",
      "     49       \u001b[36m32.2415\u001b[0m       \u001b[32m30.3367\u001b[0m  0.0116\n",
      "     50       \u001b[36m32.2366\u001b[0m       \u001b[32m30.3332\u001b[0m  0.0149\n",
      "     51       \u001b[36m32.2321\u001b[0m       \u001b[32m30.3280\u001b[0m  0.0169\n",
      "     52       \u001b[36m32.2277\u001b[0m       \u001b[32m30.3221\u001b[0m  0.0149\n",
      "     53       \u001b[36m32.2235\u001b[0m       \u001b[32m30.3155\u001b[0m  0.0149\n",
      "     54       \u001b[36m32.2195\u001b[0m       \u001b[32m30.3090\u001b[0m  0.0144\n",
      "     55       \u001b[36m32.2157\u001b[0m       \u001b[32m30.3026\u001b[0m  0.0166\n",
      "     56       \u001b[36m32.2120\u001b[0m       \u001b[32m30.2958\u001b[0m  0.0135\n",
      "     57       \u001b[36m32.2085\u001b[0m       \u001b[32m30.2883\u001b[0m  0.0123\n",
      "     58       \u001b[36m32.2051\u001b[0m       \u001b[32m30.2816\u001b[0m  0.0121\n",
      "     59       \u001b[36m32.2019\u001b[0m       \u001b[32m30.2751\u001b[0m  0.0119\n",
      "     60       \u001b[36m32.1988\u001b[0m       \u001b[32m30.2698\u001b[0m  0.0124\n",
      "     61       \u001b[36m32.1958\u001b[0m       \u001b[32m30.2640\u001b[0m  0.0124\n",
      "     62       \u001b[36m32.1930\u001b[0m       \u001b[32m30.2594\u001b[0m  0.0123\n",
      "     63       \u001b[36m32.1902\u001b[0m       \u001b[32m30.2534\u001b[0m  0.0123\n",
      "     64       \u001b[36m32.1874\u001b[0m       \u001b[32m30.2489\u001b[0m  0.0121\n",
      "     65       \u001b[36m32.1848\u001b[0m       \u001b[32m30.2429\u001b[0m  0.0125\n",
      "     66       \u001b[36m32.1823\u001b[0m       \u001b[32m30.2382\u001b[0m  0.0126\n",
      "     67       \u001b[36m32.1798\u001b[0m       \u001b[32m30.2323\u001b[0m  0.0120\n",
      "     68       \u001b[36m32.1774\u001b[0m       \u001b[32m30.2286\u001b[0m  0.0121\n",
      "     69       \u001b[36m32.1752\u001b[0m       \u001b[32m30.2229\u001b[0m  0.0122\n",
      "     70       \u001b[36m32.1729\u001b[0m       \u001b[32m30.2201\u001b[0m  0.0123\n",
      "     71       \u001b[36m32.1707\u001b[0m       \u001b[32m30.2134\u001b[0m  0.0132\n",
      "     72       \u001b[36m32.1686\u001b[0m       \u001b[32m30.2111\u001b[0m  0.0129\n",
      "     73       \u001b[36m32.1665\u001b[0m       \u001b[32m30.2044\u001b[0m  0.0131\n",
      "     74       \u001b[36m32.1646\u001b[0m       30.2049  0.0126\n",
      "     75       \u001b[36m32.1627\u001b[0m       \u001b[32m30.1979\u001b[0m  0.0123\n",
      "     76       \u001b[36m32.1608\u001b[0m       30.2018  0.0122\n",
      "     77       \u001b[36m32.1591\u001b[0m       \u001b[32m30.1917\u001b[0m  0.0123\n",
      "     78       \u001b[36m32.1575\u001b[0m       30.2017  0.0122\n",
      "     79       \u001b[36m32.1560\u001b[0m       \u001b[32m30.1856\u001b[0m  0.0121\n",
      "     80       \u001b[36m32.1552\u001b[0m       30.2062  0.0121\n",
      "     81       \u001b[36m32.1546\u001b[0m       \u001b[32m30.1801\u001b[0m  0.0123\n",
      "     82       32.1555       30.2237  0.0120\n",
      "     83       32.1576       30.1831  0.0123\n",
      "     84       32.1629       30.2581  0.0125\n",
      "     85       32.1699       30.1942  0.0122\n",
      "     86       32.1784       30.2576  0.0121\n",
      "     87       32.1778       \u001b[32m30.1726\u001b[0m  0.0123\n",
      "     88       32.1685       30.1922  0.0129\n",
      "     89       \u001b[36m32.1542\u001b[0m       \u001b[32m30.1721\u001b[0m  0.0123\n",
      "     90       \u001b[36m32.1411\u001b[0m       \u001b[32m30.1625\u001b[0m  0.0118\n",
      "     91       \u001b[36m32.1400\u001b[0m       30.1844  0.0123\n",
      "     92       \u001b[36m32.1396\u001b[0m       30.1639  0.0123\n",
      "     93       \u001b[36m32.1380\u001b[0m       30.1768  0.0114\n",
      "     94       \u001b[36m32.1367\u001b[0m       30.1692  0.0132\n",
      "     95       \u001b[36m32.1341\u001b[0m       30.1669  0.0120\n",
      "     96       \u001b[36m32.1331\u001b[0m       30.1705  0.0117\n",
      "     97       \u001b[36m32.1318\u001b[0m       30.1650  0.0120\n",
      "     98       \u001b[36m32.1307\u001b[0m       30.1694  0.0114\n",
      "     99       \u001b[36m32.1297\u001b[0m       30.1655  0.0125\n",
      "    100       \u001b[36m32.1285\u001b[0m       30.1670  0.0123\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m32.8324\u001b[0m       \u001b[32m31.5127\u001b[0m  0.0118\n",
      "      2       \u001b[36m32.1014\u001b[0m       \u001b[32m30.9205\u001b[0m  0.0124\n",
      "      3       \u001b[36m31.2903\u001b[0m       \u001b[32m30.2488\u001b[0m  0.0121\n",
      "      4       \u001b[36m30.3509\u001b[0m       \u001b[32m29.4845\u001b[0m  0.0125\n",
      "      5       \u001b[36m29.2783\u001b[0m       \u001b[32m28.6555\u001b[0m  0.0130\n",
      "      6       \u001b[36m28.1262\u001b[0m       \u001b[32m27.8440\u001b[0m  0.0132\n",
      "      7       \u001b[36m26.9968\u001b[0m       \u001b[32m27.1751\u001b[0m  0.0115\n",
      "      8       \u001b[36m25.9715\u001b[0m       \u001b[32m26.8083\u001b[0m  0.0124\n",
      "      9       \u001b[36m25.1886\u001b[0m       26.9026  0.0121\n",
      "     10       \u001b[36m24.8077\u001b[0m       27.3014  0.0123\n",
      "     11       \u001b[36m24.7185\u001b[0m       27.4840  0.0119\n",
      "     12       \u001b[36m24.6148\u001b[0m       27.2766  0.0121\n",
      "     13       \u001b[36m24.4356\u001b[0m       26.9577  0.0122\n",
      "     14       \u001b[36m24.2825\u001b[0m       \u001b[32m26.7316\u001b[0m  0.0135\n",
      "     15       \u001b[36m24.1760\u001b[0m       \u001b[32m26.6204\u001b[0m  0.0123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     16       \u001b[36m24.0809\u001b[0m       \u001b[32m26.5926\u001b[0m  0.0142\n",
      "     17       \u001b[36m23.9799\u001b[0m       26.6225  0.0119\n",
      "     18       \u001b[36m23.8851\u001b[0m       26.6812  0.0129\n",
      "     19       \u001b[36m23.8039\u001b[0m       26.7355  0.0130\n",
      "     20       \u001b[36m23.7353\u001b[0m       26.7642  0.0130\n",
      "     21       \u001b[36m23.6747\u001b[0m       26.7684  0.0126\n",
      "     22       \u001b[36m23.6212\u001b[0m       26.7643  0.0122\n",
      "     23       \u001b[36m23.5747\u001b[0m       26.7653  0.0124\n",
      "     24       \u001b[36m23.5338\u001b[0m       26.7740  0.0130\n",
      "     25       \u001b[36m23.4974\u001b[0m       26.7853  0.0185\n",
      "     26       \u001b[36m23.4639\u001b[0m       26.7946  0.0132\n",
      "     27       \u001b[36m23.4324\u001b[0m       26.8007  0.0123\n",
      "     28       \u001b[36m23.4036\u001b[0m       26.8007  0.0136\n",
      "     29       \u001b[36m23.3776\u001b[0m       26.7964  0.0137\n",
      "     30       \u001b[36m23.3537\u001b[0m       26.7918  0.0134\n",
      "     31       \u001b[36m23.3317\u001b[0m       26.7876  0.0132\n",
      "     32       \u001b[36m23.3116\u001b[0m       26.7834  0.0122\n",
      "     33       \u001b[36m23.2927\u001b[0m       26.7807  0.0124\n",
      "     34       \u001b[36m23.2753\u001b[0m       26.7773  0.0141\n",
      "     35       \u001b[36m23.2591\u001b[0m       26.7725  0.0127\n",
      "     36       \u001b[36m23.2440\u001b[0m       26.7688  0.0122\n",
      "     37       \u001b[36m23.2299\u001b[0m       26.7666  0.0123\n",
      "     38       \u001b[36m23.2171\u001b[0m       26.7652  0.0122\n",
      "     39       \u001b[36m23.2053\u001b[0m       26.7624  0.0123\n",
      "     40       \u001b[36m23.1943\u001b[0m       26.7583  0.0123\n",
      "     41       \u001b[36m23.1843\u001b[0m       26.7547  0.0126\n",
      "     42       \u001b[36m23.1750\u001b[0m       26.7525  0.0121\n",
      "     43       \u001b[36m23.1664\u001b[0m       26.7515  0.0121\n",
      "     44       \u001b[36m23.1584\u001b[0m       26.7494  0.0116\n",
      "     45       \u001b[36m23.1510\u001b[0m       26.7457  0.0117\n",
      "     46       \u001b[36m23.1441\u001b[0m       26.7430  0.0123\n",
      "     47       \u001b[36m23.1376\u001b[0m       26.7409  0.0121\n",
      "     48       \u001b[36m23.1315\u001b[0m       26.7393  0.0127\n",
      "     49       \u001b[36m23.1259\u001b[0m       26.7389  0.0122\n",
      "     50       \u001b[36m23.1205\u001b[0m       26.7387  0.0122\n",
      "     51       \u001b[36m23.1155\u001b[0m       26.7370  0.0124\n",
      "     52       \u001b[36m23.1108\u001b[0m       26.7341  0.0119\n",
      "     53       \u001b[36m23.1063\u001b[0m       26.7312  0.0127\n",
      "     54       \u001b[36m23.1021\u001b[0m       26.7278  0.0120\n",
      "     55       \u001b[36m23.0981\u001b[0m       26.7246  0.0120\n",
      "     56       \u001b[36m23.0943\u001b[0m       26.7223  0.0124\n",
      "     57       \u001b[36m23.0908\u001b[0m       26.7169  0.0123\n",
      "     58       \u001b[36m23.0875\u001b[0m       26.7112  0.0122\n",
      "     59       \u001b[36m23.0843\u001b[0m       26.7072  0.0121\n",
      "     60       \u001b[36m23.0813\u001b[0m       26.7044  0.0120\n",
      "     61       \u001b[36m23.0785\u001b[0m       26.7016  0.0126\n",
      "     62       \u001b[36m23.0757\u001b[0m       26.6974  0.0123\n",
      "     63       \u001b[36m23.0731\u001b[0m       26.6934  0.0127\n",
      "     64       \u001b[36m23.0705\u001b[0m       26.6901  0.0121\n",
      "     65       \u001b[36m23.0680\u001b[0m       26.6878  0.0122\n",
      "     66       \u001b[36m23.0657\u001b[0m       26.6861  0.0131\n",
      "     67       \u001b[36m23.0634\u001b[0m       26.6839  0.0122\n",
      "     68       \u001b[36m23.0611\u001b[0m       26.6815  0.0123\n",
      "     69       \u001b[36m23.0590\u001b[0m       26.6795  0.0122\n",
      "     70       \u001b[36m23.0569\u001b[0m       26.6776  0.0118\n",
      "     71       \u001b[36m23.0549\u001b[0m       26.6761  0.0118\n",
      "     72       \u001b[36m23.0530\u001b[0m       26.6736  0.0116\n",
      "     73       \u001b[36m23.0511\u001b[0m       26.6711  0.0117\n",
      "     74       \u001b[36m23.0493\u001b[0m       26.6690  0.0114\n",
      "     75       \u001b[36m23.0475\u001b[0m       26.6672  0.0114\n",
      "     76       \u001b[36m23.0458\u001b[0m       26.6659  0.0118\n",
      "     77       \u001b[36m23.0441\u001b[0m       26.6653  0.0116\n",
      "     78       \u001b[36m23.0424\u001b[0m       26.6634  0.0116\n",
      "     79       \u001b[36m23.0407\u001b[0m       26.6605  0.0114\n",
      "     80       \u001b[36m23.0391\u001b[0m       26.6590  0.0114\n",
      "     81       \u001b[36m23.0375\u001b[0m       26.6576  0.0116\n",
      "     82       \u001b[36m23.0360\u001b[0m       26.6555  0.0115\n",
      "     83       \u001b[36m23.0345\u001b[0m       26.6537  0.0114\n",
      "     84       \u001b[36m23.0330\u001b[0m       26.6522  0.0115\n",
      "     85       \u001b[36m23.0316\u001b[0m       26.6504  0.0112\n",
      "     86       \u001b[36m23.0302\u001b[0m       26.6484  0.0115\n",
      "     87       \u001b[36m23.0289\u001b[0m       26.6465  0.0115\n",
      "     88       \u001b[36m23.0276\u001b[0m       26.6445  0.0116\n",
      "     89       \u001b[36m23.0263\u001b[0m       26.6420  0.0113\n",
      "     90       \u001b[36m23.0251\u001b[0m       26.6394  0.0112\n",
      "     91       \u001b[36m23.0238\u001b[0m       26.6380  0.0124\n",
      "     92       \u001b[36m23.0226\u001b[0m       26.6363  0.0116\n",
      "     93       \u001b[36m23.0214\u001b[0m       26.6347  0.0118\n",
      "     94       \u001b[36m23.0203\u001b[0m       26.6337  0.0117\n",
      "     95       \u001b[36m23.0191\u001b[0m       26.6316  0.0119\n",
      "     96       \u001b[36m23.0181\u001b[0m       26.6287  0.0138\n",
      "     97       \u001b[36m23.0170\u001b[0m       26.6274  0.0134\n",
      "     98       \u001b[36m23.0159\u001b[0m       26.6280  0.0120\n",
      "     99       \u001b[36m23.0149\u001b[0m       26.6277  0.0117\n",
      "    100       \u001b[36m23.0138\u001b[0m       26.6261  0.0112\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m40.3062\u001b[0m       \u001b[32m31.8313\u001b[0m  0.0120\n",
      "      2       \u001b[36m39.4652\u001b[0m       \u001b[32m31.2101\u001b[0m  0.0190\n",
      "      3       \u001b[36m38.5031\u001b[0m       \u001b[32m30.4599\u001b[0m  0.0142\n",
      "      4       \u001b[36m37.3280\u001b[0m       \u001b[32m29.5649\u001b[0m  0.0125\n",
      "      5       \u001b[36m35.9407\u001b[0m       \u001b[32m28.5803\u001b[0m  0.0126\n",
      "      6       \u001b[36m34.4121\u001b[0m       \u001b[32m27.6430\u001b[0m  0.0146\n",
      "      7       \u001b[36m32.8376\u001b[0m       \u001b[32m26.9847\u001b[0m  0.0146\n",
      "      8       \u001b[36m31.4314\u001b[0m       \u001b[32m26.9468\u001b[0m  0.0137\n",
      "      9       \u001b[36m30.5639\u001b[0m       27.6533  0.0126\n",
      "     10       \u001b[36m30.3790\u001b[0m       28.4704  0.0126\n",
      "     11       30.4249       28.5397  0.0120\n",
      "     12       \u001b[36m30.2462\u001b[0m       28.0097  0.0119\n",
      "     13       \u001b[36m29.9554\u001b[0m       27.4688  0.0118\n",
      "     14       \u001b[36m29.7478\u001b[0m       27.1578  0.0117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     15       \u001b[36m29.6095\u001b[0m       27.0610  0.0118\n",
      "     16       \u001b[36m29.4861\u001b[0m       27.1134  0.0118\n",
      "     17       \u001b[36m29.3746\u001b[0m       27.2539  0.0119\n",
      "     18       \u001b[36m29.2876\u001b[0m       27.3994  0.0120\n",
      "     19       \u001b[36m29.2184\u001b[0m       27.4829  0.0118\n",
      "     20       \u001b[36m29.1516\u001b[0m       27.4972  0.0113\n",
      "     21       \u001b[36m29.0821\u001b[0m       27.4771  0.0113\n",
      "     22       \u001b[36m29.0150\u001b[0m       27.4587  0.0117\n",
      "     23       \u001b[36m28.9558\u001b[0m       27.4528  0.0114\n",
      "     24       \u001b[36m28.9048\u001b[0m       27.4585  0.0113\n",
      "     25       \u001b[36m28.8611\u001b[0m       27.4766  0.0112\n",
      "     26       \u001b[36m28.8231\u001b[0m       27.5008  0.0116\n",
      "     27       \u001b[36m28.7905\u001b[0m       27.5211  0.0118\n",
      "     28       \u001b[36m28.7609\u001b[0m       27.5300  0.0113\n",
      "     29       \u001b[36m28.7334\u001b[0m       27.5278  0.0114\n",
      "     30       \u001b[36m28.7075\u001b[0m       27.5250  0.0112\n",
      "     31       \u001b[36m28.6837\u001b[0m       27.5245  0.0115\n",
      "     32       \u001b[36m28.6623\u001b[0m       27.5263  0.0117\n",
      "     33       \u001b[36m28.6433\u001b[0m       27.5275  0.0114\n",
      "     34       \u001b[36m28.6262\u001b[0m       27.5276  0.0115\n",
      "     35       \u001b[36m28.6110\u001b[0m       27.5279  0.0113\n",
      "     36       \u001b[36m28.5969\u001b[0m       27.5294  0.0115\n",
      "     37       \u001b[36m28.5839\u001b[0m       27.5302  0.0118\n",
      "     38       \u001b[36m28.5721\u001b[0m       27.5280  0.0115\n",
      "     39       \u001b[36m28.5611\u001b[0m       27.5228  0.0112\n",
      "     40       \u001b[36m28.5509\u001b[0m       27.5195  0.0112\n",
      "     41       \u001b[36m28.5417\u001b[0m       27.5207  0.0115\n",
      "     42       \u001b[36m28.5334\u001b[0m       27.5207  0.0116\n",
      "     43       \u001b[36m28.5256\u001b[0m       27.5201  0.0114\n",
      "     44       \u001b[36m28.5185\u001b[0m       27.5189  0.0114\n",
      "     45       \u001b[36m28.5121\u001b[0m       27.5195  0.0112\n",
      "     46       \u001b[36m28.5062\u001b[0m       27.5218  0.0114\n",
      "     47       \u001b[36m28.5008\u001b[0m       27.5217  0.0119\n",
      "     48       \u001b[36m28.4956\u001b[0m       27.5207  0.0115\n",
      "     49       \u001b[36m28.4908\u001b[0m       27.5201  0.0119\n",
      "     50       \u001b[36m28.4862\u001b[0m       27.5201  0.0113\n",
      "     51       \u001b[36m28.4821\u001b[0m       27.5210  0.0112\n",
      "     52       \u001b[36m28.4782\u001b[0m       27.5227  0.0122\n",
      "     53       \u001b[36m28.4746\u001b[0m       27.5234  0.0112\n",
      "     54       \u001b[36m28.4711\u001b[0m       27.5220  0.0114\n",
      "     55       \u001b[36m28.4677\u001b[0m       27.5203  0.0114\n",
      "     56       \u001b[36m28.4644\u001b[0m       27.5202  0.0114\n",
      "     57       \u001b[36m28.4614\u001b[0m       27.5211  0.0115\n",
      "     58       \u001b[36m28.4586\u001b[0m       27.5211  0.0119\n",
      "     59       \u001b[36m28.4559\u001b[0m       27.5196  0.0115\n",
      "     60       \u001b[36m28.4533\u001b[0m       27.5178  0.0113\n",
      "     61       \u001b[36m28.4508\u001b[0m       27.5181  0.0110\n",
      "     62       \u001b[36m28.4485\u001b[0m       27.5171  0.0119\n",
      "     63       \u001b[36m28.4462\u001b[0m       27.5139  0.0115\n",
      "     64       \u001b[36m28.4441\u001b[0m       27.5115  0.0114\n",
      "     65       \u001b[36m28.4420\u001b[0m       27.5104  0.0110\n",
      "     66       \u001b[36m28.4400\u001b[0m       27.5094  0.0112\n",
      "     67       \u001b[36m28.4381\u001b[0m       27.5073  0.0118\n",
      "     68       \u001b[36m28.4363\u001b[0m       27.5048  0.0112\n",
      "     69       \u001b[36m28.4345\u001b[0m       27.5034  0.0116\n",
      "     70       \u001b[36m28.4329\u001b[0m       27.5021  0.0114\n",
      "     71       \u001b[36m28.4311\u001b[0m       27.5008  0.0112\n",
      "     72       \u001b[36m28.4296\u001b[0m       27.4989  0.0115\n",
      "     73       \u001b[36m28.4279\u001b[0m       27.4982  0.0115\n",
      "     74       \u001b[36m28.4265\u001b[0m       27.4958  0.0113\n",
      "     75       \u001b[36m28.4249\u001b[0m       27.4939  0.0113\n",
      "     76       \u001b[36m28.4236\u001b[0m       27.4915  0.0116\n",
      "     77       \u001b[36m28.4220\u001b[0m       27.4882  0.0127\n",
      "     78       \u001b[36m28.4208\u001b[0m       27.4839  0.0118\n",
      "     79       \u001b[36m28.4192\u001b[0m       27.4840  0.0116\n",
      "     80       \u001b[36m28.4184\u001b[0m       27.4824  0.0115\n",
      "     81       \u001b[36m28.4168\u001b[0m       27.4802  0.0116\n",
      "     82       \u001b[36m28.4161\u001b[0m       27.4757  0.0158\n",
      "     83       \u001b[36m28.4143\u001b[0m       27.4765  0.0143\n",
      "     84       \u001b[36m28.4142\u001b[0m       27.4763  0.0124\n",
      "     85       \u001b[36m28.4124\u001b[0m       27.4800  0.0116\n",
      "     86       28.4130       27.4742  0.0124\n",
      "     87       \u001b[36m28.4107\u001b[0m       27.4762  0.0137\n",
      "     88       28.4127       27.4712  0.0128\n",
      "     89       \u001b[36m28.4104\u001b[0m       27.4788  0.0129\n",
      "     90       28.4147       27.4713  0.0117\n",
      "     91       28.4126       27.4764  0.0118\n",
      "     92       28.4195       27.4658  0.0122\n",
      "     93       28.4166       27.4699  0.0120\n",
      "     94       28.4242       27.4668  0.0120\n",
      "     95       28.4162       27.4573  0.0118\n",
      "     96       28.4157       27.4665  0.0119\n",
      "     97       \u001b[36m28.4060\u001b[0m       27.4502  0.0117\n",
      "     98       \u001b[36m28.4036\u001b[0m       27.4664  0.0118\n",
      "     99       \u001b[36m28.4014\u001b[0m       27.4555  0.0120\n",
      "    100       \u001b[36m28.3999\u001b[0m       27.4612  0.0119\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m42.8756\u001b[0m       \u001b[32m44.8854\u001b[0m  0.0107\n",
      "      2       \u001b[36m42.5561\u001b[0m       \u001b[32m44.4932\u001b[0m  0.0108\n",
      "      3       \u001b[36m42.2447\u001b[0m       \u001b[32m44.1090\u001b[0m  0.0108\n",
      "      4       \u001b[36m41.9402\u001b[0m       \u001b[32m43.7314\u001b[0m  0.0108\n",
      "      5       \u001b[36m41.6411\u001b[0m       \u001b[32m43.3586\u001b[0m  0.0106\n",
      "      6       \u001b[36m41.3468\u001b[0m       \u001b[32m42.9903\u001b[0m  0.0106\n",
      "      7       \u001b[36m41.0565\u001b[0m       \u001b[32m42.6251\u001b[0m  0.0112\n",
      "      8       \u001b[36m40.7693\u001b[0m       \u001b[32m42.2625\u001b[0m  0.0113\n",
      "      9       \u001b[36m40.4848\u001b[0m       \u001b[32m41.9018\u001b[0m  0.0110\n",
      "     10       \u001b[36m40.2022\u001b[0m       \u001b[32m41.5425\u001b[0m  0.0107\n",
      "     11       \u001b[36m39.9214\u001b[0m       \u001b[32m41.1844\u001b[0m  0.0106\n",
      "     12       \u001b[36m39.6420\u001b[0m       \u001b[32m40.8262\u001b[0m  0.0109\n",
      "     13       \u001b[36m39.3634\u001b[0m       \u001b[32m40.4675\u001b[0m  0.0112\n",
      "     14       \u001b[36m39.0857\u001b[0m       \u001b[32m40.1082\u001b[0m  0.0106\n",
      "     15       \u001b[36m38.8088\u001b[0m       \u001b[32m39.7491\u001b[0m  0.0106\n",
      "     16       \u001b[36m38.5329\u001b[0m       \u001b[32m39.3896\u001b[0m  0.0105\n",
      "     17       \u001b[36m38.2581\u001b[0m       \u001b[32m39.0305\u001b[0m  0.0111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     18       \u001b[36m37.9846\u001b[0m       \u001b[32m38.6718\u001b[0m  0.0112\n",
      "     19       \u001b[36m37.7127\u001b[0m       \u001b[32m38.3135\u001b[0m  0.0110\n",
      "     20       \u001b[36m37.4427\u001b[0m       \u001b[32m37.9567\u001b[0m  0.0107\n",
      "     21       \u001b[36m37.1751\u001b[0m       \u001b[32m37.6015\u001b[0m  0.0108\n",
      "     22       \u001b[36m36.9106\u001b[0m       \u001b[32m37.2489\u001b[0m  0.0103\n",
      "     23       \u001b[36m36.6497\u001b[0m       \u001b[32m36.8995\u001b[0m  0.0107\n",
      "     24       \u001b[36m36.3931\u001b[0m       \u001b[32m36.5539\u001b[0m  0.0110\n",
      "     25       \u001b[36m36.1416\u001b[0m       \u001b[32m36.2134\u001b[0m  0.0107\n",
      "     26       \u001b[36m35.8961\u001b[0m       \u001b[32m35.8790\u001b[0m  0.0112\n",
      "     27       \u001b[36m35.6572\u001b[0m       \u001b[32m35.5514\u001b[0m  0.0105\n",
      "     28       \u001b[36m35.4256\u001b[0m       \u001b[32m35.2312\u001b[0m  0.0109\n",
      "     29       \u001b[36m35.2017\u001b[0m       \u001b[32m34.9191\u001b[0m  0.0113\n",
      "     30       \u001b[36m34.9860\u001b[0m       \u001b[32m34.6161\u001b[0m  0.0106\n",
      "     31       \u001b[36m34.7792\u001b[0m       \u001b[32m34.3225\u001b[0m  0.0107\n",
      "     32       \u001b[36m34.5818\u001b[0m       \u001b[32m34.0389\u001b[0m  0.0105\n",
      "     33       \u001b[36m34.3942\u001b[0m       \u001b[32m33.7655\u001b[0m  0.0113\n",
      "     34       \u001b[36m34.2167\u001b[0m       \u001b[32m33.5033\u001b[0m  0.0113\n",
      "     35       \u001b[36m34.0497\u001b[0m       \u001b[32m33.2525\u001b[0m  0.0107\n",
      "     36       \u001b[36m33.8932\u001b[0m       \u001b[32m33.0134\u001b[0m  0.0105\n",
      "     37       \u001b[36m33.7474\u001b[0m       \u001b[32m32.7866\u001b[0m  0.0108\n",
      "     38       \u001b[36m33.6123\u001b[0m       \u001b[32m32.5720\u001b[0m  0.0107\n",
      "     39       \u001b[36m33.4878\u001b[0m       \u001b[32m32.3696\u001b[0m  0.0112\n",
      "     40       \u001b[36m33.3736\u001b[0m       \u001b[32m32.1797\u001b[0m  0.0106\n",
      "     41       \u001b[36m33.2694\u001b[0m       \u001b[32m32.0019\u001b[0m  0.0105\n",
      "     42       \u001b[36m33.1748\u001b[0m       \u001b[32m31.8365\u001b[0m  0.0110\n",
      "     43       \u001b[36m33.0896\u001b[0m       \u001b[32m31.6833\u001b[0m  0.0113\n",
      "     44       \u001b[36m33.0133\u001b[0m       \u001b[32m31.5417\u001b[0m  0.0108\n",
      "     45       \u001b[36m32.9451\u001b[0m       \u001b[32m31.4112\u001b[0m  0.0109\n",
      "     46       \u001b[36m32.8846\u001b[0m       \u001b[32m31.2913\u001b[0m  0.0107\n",
      "     47       \u001b[36m32.8310\u001b[0m       \u001b[32m31.1813\u001b[0m  0.0106\n",
      "     48       \u001b[36m32.7837\u001b[0m       \u001b[32m31.0807\u001b[0m  0.0109\n",
      "     49       \u001b[36m32.7422\u001b[0m       \u001b[32m30.9889\u001b[0m  0.0109\n",
      "     50       \u001b[36m32.7059\u001b[0m       \u001b[32m30.9055\u001b[0m  0.0110\n",
      "     51       \u001b[36m32.6744\u001b[0m       \u001b[32m30.8298\u001b[0m  0.0106\n",
      "     52       \u001b[36m32.6469\u001b[0m       \u001b[32m30.7615\u001b[0m  0.0107\n",
      "     53       \u001b[36m32.6230\u001b[0m       \u001b[32m30.6996\u001b[0m  0.0111\n",
      "     54       \u001b[36m32.6021\u001b[0m       \u001b[32m30.6435\u001b[0m  0.0107\n",
      "     55       \u001b[36m32.5840\u001b[0m       \u001b[32m30.5927\u001b[0m  0.0110\n",
      "     56       \u001b[36m32.5681\u001b[0m       \u001b[32m30.5467\u001b[0m  0.0107\n",
      "     57       \u001b[36m32.5543\u001b[0m       \u001b[32m30.5053\u001b[0m  0.0105\n",
      "     58       \u001b[36m32.5422\u001b[0m       \u001b[32m30.4680\u001b[0m  0.0111\n",
      "     59       \u001b[36m32.5315\u001b[0m       \u001b[32m30.4342\u001b[0m  0.0111\n",
      "     60       \u001b[36m32.5221\u001b[0m       \u001b[32m30.4036\u001b[0m  0.0109\n",
      "     61       \u001b[36m32.5137\u001b[0m       \u001b[32m30.3759\u001b[0m  0.0105\n",
      "     62       \u001b[36m32.5063\u001b[0m       \u001b[32m30.3508\u001b[0m  0.0106\n",
      "     63       \u001b[36m32.4995\u001b[0m       \u001b[32m30.3279\u001b[0m  0.0110\n",
      "     64       \u001b[36m32.4934\u001b[0m       \u001b[32m30.3071\u001b[0m  0.0107\n",
      "     65       \u001b[36m32.4880\u001b[0m       \u001b[32m30.2881\u001b[0m  0.0111\n",
      "     66       \u001b[36m32.4830\u001b[0m       \u001b[32m30.2709\u001b[0m  0.0174\n",
      "     67       \u001b[36m32.4783\u001b[0m       \u001b[32m30.2551\u001b[0m  0.0174\n",
      "     68       \u001b[36m32.4740\u001b[0m       \u001b[32m30.2407\u001b[0m  0.0119\n",
      "     69       \u001b[36m32.4700\u001b[0m       \u001b[32m30.2275\u001b[0m  0.0132\n",
      "     70       \u001b[36m32.4662\u001b[0m       \u001b[32m30.2154\u001b[0m  0.0143\n",
      "     71       \u001b[36m32.4626\u001b[0m       \u001b[32m30.2042\u001b[0m  0.0128\n",
      "     72       \u001b[36m32.4592\u001b[0m       \u001b[32m30.1940\u001b[0m  0.0120\n",
      "     73       \u001b[36m32.4559\u001b[0m       \u001b[32m30.1845\u001b[0m  0.0119\n",
      "     74       \u001b[36m32.4528\u001b[0m       \u001b[32m30.1758\u001b[0m  0.0113\n",
      "     75       \u001b[36m32.4498\u001b[0m       \u001b[32m30.1677\u001b[0m  0.0110\n",
      "     76       \u001b[36m32.4469\u001b[0m       \u001b[32m30.1601\u001b[0m  0.0114\n",
      "     77       \u001b[36m32.4441\u001b[0m       \u001b[32m30.1531\u001b[0m  0.0110\n",
      "     78       \u001b[36m32.4414\u001b[0m       \u001b[32m30.1465\u001b[0m  0.0110\n",
      "     79       \u001b[36m32.4388\u001b[0m       \u001b[32m30.1404\u001b[0m  0.0108\n",
      "     80       \u001b[36m32.4362\u001b[0m       \u001b[32m30.1346\u001b[0m  0.0109\n",
      "     81       \u001b[36m32.4338\u001b[0m       \u001b[32m30.1292\u001b[0m  0.0110\n",
      "     82       \u001b[36m32.4313\u001b[0m       \u001b[32m30.1241\u001b[0m  0.0110\n",
      "     83       \u001b[36m32.4290\u001b[0m       \u001b[32m30.1193\u001b[0m  0.0112\n",
      "     84       \u001b[36m32.4267\u001b[0m       \u001b[32m30.1147\u001b[0m  0.0107\n",
      "     85       \u001b[36m32.4244\u001b[0m       \u001b[32m30.1104\u001b[0m  0.0105\n",
      "     86       \u001b[36m32.4222\u001b[0m       \u001b[32m30.1062\u001b[0m  0.0108\n",
      "     87       \u001b[36m32.4201\u001b[0m       \u001b[32m30.1023\u001b[0m  0.0109\n",
      "     88       \u001b[36m32.4179\u001b[0m       \u001b[32m30.0986\u001b[0m  0.0109\n",
      "     89       \u001b[36m32.4158\u001b[0m       \u001b[32m30.0950\u001b[0m  0.0109\n",
      "     90       \u001b[36m32.4138\u001b[0m       \u001b[32m30.0916\u001b[0m  0.0107\n",
      "     91       \u001b[36m32.4118\u001b[0m       \u001b[32m30.0883\u001b[0m  0.0113\n",
      "     92       \u001b[36m32.4098\u001b[0m       \u001b[32m30.0852\u001b[0m  0.0111\n",
      "     93       \u001b[36m32.4078\u001b[0m       \u001b[32m30.0822\u001b[0m  0.0114\n",
      "     94       \u001b[36m32.4059\u001b[0m       \u001b[32m30.0793\u001b[0m  0.0108\n",
      "     95       \u001b[36m32.4040\u001b[0m       \u001b[32m30.0765\u001b[0m  0.0110\n",
      "     96       \u001b[36m32.4021\u001b[0m       \u001b[32m30.0738\u001b[0m  0.0116\n",
      "     97       \u001b[36m32.4003\u001b[0m       \u001b[32m30.0711\u001b[0m  0.0113\n",
      "     98       \u001b[36m32.3985\u001b[0m       \u001b[32m30.0686\u001b[0m  0.0115\n",
      "     99       \u001b[36m32.3967\u001b[0m       \u001b[32m30.0661\u001b[0m  0.0110\n",
      "    100       \u001b[36m32.3949\u001b[0m       \u001b[32m30.0637\u001b[0m  0.0114\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m34.9490\u001b[0m       \u001b[32m33.2411\u001b[0m  0.0114\n",
      "      2       \u001b[36m34.5195\u001b[0m       \u001b[32m32.9227\u001b[0m  0.0114\n",
      "      3       \u001b[36m34.1094\u001b[0m       \u001b[32m32.6172\u001b[0m  0.0110\n",
      "      4       \u001b[36m33.7149\u001b[0m       \u001b[32m32.3222\u001b[0m  0.0112\n",
      "      5       \u001b[36m33.3324\u001b[0m       \u001b[32m32.0356\u001b[0m  0.0109\n",
      "      6       \u001b[36m32.9584\u001b[0m       \u001b[32m31.7546\u001b[0m  0.0107\n",
      "      7       \u001b[36m32.5903\u001b[0m       \u001b[32m31.4786\u001b[0m  0.0109\n",
      "      8       \u001b[36m32.2275\u001b[0m       \u001b[32m31.2073\u001b[0m  0.0110\n",
      "      9       \u001b[36m31.8688\u001b[0m       \u001b[32m30.9396\u001b[0m  0.0110\n",
      "     10       \u001b[36m31.5133\u001b[0m       \u001b[32m30.6757\u001b[0m  0.0108\n",
      "     11       \u001b[36m31.1611\u001b[0m       \u001b[32m30.4151\u001b[0m  0.0112\n",
      "     12       \u001b[36m30.8114\u001b[0m       \u001b[32m30.1576\u001b[0m  0.0119\n",
      "     13       \u001b[36m30.4641\u001b[0m       \u001b[32m29.9034\u001b[0m  0.0117\n",
      "     14       \u001b[36m30.1185\u001b[0m       \u001b[32m29.6522\u001b[0m  0.0113\n",
      "     15       \u001b[36m29.7743\u001b[0m       \u001b[32m29.4040\u001b[0m  0.0115\n",
      "     16       \u001b[36m29.4317\u001b[0m       \u001b[32m29.1595\u001b[0m  0.0114\n",
      "     17       \u001b[36m29.0911\u001b[0m       \u001b[32m28.9194\u001b[0m  0.0115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     18       \u001b[36m28.7535\u001b[0m       \u001b[32m28.6843\u001b[0m  0.0123\n",
      "     19       \u001b[36m28.4198\u001b[0m       \u001b[32m28.4548\u001b[0m  0.0111\n",
      "     20       \u001b[36m28.0908\u001b[0m       \u001b[32m28.2319\u001b[0m  0.0113\n",
      "     21       \u001b[36m27.7671\u001b[0m       \u001b[32m28.0166\u001b[0m  0.0112\n",
      "     22       \u001b[36m27.4501\u001b[0m       \u001b[32m27.8099\u001b[0m  0.0110\n",
      "     23       \u001b[36m27.1410\u001b[0m       \u001b[32m27.6132\u001b[0m  0.0116\n",
      "     24       \u001b[36m26.8410\u001b[0m       \u001b[32m27.4271\u001b[0m  0.0114\n",
      "     25       \u001b[36m26.5513\u001b[0m       \u001b[32m27.2529\u001b[0m  0.0113\n",
      "     26       \u001b[36m26.2732\u001b[0m       \u001b[32m27.0917\u001b[0m  0.0110\n",
      "     27       \u001b[36m26.0081\u001b[0m       \u001b[32m26.9442\u001b[0m  0.0105\n",
      "     28       \u001b[36m25.7568\u001b[0m       \u001b[32m26.8109\u001b[0m  0.0109\n",
      "     29       \u001b[36m25.5203\u001b[0m       \u001b[32m26.6923\u001b[0m  0.0109\n",
      "     30       \u001b[36m25.2993\u001b[0m       \u001b[32m26.5886\u001b[0m  0.0107\n",
      "     31       \u001b[36m25.0944\u001b[0m       \u001b[32m26.4996\u001b[0m  0.0108\n",
      "     32       \u001b[36m24.9055\u001b[0m       \u001b[32m26.4249\u001b[0m  0.0107\n",
      "     33       \u001b[36m24.7328\u001b[0m       \u001b[32m26.3641\u001b[0m  0.0108\n",
      "     34       \u001b[36m24.5759\u001b[0m       \u001b[32m26.3162\u001b[0m  0.0110\n",
      "     35       \u001b[36m24.4346\u001b[0m       \u001b[32m26.2805\u001b[0m  0.0109\n",
      "     36       \u001b[36m24.3083\u001b[0m       \u001b[32m26.2559\u001b[0m  0.0107\n",
      "     37       \u001b[36m24.1959\u001b[0m       \u001b[32m26.2408\u001b[0m  0.0107\n",
      "     38       \u001b[36m24.0964\u001b[0m       \u001b[32m26.2341\u001b[0m  0.0111\n",
      "     39       \u001b[36m24.0089\u001b[0m       26.2344  0.0116\n",
      "     40       \u001b[36m23.9321\u001b[0m       26.2404  0.0111\n",
      "     41       \u001b[36m23.8652\u001b[0m       26.2511  0.0108\n",
      "     42       \u001b[36m23.8072\u001b[0m       26.2653  0.0109\n",
      "     43       \u001b[36m23.7568\u001b[0m       26.2820  0.0109\n",
      "     44       \u001b[36m23.7132\u001b[0m       26.3003  0.0116\n",
      "     45       \u001b[36m23.6754\u001b[0m       26.3196  0.0116\n",
      "     46       \u001b[36m23.6425\u001b[0m       26.3391  0.0109\n",
      "     47       \u001b[36m23.6139\u001b[0m       26.3585  0.0106\n",
      "     48       \u001b[36m23.5888\u001b[0m       26.3774  0.0106\n",
      "     49       \u001b[36m23.5669\u001b[0m       26.3954  0.0232\n",
      "     50       \u001b[36m23.5476\u001b[0m       26.4124  0.0132\n",
      "     51       \u001b[36m23.5305\u001b[0m       26.4283  0.0122\n",
      "     52       \u001b[36m23.5152\u001b[0m       26.4429  0.0140\n",
      "     53       \u001b[36m23.5015\u001b[0m       26.4563  0.0127\n",
      "     54       \u001b[36m23.4891\u001b[0m       26.4684  0.0198\n",
      "     55       \u001b[36m23.4778\u001b[0m       26.4793  0.0156\n",
      "     56       \u001b[36m23.4674\u001b[0m       26.4890  0.0109\n",
      "     57       \u001b[36m23.4579\u001b[0m       26.4977  0.0110\n",
      "     58       \u001b[36m23.4490\u001b[0m       26.5053  0.0117\n",
      "     59       \u001b[36m23.4407\u001b[0m       26.5122  0.0112\n",
      "     60       \u001b[36m23.4329\u001b[0m       26.5180  0.0110\n",
      "     61       \u001b[36m23.4256\u001b[0m       26.5231  0.0109\n",
      "     62       \u001b[36m23.4186\u001b[0m       26.5275  0.0110\n",
      "     63       \u001b[36m23.4120\u001b[0m       26.5313  0.0112\n",
      "     64       \u001b[36m23.4057\u001b[0m       26.5344  0.0113\n",
      "     65       \u001b[36m23.3997\u001b[0m       26.5370  0.0113\n",
      "     66       \u001b[36m23.3939\u001b[0m       26.5391  0.0113\n",
      "     67       \u001b[36m23.3883\u001b[0m       26.5409  0.0110\n",
      "     68       \u001b[36m23.3828\u001b[0m       26.5422  0.0109\n",
      "     69       \u001b[36m23.3776\u001b[0m       26.5432  0.0109\n",
      "     70       \u001b[36m23.3725\u001b[0m       26.5440  0.0108\n",
      "     71       \u001b[36m23.3676\u001b[0m       26.5445  0.0107\n",
      "     72       \u001b[36m23.3628\u001b[0m       26.5447  0.0108\n",
      "     73       \u001b[36m23.3581\u001b[0m       26.5448  0.0113\n",
      "     74       \u001b[36m23.3536\u001b[0m       26.5448  0.0113\n",
      "     75       \u001b[36m23.3492\u001b[0m       26.5446  0.0108\n",
      "     76       \u001b[36m23.3449\u001b[0m       26.5442  0.0108\n",
      "     77       \u001b[36m23.3407\u001b[0m       26.5437  0.0110\n",
      "     78       \u001b[36m23.3366\u001b[0m       26.5432  0.0109\n",
      "     79       \u001b[36m23.3325\u001b[0m       26.5425  0.0110\n",
      "     80       \u001b[36m23.3286\u001b[0m       26.5418  0.0110\n",
      "     81       \u001b[36m23.3248\u001b[0m       26.5410  0.0108\n",
      "     82       \u001b[36m23.3210\u001b[0m       26.5402  0.0108\n",
      "     83       \u001b[36m23.3173\u001b[0m       26.5394  0.0112\n",
      "     84       \u001b[36m23.3137\u001b[0m       26.5385  0.0111\n",
      "     85       \u001b[36m23.3102\u001b[0m       26.5376  0.0110\n",
      "     86       \u001b[36m23.3067\u001b[0m       26.5367  0.0112\n",
      "     87       \u001b[36m23.3034\u001b[0m       26.5358  0.0116\n",
      "     88       \u001b[36m23.3001\u001b[0m       26.5349  0.0114\n",
      "     89       \u001b[36m23.2968\u001b[0m       26.5340  0.0112\n",
      "     90       \u001b[36m23.2937\u001b[0m       26.5331  0.0112\n",
      "     91       \u001b[36m23.2906\u001b[0m       26.5323  0.0108\n",
      "     92       \u001b[36m23.2875\u001b[0m       26.5315  0.0106\n",
      "     93       \u001b[36m23.2846\u001b[0m       26.5307  0.0117\n",
      "     94       \u001b[36m23.2817\u001b[0m       26.5298  0.0115\n",
      "     95       \u001b[36m23.2788\u001b[0m       26.5290  0.0111\n",
      "     96       \u001b[36m23.2760\u001b[0m       26.5281  0.0108\n",
      "     97       \u001b[36m23.2733\u001b[0m       26.5272  0.0108\n",
      "     98       \u001b[36m23.2706\u001b[0m       26.5264  0.0118\n",
      "     99       \u001b[36m23.2679\u001b[0m       26.5256  0.0110\n",
      "    100       \u001b[36m23.2653\u001b[0m       26.5249  0.0115\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m40.2505\u001b[0m       \u001b[32m31.9719\u001b[0m  0.0107\n",
      "      2       \u001b[36m39.7942\u001b[0m       \u001b[32m31.6639\u001b[0m  0.0112\n",
      "      3       \u001b[36m39.3583\u001b[0m       \u001b[32m31.3703\u001b[0m  0.0128\n",
      "      4       \u001b[36m38.9401\u001b[0m       \u001b[32m31.0886\u001b[0m  0.0111\n",
      "      5       \u001b[36m38.5353\u001b[0m       \u001b[32m30.8168\u001b[0m  0.0109\n",
      "      6       \u001b[36m38.1404\u001b[0m       \u001b[32m30.5522\u001b[0m  0.0108\n",
      "      7       \u001b[36m37.7526\u001b[0m       \u001b[32m30.2934\u001b[0m  0.0117\n",
      "      8       \u001b[36m37.3709\u001b[0m       \u001b[32m30.0400\u001b[0m  0.0115\n",
      "      9       \u001b[36m36.9932\u001b[0m       \u001b[32m29.7913\u001b[0m  0.0114\n",
      "     10       \u001b[36m36.6186\u001b[0m       \u001b[32m29.5466\u001b[0m  0.0107\n",
      "     11       \u001b[36m36.2466\u001b[0m       \u001b[32m29.3060\u001b[0m  0.0106\n",
      "     12       \u001b[36m35.8770\u001b[0m       \u001b[32m29.0689\u001b[0m  0.0110\n",
      "     13       \u001b[36m35.5085\u001b[0m       \u001b[32m28.8365\u001b[0m  0.0109\n",
      "     14       \u001b[36m35.1426\u001b[0m       \u001b[32m28.6085\u001b[0m  0.0112\n",
      "     15       \u001b[36m34.7794\u001b[0m       \u001b[32m28.3857\u001b[0m  0.0108\n",
      "     16       \u001b[36m34.4199\u001b[0m       \u001b[32m28.1686\u001b[0m  0.0111\n",
      "     17       \u001b[36m34.0648\u001b[0m       \u001b[32m27.9576\u001b[0m  0.0122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     18       \u001b[36m33.7152\u001b[0m       \u001b[32m27.7538\u001b[0m  0.0116\n",
      "     19       \u001b[36m33.3722\u001b[0m       \u001b[32m27.5581\u001b[0m  0.0108\n",
      "     20       \u001b[36m33.0367\u001b[0m       \u001b[32m27.3716\u001b[0m  0.0108\n",
      "     21       \u001b[36m32.7099\u001b[0m       \u001b[32m27.1951\u001b[0m  0.0108\n",
      "     22       \u001b[36m32.3929\u001b[0m       \u001b[32m27.0297\u001b[0m  0.0108\n",
      "     23       \u001b[36m32.0868\u001b[0m       \u001b[32m26.8762\u001b[0m  0.0109\n",
      "     24       \u001b[36m31.7925\u001b[0m       \u001b[32m26.7356\u001b[0m  0.0108\n",
      "     25       \u001b[36m31.5109\u001b[0m       \u001b[32m26.6087\u001b[0m  0.0108\n",
      "     26       \u001b[36m31.2429\u001b[0m       \u001b[32m26.4961\u001b[0m  0.0109\n",
      "     27       \u001b[36m30.9895\u001b[0m       \u001b[32m26.3982\u001b[0m  0.0108\n",
      "     28       \u001b[36m30.7512\u001b[0m       \u001b[32m26.3156\u001b[0m  0.0111\n",
      "     29       \u001b[36m30.5287\u001b[0m       \u001b[32m26.2481\u001b[0m  0.0110\n",
      "     30       \u001b[36m30.3223\u001b[0m       \u001b[32m26.1957\u001b[0m  0.0129\n",
      "     31       \u001b[36m30.1324\u001b[0m       \u001b[32m26.1579\u001b[0m  0.0151\n",
      "     32       \u001b[36m29.9589\u001b[0m       \u001b[32m26.1341\u001b[0m  0.0158\n",
      "     33       \u001b[36m29.8016\u001b[0m       \u001b[32m26.1231\u001b[0m  0.0127\n",
      "     34       \u001b[36m29.6604\u001b[0m       26.1241  0.0121\n",
      "     35       \u001b[36m29.5348\u001b[0m       26.1355  0.0124\n",
      "     36       \u001b[36m29.4237\u001b[0m       26.1560  0.0117\n",
      "     37       \u001b[36m29.3262\u001b[0m       26.1843  0.0141\n",
      "     38       \u001b[36m29.2417\u001b[0m       26.2187  0.0115\n",
      "     39       \u001b[36m29.1685\u001b[0m       26.2575  0.0115\n",
      "     40       \u001b[36m29.1054\u001b[0m       26.2996  0.0137\n",
      "     41       \u001b[36m29.0514\u001b[0m       26.3436  0.0110\n",
      "     42       \u001b[36m29.0051\u001b[0m       26.3886  0.0120\n",
      "     43       \u001b[36m28.9657\u001b[0m       26.4334  0.0111\n",
      "     44       \u001b[36m28.9319\u001b[0m       26.4776  0.0112\n",
      "     45       \u001b[36m28.9029\u001b[0m       26.5204  0.0107\n",
      "     46       \u001b[36m28.8779\u001b[0m       26.5614  0.0109\n",
      "     47       \u001b[36m28.8563\u001b[0m       26.6002  0.0117\n",
      "     48       \u001b[36m28.8374\u001b[0m       26.6367  0.0114\n",
      "     49       \u001b[36m28.8210\u001b[0m       26.6709  0.0111\n",
      "     50       \u001b[36m28.8064\u001b[0m       26.7025  0.0111\n",
      "     51       \u001b[36m28.7934\u001b[0m       26.7320  0.0117\n",
      "     52       \u001b[36m28.7818\u001b[0m       26.7591  0.0116\n",
      "     53       \u001b[36m28.7712\u001b[0m       26.7839  0.0109\n",
      "     54       \u001b[36m28.7615\u001b[0m       26.8066  0.0111\n",
      "     55       \u001b[36m28.7525\u001b[0m       26.8273  0.0107\n",
      "     56       \u001b[36m28.7442\u001b[0m       26.8460  0.0107\n",
      "     57       \u001b[36m28.7364\u001b[0m       26.8631  0.0122\n",
      "     58       \u001b[36m28.7290\u001b[0m       26.8786  0.0115\n",
      "     59       \u001b[36m28.7220\u001b[0m       26.8927  0.0112\n",
      "     60       \u001b[36m28.7153\u001b[0m       26.9055  0.0110\n",
      "     61       \u001b[36m28.7090\u001b[0m       26.9170  0.0129\n",
      "     62       \u001b[36m28.7028\u001b[0m       26.9275  0.0112\n",
      "     63       \u001b[36m28.6970\u001b[0m       26.9371  0.0110\n",
      "     64       \u001b[36m28.6913\u001b[0m       26.9457  0.0113\n",
      "     65       \u001b[36m28.6859\u001b[0m       26.9535  0.0125\n",
      "     66       \u001b[36m28.6806\u001b[0m       26.9607  0.0114\n",
      "     67       \u001b[36m28.6755\u001b[0m       26.9672  0.0116\n",
      "     68       \u001b[36m28.6706\u001b[0m       26.9732  0.0114\n",
      "     69       \u001b[36m28.6659\u001b[0m       26.9787  0.0114\n",
      "     70       \u001b[36m28.6613\u001b[0m       26.9837  0.0109\n",
      "     71       \u001b[36m28.6568\u001b[0m       26.9883  0.0108\n",
      "     72       \u001b[36m28.6524\u001b[0m       26.9929  0.0113\n",
      "     73       \u001b[36m28.6483\u001b[0m       26.9972  0.0113\n",
      "     74       \u001b[36m28.6442\u001b[0m       27.0011  0.0113\n",
      "     75       \u001b[36m28.6403\u001b[0m       27.0046  0.0109\n",
      "     76       \u001b[36m28.6364\u001b[0m       27.0078  0.0111\n",
      "     77       \u001b[36m28.6327\u001b[0m       27.0108  0.0116\n",
      "     78       \u001b[36m28.6290\u001b[0m       27.0136  0.0113\n",
      "     79       \u001b[36m28.6254\u001b[0m       27.0162  0.0111\n",
      "     80       \u001b[36m28.6219\u001b[0m       27.0186  0.0108\n",
      "     81       \u001b[36m28.6185\u001b[0m       27.0208  0.0110\n",
      "     82       \u001b[36m28.6152\u001b[0m       27.0229  0.0117\n",
      "     83       \u001b[36m28.6120\u001b[0m       27.0250  0.0112\n",
      "     84       \u001b[36m28.6088\u001b[0m       27.0269  0.0116\n",
      "     85       \u001b[36m28.6058\u001b[0m       27.0287  0.0111\n",
      "     86       \u001b[36m28.6028\u001b[0m       27.0305  0.0109\n",
      "     87       \u001b[36m28.5999\u001b[0m       27.0321  0.0113\n",
      "     88       \u001b[36m28.5970\u001b[0m       27.0336  0.0112\n",
      "     89       \u001b[36m28.5943\u001b[0m       27.0351  0.0112\n",
      "     90       \u001b[36m28.5916\u001b[0m       27.0365  0.0110\n",
      "     91       \u001b[36m28.5890\u001b[0m       27.0378  0.0109\n",
      "     92       \u001b[36m28.5864\u001b[0m       27.0390  0.0110\n",
      "     93       \u001b[36m28.5840\u001b[0m       27.0402  0.0112\n",
      "     94       \u001b[36m28.5815\u001b[0m       27.0413  0.0112\n",
      "     95       \u001b[36m28.5791\u001b[0m       27.0424  0.0111\n",
      "     96       \u001b[36m28.5768\u001b[0m       27.0434  0.0108\n",
      "     97       \u001b[36m28.5746\u001b[0m       27.0443  0.0110\n",
      "     98       \u001b[36m28.5724\u001b[0m       27.0452  0.0110\n",
      "     99       \u001b[36m28.5702\u001b[0m       27.0461  0.0111\n",
      "    100       \u001b[36m28.5681\u001b[0m       27.0469  0.0110\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m43.4499\u001b[0m       \u001b[32m44.7524\u001b[0m  0.0115\n",
      "      2       \u001b[36m42.3308\u001b[0m       \u001b[32m43.3367\u001b[0m  0.0120\n",
      "      3       \u001b[36m41.2481\u001b[0m       \u001b[32m41.9770\u001b[0m  0.0120\n",
      "      4       \u001b[36m40.1568\u001b[0m       \u001b[32m40.5465\u001b[0m  0.0115\n",
      "      5       \u001b[36m38.9087\u001b[0m       \u001b[32m38.6804\u001b[0m  0.0115\n",
      "      6       \u001b[36m37.3550\u001b[0m       \u001b[32m36.3528\u001b[0m  0.0116\n",
      "      7       \u001b[36m35.6539\u001b[0m       \u001b[32m33.9078\u001b[0m  0.0121\n",
      "      8       \u001b[36m34.2599\u001b[0m       \u001b[32m32.0238\u001b[0m  0.0119\n",
      "      9       \u001b[36m33.6821\u001b[0m       \u001b[32m31.2011\u001b[0m  0.0119\n",
      "     10       33.6988       \u001b[32m31.0136\u001b[0m  0.0114\n",
      "     11       \u001b[36m33.5480\u001b[0m       31.0736  0.0116\n",
      "     12       \u001b[36m33.2800\u001b[0m       31.2735  0.0185\n",
      "     13       \u001b[36m33.1294\u001b[0m       31.3550  0.0147\n",
      "     14       \u001b[36m33.0336\u001b[0m       31.2606  0.0141\n",
      "     15       \u001b[36m32.9402\u001b[0m       31.0950  0.0122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     16       \u001b[36m32.8574\u001b[0m       \u001b[32m30.9523\u001b[0m  0.0130\n",
      "     17       \u001b[36m32.7908\u001b[0m       \u001b[32m30.8768\u001b[0m  0.0148\n",
      "     18       \u001b[36m32.7379\u001b[0m       \u001b[32m30.8568\u001b[0m  0.0129\n",
      "     19       \u001b[36m32.6931\u001b[0m       \u001b[32m30.8450\u001b[0m  0.0122\n",
      "     20       \u001b[36m32.6502\u001b[0m       \u001b[32m30.8285\u001b[0m  0.0119\n",
      "     21       \u001b[36m32.6114\u001b[0m       \u001b[32m30.8204\u001b[0m  0.0132\n",
      "     22       \u001b[36m32.5770\u001b[0m       30.8226  0.0131\n",
      "     23       \u001b[36m32.5459\u001b[0m       30.8212  0.0120\n",
      "     24       \u001b[36m32.5174\u001b[0m       \u001b[32m30.8032\u001b[0m  0.0120\n",
      "     25       \u001b[36m32.4913\u001b[0m       \u001b[32m30.7773\u001b[0m  0.0118\n",
      "     26       \u001b[36m32.4679\u001b[0m       \u001b[32m30.7627\u001b[0m  0.0117\n",
      "     27       \u001b[36m32.4472\u001b[0m       \u001b[32m30.7613\u001b[0m  0.0134\n",
      "     28       \u001b[36m32.4283\u001b[0m       \u001b[32m30.7601\u001b[0m  0.0124\n",
      "     29       \u001b[36m32.4107\u001b[0m       \u001b[32m30.7562\u001b[0m  0.0125\n",
      "     30       \u001b[36m32.3943\u001b[0m       \u001b[32m30.7513\u001b[0m  0.0115\n",
      "     31       \u001b[36m32.3791\u001b[0m       \u001b[32m30.7498\u001b[0m  0.0116\n",
      "     32       \u001b[36m32.3651\u001b[0m       \u001b[32m30.7459\u001b[0m  0.0131\n",
      "     33       \u001b[36m32.3518\u001b[0m       \u001b[32m30.7411\u001b[0m  0.0120\n",
      "     34       \u001b[36m32.3398\u001b[0m       \u001b[32m30.7399\u001b[0m  0.0121\n",
      "     35       \u001b[36m32.3286\u001b[0m       30.7405  0.0115\n",
      "     36       \u001b[36m32.3184\u001b[0m       30.7415  0.0113\n",
      "     37       \u001b[36m32.3086\u001b[0m       30.7423  0.0128\n",
      "     38       \u001b[36m32.2995\u001b[0m       30.7410  0.0120\n",
      "     39       \u001b[36m32.2910\u001b[0m       30.7400  0.0120\n",
      "     40       \u001b[36m32.2831\u001b[0m       \u001b[32m30.7391\u001b[0m  0.0114\n",
      "     41       \u001b[36m32.2758\u001b[0m       \u001b[32m30.7371\u001b[0m  0.0119\n",
      "     42       \u001b[36m32.2690\u001b[0m       \u001b[32m30.7357\u001b[0m  0.0126\n",
      "     43       \u001b[36m32.2625\u001b[0m       \u001b[32m30.7321\u001b[0m  0.0117\n",
      "     44       \u001b[36m32.2564\u001b[0m       \u001b[32m30.7270\u001b[0m  0.0121\n",
      "     45       \u001b[36m32.2507\u001b[0m       \u001b[32m30.7223\u001b[0m  0.0111\n",
      "     46       \u001b[36m32.2452\u001b[0m       \u001b[32m30.7167\u001b[0m  0.0114\n",
      "     47       \u001b[36m32.2401\u001b[0m       \u001b[32m30.7113\u001b[0m  0.0135\n",
      "     48       \u001b[36m32.2351\u001b[0m       \u001b[32m30.7066\u001b[0m  0.0123\n",
      "     49       \u001b[36m32.2303\u001b[0m       \u001b[32m30.7000\u001b[0m  0.0118\n",
      "     50       \u001b[36m32.2259\u001b[0m       \u001b[32m30.6907\u001b[0m  0.0115\n",
      "     51       \u001b[36m32.2215\u001b[0m       \u001b[32m30.6807\u001b[0m  0.0115\n",
      "     52       \u001b[36m32.2173\u001b[0m       \u001b[32m30.6729\u001b[0m  0.0135\n",
      "     53       \u001b[36m32.2134\u001b[0m       \u001b[32m30.6644\u001b[0m  0.0120\n",
      "     54       \u001b[36m32.2095\u001b[0m       \u001b[32m30.6564\u001b[0m  0.0118\n",
      "     55       \u001b[36m32.2060\u001b[0m       \u001b[32m30.6493\u001b[0m  0.0117\n",
      "     56       \u001b[36m32.2025\u001b[0m       \u001b[32m30.6441\u001b[0m  0.0116\n",
      "     57       \u001b[36m32.1992\u001b[0m       \u001b[32m30.6372\u001b[0m  0.0128\n",
      "     58       \u001b[36m32.1959\u001b[0m       \u001b[32m30.6319\u001b[0m  0.0119\n",
      "     59       \u001b[36m32.1928\u001b[0m       \u001b[32m30.6243\u001b[0m  0.0119\n",
      "     60       \u001b[36m32.1898\u001b[0m       \u001b[32m30.6180\u001b[0m  0.0117\n",
      "     61       \u001b[36m32.1869\u001b[0m       \u001b[32m30.6124\u001b[0m  0.0114\n",
      "     62       \u001b[36m32.1841\u001b[0m       \u001b[32m30.6053\u001b[0m  0.0126\n",
      "     63       \u001b[36m32.1814\u001b[0m       \u001b[32m30.5985\u001b[0m  0.0121\n",
      "     64       \u001b[36m32.1788\u001b[0m       \u001b[32m30.5911\u001b[0m  0.0121\n",
      "     65       \u001b[36m32.1762\u001b[0m       \u001b[32m30.5835\u001b[0m  0.0115\n",
      "     66       \u001b[36m32.1738\u001b[0m       \u001b[32m30.5787\u001b[0m  0.0112\n",
      "     67       \u001b[36m32.1714\u001b[0m       \u001b[32m30.5705\u001b[0m  0.0132\n",
      "     68       \u001b[36m32.1691\u001b[0m       \u001b[32m30.5657\u001b[0m  0.0126\n",
      "     69       \u001b[36m32.1669\u001b[0m       \u001b[32m30.5586\u001b[0m  0.0123\n",
      "     70       \u001b[36m32.1646\u001b[0m       \u001b[32m30.5570\u001b[0m  0.0116\n",
      "     71       \u001b[36m32.1625\u001b[0m       \u001b[32m30.5506\u001b[0m  0.0115\n",
      "     72       \u001b[36m32.1603\u001b[0m       \u001b[32m30.5453\u001b[0m  0.0125\n",
      "     73       \u001b[36m32.1583\u001b[0m       \u001b[32m30.5391\u001b[0m  0.0125\n",
      "     74       \u001b[36m32.1563\u001b[0m       \u001b[32m30.5360\u001b[0m  0.0121\n",
      "     75       \u001b[36m32.1544\u001b[0m       \u001b[32m30.5338\u001b[0m  0.0116\n",
      "     76       \u001b[36m32.1525\u001b[0m       \u001b[32m30.5336\u001b[0m  0.0115\n",
      "     77       \u001b[36m32.1507\u001b[0m       \u001b[32m30.5277\u001b[0m  0.0130\n",
      "     78       \u001b[36m32.1489\u001b[0m       \u001b[32m30.5255\u001b[0m  0.0116\n",
      "     79       \u001b[36m32.1471\u001b[0m       \u001b[32m30.5219\u001b[0m  0.0119\n",
      "     80       \u001b[36m32.1454\u001b[0m       30.5235  0.0119\n",
      "     81       \u001b[36m32.1437\u001b[0m       \u001b[32m30.5178\u001b[0m  0.0113\n",
      "     82       \u001b[36m32.1420\u001b[0m       30.5215  0.0127\n",
      "     83       \u001b[36m32.1404\u001b[0m       \u001b[32m30.5134\u001b[0m  0.0119\n",
      "     84       \u001b[36m32.1388\u001b[0m       30.5188  0.0122\n",
      "     85       \u001b[36m32.1373\u001b[0m       \u001b[32m30.5071\u001b[0m  0.0156\n",
      "     86       \u001b[36m32.1358\u001b[0m       30.5197  0.0217\n",
      "     87       \u001b[36m32.1344\u001b[0m       \u001b[32m30.5005\u001b[0m  0.0122\n",
      "     88       \u001b[36m32.1329\u001b[0m       30.5229  0.0149\n",
      "     89       \u001b[36m32.1318\u001b[0m       \u001b[32m30.4890\u001b[0m  0.0155\n",
      "     90       \u001b[36m32.1304\u001b[0m       30.5324  0.0126\n",
      "     91       \u001b[36m32.1299\u001b[0m       \u001b[32m30.4732\u001b[0m  0.0124\n",
      "     92       \u001b[36m32.1291\u001b[0m       30.5532  0.0124\n",
      "     93       32.1299       \u001b[32m30.4517\u001b[0m  0.0133\n",
      "     94       32.1309       30.5891  0.0127\n",
      "     95       32.1346       \u001b[32m30.4306\u001b[0m  0.0124\n",
      "     96       32.1374       30.6046  0.0119\n",
      "     97       32.1417       \u001b[32m30.4230\u001b[0m  0.0120\n",
      "     98       32.1393       30.5516  0.0119\n",
      "     99       32.1339       30.4527  0.0122\n",
      "    100       \u001b[36m32.1228\u001b[0m       30.4761  0.0119\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m34.4110\u001b[0m       \u001b[32m32.4782\u001b[0m  0.0120\n",
      "      2       \u001b[36m33.2911\u001b[0m       \u001b[32m31.6224\u001b[0m  0.0118\n",
      "      3       \u001b[36m32.0924\u001b[0m       \u001b[32m30.7069\u001b[0m  0.0166\n",
      "      4       \u001b[36m30.7888\u001b[0m       \u001b[32m29.7008\u001b[0m  0.0121\n",
      "      5       \u001b[36m29.3302\u001b[0m       \u001b[32m28.5498\u001b[0m  0.0118\n",
      "      6       \u001b[36m27.6548\u001b[0m       \u001b[32m27.4495\u001b[0m  0.0113\n",
      "      7       \u001b[36m25.9666\u001b[0m       \u001b[32m26.9153\u001b[0m  0.0119\n",
      "      8       \u001b[36m24.7854\u001b[0m       27.3558  0.0118\n",
      "      9       \u001b[36m24.4483\u001b[0m       27.9971  0.0115\n",
      "     10       \u001b[36m24.3963\u001b[0m       27.7472  0.0120\n",
      "     11       \u001b[36m24.1268\u001b[0m       27.1183  0.0113\n",
      "     12       \u001b[36m23.9087\u001b[0m       \u001b[32m26.7228\u001b[0m  0.0114\n",
      "     13       \u001b[36m23.8282\u001b[0m       \u001b[32m26.5750\u001b[0m  0.0118\n",
      "     14       \u001b[36m23.7664\u001b[0m       \u001b[32m26.5746\u001b[0m  0.0114\n",
      "     15       \u001b[36m23.6873\u001b[0m       26.6728  0.0115\n",
      "     16       \u001b[36m23.6167\u001b[0m       26.8067  0.0115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m23.5630\u001b[0m       26.8982  0.0114\n",
      "     18       \u001b[36m23.5170\u001b[0m       26.9224  0.0117\n",
      "     19       \u001b[36m23.4767\u001b[0m       26.9044  0.0114\n",
      "     20       \u001b[36m23.4424\u001b[0m       26.8792  0.0115\n",
      "     21       \u001b[36m23.4122\u001b[0m       26.8663  0.0114\n",
      "     22       \u001b[36m23.3846\u001b[0m       26.8668  0.0114\n",
      "     23       \u001b[36m23.3583\u001b[0m       26.8744  0.0120\n",
      "     24       \u001b[36m23.3335\u001b[0m       26.8848  0.0115\n",
      "     25       \u001b[36m23.3109\u001b[0m       26.8910  0.0117\n",
      "     26       \u001b[36m23.2908\u001b[0m       26.8905  0.0114\n",
      "     27       \u001b[36m23.2729\u001b[0m       26.8859  0.0116\n",
      "     28       \u001b[36m23.2568\u001b[0m       26.8812  0.0117\n",
      "     29       \u001b[36m23.2421\u001b[0m       26.8781  0.0115\n",
      "     30       \u001b[36m23.2284\u001b[0m       26.8758  0.0115\n",
      "     31       \u001b[36m23.2158\u001b[0m       26.8741  0.0115\n",
      "     32       \u001b[36m23.2041\u001b[0m       26.8700  0.0115\n",
      "     33       \u001b[36m23.1933\u001b[0m       26.8626  0.0121\n",
      "     34       \u001b[36m23.1833\u001b[0m       26.8557  0.0116\n",
      "     35       \u001b[36m23.1741\u001b[0m       26.8487  0.0114\n",
      "     36       \u001b[36m23.1655\u001b[0m       26.8425  0.0115\n",
      "     37       \u001b[36m23.1575\u001b[0m       26.8358  0.0113\n",
      "     38       \u001b[36m23.1500\u001b[0m       26.8278  0.0120\n",
      "     39       \u001b[36m23.1429\u001b[0m       26.8198  0.0116\n",
      "     40       \u001b[36m23.1362\u001b[0m       26.8120  0.0118\n",
      "     41       \u001b[36m23.1300\u001b[0m       26.8048  0.0113\n",
      "     42       \u001b[36m23.1241\u001b[0m       26.7980  0.0114\n",
      "     43       \u001b[36m23.1185\u001b[0m       26.7910  0.0119\n",
      "     44       \u001b[36m23.1132\u001b[0m       26.7829  0.0115\n",
      "     45       \u001b[36m23.1082\u001b[0m       26.7760  0.0119\n",
      "     46       \u001b[36m23.1035\u001b[0m       26.7718  0.0114\n",
      "     47       \u001b[36m23.0990\u001b[0m       26.7690  0.0115\n",
      "     48       \u001b[36m23.0947\u001b[0m       26.7663  0.0116\n",
      "     49       \u001b[36m23.0907\u001b[0m       26.7625  0.0119\n",
      "     50       \u001b[36m23.0868\u001b[0m       26.7591  0.0113\n",
      "     51       \u001b[36m23.0832\u001b[0m       26.7551  0.0114\n",
      "     52       \u001b[36m23.0797\u001b[0m       26.7507  0.0114\n",
      "     53       \u001b[36m23.0763\u001b[0m       26.7466  0.0123\n",
      "     54       \u001b[36m23.0730\u001b[0m       26.7425  0.0115\n",
      "     55       \u001b[36m23.0699\u001b[0m       26.7389  0.0118\n",
      "     56       \u001b[36m23.0669\u001b[0m       26.7362  0.0113\n",
      "     57       \u001b[36m23.0641\u001b[0m       26.7338  0.0113\n",
      "     58       \u001b[36m23.0613\u001b[0m       26.7308  0.0119\n",
      "     59       \u001b[36m23.0587\u001b[0m       26.7283  0.0115\n",
      "     60       \u001b[36m23.0562\u001b[0m       26.7263  0.0113\n",
      "     61       \u001b[36m23.0537\u001b[0m       26.7228  0.0113\n",
      "     62       \u001b[36m23.0514\u001b[0m       26.7189  0.0113\n",
      "     63       \u001b[36m23.0491\u001b[0m       26.7165  0.0116\n",
      "     64       \u001b[36m23.0469\u001b[0m       26.7140  0.0117\n",
      "     65       \u001b[36m23.0448\u001b[0m       26.7108  0.0117\n",
      "     66       \u001b[36m23.0427\u001b[0m       26.7089  0.0113\n",
      "     67       \u001b[36m23.0408\u001b[0m       26.7071  0.0119\n",
      "     68       \u001b[36m23.0389\u001b[0m       26.7047  0.0161\n",
      "     69       \u001b[36m23.0371\u001b[0m       26.7021  0.0149\n",
      "     70       \u001b[36m23.0353\u001b[0m       26.6997  0.0125\n",
      "     71       \u001b[36m23.0336\u001b[0m       26.6983  0.0131\n",
      "     72       \u001b[36m23.0319\u001b[0m       26.6969  0.0159\n",
      "     73       \u001b[36m23.0302\u001b[0m       26.6965  0.0168\n",
      "     74       \u001b[36m23.0287\u001b[0m       26.6954  0.0147\n",
      "     75       \u001b[36m23.0272\u001b[0m       26.6924  0.0132\n",
      "     76       \u001b[36m23.0257\u001b[0m       26.6902  0.0118\n",
      "     77       \u001b[36m23.0243\u001b[0m       26.6895  0.0120\n",
      "     78       \u001b[36m23.0229\u001b[0m       26.6893  0.0135\n",
      "     79       \u001b[36m23.0216\u001b[0m       26.6878  0.0121\n",
      "     80       \u001b[36m23.0203\u001b[0m       26.6871  0.0117\n",
      "     81       \u001b[36m23.0191\u001b[0m       26.6875  0.0119\n",
      "     82       \u001b[36m23.0179\u001b[0m       26.6873  0.0119\n",
      "     83       \u001b[36m23.0167\u001b[0m       26.6853  0.0120\n",
      "     84       \u001b[36m23.0156\u001b[0m       26.6844  0.0120\n",
      "     85       \u001b[36m23.0145\u001b[0m       26.6848  0.0118\n",
      "     86       \u001b[36m23.0134\u001b[0m       26.6853  0.0116\n",
      "     87       \u001b[36m23.0123\u001b[0m       26.6855  0.0117\n",
      "     88       \u001b[36m23.0113\u001b[0m       26.6856  0.0116\n",
      "     89       \u001b[36m23.0103\u001b[0m       26.6854  0.0123\n",
      "     90       \u001b[36m23.0092\u001b[0m       26.6859  0.0113\n",
      "     91       \u001b[36m23.0082\u001b[0m       26.6860  0.0123\n",
      "     92       \u001b[36m23.0073\u001b[0m       26.6866  0.0115\n",
      "     93       \u001b[36m23.0063\u001b[0m       26.6873  0.0118\n",
      "     94       \u001b[36m23.0055\u001b[0m       26.6884  0.0117\n",
      "     95       \u001b[36m23.0045\u001b[0m       26.6887  0.0115\n",
      "     96       \u001b[36m23.0037\u001b[0m       26.6894  0.0121\n",
      "     97       \u001b[36m23.0028\u001b[0m       26.6893  0.0114\n",
      "     98       \u001b[36m23.0021\u001b[0m       26.6921  0.0117\n",
      "     99       \u001b[36m23.0012\u001b[0m       26.6924  0.0113\n",
      "    100       \u001b[36m23.0007\u001b[0m       26.6944  0.0113\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m41.2294\u001b[0m       \u001b[32m32.4547\u001b[0m  0.0124\n",
      "      2       \u001b[36m40.0287\u001b[0m       \u001b[32m31.6729\u001b[0m  0.0117\n",
      "      3       \u001b[36m38.8481\u001b[0m       \u001b[32m30.7789\u001b[0m  0.0119\n",
      "      4       \u001b[36m37.5559\u001b[0m       \u001b[32m29.6910\u001b[0m  0.0114\n",
      "      5       \u001b[36m35.9198\u001b[0m       \u001b[32m28.3967\u001b[0m  0.0111\n",
      "      6       \u001b[36m33.8986\u001b[0m       \u001b[32m27.1906\u001b[0m  0.0112\n",
      "      7       \u001b[36m31.8748\u001b[0m       \u001b[32m26.7443\u001b[0m  0.0119\n",
      "      8       \u001b[36m30.5435\u001b[0m       27.5974  0.0122\n",
      "      9       \u001b[36m30.3395\u001b[0m       28.7765  0.0117\n",
      "     10       30.4381       28.6196  0.0119\n",
      "     11       \u001b[36m30.0748\u001b[0m       27.7529  0.0116\n",
      "     12       \u001b[36m29.6872\u001b[0m       27.1603  0.0115\n",
      "     13       \u001b[36m29.4861\u001b[0m       26.9627  0.0121\n",
      "     14       \u001b[36m29.3513\u001b[0m       27.0281  0.0115\n",
      "     15       \u001b[36m29.2331\u001b[0m       27.2439  0.0117\n",
      "     16       \u001b[36m29.1505\u001b[0m       27.4794  0.0116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m29.0975\u001b[0m       27.6095  0.0118\n",
      "     18       \u001b[36m29.0407\u001b[0m       27.6086  0.0115\n",
      "     19       \u001b[36m28.9727\u001b[0m       27.5344  0.0117\n",
      "     20       \u001b[36m28.9083\u001b[0m       27.4600  0.0116\n",
      "     21       \u001b[36m28.8552\u001b[0m       27.4327  0.0113\n",
      "     22       \u001b[36m28.8131\u001b[0m       27.4470  0.0112\n",
      "     23       \u001b[36m28.7791\u001b[0m       27.4733  0.0120\n",
      "     24       \u001b[36m28.7500\u001b[0m       27.4843  0.0117\n",
      "     25       \u001b[36m28.7222\u001b[0m       27.4712  0.0115\n",
      "     26       \u001b[36m28.6949\u001b[0m       27.4498  0.0115\n",
      "     27       \u001b[36m28.6696\u001b[0m       27.4354  0.0113\n",
      "     28       \u001b[36m28.6473\u001b[0m       27.4296  0.0121\n",
      "     29       \u001b[36m28.6278\u001b[0m       27.4303  0.0117\n",
      "     30       \u001b[36m28.6109\u001b[0m       27.4325  0.0116\n",
      "     31       \u001b[36m28.5954\u001b[0m       27.4274  0.0116\n",
      "     32       \u001b[36m28.5809\u001b[0m       27.4212  0.0114\n",
      "     33       \u001b[36m28.5676\u001b[0m       27.4178  0.0114\n",
      "     34       \u001b[36m28.5555\u001b[0m       27.4154  0.0114\n",
      "     35       \u001b[36m28.5448\u001b[0m       27.4171  0.0110\n",
      "     36       \u001b[36m28.5354\u001b[0m       27.4198  0.0110\n",
      "     37       \u001b[36m28.5268\u001b[0m       27.4197  0.0113\n",
      "     38       \u001b[36m28.5189\u001b[0m       27.4191  0.0116\n",
      "     39       \u001b[36m28.5116\u001b[0m       27.4190  0.0113\n",
      "     40       \u001b[36m28.5048\u001b[0m       27.4193  0.0113\n",
      "     41       \u001b[36m28.4988\u001b[0m       27.4202  0.0110\n",
      "     42       \u001b[36m28.4932\u001b[0m       27.4197  0.0116\n",
      "     43       \u001b[36m28.4881\u001b[0m       27.4180  0.0117\n",
      "     44       \u001b[36m28.4833\u001b[0m       27.4168  0.0115\n",
      "     45       \u001b[36m28.4788\u001b[0m       27.4159  0.0117\n",
      "     46       \u001b[36m28.4746\u001b[0m       27.4144  0.0123\n",
      "     47       \u001b[36m28.4706\u001b[0m       27.4129  0.0175\n",
      "     48       \u001b[36m28.4668\u001b[0m       27.4111  0.0205\n",
      "     49       \u001b[36m28.4633\u001b[0m       27.4090  0.0200\n",
      "     50       \u001b[36m28.4598\u001b[0m       27.4062  0.0178\n",
      "     51       \u001b[36m28.4566\u001b[0m       27.4047  0.0139\n",
      "     52       \u001b[36m28.4536\u001b[0m       27.4030  0.0152\n",
      "     53       \u001b[36m28.4509\u001b[0m       27.4017  0.0124\n",
      "     54       \u001b[36m28.4481\u001b[0m       27.3981  0.0119\n",
      "     55       \u001b[36m28.4454\u001b[0m       27.3941  0.0118\n",
      "     56       \u001b[36m28.4429\u001b[0m       27.3932  0.0121\n",
      "     57       \u001b[36m28.4406\u001b[0m       27.3923  0.0120\n",
      "     58       \u001b[36m28.4385\u001b[0m       27.3920  0.0118\n",
      "     59       \u001b[36m28.4365\u001b[0m       27.3911  0.0120\n",
      "     60       \u001b[36m28.4344\u001b[0m       27.3889  0.0116\n",
      "     61       \u001b[36m28.4324\u001b[0m       27.3870  0.0122\n",
      "     62       \u001b[36m28.4306\u001b[0m       27.3870  0.0121\n",
      "     63       \u001b[36m28.4289\u001b[0m       27.3855  0.0120\n",
      "     64       \u001b[36m28.4271\u001b[0m       27.3855  0.0117\n",
      "     65       \u001b[36m28.4255\u001b[0m       27.3850  0.0112\n",
      "     66       \u001b[36m28.4238\u001b[0m       27.3847  0.0117\n",
      "     67       \u001b[36m28.4224\u001b[0m       27.3818  0.0119\n",
      "     68       \u001b[36m28.4208\u001b[0m       27.3792  0.0116\n",
      "     69       \u001b[36m28.4194\u001b[0m       27.3794  0.0118\n",
      "     70       \u001b[36m28.4180\u001b[0m       27.3791  0.0122\n",
      "     71       \u001b[36m28.4167\u001b[0m       27.3760  0.0119\n",
      "     72       \u001b[36m28.4153\u001b[0m       27.3736  0.0114\n",
      "     73       \u001b[36m28.4141\u001b[0m       27.3712  0.0121\n",
      "     74       \u001b[36m28.4128\u001b[0m       27.3692  0.0116\n",
      "     75       \u001b[36m28.4117\u001b[0m       27.3682  0.0120\n",
      "     76       \u001b[36m28.4105\u001b[0m       27.3643  0.0127\n",
      "     77       \u001b[36m28.4094\u001b[0m       27.3631  0.0121\n",
      "     78       \u001b[36m28.4083\u001b[0m       27.3619  0.0122\n",
      "     79       \u001b[36m28.4072\u001b[0m       27.3613  0.0123\n",
      "     80       \u001b[36m28.4062\u001b[0m       27.3581  0.0116\n",
      "     81       \u001b[36m28.4052\u001b[0m       27.3566  0.0123\n",
      "     82       \u001b[36m28.4043\u001b[0m       27.3545  0.0120\n",
      "     83       \u001b[36m28.4033\u001b[0m       27.3544  0.0119\n",
      "     84       \u001b[36m28.4025\u001b[0m       27.3534  0.0124\n",
      "     85       \u001b[36m28.4016\u001b[0m       27.3495  0.0119\n",
      "     86       \u001b[36m28.4006\u001b[0m       27.3478  0.0121\n",
      "     87       \u001b[36m28.3999\u001b[0m       27.3464  0.0120\n",
      "     88       \u001b[36m28.3992\u001b[0m       27.3452  0.0123\n",
      "     89       \u001b[36m28.3982\u001b[0m       27.3446  0.0122\n",
      "     90       28.3983       27.3421  0.0120\n",
      "     91       \u001b[36m28.3977\u001b[0m       27.3389  0.0126\n",
      "     92       \u001b[36m28.3975\u001b[0m       27.3454  0.0122\n",
      "     93       28.4015       27.3412  0.0120\n",
      "     94       28.3998       27.3372  0.0121\n",
      "     95       28.3989       27.3482  0.0117\n",
      "     96       28.4124       27.3383  0.0123\n",
      "     97       28.4058       27.3434  0.0127\n",
      "     98       28.3980       27.3288  0.0126\n",
      "     99       \u001b[36m28.3919\u001b[0m       27.3300  0.0122\n",
      "    100       \u001b[36m28.3906\u001b[0m       27.3332  0.0121\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m43.2036\u001b[0m       \u001b[32m45.2086\u001b[0m  0.0117\n",
      "      2       \u001b[36m42.8138\u001b[0m       \u001b[32m44.7317\u001b[0m  0.0115\n",
      "      3       \u001b[36m42.4424\u001b[0m       \u001b[32m44.2747\u001b[0m  0.0116\n",
      "      4       \u001b[36m42.0858\u001b[0m       \u001b[32m43.8340\u001b[0m  0.0115\n",
      "      5       \u001b[36m41.7420\u001b[0m       \u001b[32m43.4065\u001b[0m  0.0114\n",
      "      6       \u001b[36m41.4090\u001b[0m       \u001b[32m42.9896\u001b[0m  0.0112\n",
      "      7       \u001b[36m41.0850\u001b[0m       \u001b[32m42.5821\u001b[0m  0.0116\n",
      "      8       \u001b[36m40.7689\u001b[0m       \u001b[32m42.1824\u001b[0m  0.0113\n",
      "      9       \u001b[36m40.4596\u001b[0m       \u001b[32m41.7894\u001b[0m  0.0112\n",
      "     10       \u001b[36m40.1562\u001b[0m       \u001b[32m41.4015\u001b[0m  0.0112\n",
      "     11       \u001b[36m39.8580\u001b[0m       \u001b[32m41.0180\u001b[0m  0.0113\n",
      "     12       \u001b[36m39.5641\u001b[0m       \u001b[32m40.6382\u001b[0m  0.0112\n",
      "     13       \u001b[36m39.2740\u001b[0m       \u001b[32m40.2613\u001b[0m  0.0115\n",
      "     14       \u001b[36m38.9874\u001b[0m       \u001b[32m39.8875\u001b[0m  0.0114\n",
      "     15       \u001b[36m38.7046\u001b[0m       \u001b[32m39.5172\u001b[0m  0.0112\n",
      "     16       \u001b[36m38.4255\u001b[0m       \u001b[32m39.1506\u001b[0m  0.0114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m38.1504\u001b[0m       \u001b[32m38.7876\u001b[0m  0.0118\n",
      "     18       \u001b[36m37.8793\u001b[0m       \u001b[32m38.4283\u001b[0m  0.0115\n",
      "     19       \u001b[36m37.6123\u001b[0m       \u001b[32m38.0732\u001b[0m  0.0109\n",
      "     20       \u001b[36m37.3498\u001b[0m       \u001b[32m37.7229\u001b[0m  0.0113\n",
      "     21       \u001b[36m37.0924\u001b[0m       \u001b[32m37.3777\u001b[0m  0.0118\n",
      "     22       \u001b[36m36.8405\u001b[0m       \u001b[32m37.0382\u001b[0m  0.0117\n",
      "     23       \u001b[36m36.5944\u001b[0m       \u001b[32m36.7045\u001b[0m  0.0139\n",
      "     24       \u001b[36m36.3545\u001b[0m       \u001b[32m36.3772\u001b[0m  0.0149\n",
      "     25       \u001b[36m36.1211\u001b[0m       \u001b[32m36.0564\u001b[0m  0.0117\n",
      "     26       \u001b[36m35.8944\u001b[0m       \u001b[32m35.7426\u001b[0m  0.0112\n",
      "     27       \u001b[36m35.6747\u001b[0m       \u001b[32m35.4361\u001b[0m  0.0116\n",
      "     28       \u001b[36m35.4625\u001b[0m       \u001b[32m35.1375\u001b[0m  0.0118\n",
      "     29       \u001b[36m35.2578\u001b[0m       \u001b[32m34.8469\u001b[0m  0.0138\n",
      "     30       \u001b[36m35.0609\u001b[0m       \u001b[32m34.5646\u001b[0m  0.0114\n",
      "     31       \u001b[36m34.8719\u001b[0m       \u001b[32m34.2909\u001b[0m  0.0119\n",
      "     32       \u001b[36m34.6911\u001b[0m       \u001b[32m34.0262\u001b[0m  0.0110\n",
      "     33       \u001b[36m34.5186\u001b[0m       \u001b[32m33.7707\u001b[0m  0.0114\n",
      "     34       \u001b[36m34.3545\u001b[0m       \u001b[32m33.5247\u001b[0m  0.0114\n",
      "     35       \u001b[36m34.1988\u001b[0m       \u001b[32m33.2884\u001b[0m  0.0117\n",
      "     36       \u001b[36m34.0520\u001b[0m       \u001b[32m33.0621\u001b[0m  0.0120\n",
      "     37       \u001b[36m33.9139\u001b[0m       \u001b[32m32.8459\u001b[0m  0.0117\n",
      "     38       \u001b[36m33.7845\u001b[0m       \u001b[32m32.6402\u001b[0m  0.0115\n",
      "     39       \u001b[36m33.6637\u001b[0m       \u001b[32m32.4450\u001b[0m  0.0111\n",
      "     40       \u001b[36m33.5513\u001b[0m       \u001b[32m32.2604\u001b[0m  0.0111\n",
      "     41       \u001b[36m33.4473\u001b[0m       \u001b[32m32.0863\u001b[0m  0.0111\n",
      "     42       \u001b[36m33.3515\u001b[0m       \u001b[32m31.9230\u001b[0m  0.0114\n",
      "     43       \u001b[36m33.2635\u001b[0m       \u001b[32m31.7701\u001b[0m  0.0114\n",
      "     44       \u001b[36m33.1831\u001b[0m       \u001b[32m31.6274\u001b[0m  0.0114\n",
      "     45       \u001b[36m33.1099\u001b[0m       \u001b[32m31.4948\u001b[0m  0.0116\n",
      "     46       \u001b[36m33.0437\u001b[0m       \u001b[32m31.3718\u001b[0m  0.0112\n",
      "     47       \u001b[36m32.9839\u001b[0m       \u001b[32m31.2579\u001b[0m  0.0115\n",
      "     48       \u001b[36m32.9299\u001b[0m       \u001b[32m31.1527\u001b[0m  0.0114\n",
      "     49       \u001b[36m32.8816\u001b[0m       \u001b[32m31.0559\u001b[0m  0.0112\n",
      "     50       \u001b[36m32.8385\u001b[0m       \u001b[32m30.9672\u001b[0m  0.0115\n",
      "     51       \u001b[36m32.7999\u001b[0m       \u001b[32m30.8857\u001b[0m  0.0116\n",
      "     52       \u001b[36m32.7654\u001b[0m       \u001b[32m30.8110\u001b[0m  0.0118\n",
      "     53       \u001b[36m32.7346\u001b[0m       \u001b[32m30.7427\u001b[0m  0.0116\n",
      "     54       \u001b[36m32.7071\u001b[0m       \u001b[32m30.6802\u001b[0m  0.0115\n",
      "     55       \u001b[36m32.6824\u001b[0m       \u001b[32m30.6232\u001b[0m  0.0115\n",
      "     56       \u001b[36m32.6604\u001b[0m       \u001b[32m30.5711\u001b[0m  0.0114\n",
      "     57       \u001b[36m32.6406\u001b[0m       \u001b[32m30.5237\u001b[0m  0.0116\n",
      "     58       \u001b[36m32.6227\u001b[0m       \u001b[32m30.4803\u001b[0m  0.0116\n",
      "     59       \u001b[36m32.6066\u001b[0m       \u001b[32m30.4408\u001b[0m  0.0114\n",
      "     60       \u001b[36m32.5921\u001b[0m       \u001b[32m30.4047\u001b[0m  0.0114\n",
      "     61       \u001b[36m32.5788\u001b[0m       \u001b[32m30.3718\u001b[0m  0.0113\n",
      "     62       \u001b[36m32.5669\u001b[0m       \u001b[32m30.3418\u001b[0m  0.0115\n",
      "     63       \u001b[36m32.5560\u001b[0m       \u001b[32m30.3143\u001b[0m  0.0113\n",
      "     64       \u001b[36m32.5460\u001b[0m       \u001b[32m30.2892\u001b[0m  0.0117\n",
      "     65       \u001b[36m32.5367\u001b[0m       \u001b[32m30.2662\u001b[0m  0.0117\n",
      "     66       \u001b[36m32.5282\u001b[0m       \u001b[32m30.2452\u001b[0m  0.0115\n",
      "     67       \u001b[36m32.5202\u001b[0m       \u001b[32m30.2259\u001b[0m  0.0112\n",
      "     68       \u001b[36m32.5128\u001b[0m       \u001b[32m30.2081\u001b[0m  0.0112\n",
      "     69       \u001b[36m32.5058\u001b[0m       \u001b[32m30.1917\u001b[0m  0.0119\n",
      "     70       \u001b[36m32.4993\u001b[0m       \u001b[32m30.1766\u001b[0m  0.0117\n",
      "     71       \u001b[36m32.4933\u001b[0m       \u001b[32m30.1627\u001b[0m  0.0113\n",
      "     72       \u001b[36m32.4875\u001b[0m       \u001b[32m30.1498\u001b[0m  0.0115\n",
      "     73       \u001b[36m32.4821\u001b[0m       \u001b[32m30.1379\u001b[0m  0.0113\n",
      "     74       \u001b[36m32.4769\u001b[0m       \u001b[32m30.1269\u001b[0m  0.0112\n",
      "     75       \u001b[36m32.4720\u001b[0m       \u001b[32m30.1166\u001b[0m  0.0115\n",
      "     76       \u001b[36m32.4673\u001b[0m       \u001b[32m30.1071\u001b[0m  0.0116\n",
      "     77       \u001b[36m32.4628\u001b[0m       \u001b[32m30.0982\u001b[0m  0.0116\n",
      "     78       \u001b[36m32.4585\u001b[0m       \u001b[32m30.0900\u001b[0m  0.0114\n",
      "     79       \u001b[36m32.4543\u001b[0m       \u001b[32m30.0823\u001b[0m  0.0116\n",
      "     80       \u001b[36m32.4503\u001b[0m       \u001b[32m30.0750\u001b[0m  0.0112\n",
      "     81       \u001b[36m32.4464\u001b[0m       \u001b[32m30.0683\u001b[0m  0.0115\n",
      "     82       \u001b[36m32.4427\u001b[0m       \u001b[32m30.0619\u001b[0m  0.0115\n",
      "     83       \u001b[36m32.4391\u001b[0m       \u001b[32m30.0559\u001b[0m  0.0118\n",
      "     84       \u001b[36m32.4356\u001b[0m       \u001b[32m30.0503\u001b[0m  0.0115\n",
      "     85       \u001b[36m32.4322\u001b[0m       \u001b[32m30.0449\u001b[0m  0.0115\n",
      "     86       \u001b[36m32.4289\u001b[0m       \u001b[32m30.0399\u001b[0m  0.0114\n",
      "     87       \u001b[36m32.4258\u001b[0m       \u001b[32m30.0351\u001b[0m  0.0112\n",
      "     88       \u001b[36m32.4228\u001b[0m       \u001b[32m30.0305\u001b[0m  0.0115\n",
      "     89       \u001b[36m32.4199\u001b[0m       \u001b[32m30.0262\u001b[0m  0.0110\n",
      "     90       \u001b[36m32.4171\u001b[0m       \u001b[32m30.0221\u001b[0m  0.0116\n",
      "     91       \u001b[36m32.4143\u001b[0m       \u001b[32m30.0182\u001b[0m  0.0114\n",
      "     92       \u001b[36m32.4116\u001b[0m       \u001b[32m30.0145\u001b[0m  0.0113\n",
      "     93       \u001b[36m32.4090\u001b[0m       \u001b[32m30.0110\u001b[0m  0.0114\n",
      "     94       \u001b[36m32.4064\u001b[0m       \u001b[32m30.0076\u001b[0m  0.0114\n",
      "     95       \u001b[36m32.4039\u001b[0m       \u001b[32m30.0044\u001b[0m  0.0115\n",
      "     96       \u001b[36m32.4015\u001b[0m       \u001b[32m30.0013\u001b[0m  0.0120\n",
      "     97       \u001b[36m32.3991\u001b[0m       \u001b[32m29.9983\u001b[0m  0.0116\n",
      "     98       \u001b[36m32.3968\u001b[0m       \u001b[32m29.9954\u001b[0m  0.0113\n",
      "     99       \u001b[36m32.3945\u001b[0m       \u001b[32m29.9927\u001b[0m  0.0122\n",
      "    100       \u001b[36m32.3922\u001b[0m       \u001b[32m29.9900\u001b[0m  0.0126\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m34.3610\u001b[0m       \u001b[32m32.8402\u001b[0m  0.0117\n",
      "      2       \u001b[36m33.9288\u001b[0m       \u001b[32m32.5135\u001b[0m  0.0116\n",
      "      3       \u001b[36m33.5070\u001b[0m       \u001b[32m32.1946\u001b[0m  0.0115\n",
      "      4       \u001b[36m33.0934\u001b[0m       \u001b[32m31.8825\u001b[0m  0.0156\n",
      "      5       \u001b[36m32.6870\u001b[0m       \u001b[32m31.5765\u001b[0m  0.0174\n",
      "      6       \u001b[36m32.2861\u001b[0m       \u001b[32m31.2757\u001b[0m  0.0171\n",
      "      7       \u001b[36m31.8899\u001b[0m       \u001b[32m30.9795\u001b[0m  0.0121\n",
      "      8       \u001b[36m31.4980\u001b[0m       \u001b[32m30.6879\u001b[0m  0.0119\n",
      "      9       \u001b[36m31.1102\u001b[0m       \u001b[32m30.4011\u001b[0m  0.0112\n",
      "     10       \u001b[36m30.7268\u001b[0m       \u001b[32m30.1192\u001b[0m  0.0129\n",
      "     11       \u001b[36m30.3480\u001b[0m       \u001b[32m29.8425\u001b[0m  0.0125\n",
      "     12       \u001b[36m29.9739\u001b[0m       \u001b[32m29.5713\u001b[0m  0.0124\n",
      "     13       \u001b[36m29.6049\u001b[0m       \u001b[32m29.3064\u001b[0m  0.0116\n",
      "     14       \u001b[36m29.2418\u001b[0m       \u001b[32m29.0485\u001b[0m  0.0117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     15       \u001b[36m28.8851\u001b[0m       \u001b[32m28.7983\u001b[0m  0.0120\n",
      "     16       \u001b[36m28.5357\u001b[0m       \u001b[32m28.5567\u001b[0m  0.0122\n",
      "     17       \u001b[36m28.1942\u001b[0m       \u001b[32m28.3244\u001b[0m  0.0114\n",
      "     18       \u001b[36m27.8613\u001b[0m       \u001b[32m28.1027\u001b[0m  0.0112\n",
      "     19       \u001b[36m27.5380\u001b[0m       \u001b[32m27.8925\u001b[0m  0.0112\n",
      "     20       \u001b[36m27.2248\u001b[0m       \u001b[32m27.6944\u001b[0m  0.0110\n",
      "     21       \u001b[36m26.9223\u001b[0m       \u001b[32m27.5089\u001b[0m  0.0112\n",
      "     22       \u001b[36m26.6315\u001b[0m       \u001b[32m27.3372\u001b[0m  0.0110\n",
      "     23       \u001b[36m26.3532\u001b[0m       \u001b[32m27.1799\u001b[0m  0.0112\n",
      "     24       \u001b[36m26.0882\u001b[0m       \u001b[32m27.0373\u001b[0m  0.0110\n",
      "     25       \u001b[36m25.8372\u001b[0m       \u001b[32m26.9102\u001b[0m  0.0110\n",
      "     26       \u001b[36m25.6006\u001b[0m       \u001b[32m26.7985\u001b[0m  0.0112\n",
      "     27       \u001b[36m25.3789\u001b[0m       \u001b[32m26.7024\u001b[0m  0.0112\n",
      "     28       \u001b[36m25.1727\u001b[0m       \u001b[32m26.6216\u001b[0m  0.0111\n",
      "     29       \u001b[36m24.9821\u001b[0m       \u001b[32m26.5564\u001b[0m  0.0111\n",
      "     30       \u001b[36m24.8075\u001b[0m       \u001b[32m26.5055\u001b[0m  0.0112\n",
      "     31       \u001b[36m24.6489\u001b[0m       \u001b[32m26.4677\u001b[0m  0.0112\n",
      "     32       \u001b[36m24.5055\u001b[0m       \u001b[32m26.4419\u001b[0m  0.0107\n",
      "     33       \u001b[36m24.3770\u001b[0m       \u001b[32m26.4267\u001b[0m  0.0113\n",
      "     34       \u001b[36m24.2624\u001b[0m       \u001b[32m26.4207\u001b[0m  0.0115\n",
      "     35       \u001b[36m24.1609\u001b[0m       26.4225  0.0116\n",
      "     36       \u001b[36m24.0713\u001b[0m       26.4309  0.0117\n",
      "     37       \u001b[36m23.9930\u001b[0m       26.4441  0.0118\n",
      "     38       \u001b[36m23.9247\u001b[0m       26.4610  0.0117\n",
      "     39       \u001b[36m23.8652\u001b[0m       26.4805  0.0113\n",
      "     40       \u001b[36m23.8135\u001b[0m       26.5015  0.0112\n",
      "     41       \u001b[36m23.7687\u001b[0m       26.5232  0.0111\n",
      "     42       \u001b[36m23.7298\u001b[0m       26.5450  0.0112\n",
      "     43       \u001b[36m23.6960\u001b[0m       26.5661  0.0113\n",
      "     44       \u001b[36m23.6666\u001b[0m       26.5864  0.0112\n",
      "     45       \u001b[36m23.6408\u001b[0m       26.6055  0.0112\n",
      "     46       \u001b[36m23.6181\u001b[0m       26.6233  0.0112\n",
      "     47       \u001b[36m23.5980\u001b[0m       26.6395  0.0112\n",
      "     48       \u001b[36m23.5801\u001b[0m       26.6541  0.0110\n",
      "     49       \u001b[36m23.5641\u001b[0m       26.6672  0.0110\n",
      "     50       \u001b[36m23.5497\u001b[0m       26.6790  0.0108\n",
      "     51       \u001b[36m23.5366\u001b[0m       26.6892  0.0108\n",
      "     52       \u001b[36m23.5247\u001b[0m       26.6980  0.0108\n",
      "     53       \u001b[36m23.5137\u001b[0m       26.7055  0.0107\n",
      "     54       \u001b[36m23.5035\u001b[0m       26.7118  0.0109\n",
      "     55       \u001b[36m23.4940\u001b[0m       26.7169  0.0108\n",
      "     56       \u001b[36m23.4851\u001b[0m       26.7211  0.0113\n",
      "     57       \u001b[36m23.4768\u001b[0m       26.7244  0.0111\n",
      "     58       \u001b[36m23.4688\u001b[0m       26.7269  0.0119\n",
      "     59       \u001b[36m23.4613\u001b[0m       26.7287  0.0115\n",
      "     60       \u001b[36m23.4541\u001b[0m       26.7298  0.0108\n",
      "     61       \u001b[36m23.4472\u001b[0m       26.7305  0.0120\n",
      "     62       \u001b[36m23.4406\u001b[0m       26.7306  0.0117\n",
      "     63       \u001b[36m23.4343\u001b[0m       26.7304  0.0123\n",
      "     64       \u001b[36m23.4281\u001b[0m       26.7297  0.0120\n",
      "     65       \u001b[36m23.4222\u001b[0m       26.7287  0.0117\n",
      "     66       \u001b[36m23.4164\u001b[0m       26.7275  0.0113\n",
      "     67       \u001b[36m23.4109\u001b[0m       26.7260  0.0112\n",
      "     68       \u001b[36m23.4054\u001b[0m       26.7243  0.0121\n",
      "     69       \u001b[36m23.4002\u001b[0m       26.7225  0.0117\n",
      "     70       \u001b[36m23.3950\u001b[0m       26.7205  0.0114\n",
      "     71       \u001b[36m23.3901\u001b[0m       26.7184  0.0112\n",
      "     72       \u001b[36m23.3853\u001b[0m       26.7162  0.0111\n",
      "     73       \u001b[36m23.3805\u001b[0m       26.7139  0.0112\n",
      "     74       \u001b[36m23.3759\u001b[0m       26.7116  0.0113\n",
      "     75       \u001b[36m23.3714\u001b[0m       26.7092  0.0110\n",
      "     76       \u001b[36m23.3670\u001b[0m       26.7068  0.0110\n",
      "     77       \u001b[36m23.3627\u001b[0m       26.7044  0.0112\n",
      "     78       \u001b[36m23.3585\u001b[0m       26.7020  0.0110\n",
      "     79       \u001b[36m23.3544\u001b[0m       26.6996  0.0113\n",
      "     80       \u001b[36m23.3503\u001b[0m       26.6973  0.0112\n",
      "     81       \u001b[36m23.3464\u001b[0m       26.6951  0.0107\n",
      "     82       \u001b[36m23.3425\u001b[0m       26.6930  0.0107\n",
      "     83       \u001b[36m23.3387\u001b[0m       26.6909  0.0110\n",
      "     84       \u001b[36m23.3351\u001b[0m       26.6889  0.0110\n",
      "     85       \u001b[36m23.3314\u001b[0m       26.6868  0.0107\n",
      "     86       \u001b[36m23.3278\u001b[0m       26.6847  0.0136\n",
      "     87       \u001b[36m23.3244\u001b[0m       26.6827  0.0152\n",
      "     88       \u001b[36m23.3209\u001b[0m       26.6806  0.0124\n",
      "     89       \u001b[36m23.3175\u001b[0m       26.6786  0.0117\n",
      "     90       \u001b[36m23.3142\u001b[0m       26.6766  0.0116\n",
      "     91       \u001b[36m23.3110\u001b[0m       26.6747  0.0127\n",
      "     92       \u001b[36m23.3078\u001b[0m       26.6728  0.0130\n",
      "     93       \u001b[36m23.3046\u001b[0m       26.6709  0.0138\n",
      "     94       \u001b[36m23.3015\u001b[0m       26.6691  0.0128\n",
      "     95       \u001b[36m23.2985\u001b[0m       26.6672  0.0112\n",
      "     96       \u001b[36m23.2955\u001b[0m       26.6655  0.0110\n",
      "     97       \u001b[36m23.2926\u001b[0m       26.6638  0.0111\n",
      "     98       \u001b[36m23.2897\u001b[0m       26.6621  0.0124\n",
      "     99       \u001b[36m23.2869\u001b[0m       26.6604  0.0116\n",
      "    100       \u001b[36m23.2841\u001b[0m       26.6588  0.0110\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m40.1509\u001b[0m       \u001b[32m31.8141\u001b[0m  0.0108\n",
      "      2       \u001b[36m39.6620\u001b[0m       \u001b[32m31.4775\u001b[0m  0.0113\n",
      "      3       \u001b[36m39.1868\u001b[0m       \u001b[32m31.1514\u001b[0m  0.0113\n",
      "      4       \u001b[36m38.7223\u001b[0m       \u001b[32m30.8338\u001b[0m  0.0113\n",
      "      5       \u001b[36m38.2662\u001b[0m       \u001b[32m30.5235\u001b[0m  0.0108\n",
      "      6       \u001b[36m37.8162\u001b[0m       \u001b[32m30.2194\u001b[0m  0.0109\n",
      "      7       \u001b[36m37.3706\u001b[0m       \u001b[32m29.9208\u001b[0m  0.0115\n",
      "      8       \u001b[36m36.9285\u001b[0m       \u001b[32m29.6275\u001b[0m  0.0110\n",
      "      9       \u001b[36m36.4893\u001b[0m       \u001b[32m29.3394\u001b[0m  0.0111\n",
      "     10       \u001b[36m36.0520\u001b[0m       \u001b[32m29.0566\u001b[0m  0.0108\n",
      "     11       \u001b[36m35.6164\u001b[0m       \u001b[32m28.7794\u001b[0m  0.0108\n",
      "     12       \u001b[36m35.1823\u001b[0m       \u001b[32m28.5088\u001b[0m  0.0110\n",
      "     13       \u001b[36m34.7513\u001b[0m       \u001b[32m28.2458\u001b[0m  0.0111\n",
      "     14       \u001b[36m34.3242\u001b[0m       \u001b[32m27.9917\u001b[0m  0.0111\n",
      "     15       \u001b[36m33.9028\u001b[0m       \u001b[32m27.7479\u001b[0m  0.0108\n",
      "     16       \u001b[36m33.4883\u001b[0m       \u001b[32m27.5159\u001b[0m  0.0110\n",
      "     17       \u001b[36m33.0824\u001b[0m       \u001b[32m27.2976\u001b[0m  0.0111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     18       \u001b[36m32.6874\u001b[0m       \u001b[32m27.0953\u001b[0m  0.0114\n",
      "     19       \u001b[36m32.3060\u001b[0m       \u001b[32m26.9107\u001b[0m  0.0110\n",
      "     20       \u001b[36m31.9405\u001b[0m       \u001b[32m26.7457\u001b[0m  0.0108\n",
      "     21       \u001b[36m31.5935\u001b[0m       \u001b[32m26.6020\u001b[0m  0.0107\n",
      "     22       \u001b[36m31.2676\u001b[0m       \u001b[32m26.4804\u001b[0m  0.0110\n",
      "     23       \u001b[36m30.9646\u001b[0m       \u001b[32m26.3817\u001b[0m  0.0112\n",
      "     24       \u001b[36m30.6857\u001b[0m       \u001b[32m26.3056\u001b[0m  0.0109\n",
      "     25       \u001b[36m30.4319\u001b[0m       \u001b[32m26.2519\u001b[0m  0.0112\n",
      "     26       \u001b[36m30.2034\u001b[0m       \u001b[32m26.2194\u001b[0m  0.0109\n",
      "     27       \u001b[36m30.0000\u001b[0m       \u001b[32m26.2071\u001b[0m  0.0109\n",
      "     28       \u001b[36m29.8212\u001b[0m       26.2122  0.0109\n",
      "     29       \u001b[36m29.6657\u001b[0m       26.2326  0.0110\n",
      "     30       \u001b[36m29.5318\u001b[0m       26.2655  0.0110\n",
      "     31       \u001b[36m29.4175\u001b[0m       26.3084  0.0110\n",
      "     32       \u001b[36m29.3209\u001b[0m       26.3587  0.0116\n",
      "     33       \u001b[36m29.2398\u001b[0m       26.4140  0.0119\n",
      "     34       \u001b[36m29.1722\u001b[0m       26.4723  0.0117\n",
      "     35       \u001b[36m29.1160\u001b[0m       26.5316  0.0116\n",
      "     36       \u001b[36m29.0695\u001b[0m       26.5905  0.0110\n",
      "     37       \u001b[36m29.0311\u001b[0m       26.6477  0.0110\n",
      "     38       \u001b[36m28.9993\u001b[0m       26.7023  0.0114\n",
      "     39       \u001b[36m28.9727\u001b[0m       26.7537  0.0120\n",
      "     40       \u001b[36m28.9505\u001b[0m       26.8016  0.0116\n",
      "     41       \u001b[36m28.9318\u001b[0m       26.8456  0.0145\n",
      "     42       \u001b[36m28.9158\u001b[0m       26.8858  0.0113\n",
      "     43       \u001b[36m28.9019\u001b[0m       26.9225  0.0117\n",
      "     44       \u001b[36m28.8896\u001b[0m       26.9551  0.0111\n",
      "     45       \u001b[36m28.8786\u001b[0m       26.9847  0.0112\n",
      "     46       \u001b[36m28.8687\u001b[0m       27.0110  0.0110\n",
      "     47       \u001b[36m28.8594\u001b[0m       27.0345  0.0111\n",
      "     48       \u001b[36m28.8509\u001b[0m       27.0548  0.0112\n",
      "     49       \u001b[36m28.8427\u001b[0m       27.0731  0.0112\n",
      "     50       \u001b[36m28.8350\u001b[0m       27.0886  0.0109\n",
      "     51       \u001b[36m28.8276\u001b[0m       27.1024  0.0109\n",
      "     52       \u001b[36m28.8205\u001b[0m       27.1140  0.0108\n",
      "     53       \u001b[36m28.8136\u001b[0m       27.1243  0.0112\n",
      "     54       \u001b[36m28.8068\u001b[0m       27.1332  0.0110\n",
      "     55       \u001b[36m28.8002\u001b[0m       27.1411  0.0111\n",
      "     56       \u001b[36m28.7939\u001b[0m       27.1476  0.0111\n",
      "     57       \u001b[36m28.7876\u001b[0m       27.1534  0.0115\n",
      "     58       \u001b[36m28.7815\u001b[0m       27.1584  0.0126\n",
      "     59       \u001b[36m28.7755\u001b[0m       27.1626  0.0118\n",
      "     60       \u001b[36m28.7697\u001b[0m       27.1659  0.0121\n",
      "     61       \u001b[36m28.7640\u001b[0m       27.1686  0.0113\n",
      "     62       \u001b[36m28.7584\u001b[0m       27.1713  0.0110\n",
      "     63       \u001b[36m28.7530\u001b[0m       27.1734  0.0112\n",
      "     64       \u001b[36m28.7477\u001b[0m       27.1751  0.0115\n",
      "     65       \u001b[36m28.7424\u001b[0m       27.1767  0.0115\n",
      "     66       \u001b[36m28.7373\u001b[0m       27.1781  0.0113\n",
      "     67       \u001b[36m28.7323\u001b[0m       27.1788  0.0117\n",
      "     68       \u001b[36m28.7274\u001b[0m       27.1795  0.0162\n",
      "     69       \u001b[36m28.7226\u001b[0m       27.1803  0.0142\n",
      "     70       \u001b[36m28.7179\u001b[0m       27.1807  0.0145\n",
      "     71       \u001b[36m28.7133\u001b[0m       27.1812  0.0130\n",
      "     72       \u001b[36m28.7087\u001b[0m       27.1814  0.0114\n",
      "     73       \u001b[36m28.7043\u001b[0m       27.1815  0.0135\n",
      "     74       \u001b[36m28.7000\u001b[0m       27.1815  0.0112\n",
      "     75       \u001b[36m28.6957\u001b[0m       27.1816  0.0152\n",
      "     76       \u001b[36m28.6915\u001b[0m       27.1816  0.0110\n",
      "     77       \u001b[36m28.6874\u001b[0m       27.1817  0.0109\n",
      "     78       \u001b[36m28.6834\u001b[0m       27.1812  0.0111\n",
      "     79       \u001b[36m28.6795\u001b[0m       27.1812  0.0111\n",
      "     80       \u001b[36m28.6757\u001b[0m       27.1810  0.0109\n",
      "     81       \u001b[36m28.6719\u001b[0m       27.1809  0.0107\n",
      "     82       \u001b[36m28.6682\u001b[0m       27.1810  0.0107\n",
      "     83       \u001b[36m28.6646\u001b[0m       27.1807  0.0108\n",
      "     84       \u001b[36m28.6611\u001b[0m       27.1803  0.0108\n",
      "     85       \u001b[36m28.6576\u001b[0m       27.1804  0.0107\n",
      "     86       \u001b[36m28.6542\u001b[0m       27.1801  0.0107\n",
      "     87       \u001b[36m28.6509\u001b[0m       27.1801  0.0107\n",
      "     88       \u001b[36m28.6476\u001b[0m       27.1796  0.0109\n",
      "     89       \u001b[36m28.6444\u001b[0m       27.1795  0.0108\n",
      "     90       \u001b[36m28.6412\u001b[0m       27.1793  0.0108\n",
      "     91       \u001b[36m28.6381\u001b[0m       27.1792  0.0104\n",
      "     92       \u001b[36m28.6351\u001b[0m       27.1792  0.0105\n",
      "     93       \u001b[36m28.6321\u001b[0m       27.1788  0.0109\n",
      "     94       \u001b[36m28.6292\u001b[0m       27.1788  0.0108\n",
      "     95       \u001b[36m28.6263\u001b[0m       27.1784  0.0109\n",
      "     96       \u001b[36m28.6235\u001b[0m       27.1784  0.0109\n",
      "     97       \u001b[36m28.6207\u001b[0m       27.1781  0.0107\n",
      "     98       \u001b[36m28.6180\u001b[0m       27.1779  0.0107\n",
      "     99       \u001b[36m28.6153\u001b[0m       27.1780  0.0111\n",
      "    100       \u001b[36m28.6128\u001b[0m       27.1777  0.0112\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m44.8895\u001b[0m       \u001b[32m46.7521\u001b[0m  0.0114\n",
      "      2       \u001b[36m43.9192\u001b[0m       \u001b[32m45.3018\u001b[0m  0.0111\n",
      "      3       \u001b[36m42.6908\u001b[0m       \u001b[32m43.4500\u001b[0m  0.0117\n",
      "      4       \u001b[36m41.1607\u001b[0m       \u001b[32m41.2755\u001b[0m  0.0116\n",
      "      5       \u001b[36m39.4419\u001b[0m       \u001b[32m38.9606\u001b[0m  0.0112\n",
      "      6       \u001b[36m37.6966\u001b[0m       \u001b[32m36.5742\u001b[0m  0.0111\n",
      "      7       \u001b[36m36.0507\u001b[0m       \u001b[32m34.2636\u001b[0m  0.0119\n",
      "      8       \u001b[36m34.7965\u001b[0m       \u001b[32m32.5771\u001b[0m  0.0121\n",
      "      9       \u001b[36m34.2826\u001b[0m       \u001b[32m31.8231\u001b[0m  0.0118\n",
      "     10       \u001b[36m34.2242\u001b[0m       \u001b[32m31.5651\u001b[0m  0.0112\n",
      "     11       \u001b[36m34.0389\u001b[0m       \u001b[32m31.4868\u001b[0m  0.0113\n",
      "     12       \u001b[36m33.7434\u001b[0m       31.5646  0.0116\n",
      "     13       \u001b[36m33.5457\u001b[0m       31.6509  0.0114\n",
      "     14       \u001b[36m33.4243\u001b[0m       31.5934  0.0116\n",
      "     15       \u001b[36m33.3023\u001b[0m       \u001b[32m31.4043\u001b[0m  0.0115\n",
      "     16       \u001b[36m33.1798\u001b[0m       \u001b[32m31.1852\u001b[0m  0.0113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m33.0766\u001b[0m       \u001b[32m31.0095\u001b[0m  0.0126\n",
      "     18       \u001b[36m32.9924\u001b[0m       \u001b[32m30.8997\u001b[0m  0.0114\n",
      "     19       \u001b[36m32.9202\u001b[0m       \u001b[32m30.8343\u001b[0m  0.0120\n",
      "     20       \u001b[36m32.8558\u001b[0m       \u001b[32m30.7852\u001b[0m  0.0113\n",
      "     21       \u001b[36m32.7973\u001b[0m       \u001b[32m30.7430\u001b[0m  0.0113\n",
      "     22       \u001b[36m32.7449\u001b[0m       \u001b[32m30.7054\u001b[0m  0.0117\n",
      "     23       \u001b[36m32.6972\u001b[0m       \u001b[32m30.6670\u001b[0m  0.0119\n",
      "     24       \u001b[36m32.6530\u001b[0m       \u001b[32m30.6233\u001b[0m  0.0117\n",
      "     25       \u001b[36m32.6124\u001b[0m       \u001b[32m30.5801\u001b[0m  0.0114\n",
      "     26       \u001b[36m32.5757\u001b[0m       \u001b[32m30.5473\u001b[0m  0.0119\n",
      "     27       \u001b[36m32.5425\u001b[0m       \u001b[32m30.5257\u001b[0m  0.0121\n",
      "     28       \u001b[36m32.5123\u001b[0m       \u001b[32m30.5091\u001b[0m  0.0119\n",
      "     29       \u001b[36m32.4848\u001b[0m       \u001b[32m30.4942\u001b[0m  0.0114\n",
      "     30       \u001b[36m32.4598\u001b[0m       \u001b[32m30.4830\u001b[0m  0.0115\n",
      "     31       \u001b[36m32.4370\u001b[0m       \u001b[32m30.4722\u001b[0m  0.0111\n",
      "     32       \u001b[36m32.4164\u001b[0m       \u001b[32m30.4592\u001b[0m  0.0119\n",
      "     33       \u001b[36m32.3977\u001b[0m       \u001b[32m30.4487\u001b[0m  0.0117\n",
      "     34       \u001b[36m32.3808\u001b[0m       \u001b[32m30.4401\u001b[0m  0.0115\n",
      "     35       \u001b[36m32.3655\u001b[0m       \u001b[32m30.4307\u001b[0m  0.0111\n",
      "     36       \u001b[36m32.3516\u001b[0m       \u001b[32m30.4213\u001b[0m  0.0113\n",
      "     37       \u001b[36m32.3390\u001b[0m       \u001b[32m30.4130\u001b[0m  0.0121\n",
      "     38       \u001b[36m32.3275\u001b[0m       \u001b[32m30.4043\u001b[0m  0.0119\n",
      "     39       \u001b[36m32.3168\u001b[0m       \u001b[32m30.3966\u001b[0m  0.0124\n",
      "     40       \u001b[36m32.3071\u001b[0m       \u001b[32m30.3904\u001b[0m  0.0122\n",
      "     41       \u001b[36m32.2982\u001b[0m       \u001b[32m30.3845\u001b[0m  0.0121\n",
      "     42       \u001b[36m32.2899\u001b[0m       \u001b[32m30.3794\u001b[0m  0.0127\n",
      "     43       \u001b[36m32.2822\u001b[0m       \u001b[32m30.3747\u001b[0m  0.0122\n",
      "     44       \u001b[36m32.2749\u001b[0m       \u001b[32m30.3694\u001b[0m  0.0155\n",
      "     45       \u001b[36m32.2682\u001b[0m       \u001b[32m30.3648\u001b[0m  0.0154\n",
      "     46       \u001b[36m32.2619\u001b[0m       \u001b[32m30.3607\u001b[0m  0.0119\n",
      "     47       \u001b[36m32.2559\u001b[0m       \u001b[32m30.3573\u001b[0m  0.0120\n",
      "     48       \u001b[36m32.2504\u001b[0m       \u001b[32m30.3540\u001b[0m  0.0156\n",
      "     49       \u001b[36m32.2452\u001b[0m       \u001b[32m30.3499\u001b[0m  0.0169\n",
      "     50       \u001b[36m32.2402\u001b[0m       \u001b[32m30.3464\u001b[0m  0.0130\n",
      "     51       \u001b[36m32.2356\u001b[0m       \u001b[32m30.3428\u001b[0m  0.0125\n",
      "     52       \u001b[36m32.2311\u001b[0m       \u001b[32m30.3402\u001b[0m  0.0166\n",
      "     53       \u001b[36m32.2269\u001b[0m       \u001b[32m30.3362\u001b[0m  0.0157\n",
      "     54       \u001b[36m32.2228\u001b[0m       \u001b[32m30.3334\u001b[0m  0.0166\n",
      "     55       \u001b[36m32.2190\u001b[0m       \u001b[32m30.3312\u001b[0m  0.0164\n",
      "     56       \u001b[36m32.2153\u001b[0m       \u001b[32m30.3292\u001b[0m  0.0130\n",
      "     57       \u001b[36m32.2118\u001b[0m       \u001b[32m30.3274\u001b[0m  0.0119\n",
      "     58       \u001b[36m32.2084\u001b[0m       \u001b[32m30.3253\u001b[0m  0.0118\n",
      "     59       \u001b[36m32.2051\u001b[0m       \u001b[32m30.3238\u001b[0m  0.0130\n",
      "     60       \u001b[36m32.2020\u001b[0m       \u001b[32m30.3222\u001b[0m  0.0122\n",
      "     61       \u001b[36m32.1989\u001b[0m       \u001b[32m30.3207\u001b[0m  0.0120\n",
      "     62       \u001b[36m32.1959\u001b[0m       \u001b[32m30.3187\u001b[0m  0.0119\n",
      "     63       \u001b[36m32.1931\u001b[0m       \u001b[32m30.3172\u001b[0m  0.0120\n",
      "     64       \u001b[36m32.1903\u001b[0m       \u001b[32m30.3161\u001b[0m  0.0140\n",
      "     65       \u001b[36m32.1876\u001b[0m       \u001b[32m30.3156\u001b[0m  0.0132\n",
      "     66       \u001b[36m32.1850\u001b[0m       \u001b[32m30.3145\u001b[0m  0.0123\n",
      "     67       \u001b[36m32.1824\u001b[0m       \u001b[32m30.3132\u001b[0m  0.0119\n",
      "     68       \u001b[36m32.1800\u001b[0m       \u001b[32m30.3118\u001b[0m  0.0116\n",
      "     69       \u001b[36m32.1776\u001b[0m       \u001b[32m30.3110\u001b[0m  0.0138\n",
      "     70       \u001b[36m32.1753\u001b[0m       \u001b[32m30.3101\u001b[0m  0.0121\n",
      "     71       \u001b[36m32.1731\u001b[0m       \u001b[32m30.3094\u001b[0m  0.0123\n",
      "     72       \u001b[36m32.1710\u001b[0m       \u001b[32m30.3078\u001b[0m  0.0118\n",
      "     73       \u001b[36m32.1689\u001b[0m       \u001b[32m30.3074\u001b[0m  0.0114\n",
      "     74       \u001b[36m32.1669\u001b[0m       \u001b[32m30.3060\u001b[0m  0.0132\n",
      "     75       \u001b[36m32.1649\u001b[0m       \u001b[32m30.3052\u001b[0m  0.0127\n",
      "     76       \u001b[36m32.1630\u001b[0m       \u001b[32m30.3031\u001b[0m  0.0122\n",
      "     77       \u001b[36m32.1611\u001b[0m       \u001b[32m30.3029\u001b[0m  0.0116\n",
      "     78       \u001b[36m32.1592\u001b[0m       \u001b[32m30.3013\u001b[0m  0.0113\n",
      "     79       \u001b[36m32.1574\u001b[0m       30.3018  0.0131\n",
      "     80       \u001b[36m32.1556\u001b[0m       \u001b[32m30.2989\u001b[0m  0.0122\n",
      "     81       \u001b[36m32.1538\u001b[0m       30.3007  0.0121\n",
      "     82       \u001b[36m32.1523\u001b[0m       \u001b[32m30.2964\u001b[0m  0.0121\n",
      "     83       \u001b[36m32.1505\u001b[0m       30.3013  0.0122\n",
      "     84       \u001b[36m32.1491\u001b[0m       \u001b[32m30.2948\u001b[0m  0.0126\n",
      "     85       \u001b[36m32.1474\u001b[0m       30.3051  0.0122\n",
      "     86       \u001b[36m32.1463\u001b[0m       \u001b[32m30.2918\u001b[0m  0.0119\n",
      "     87       \u001b[36m32.1447\u001b[0m       30.3109  0.0121\n",
      "     88       \u001b[36m32.1444\u001b[0m       \u001b[32m30.2866\u001b[0m  0.0122\n",
      "     89       \u001b[36m32.1434\u001b[0m       30.3235  0.0127\n",
      "     90       32.1449       \u001b[32m30.2800\u001b[0m  0.0122\n",
      "     91       32.1455       30.3426  0.0120\n",
      "     92       32.1500       \u001b[32m30.2738\u001b[0m  0.0118\n",
      "     93       32.1522       30.3362  0.0116\n",
      "     94       32.1502       \u001b[32m30.2710\u001b[0m  0.0135\n",
      "     95       \u001b[36m32.1418\u001b[0m       30.2941  0.0119\n",
      "     96       \u001b[36m32.1371\u001b[0m       30.2873  0.0120\n",
      "     97       \u001b[36m32.1307\u001b[0m       \u001b[32m30.2681\u001b[0m  0.0114\n",
      "     98       \u001b[36m32.1302\u001b[0m       30.2903  0.0115\n",
      "     99       \u001b[36m32.1298\u001b[0m       30.2714  0.0128\n",
      "    100       \u001b[36m32.1275\u001b[0m       30.2816  0.0121\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m32.2306\u001b[0m       \u001b[32m31.0290\u001b[0m  0.0120\n",
      "      2       \u001b[36m31.3229\u001b[0m       \u001b[32m30.4156\u001b[0m  0.0116\n",
      "      3       \u001b[36m30.4011\u001b[0m       \u001b[32m29.7186\u001b[0m  0.0113\n",
      "      4       \u001b[36m29.4332\u001b[0m       \u001b[32m28.8860\u001b[0m  0.0133\n",
      "      5       \u001b[36m28.2877\u001b[0m       \u001b[32m27.9200\u001b[0m  0.0121\n",
      "      6       \u001b[36m26.9445\u001b[0m       \u001b[32m27.0283\u001b[0m  0.0116\n",
      "      7       \u001b[36m25.6095\u001b[0m       \u001b[32m26.5671\u001b[0m  0.0112\n",
      "      8       \u001b[36m24.6424\u001b[0m       26.8375  0.0151\n",
      "      9       \u001b[36m24.3025\u001b[0m       27.4632  0.0117\n",
      "     10       \u001b[36m24.2861\u001b[0m       27.5701  0.0121\n",
      "     11       \u001b[36m24.1376\u001b[0m       27.1989  0.0112\n",
      "     12       \u001b[36m23.9406\u001b[0m       26.8510  0.0114\n",
      "     13       \u001b[36m23.8311\u001b[0m       26.6848  0.0129\n",
      "     14       \u001b[36m23.7671\u001b[0m       26.6600  0.0119\n",
      "     15       \u001b[36m23.7002\u001b[0m       26.7318  0.0123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     16       \u001b[36m23.6295\u001b[0m       26.8588  0.0116\n",
      "     17       \u001b[36m23.5667\u001b[0m       26.9850  0.0112\n",
      "     18       \u001b[36m23.5162\u001b[0m       27.0599  0.0132\n",
      "     19       \u001b[36m23.4732\u001b[0m       27.0723  0.0121\n",
      "     20       \u001b[36m23.4350\u001b[0m       27.0510  0.0121\n",
      "     21       \u001b[36m23.4026\u001b[0m       27.0267  0.0114\n",
      "     22       \u001b[36m23.3743\u001b[0m       27.0168  0.0113\n",
      "     23       \u001b[36m23.3480\u001b[0m       27.0212  0.0148\n",
      "     24       \u001b[36m23.3231\u001b[0m       27.0308  0.0177\n",
      "     25       \u001b[36m23.3002\u001b[0m       27.0378  0.0131\n",
      "     26       \u001b[36m23.2795\u001b[0m       27.0364  0.0120\n",
      "     27       \u001b[36m23.2608\u001b[0m       27.0282  0.0127\n",
      "     28       \u001b[36m23.2441\u001b[0m       27.0162  0.0138\n",
      "     29       \u001b[36m23.2291\u001b[0m       27.0047  0.0130\n",
      "     30       \u001b[36m23.2153\u001b[0m       26.9967  0.0128\n",
      "     31       \u001b[36m23.2026\u001b[0m       26.9897  0.0122\n",
      "     32       \u001b[36m23.1908\u001b[0m       26.9810  0.0119\n",
      "     33       \u001b[36m23.1799\u001b[0m       26.9695  0.0121\n",
      "     34       \u001b[36m23.1700\u001b[0m       26.9567  0.0127\n",
      "     35       \u001b[36m23.1609\u001b[0m       26.9433  0.0123\n",
      "     36       \u001b[36m23.1525\u001b[0m       26.9317  0.0117\n",
      "     37       \u001b[36m23.1447\u001b[0m       26.9209  0.0116\n",
      "     38       \u001b[36m23.1375\u001b[0m       26.9094  0.0132\n",
      "     39       \u001b[36m23.1309\u001b[0m       26.8972  0.0120\n",
      "     40       \u001b[36m23.1247\u001b[0m       26.8855  0.0129\n",
      "     41       \u001b[36m23.1189\u001b[0m       26.8754  0.0116\n",
      "     42       \u001b[36m23.1135\u001b[0m       26.8663  0.0116\n",
      "     43       \u001b[36m23.1084\u001b[0m       26.8568  0.0126\n",
      "     44       \u001b[36m23.1036\u001b[0m       26.8470  0.0123\n",
      "     45       \u001b[36m23.0992\u001b[0m       26.8388  0.0121\n",
      "     46       \u001b[36m23.0950\u001b[0m       26.8316  0.0114\n",
      "     47       \u001b[36m23.0910\u001b[0m       26.8261  0.0115\n",
      "     48       \u001b[36m23.0873\u001b[0m       26.8214  0.0130\n",
      "     49       \u001b[36m23.0837\u001b[0m       26.8169  0.0122\n",
      "     50       \u001b[36m23.0803\u001b[0m       26.8125  0.0122\n",
      "     51       \u001b[36m23.0771\u001b[0m       26.8082  0.0112\n",
      "     52       \u001b[36m23.0740\u001b[0m       26.8034  0.0115\n",
      "     53       \u001b[36m23.0711\u001b[0m       26.7980  0.0128\n",
      "     54       \u001b[36m23.0683\u001b[0m       26.7934  0.0119\n",
      "     55       \u001b[36m23.0657\u001b[0m       26.7889  0.0121\n",
      "     56       \u001b[36m23.0631\u001b[0m       26.7841  0.0122\n",
      "     57       \u001b[36m23.0607\u001b[0m       26.7792  0.0115\n",
      "     58       \u001b[36m23.0584\u001b[0m       26.7756  0.0128\n",
      "     59       \u001b[36m23.0561\u001b[0m       26.7724  0.0123\n",
      "     60       \u001b[36m23.0540\u001b[0m       26.7684  0.0119\n",
      "     61       \u001b[36m23.0519\u001b[0m       26.7645  0.0117\n",
      "     62       \u001b[36m23.0499\u001b[0m       26.7609  0.0113\n",
      "     63       \u001b[36m23.0479\u001b[0m       26.7576  0.0131\n",
      "     64       \u001b[36m23.0460\u001b[0m       26.7553  0.0122\n",
      "     65       \u001b[36m23.0442\u001b[0m       26.7533  0.0121\n",
      "     66       \u001b[36m23.0424\u001b[0m       26.7514  0.0113\n",
      "     67       \u001b[36m23.0406\u001b[0m       26.7491  0.0112\n",
      "     68       \u001b[36m23.0389\u001b[0m       26.7467  0.0132\n",
      "     69       \u001b[36m23.0373\u001b[0m       26.7439  0.0125\n",
      "     70       \u001b[36m23.0357\u001b[0m       26.7405  0.0120\n",
      "     71       \u001b[36m23.0342\u001b[0m       26.7380  0.0116\n",
      "     72       \u001b[36m23.0327\u001b[0m       26.7372  0.0114\n",
      "     73       \u001b[36m23.0312\u001b[0m       26.7358  0.0132\n",
      "     74       \u001b[36m23.0298\u001b[0m       26.7334  0.0119\n",
      "     75       \u001b[36m23.0284\u001b[0m       26.7310  0.0119\n",
      "     76       \u001b[36m23.0271\u001b[0m       26.7289  0.0116\n",
      "     77       \u001b[36m23.0257\u001b[0m       26.7279  0.0115\n",
      "     78       \u001b[36m23.0244\u001b[0m       26.7262  0.0132\n",
      "     79       \u001b[36m23.0231\u001b[0m       26.7238  0.0120\n",
      "     80       \u001b[36m23.0219\u001b[0m       26.7214  0.0120\n",
      "     81       \u001b[36m23.0206\u001b[0m       26.7200  0.0116\n",
      "     82       \u001b[36m23.0194\u001b[0m       26.7190  0.0115\n",
      "     83       \u001b[36m23.0182\u001b[0m       26.7177  0.0128\n",
      "     84       \u001b[36m23.0170\u001b[0m       26.7157  0.0119\n",
      "     85       \u001b[36m23.0158\u001b[0m       26.7132  0.0122\n",
      "     86       \u001b[36m23.0147\u001b[0m       26.7119  0.0115\n",
      "     87       \u001b[36m23.0136\u001b[0m       26.7117  0.0114\n",
      "     88       \u001b[36m23.0125\u001b[0m       26.7104  0.0126\n",
      "     89       \u001b[36m23.0114\u001b[0m       26.7089  0.0119\n",
      "     90       \u001b[36m23.0103\u001b[0m       26.7073  0.0121\n",
      "     91       \u001b[36m23.0093\u001b[0m       26.7064  0.0114\n",
      "     92       \u001b[36m23.0082\u001b[0m       26.7062  0.0113\n",
      "     93       \u001b[36m23.0072\u001b[0m       26.7059  0.0129\n",
      "     94       \u001b[36m23.0062\u001b[0m       26.7054  0.0119\n",
      "     95       \u001b[36m23.0052\u001b[0m       26.7055  0.0120\n",
      "     96       \u001b[36m23.0042\u001b[0m       26.7049  0.0114\n",
      "     97       \u001b[36m23.0033\u001b[0m       26.7049  0.0114\n",
      "     98       \u001b[36m23.0023\u001b[0m       26.7050  0.0127\n",
      "     99       \u001b[36m23.0014\u001b[0m       26.7047  0.0121\n",
      "    100       \u001b[36m23.0004\u001b[0m       26.7047  0.0118\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m41.0053\u001b[0m       \u001b[32m32.2555\u001b[0m  0.0171\n",
      "      2       \u001b[36m39.9419\u001b[0m       \u001b[32m31.5536\u001b[0m  0.0155\n",
      "      3       \u001b[36m38.8497\u001b[0m       \u001b[32m30.7953\u001b[0m  0.0128\n",
      "      4       \u001b[36m37.6915\u001b[0m       \u001b[32m29.9677\u001b[0m  0.0132\n",
      "      5       \u001b[36m36.4797\u001b[0m       \u001b[32m29.0815\u001b[0m  0.0133\n",
      "      6       \u001b[36m35.1601\u001b[0m       \u001b[32m28.1613\u001b[0m  0.0142\n",
      "      7       \u001b[36m33.7070\u001b[0m       \u001b[32m27.3373\u001b[0m  0.0132\n",
      "      8       \u001b[36m32.2774\u001b[0m       \u001b[32m26.8644\u001b[0m  0.0125\n",
      "      9       \u001b[36m31.1422\u001b[0m       26.9918  0.0120\n",
      "     10       \u001b[36m30.5442\u001b[0m       27.6556  0.0118\n",
      "     11       \u001b[36m30.4398\u001b[0m       28.2312  0.0122\n",
      "     12       \u001b[36m30.4097\u001b[0m       28.1896  0.0120\n",
      "     13       \u001b[36m30.2036\u001b[0m       27.7573  0.0135\n",
      "     14       \u001b[36m29.9484\u001b[0m       27.3474  0.0120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     15       \u001b[36m29.7549\u001b[0m       27.1151  0.0121\n",
      "     16       \u001b[36m29.6116\u001b[0m       27.0480  0.0124\n",
      "     17       \u001b[36m29.4896\u001b[0m       27.0967  0.0122\n",
      "     18       \u001b[36m29.3843\u001b[0m       27.2110  0.0122\n",
      "     19       \u001b[36m29.2982\u001b[0m       27.3352  0.0116\n",
      "     20       \u001b[36m29.2270\u001b[0m       27.4216  0.0115\n",
      "     21       \u001b[36m29.1596\u001b[0m       27.4521  0.0121\n",
      "     22       \u001b[36m29.0929\u001b[0m       27.4398  0.0117\n",
      "     23       \u001b[36m29.0292\u001b[0m       27.4136  0.0120\n",
      "     24       \u001b[36m28.9710\u001b[0m       27.3996  0.0117\n",
      "     25       \u001b[36m28.9205\u001b[0m       27.4011  0.0118\n",
      "     26       \u001b[36m28.8767\u001b[0m       27.4117  0.0149\n",
      "     27       \u001b[36m28.8379\u001b[0m       27.4263  0.0119\n",
      "     28       \u001b[36m28.8029\u001b[0m       27.4393  0.0117\n",
      "     29       \u001b[36m28.7711\u001b[0m       27.4467  0.0117\n",
      "     30       \u001b[36m28.7415\u001b[0m       27.4526  0.0117\n",
      "     31       \u001b[36m28.7145\u001b[0m       27.4560  0.0117\n",
      "     32       \u001b[36m28.6898\u001b[0m       27.4551  0.0117\n",
      "     33       \u001b[36m28.6675\u001b[0m       27.4574  0.0118\n",
      "     34       \u001b[36m28.6478\u001b[0m       27.4603  0.0116\n",
      "     35       \u001b[36m28.6301\u001b[0m       27.4624  0.0111\n",
      "     36       \u001b[36m28.6143\u001b[0m       27.4644  0.0124\n",
      "     37       \u001b[36m28.6002\u001b[0m       27.4643  0.0119\n",
      "     38       \u001b[36m28.5873\u001b[0m       27.4604  0.0118\n",
      "     39       \u001b[36m28.5754\u001b[0m       27.4555  0.0118\n",
      "     40       \u001b[36m28.5646\u001b[0m       27.4521  0.0116\n",
      "     41       \u001b[36m28.5547\u001b[0m       27.4495  0.0124\n",
      "     42       \u001b[36m28.5457\u001b[0m       27.4476  0.0119\n",
      "     43       \u001b[36m28.5375\u001b[0m       27.4450  0.0120\n",
      "     44       \u001b[36m28.5300\u001b[0m       27.4409  0.0116\n",
      "     45       \u001b[36m28.5232\u001b[0m       27.4365  0.0118\n",
      "     46       \u001b[36m28.5168\u001b[0m       27.4312  0.0120\n",
      "     47       \u001b[36m28.5108\u001b[0m       27.4268  0.0116\n",
      "     48       \u001b[36m28.5053\u001b[0m       27.4230  0.0117\n",
      "     49       \u001b[36m28.5000\u001b[0m       27.4195  0.0117\n",
      "     50       \u001b[36m28.4951\u001b[0m       27.4151  0.0115\n",
      "     51       \u001b[36m28.4905\u001b[0m       27.4104  0.0151\n",
      "     52       \u001b[36m28.4863\u001b[0m       27.4057  0.0130\n",
      "     53       \u001b[36m28.4822\u001b[0m       27.4017  0.0118\n",
      "     54       \u001b[36m28.4784\u001b[0m       27.3983  0.0117\n",
      "     55       \u001b[36m28.4747\u001b[0m       27.3954  0.0116\n",
      "     56       \u001b[36m28.4714\u001b[0m       27.3924  0.0123\n",
      "     57       \u001b[36m28.4681\u001b[0m       27.3902  0.0120\n",
      "     58       \u001b[36m28.4653\u001b[0m       27.3869  0.0121\n",
      "     59       \u001b[36m28.4622\u001b[0m       27.3858  0.0116\n",
      "     60       \u001b[36m28.4598\u001b[0m       27.3823  0.0116\n",
      "     61       \u001b[36m28.4568\u001b[0m       27.3817  0.0119\n",
      "     62       \u001b[36m28.4550\u001b[0m       27.3767  0.0118\n",
      "     63       \u001b[36m28.4518\u001b[0m       27.3773  0.0116\n",
      "     64       \u001b[36m28.4509\u001b[0m       27.3695  0.0118\n",
      "     65       \u001b[36m28.4475\u001b[0m       27.3732  0.0118\n",
      "     66       28.4484       27.3610  0.0117\n",
      "     67       \u001b[36m28.4449\u001b[0m       27.3713  0.0116\n",
      "     68       28.4493       27.3524  0.0117\n",
      "     69       28.4464       27.3701  0.0117\n",
      "     70       28.4579       27.3451  0.0118\n",
      "     71       28.4567       27.3583  0.0115\n",
      "     72       28.4772       27.3512  0.0114\n",
      "     73       28.4597       27.3266  0.0121\n",
      "     74       28.4581       27.3691  0.0116\n",
      "     75       \u001b[36m28.4361\u001b[0m       27.3142  0.0113\n",
      "     76       \u001b[36m28.4309\u001b[0m       27.3539  0.0114\n",
      "     77       28.4311       27.3314  0.0117\n",
      "     78       \u001b[36m28.4270\u001b[0m       27.3335  0.0118\n",
      "     79       28.4271       27.3412  0.0159\n",
      "     80       \u001b[36m28.4237\u001b[0m       27.3277  0.0164\n",
      "     81       \u001b[36m28.4224\u001b[0m       27.3385  0.0128\n",
      "     82       \u001b[36m28.4215\u001b[0m       27.3309  0.0128\n",
      "     83       \u001b[36m28.4198\u001b[0m       27.3316  0.0126\n",
      "     84       \u001b[36m28.4193\u001b[0m       27.3290  0.0139\n",
      "     85       \u001b[36m28.4177\u001b[0m       27.3243  0.0123\n",
      "     86       \u001b[36m28.4169\u001b[0m       27.3251  0.0122\n",
      "     87       \u001b[36m28.4157\u001b[0m       27.3212  0.0122\n",
      "     88       \u001b[36m28.4147\u001b[0m       27.3210  0.0120\n",
      "     89       \u001b[36m28.4138\u001b[0m       27.3183  0.0122\n",
      "     90       \u001b[36m28.4128\u001b[0m       27.3174  0.0120\n",
      "     91       \u001b[36m28.4120\u001b[0m       27.3159  0.0119\n",
      "     92       \u001b[36m28.4110\u001b[0m       27.3143  0.0119\n",
      "     93       \u001b[36m28.4102\u001b[0m       27.3127  0.0119\n",
      "     94       \u001b[36m28.4093\u001b[0m       27.3111  0.0117\n",
      "     95       \u001b[36m28.4085\u001b[0m       27.3098  0.0120\n",
      "     96       \u001b[36m28.4076\u001b[0m       27.3081  0.0119\n",
      "     97       \u001b[36m28.4069\u001b[0m       27.3068  0.0118\n",
      "     98       \u001b[36m28.4060\u001b[0m       27.3054  0.0114\n",
      "     99       \u001b[36m28.4054\u001b[0m       27.3040  0.0114\n",
      "    100       \u001b[36m28.4045\u001b[0m       27.3027  0.0119\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m42.8158\u001b[0m       \u001b[32m44.6794\u001b[0m  0.0109\n",
      "      2       \u001b[36m42.4562\u001b[0m       \u001b[32m44.2344\u001b[0m  0.0112\n",
      "      3       \u001b[36m42.1049\u001b[0m       \u001b[32m43.7963\u001b[0m  0.0114\n",
      "      4       \u001b[36m41.7577\u001b[0m       \u001b[32m43.3609\u001b[0m  0.0113\n",
      "      5       \u001b[36m41.4132\u001b[0m       \u001b[32m42.9274\u001b[0m  0.0113\n",
      "      6       \u001b[36m41.0704\u001b[0m       \u001b[32m42.4922\u001b[0m  0.0109\n",
      "      7       \u001b[36m40.7262\u001b[0m       \u001b[32m42.0542\u001b[0m  0.0110\n",
      "      8       \u001b[36m40.3798\u001b[0m       \u001b[32m41.6108\u001b[0m  0.0112\n",
      "      9       \u001b[36m40.0312\u001b[0m       \u001b[32m41.1661\u001b[0m  0.0109\n",
      "     10       \u001b[36m39.6821\u001b[0m       \u001b[32m40.7200\u001b[0m  0.0109\n",
      "     11       \u001b[36m39.3326\u001b[0m       \u001b[32m40.2717\u001b[0m  0.0110\n",
      "     12       \u001b[36m38.9845\u001b[0m       \u001b[32m39.8243\u001b[0m  0.0111\n",
      "     13       \u001b[36m38.6396\u001b[0m       \u001b[32m39.3806\u001b[0m  0.0110\n",
      "     14       \u001b[36m38.2987\u001b[0m       \u001b[32m38.9415\u001b[0m  0.0108\n",
      "     15       \u001b[36m37.9638\u001b[0m       \u001b[32m38.5083\u001b[0m  0.0110\n",
      "     16       \u001b[36m37.6356\u001b[0m       \u001b[32m38.0827\u001b[0m  0.0113\n",
      "     17       \u001b[36m37.3151\u001b[0m       \u001b[32m37.6657\u001b[0m  0.0109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     18       \u001b[36m37.0033\u001b[0m       \u001b[32m37.2586\u001b[0m  0.0114\n",
      "     19       \u001b[36m36.7016\u001b[0m       \u001b[32m36.8617\u001b[0m  0.0109\n",
      "     20       \u001b[36m36.4104\u001b[0m       \u001b[32m36.4761\u001b[0m  0.0107\n",
      "     21       \u001b[36m36.1302\u001b[0m       \u001b[32m36.1020\u001b[0m  0.0107\n",
      "     22       \u001b[36m35.8615\u001b[0m       \u001b[32m35.7396\u001b[0m  0.0108\n",
      "     23       \u001b[36m35.6043\u001b[0m       \u001b[32m35.3891\u001b[0m  0.0111\n",
      "     24       \u001b[36m35.3589\u001b[0m       \u001b[32m35.0505\u001b[0m  0.0111\n",
      "     25       \u001b[36m35.1255\u001b[0m       \u001b[32m34.7237\u001b[0m  0.0113\n",
      "     26       \u001b[36m34.9041\u001b[0m       \u001b[32m34.4090\u001b[0m  0.0107\n",
      "     27       \u001b[36m34.6945\u001b[0m       \u001b[32m34.1065\u001b[0m  0.0107\n",
      "     28       \u001b[36m34.4967\u001b[0m       \u001b[32m33.8167\u001b[0m  0.0114\n",
      "     29       \u001b[36m34.3106\u001b[0m       \u001b[32m33.5394\u001b[0m  0.0110\n",
      "     30       \u001b[36m34.1364\u001b[0m       \u001b[32m33.2750\u001b[0m  0.0107\n",
      "     31       \u001b[36m33.9741\u001b[0m       \u001b[32m33.0237\u001b[0m  0.0110\n",
      "     32       \u001b[36m33.8237\u001b[0m       \u001b[32m32.7859\u001b[0m  0.0109\n",
      "     33       \u001b[36m33.6850\u001b[0m       \u001b[32m32.5622\u001b[0m  0.0111\n",
      "     34       \u001b[36m33.5579\u001b[0m       \u001b[32m32.3520\u001b[0m  0.0111\n",
      "     35       \u001b[36m33.4419\u001b[0m       \u001b[32m32.1556\u001b[0m  0.0110\n",
      "     36       \u001b[36m33.3365\u001b[0m       \u001b[32m31.9728\u001b[0m  0.0110\n",
      "     37       \u001b[36m33.2414\u001b[0m       \u001b[32m31.8037\u001b[0m  0.0110\n",
      "     38       \u001b[36m33.1561\u001b[0m       \u001b[32m31.6479\u001b[0m  0.0110\n",
      "     39       \u001b[36m33.0799\u001b[0m       \u001b[32m31.5047\u001b[0m  0.0110\n",
      "     40       \u001b[36m33.0121\u001b[0m       \u001b[32m31.3737\u001b[0m  0.0112\n",
      "     41       \u001b[36m32.9520\u001b[0m       \u001b[32m31.2542\u001b[0m  0.0109\n",
      "     42       \u001b[36m32.8991\u001b[0m       \u001b[32m31.1457\u001b[0m  0.0108\n",
      "     43       \u001b[36m32.8525\u001b[0m       \u001b[32m31.0474\u001b[0m  0.0109\n",
      "     44       \u001b[36m32.8118\u001b[0m       \u001b[32m30.9585\u001b[0m  0.0111\n",
      "     45       \u001b[36m32.7760\u001b[0m       \u001b[32m30.8783\u001b[0m  0.0110\n",
      "     46       \u001b[36m32.7448\u001b[0m       \u001b[32m30.8061\u001b[0m  0.0107\n",
      "     47       \u001b[36m32.7174\u001b[0m       \u001b[32m30.7412\u001b[0m  0.0107\n",
      "     48       \u001b[36m32.6935\u001b[0m       \u001b[32m30.6829\u001b[0m  0.0115\n",
      "     49       \u001b[36m32.6726\u001b[0m       \u001b[32m30.6305\u001b[0m  0.0111\n",
      "     50       \u001b[36m32.6541\u001b[0m       \u001b[32m30.5835\u001b[0m  0.0108\n",
      "     51       \u001b[36m32.6378\u001b[0m       \u001b[32m30.5412\u001b[0m  0.0109\n",
      "     52       \u001b[36m32.6233\u001b[0m       \u001b[32m30.5033\u001b[0m  0.0108\n",
      "     53       \u001b[36m32.6104\u001b[0m       \u001b[32m30.4692\u001b[0m  0.0110\n",
      "     54       \u001b[36m32.5988\u001b[0m       \u001b[32m30.4386\u001b[0m  0.0111\n",
      "     55       \u001b[36m32.5884\u001b[0m       \u001b[32m30.4110\u001b[0m  0.0110\n",
      "     56       \u001b[36m32.5790\u001b[0m       \u001b[32m30.3861\u001b[0m  0.0106\n",
      "     57       \u001b[36m32.5704\u001b[0m       \u001b[32m30.3635\u001b[0m  0.0106\n",
      "     58       \u001b[36m32.5625\u001b[0m       \u001b[32m30.3432\u001b[0m  0.0111\n",
      "     59       \u001b[36m32.5553\u001b[0m       \u001b[32m30.3247\u001b[0m  0.0112\n",
      "     60       \u001b[36m32.5485\u001b[0m       \u001b[32m30.3080\u001b[0m  0.0109\n",
      "     61       \u001b[36m32.5422\u001b[0m       \u001b[32m30.2927\u001b[0m  0.0115\n",
      "     62       \u001b[36m32.5363\u001b[0m       \u001b[32m30.2788\u001b[0m  0.0170\n",
      "     63       \u001b[36m32.5306\u001b[0m       \u001b[32m30.2661\u001b[0m  0.0135\n",
      "     64       \u001b[36m32.5253\u001b[0m       \u001b[32m30.2545\u001b[0m  0.0114\n",
      "     65       \u001b[36m32.5203\u001b[0m       \u001b[32m30.2438\u001b[0m  0.0117\n",
      "     66       \u001b[36m32.5154\u001b[0m       \u001b[32m30.2338\u001b[0m  0.0125\n",
      "     67       \u001b[36m32.5108\u001b[0m       \u001b[32m30.2248\u001b[0m  0.0195\n",
      "     68       \u001b[36m32.5063\u001b[0m       \u001b[32m30.2163\u001b[0m  0.0193\n",
      "     69       \u001b[36m32.5020\u001b[0m       \u001b[32m30.2085\u001b[0m  0.0113\n",
      "     70       \u001b[36m32.4979\u001b[0m       \u001b[32m30.2013\u001b[0m  0.0111\n",
      "     71       \u001b[36m32.4939\u001b[0m       \u001b[32m30.1944\u001b[0m  0.0112\n",
      "     72       \u001b[36m32.4900\u001b[0m       \u001b[32m30.1881\u001b[0m  0.0115\n",
      "     73       \u001b[36m32.4862\u001b[0m       \u001b[32m30.1822\u001b[0m  0.0111\n",
      "     74       \u001b[36m32.4826\u001b[0m       \u001b[32m30.1766\u001b[0m  0.0121\n",
      "     75       \u001b[36m32.4790\u001b[0m       \u001b[32m30.1714\u001b[0m  0.0114\n",
      "     76       \u001b[36m32.4756\u001b[0m       \u001b[32m30.1664\u001b[0m  0.0112\n",
      "     77       \u001b[36m32.4722\u001b[0m       \u001b[32m30.1618\u001b[0m  0.0116\n",
      "     78       \u001b[36m32.4689\u001b[0m       \u001b[32m30.1574\u001b[0m  0.0113\n",
      "     79       \u001b[36m32.4657\u001b[0m       \u001b[32m30.1532\u001b[0m  0.0114\n",
      "     80       \u001b[36m32.4626\u001b[0m       \u001b[32m30.1491\u001b[0m  0.0106\n",
      "     81       \u001b[36m32.4595\u001b[0m       \u001b[32m30.1453\u001b[0m  0.0110\n",
      "     82       \u001b[36m32.4565\u001b[0m       \u001b[32m30.1416\u001b[0m  0.0114\n",
      "     83       \u001b[36m32.4536\u001b[0m       \u001b[32m30.1381\u001b[0m  0.0112\n",
      "     84       \u001b[36m32.4507\u001b[0m       \u001b[32m30.1347\u001b[0m  0.0110\n",
      "     85       \u001b[36m32.4479\u001b[0m       \u001b[32m30.1316\u001b[0m  0.0109\n",
      "     86       \u001b[36m32.4452\u001b[0m       \u001b[32m30.1284\u001b[0m  0.0112\n",
      "     87       \u001b[36m32.4425\u001b[0m       \u001b[32m30.1254\u001b[0m  0.0121\n",
      "     88       \u001b[36m32.4399\u001b[0m       \u001b[32m30.1225\u001b[0m  0.0114\n",
      "     89       \u001b[36m32.4373\u001b[0m       \u001b[32m30.1196\u001b[0m  0.0118\n",
      "     90       \u001b[36m32.4348\u001b[0m       \u001b[32m30.1169\u001b[0m  0.0115\n",
      "     91       \u001b[36m32.4323\u001b[0m       \u001b[32m30.1142\u001b[0m  0.0111\n",
      "     92       \u001b[36m32.4299\u001b[0m       \u001b[32m30.1116\u001b[0m  0.0121\n",
      "     93       \u001b[36m32.4276\u001b[0m       \u001b[32m30.1091\u001b[0m  0.0120\n",
      "     94       \u001b[36m32.4252\u001b[0m       \u001b[32m30.1066\u001b[0m  0.0117\n",
      "     95       \u001b[36m32.4230\u001b[0m       \u001b[32m30.1042\u001b[0m  0.0112\n",
      "     96       \u001b[36m32.4207\u001b[0m       \u001b[32m30.1019\u001b[0m  0.0110\n",
      "     97       \u001b[36m32.4185\u001b[0m       \u001b[32m30.0996\u001b[0m  0.0120\n",
      "     98       \u001b[36m32.4164\u001b[0m       \u001b[32m30.0974\u001b[0m  0.0116\n",
      "     99       \u001b[36m32.4142\u001b[0m       \u001b[32m30.0952\u001b[0m  0.0114\n",
      "    100       \u001b[36m32.4121\u001b[0m       \u001b[32m30.0930\u001b[0m  0.0116\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m32.9151\u001b[0m       \u001b[32m31.7525\u001b[0m  0.0114\n",
      "      2       \u001b[36m32.6426\u001b[0m       \u001b[32m31.5424\u001b[0m  0.0118\n",
      "      3       \u001b[36m32.3733\u001b[0m       \u001b[32m31.3359\u001b[0m  0.0113\n",
      "      4       \u001b[36m32.1060\u001b[0m       \u001b[32m31.1312\u001b[0m  0.0117\n",
      "      5       \u001b[36m31.8387\u001b[0m       \u001b[32m30.9275\u001b[0m  0.0114\n",
      "      6       \u001b[36m31.5711\u001b[0m       \u001b[32m30.7249\u001b[0m  0.0114\n",
      "      7       \u001b[36m31.3035\u001b[0m       \u001b[32m30.5223\u001b[0m  0.0115\n",
      "      8       \u001b[36m31.0342\u001b[0m       \u001b[32m30.3197\u001b[0m  0.0112\n",
      "      9       \u001b[36m30.7634\u001b[0m       \u001b[32m30.1175\u001b[0m  0.0112\n",
      "     10       \u001b[36m30.4911\u001b[0m       \u001b[32m29.9154\u001b[0m  0.0111\n",
      "     11       \u001b[36m30.2171\u001b[0m       \u001b[32m29.7131\u001b[0m  0.0114\n",
      "     12       \u001b[36m29.9415\u001b[0m       \u001b[32m29.5113\u001b[0m  0.0116\n",
      "     13       \u001b[36m29.6639\u001b[0m       \u001b[32m29.3098\u001b[0m  0.0112\n",
      "     14       \u001b[36m29.3846\u001b[0m       \u001b[32m29.1093\u001b[0m  0.0110\n",
      "     15       \u001b[36m29.1040\u001b[0m       \u001b[32m28.9101\u001b[0m  0.0113\n",
      "     16       \u001b[36m28.8225\u001b[0m       \u001b[32m28.7128\u001b[0m  0.0112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m28.5411\u001b[0m       \u001b[32m28.5178\u001b[0m  0.0118\n",
      "     18       \u001b[36m28.2605\u001b[0m       \u001b[32m28.3261\u001b[0m  0.0112\n",
      "     19       \u001b[36m27.9813\u001b[0m       \u001b[32m28.1384\u001b[0m  0.0111\n",
      "     20       \u001b[36m27.7045\u001b[0m       \u001b[32m27.9557\u001b[0m  0.0109\n",
      "     21       \u001b[36m27.4317\u001b[0m       \u001b[32m27.7790\u001b[0m  0.0108\n",
      "     22       \u001b[36m27.1638\u001b[0m       \u001b[32m27.6092\u001b[0m  0.0116\n",
      "     23       \u001b[36m26.9021\u001b[0m       \u001b[32m27.4475\u001b[0m  0.0113\n",
      "     24       \u001b[36m26.6479\u001b[0m       \u001b[32m27.2945\u001b[0m  0.0115\n",
      "     25       \u001b[36m26.4022\u001b[0m       \u001b[32m27.1512\u001b[0m  0.0106\n",
      "     26       \u001b[36m26.1662\u001b[0m       \u001b[32m27.0181\u001b[0m  0.0106\n",
      "     27       \u001b[36m25.9406\u001b[0m       \u001b[32m26.8957\u001b[0m  0.0112\n",
      "     28       \u001b[36m25.7263\u001b[0m       \u001b[32m26.7844\u001b[0m  0.0112\n",
      "     29       \u001b[36m25.5236\u001b[0m       \u001b[32m26.6843\u001b[0m  0.0110\n",
      "     30       \u001b[36m25.3330\u001b[0m       \u001b[32m26.5953\u001b[0m  0.0108\n",
      "     31       \u001b[36m25.1546\u001b[0m       \u001b[32m26.5174\u001b[0m  0.0108\n",
      "     32       \u001b[36m24.9884\u001b[0m       \u001b[32m26.4503\u001b[0m  0.0112\n",
      "     33       \u001b[36m24.8345\u001b[0m       \u001b[32m26.3935\u001b[0m  0.0111\n",
      "     34       \u001b[36m24.6926\u001b[0m       \u001b[32m26.3463\u001b[0m  0.0109\n",
      "     35       \u001b[36m24.5621\u001b[0m       \u001b[32m26.3083\u001b[0m  0.0109\n",
      "     36       \u001b[36m24.4429\u001b[0m       \u001b[32m26.2788\u001b[0m  0.0108\n",
      "     37       \u001b[36m24.3341\u001b[0m       \u001b[32m26.2568\u001b[0m  0.0108\n",
      "     38       \u001b[36m24.2353\u001b[0m       \u001b[32m26.2417\u001b[0m  0.0107\n",
      "     39       \u001b[36m24.1460\u001b[0m       \u001b[32m26.2327\u001b[0m  0.0111\n",
      "     40       \u001b[36m24.0655\u001b[0m       \u001b[32m26.2288\u001b[0m  0.0109\n",
      "     41       \u001b[36m23.9931\u001b[0m       26.2295  0.0108\n",
      "     42       \u001b[36m23.9281\u001b[0m       26.2338  0.0118\n",
      "     43       \u001b[36m23.8700\u001b[0m       26.2413  0.0193\n",
      "     44       \u001b[36m23.8180\u001b[0m       26.2513  0.0136\n",
      "     45       \u001b[36m23.7716\u001b[0m       26.2632  0.0120\n",
      "     46       \u001b[36m23.7303\u001b[0m       26.2765  0.0128\n",
      "     47       \u001b[36m23.6934\u001b[0m       26.2907  0.0138\n",
      "     48       \u001b[36m23.6605\u001b[0m       26.3054  0.0156\n",
      "     49       \u001b[36m23.6312\u001b[0m       26.3204  0.0123\n",
      "     50       \u001b[36m23.6051\u001b[0m       26.3352  0.0113\n",
      "     51       \u001b[36m23.5819\u001b[0m       26.3500  0.0113\n",
      "     52       \u001b[36m23.5611\u001b[0m       26.3642  0.0114\n",
      "     53       \u001b[36m23.5425\u001b[0m       26.3779  0.0115\n",
      "     54       \u001b[36m23.5257\u001b[0m       26.3909  0.0114\n",
      "     55       \u001b[36m23.5105\u001b[0m       26.4031  0.0112\n",
      "     56       \u001b[36m23.4968\u001b[0m       26.4145  0.0113\n",
      "     57       \u001b[36m23.4843\u001b[0m       26.4251  0.0113\n",
      "     58       \u001b[36m23.4729\u001b[0m       26.4349  0.0112\n",
      "     59       \u001b[36m23.4624\u001b[0m       26.4439  0.0114\n",
      "     60       \u001b[36m23.4527\u001b[0m       26.4522  0.0113\n",
      "     61       \u001b[36m23.4438\u001b[0m       26.4598  0.0111\n",
      "     62       \u001b[36m23.4354\u001b[0m       26.4668  0.0110\n",
      "     63       \u001b[36m23.4277\u001b[0m       26.4731  0.0109\n",
      "     64       \u001b[36m23.4204\u001b[0m       26.4788  0.0112\n",
      "     65       \u001b[36m23.4136\u001b[0m       26.4839  0.0109\n",
      "     66       \u001b[36m23.4072\u001b[0m       26.4885  0.0110\n",
      "     67       \u001b[36m23.4011\u001b[0m       26.4926  0.0112\n",
      "     68       \u001b[36m23.3953\u001b[0m       26.4963  0.0112\n",
      "     69       \u001b[36m23.3897\u001b[0m       26.4995  0.0112\n",
      "     70       \u001b[36m23.3844\u001b[0m       26.5023  0.0109\n",
      "     71       \u001b[36m23.3793\u001b[0m       26.5049  0.0112\n",
      "     72       \u001b[36m23.3745\u001b[0m       26.5071  0.0114\n",
      "     73       \u001b[36m23.3698\u001b[0m       26.5091  0.0108\n",
      "     74       \u001b[36m23.3652\u001b[0m       26.5108  0.0107\n",
      "     75       \u001b[36m23.3609\u001b[0m       26.5123  0.0110\n",
      "     76       \u001b[36m23.3566\u001b[0m       26.5136  0.0107\n",
      "     77       \u001b[36m23.3525\u001b[0m       26.5147  0.0112\n",
      "     78       \u001b[36m23.3485\u001b[0m       26.5156  0.0109\n",
      "     79       \u001b[36m23.3447\u001b[0m       26.5163  0.0111\n",
      "     80       \u001b[36m23.3409\u001b[0m       26.5170  0.0108\n",
      "     81       \u001b[36m23.3372\u001b[0m       26.5175  0.0109\n",
      "     82       \u001b[36m23.3336\u001b[0m       26.5178  0.0113\n",
      "     83       \u001b[36m23.3302\u001b[0m       26.5181  0.0110\n",
      "     84       \u001b[36m23.3268\u001b[0m       26.5183  0.0110\n",
      "     85       \u001b[36m23.3235\u001b[0m       26.5184  0.0108\n",
      "     86       \u001b[36m23.3202\u001b[0m       26.5184  0.0109\n",
      "     87       \u001b[36m23.3170\u001b[0m       26.5184  0.0112\n",
      "     88       \u001b[36m23.3139\u001b[0m       26.5183  0.0110\n",
      "     89       \u001b[36m23.3109\u001b[0m       26.5182  0.0113\n",
      "     90       \u001b[36m23.3079\u001b[0m       26.5180  0.0109\n",
      "     91       \u001b[36m23.3050\u001b[0m       26.5178  0.0108\n",
      "     92       \u001b[36m23.3021\u001b[0m       26.5176  0.0118\n",
      "     93       \u001b[36m23.2993\u001b[0m       26.5173  0.0112\n",
      "     94       \u001b[36m23.2965\u001b[0m       26.5170  0.0110\n",
      "     95       \u001b[36m23.2938\u001b[0m       26.5167  0.0109\n",
      "     96       \u001b[36m23.2912\u001b[0m       26.5164  0.0108\n",
      "     97       \u001b[36m23.2886\u001b[0m       26.5160  0.0110\n",
      "     98       \u001b[36m23.2860\u001b[0m       26.5157  0.0111\n",
      "     99       \u001b[36m23.2835\u001b[0m       26.5153  0.0111\n",
      "    100       \u001b[36m23.2810\u001b[0m       26.5150  0.0108\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m38.9664\u001b[0m       \u001b[32m31.0850\u001b[0m  0.0107\n",
      "      2       \u001b[36m38.4638\u001b[0m       \u001b[32m30.7531\u001b[0m  0.0118\n",
      "      3       \u001b[36m37.9717\u001b[0m       \u001b[32m30.4284\u001b[0m  0.0109\n",
      "      4       \u001b[36m37.4883\u001b[0m       \u001b[32m30.1110\u001b[0m  0.0108\n",
      "      5       \u001b[36m37.0127\u001b[0m       \u001b[32m29.7998\u001b[0m  0.0109\n",
      "      6       \u001b[36m36.5448\u001b[0m       \u001b[32m29.4950\u001b[0m  0.0109\n",
      "      7       \u001b[36m36.0827\u001b[0m       \u001b[32m29.1957\u001b[0m  0.0112\n",
      "      8       \u001b[36m35.6256\u001b[0m       \u001b[32m28.9025\u001b[0m  0.0109\n",
      "      9       \u001b[36m35.1753\u001b[0m       \u001b[32m28.6172\u001b[0m  0.0109\n",
      "     10       \u001b[36m34.7341\u001b[0m       \u001b[32m28.3411\u001b[0m  0.0106\n",
      "     11       \u001b[36m34.3030\u001b[0m       \u001b[32m28.0751\u001b[0m  0.0108\n",
      "     12       \u001b[36m33.8836\u001b[0m       \u001b[32m27.8214\u001b[0m  0.0139\n",
      "     13       \u001b[36m33.4774\u001b[0m       \u001b[32m27.5815\u001b[0m  0.0115\n",
      "     14       \u001b[36m33.0855\u001b[0m       \u001b[32m27.3573\u001b[0m  0.0129\n",
      "     15       \u001b[36m32.7096\u001b[0m       \u001b[32m27.1500\u001b[0m  0.0109\n",
      "     16       \u001b[36m32.3505\u001b[0m       \u001b[32m26.9608\u001b[0m  0.0106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m32.0091\u001b[0m       \u001b[32m26.7908\u001b[0m  0.0120\n",
      "     18       \u001b[36m31.6864\u001b[0m       \u001b[32m26.6404\u001b[0m  0.0114\n",
      "     19       \u001b[36m31.3826\u001b[0m       \u001b[32m26.5103\u001b[0m  0.0108\n",
      "     20       \u001b[36m31.0986\u001b[0m       \u001b[32m26.4009\u001b[0m  0.0108\n",
      "     21       \u001b[36m30.8348\u001b[0m       \u001b[32m26.3124\u001b[0m  0.0111\n",
      "     22       \u001b[36m30.5917\u001b[0m       \u001b[32m26.2442\u001b[0m  0.0130\n",
      "     23       \u001b[36m30.3695\u001b[0m       \u001b[32m26.1955\u001b[0m  0.0110\n",
      "     24       \u001b[36m30.1680\u001b[0m       \u001b[32m26.1652\u001b[0m  0.0112\n",
      "     25       \u001b[36m29.9872\u001b[0m       \u001b[32m26.1520\u001b[0m  0.0142\n",
      "     26       \u001b[36m29.8264\u001b[0m       26.1541  0.0142\n",
      "     27       \u001b[36m29.6843\u001b[0m       26.1694  0.0115\n",
      "     28       \u001b[36m29.5599\u001b[0m       26.1960  0.0114\n",
      "     29       \u001b[36m29.4520\u001b[0m       26.2316  0.0117\n",
      "     30       \u001b[36m29.3591\u001b[0m       26.2743  0.0126\n",
      "     31       \u001b[36m29.2798\u001b[0m       26.3220  0.0123\n",
      "     32       \u001b[36m29.2122\u001b[0m       26.3730  0.0125\n",
      "     33       \u001b[36m29.1549\u001b[0m       26.4257  0.0114\n",
      "     34       \u001b[36m29.1063\u001b[0m       26.4786  0.0113\n",
      "     35       \u001b[36m29.0653\u001b[0m       26.5308  0.0113\n",
      "     36       \u001b[36m29.0306\u001b[0m       26.5812  0.0116\n",
      "     37       \u001b[36m29.0013\u001b[0m       26.6293  0.0111\n",
      "     38       \u001b[36m28.9764\u001b[0m       26.6747  0.0111\n",
      "     39       \u001b[36m28.9549\u001b[0m       26.7170  0.0134\n",
      "     40       \u001b[36m28.9364\u001b[0m       26.7562  0.0113\n",
      "     41       \u001b[36m28.9202\u001b[0m       26.7921  0.0114\n",
      "     42       \u001b[36m28.9059\u001b[0m       26.8249  0.0111\n",
      "     43       \u001b[36m28.8931\u001b[0m       26.8546  0.0115\n",
      "     44       \u001b[36m28.8815\u001b[0m       26.8815  0.0112\n",
      "     45       \u001b[36m28.8710\u001b[0m       26.9057  0.0111\n",
      "     46       \u001b[36m28.8613\u001b[0m       26.9273  0.0111\n",
      "     47       \u001b[36m28.8523\u001b[0m       26.9467  0.0114\n",
      "     48       \u001b[36m28.8438\u001b[0m       26.9639  0.0111\n",
      "     49       \u001b[36m28.8358\u001b[0m       26.9792  0.0110\n",
      "     50       \u001b[36m28.8281\u001b[0m       26.9928  0.0109\n",
      "     51       \u001b[36m28.8207\u001b[0m       27.0049  0.0111\n",
      "     52       \u001b[36m28.8137\u001b[0m       27.0155  0.0110\n",
      "     53       \u001b[36m28.8068\u001b[0m       27.0249  0.0110\n",
      "     54       \u001b[36m28.8002\u001b[0m       27.0333  0.0112\n",
      "     55       \u001b[36m28.7938\u001b[0m       27.0406  0.0107\n",
      "     56       \u001b[36m28.7875\u001b[0m       27.0471  0.0110\n",
      "     57       \u001b[36m28.7815\u001b[0m       27.0528  0.0106\n",
      "     58       \u001b[36m28.7755\u001b[0m       27.0579  0.0110\n",
      "     59       \u001b[36m28.7697\u001b[0m       27.0625  0.0106\n",
      "     60       \u001b[36m28.7641\u001b[0m       27.0665  0.0107\n",
      "     61       \u001b[36m28.7586\u001b[0m       27.0701  0.0117\n",
      "     62       \u001b[36m28.7532\u001b[0m       27.0733  0.0112\n",
      "     63       \u001b[36m28.7479\u001b[0m       27.0761  0.0111\n",
      "     64       \u001b[36m28.7427\u001b[0m       27.0786  0.0109\n",
      "     65       \u001b[36m28.7377\u001b[0m       27.0809  0.0109\n",
      "     66       \u001b[36m28.7328\u001b[0m       27.0829  0.0126\n",
      "     67       \u001b[36m28.7280\u001b[0m       27.0849  0.0111\n",
      "     68       \u001b[36m28.7234\u001b[0m       27.0866  0.0112\n",
      "     69       \u001b[36m28.7188\u001b[0m       27.0882  0.0108\n",
      "     70       \u001b[36m28.7143\u001b[0m       27.0897  0.0107\n",
      "     71       \u001b[36m28.7099\u001b[0m       27.0909  0.0120\n",
      "     72       \u001b[36m28.7057\u001b[0m       27.0921  0.0115\n",
      "     73       \u001b[36m28.7015\u001b[0m       27.0932  0.0115\n",
      "     74       \u001b[36m28.6974\u001b[0m       27.0942  0.0108\n",
      "     75       \u001b[36m28.6935\u001b[0m       27.0951  0.0105\n",
      "     76       \u001b[36m28.6896\u001b[0m       27.0960  0.0116\n",
      "     77       \u001b[36m28.6858\u001b[0m       27.0966  0.0114\n",
      "     78       \u001b[36m28.6820\u001b[0m       27.0973  0.0117\n",
      "     79       \u001b[36m28.6784\u001b[0m       27.0980  0.0110\n",
      "     80       \u001b[36m28.6748\u001b[0m       27.0986  0.0109\n",
      "     81       \u001b[36m28.6712\u001b[0m       27.0991  0.0121\n",
      "     82       \u001b[36m28.6678\u001b[0m       27.0997  0.0119\n",
      "     83       \u001b[36m28.6644\u001b[0m       27.1000  0.0114\n",
      "     84       \u001b[36m28.6611\u001b[0m       27.1004  0.0110\n",
      "     85       \u001b[36m28.6578\u001b[0m       27.1009  0.0108\n",
      "     86       \u001b[36m28.6547\u001b[0m       27.1014  0.0126\n",
      "     87       \u001b[36m28.6516\u001b[0m       27.1018  0.0117\n",
      "     88       \u001b[36m28.6485\u001b[0m       27.1022  0.0118\n",
      "     89       \u001b[36m28.6456\u001b[0m       27.1027  0.0111\n",
      "     90       \u001b[36m28.6427\u001b[0m       27.1032  0.0107\n",
      "     91       \u001b[36m28.6398\u001b[0m       27.1038  0.0120\n",
      "     92       \u001b[36m28.6370\u001b[0m       27.1042  0.0112\n",
      "     93       \u001b[36m28.6343\u001b[0m       27.1047  0.0113\n",
      "     94       \u001b[36m28.6316\u001b[0m       27.1052  0.0108\n",
      "     95       \u001b[36m28.6290\u001b[0m       27.1056  0.0107\n",
      "     96       \u001b[36m28.6265\u001b[0m       27.1061  0.0116\n",
      "     97       \u001b[36m28.6239\u001b[0m       27.1066  0.0115\n",
      "     98       \u001b[36m28.6215\u001b[0m       27.1070  0.0115\n",
      "     99       \u001b[36m28.6191\u001b[0m       27.1074  0.0109\n",
      "    100       \u001b[36m28.6167\u001b[0m       27.1078  0.0106\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m43.0812\u001b[0m       \u001b[32m44.2364\u001b[0m  0.0143\n",
      "      2       \u001b[36m41.8746\u001b[0m       \u001b[32m42.6922\u001b[0m  0.0122\n",
      "      3       \u001b[36m40.6612\u001b[0m       \u001b[32m41.0233\u001b[0m  0.0135\n",
      "      4       \u001b[36m39.3336\u001b[0m       \u001b[32m39.1018\u001b[0m  0.0121\n",
      "      5       \u001b[36m37.8103\u001b[0m       \u001b[32m36.8242\u001b[0m  0.0115\n",
      "      6       \u001b[36m36.1248\u001b[0m       \u001b[32m34.4148\u001b[0m  0.0114\n",
      "      7       \u001b[36m34.6604\u001b[0m       \u001b[32m32.4788\u001b[0m  0.0171\n",
      "      8       \u001b[36m33.9334\u001b[0m       \u001b[32m31.5418\u001b[0m  0.0153\n",
      "      9       \u001b[36m33.8687\u001b[0m       \u001b[32m31.2683\u001b[0m  0.0133\n",
      "     10       \u001b[36m33.7305\u001b[0m       \u001b[32m31.2129\u001b[0m  0.0121\n",
      "     11       \u001b[36m33.4378\u001b[0m       31.3324  0.0122\n",
      "     12       \u001b[36m33.2567\u001b[0m       31.4202  0.0152\n",
      "     13       \u001b[36m33.1494\u001b[0m       31.3412  0.0127\n",
      "     14       \u001b[36m33.0425\u001b[0m       \u001b[32m31.1531\u001b[0m  0.0128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     15       \u001b[36m32.9362\u001b[0m       \u001b[32m30.9502\u001b[0m  0.0122\n",
      "     16       \u001b[36m32.8461\u001b[0m       \u001b[32m30.7924\u001b[0m  0.0118\n",
      "     17       \u001b[36m32.7778\u001b[0m       \u001b[32m30.6941\u001b[0m  0.0132\n",
      "     18       \u001b[36m32.7229\u001b[0m       \u001b[32m30.6332\u001b[0m  0.0123\n",
      "     19       \u001b[36m32.6705\u001b[0m       \u001b[32m30.5985\u001b[0m  0.0131\n",
      "     20       \u001b[36m32.6237\u001b[0m       \u001b[32m30.5780\u001b[0m  0.0131\n",
      "     21       \u001b[36m32.5823\u001b[0m       \u001b[32m30.5553\u001b[0m  0.0138\n",
      "     22       \u001b[36m32.5449\u001b[0m       \u001b[32m30.5214\u001b[0m  0.0178\n",
      "     23       \u001b[36m32.5110\u001b[0m       \u001b[32m30.4797\u001b[0m  0.0122\n",
      "     24       \u001b[36m32.4808\u001b[0m       \u001b[32m30.4438\u001b[0m  0.0120\n",
      "     25       \u001b[36m32.4545\u001b[0m       \u001b[32m30.4208\u001b[0m  0.0120\n",
      "     26       \u001b[36m32.4312\u001b[0m       \u001b[32m30.4051\u001b[0m  0.0121\n",
      "     27       \u001b[36m32.4102\u001b[0m       \u001b[32m30.3898\u001b[0m  0.0131\n",
      "     28       \u001b[36m32.3915\u001b[0m       \u001b[32m30.3756\u001b[0m  0.0122\n",
      "     29       \u001b[36m32.3749\u001b[0m       \u001b[32m30.3620\u001b[0m  0.0120\n",
      "     30       \u001b[36m32.3600\u001b[0m       \u001b[32m30.3465\u001b[0m  0.0129\n",
      "     31       \u001b[36m32.3466\u001b[0m       \u001b[32m30.3311\u001b[0m  0.0122\n",
      "     32       \u001b[36m32.3346\u001b[0m       \u001b[32m30.3190\u001b[0m  0.0118\n",
      "     33       \u001b[36m32.3238\u001b[0m       \u001b[32m30.3092\u001b[0m  0.0117\n",
      "     34       \u001b[36m32.3138\u001b[0m       \u001b[32m30.3010\u001b[0m  0.0116\n",
      "     35       \u001b[36m32.3047\u001b[0m       \u001b[32m30.2934\u001b[0m  0.0115\n",
      "     36       \u001b[36m32.2962\u001b[0m       \u001b[32m30.2850\u001b[0m  0.0116\n",
      "     37       \u001b[36m32.2883\u001b[0m       \u001b[32m30.2769\u001b[0m  0.0116\n",
      "     38       \u001b[36m32.2810\u001b[0m       \u001b[32m30.2700\u001b[0m  0.0116\n",
      "     39       \u001b[36m32.2741\u001b[0m       \u001b[32m30.2635\u001b[0m  0.0113\n",
      "     40       \u001b[36m32.2677\u001b[0m       \u001b[32m30.2578\u001b[0m  0.0113\n",
      "     41       \u001b[36m32.2616\u001b[0m       \u001b[32m30.2525\u001b[0m  0.0122\n",
      "     42       \u001b[36m32.2558\u001b[0m       \u001b[32m30.2469\u001b[0m  0.0116\n",
      "     43       \u001b[36m32.2504\u001b[0m       \u001b[32m30.2425\u001b[0m  0.0117\n",
      "     44       \u001b[36m32.2452\u001b[0m       \u001b[32m30.2379\u001b[0m  0.0126\n",
      "     45       \u001b[36m32.2403\u001b[0m       \u001b[32m30.2332\u001b[0m  0.0117\n",
      "     46       \u001b[36m32.2356\u001b[0m       \u001b[32m30.2291\u001b[0m  0.0133\n",
      "     47       \u001b[36m32.2311\u001b[0m       \u001b[32m30.2251\u001b[0m  0.0125\n",
      "     48       \u001b[36m32.2269\u001b[0m       \u001b[32m30.2217\u001b[0m  0.0118\n",
      "     49       \u001b[36m32.2228\u001b[0m       \u001b[32m30.2178\u001b[0m  0.0115\n",
      "     50       \u001b[36m32.2189\u001b[0m       \u001b[32m30.2145\u001b[0m  0.0113\n",
      "     51       \u001b[36m32.2152\u001b[0m       \u001b[32m30.2108\u001b[0m  0.0117\n",
      "     52       \u001b[36m32.2115\u001b[0m       \u001b[32m30.2077\u001b[0m  0.0118\n",
      "     53       \u001b[36m32.2081\u001b[0m       \u001b[32m30.2051\u001b[0m  0.0116\n",
      "     54       \u001b[36m32.2047\u001b[0m       \u001b[32m30.2022\u001b[0m  0.0118\n",
      "     55       \u001b[36m32.2015\u001b[0m       \u001b[32m30.1988\u001b[0m  0.0115\n",
      "     56       \u001b[36m32.1983\u001b[0m       \u001b[32m30.1947\u001b[0m  0.0117\n",
      "     57       \u001b[36m32.1952\u001b[0m       \u001b[32m30.1918\u001b[0m  0.0127\n",
      "     58       \u001b[36m32.1923\u001b[0m       \u001b[32m30.1895\u001b[0m  0.0120\n",
      "     59       \u001b[36m32.1894\u001b[0m       \u001b[32m30.1867\u001b[0m  0.0124\n",
      "     60       \u001b[36m32.1866\u001b[0m       \u001b[32m30.1832\u001b[0m  0.0117\n",
      "     61       \u001b[36m32.1838\u001b[0m       \u001b[32m30.1816\u001b[0m  0.0118\n",
      "     62       \u001b[36m32.1812\u001b[0m       \u001b[32m30.1785\u001b[0m  0.0125\n",
      "     63       \u001b[36m32.1786\u001b[0m       \u001b[32m30.1761\u001b[0m  0.0127\n",
      "     64       \u001b[36m32.1761\u001b[0m       \u001b[32m30.1733\u001b[0m  0.0122\n",
      "     65       \u001b[36m32.1737\u001b[0m       \u001b[32m30.1710\u001b[0m  0.0121\n",
      "     66       \u001b[36m32.1714\u001b[0m       \u001b[32m30.1688\u001b[0m  0.0130\n",
      "     67       \u001b[36m32.1691\u001b[0m       \u001b[32m30.1665\u001b[0m  0.0120\n",
      "     68       \u001b[36m32.1668\u001b[0m       \u001b[32m30.1649\u001b[0m  0.0122\n",
      "     69       \u001b[36m32.1646\u001b[0m       \u001b[32m30.1616\u001b[0m  0.0145\n",
      "     70       \u001b[36m32.1624\u001b[0m       \u001b[32m30.1596\u001b[0m  0.0119\n",
      "     71       \u001b[36m32.1604\u001b[0m       \u001b[32m30.1579\u001b[0m  0.0118\n",
      "     72       \u001b[36m32.1583\u001b[0m       \u001b[32m30.1550\u001b[0m  0.0122\n",
      "     73       \u001b[36m32.1562\u001b[0m       \u001b[32m30.1533\u001b[0m  0.0119\n",
      "     74       \u001b[36m32.1542\u001b[0m       \u001b[32m30.1510\u001b[0m  0.0118\n",
      "     75       \u001b[36m32.1522\u001b[0m       \u001b[32m30.1496\u001b[0m  0.0116\n",
      "     76       \u001b[36m32.1503\u001b[0m       \u001b[32m30.1469\u001b[0m  0.0120\n",
      "     77       \u001b[36m32.1484\u001b[0m       \u001b[32m30.1460\u001b[0m  0.0129\n",
      "     78       \u001b[36m32.1466\u001b[0m       \u001b[32m30.1437\u001b[0m  0.0122\n",
      "     79       \u001b[36m32.1447\u001b[0m       \u001b[32m30.1426\u001b[0m  0.0120\n",
      "     80       \u001b[36m32.1429\u001b[0m       \u001b[32m30.1397\u001b[0m  0.0137\n",
      "     81       \u001b[36m32.1411\u001b[0m       \u001b[32m30.1388\u001b[0m  0.0117\n",
      "     82       \u001b[36m32.1394\u001b[0m       \u001b[32m30.1362\u001b[0m  0.0131\n",
      "     83       \u001b[36m32.1376\u001b[0m       30.1365  0.0143\n",
      "     84       \u001b[36m32.1360\u001b[0m       \u001b[32m30.1341\u001b[0m  0.0183\n",
      "     85       \u001b[36m32.1342\u001b[0m       \u001b[32m30.1332\u001b[0m  0.0134\n",
      "     86       \u001b[36m32.1326\u001b[0m       \u001b[32m30.1314\u001b[0m  0.0135\n",
      "     87       \u001b[36m32.1310\u001b[0m       30.1319  0.0142\n",
      "     88       \u001b[36m32.1295\u001b[0m       \u001b[32m30.1288\u001b[0m  0.0149\n",
      "     89       \u001b[36m32.1279\u001b[0m       30.1299  0.0128\n",
      "     90       \u001b[36m32.1264\u001b[0m       \u001b[32m30.1273\u001b[0m  0.0200\n",
      "     91       \u001b[36m32.1249\u001b[0m       \u001b[32m30.1267\u001b[0m  0.0180\n",
      "     92       \u001b[36m32.1234\u001b[0m       \u001b[32m30.1223\u001b[0m  0.0119\n",
      "     93       \u001b[36m32.1219\u001b[0m       30.1257  0.0117\n",
      "     94       \u001b[36m32.1207\u001b[0m       \u001b[32m30.1218\u001b[0m  0.0118\n",
      "     95       \u001b[36m32.1191\u001b[0m       30.1243  0.0138\n",
      "     96       \u001b[36m32.1180\u001b[0m       \u001b[32m30.1184\u001b[0m  0.0125\n",
      "     97       \u001b[36m32.1165\u001b[0m       30.1261  0.0121\n",
      "     98       \u001b[36m32.1157\u001b[0m       30.1186  0.0126\n",
      "     99       \u001b[36m32.1141\u001b[0m       30.1289  0.0127\n",
      "    100       \u001b[36m32.1136\u001b[0m       \u001b[32m30.1172\u001b[0m  0.0125\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m32.9766\u001b[0m       \u001b[32m31.3100\u001b[0m  0.0122\n",
      "      2       \u001b[36m31.6786\u001b[0m       \u001b[32m30.3253\u001b[0m  0.0128\n",
      "      3       \u001b[36m30.2992\u001b[0m       \u001b[32m29.2814\u001b[0m  0.0127\n",
      "      4       \u001b[36m28.8845\u001b[0m       \u001b[32m28.2412\u001b[0m  0.0154\n",
      "      5       \u001b[36m27.4425\u001b[0m       \u001b[32m27.2781\u001b[0m  0.0123\n",
      "      6       \u001b[36m25.9724\u001b[0m       \u001b[32m26.7604\u001b[0m  0.0123\n",
      "      7       \u001b[36m24.8714\u001b[0m       27.1130  0.0129\n",
      "      8       \u001b[36m24.5220\u001b[0m       27.7485  0.0143\n",
      "      9       \u001b[36m24.4824\u001b[0m       27.6155  0.0129\n",
      "     10       \u001b[36m24.2468\u001b[0m       27.0726  0.0131\n",
      "     11       \u001b[36m24.0172\u001b[0m       \u001b[32m26.7120\u001b[0m  0.0121\n",
      "     12       \u001b[36m23.9146\u001b[0m       \u001b[32m26.5895\u001b[0m  0.0162\n",
      "     13       \u001b[36m23.8349\u001b[0m       26.6118  0.0125\n",
      "     14       \u001b[36m23.7310\u001b[0m       26.7371  0.0124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     15       \u001b[36m23.6393\u001b[0m       26.9030  0.0124\n",
      "     16       \u001b[36m23.5724\u001b[0m       27.0214  0.0126\n",
      "     17       \u001b[36m23.5172\u001b[0m       27.0578  0.0125\n",
      "     18       \u001b[36m23.4698\u001b[0m       27.0392  0.0127\n",
      "     19       \u001b[36m23.4290\u001b[0m       27.0054  0.0120\n",
      "     20       \u001b[36m23.3956\u001b[0m       26.9861  0.0120\n",
      "     21       \u001b[36m23.3650\u001b[0m       26.9866  0.0127\n",
      "     22       \u001b[36m23.3349\u001b[0m       26.9990  0.0120\n",
      "     23       \u001b[36m23.3071\u001b[0m       27.0078  0.0120\n",
      "     24       \u001b[36m23.2829\u001b[0m       27.0058  0.0121\n",
      "     25       \u001b[36m23.2620\u001b[0m       26.9952  0.0117\n",
      "     26       \u001b[36m23.2435\u001b[0m       26.9807  0.0124\n",
      "     27       \u001b[36m23.2269\u001b[0m       26.9696  0.0123\n",
      "     28       \u001b[36m23.2120\u001b[0m       26.9625  0.0126\n",
      "     29       \u001b[36m23.1981\u001b[0m       26.9558  0.0119\n",
      "     30       \u001b[36m23.1855\u001b[0m       26.9472  0.0125\n",
      "     31       \u001b[36m23.1740\u001b[0m       26.9360  0.0126\n",
      "     32       \u001b[36m23.1636\u001b[0m       26.9257  0.0118\n",
      "     33       \u001b[36m23.1543\u001b[0m       26.9186  0.0124\n",
      "     34       \u001b[36m23.1459\u001b[0m       26.9133  0.0124\n",
      "     35       \u001b[36m23.1381\u001b[0m       26.9076  0.0123\n",
      "     36       \u001b[36m23.1311\u001b[0m       26.9007  0.0125\n",
      "     37       \u001b[36m23.1245\u001b[0m       26.8941  0.0126\n",
      "     38       \u001b[36m23.1185\u001b[0m       26.8902  0.0149\n",
      "     39       \u001b[36m23.1129\u001b[0m       26.8860  0.0123\n",
      "     40       \u001b[36m23.1076\u001b[0m       26.8799  0.0127\n",
      "     41       \u001b[36m23.1028\u001b[0m       26.8727  0.0131\n",
      "     42       \u001b[36m23.0982\u001b[0m       26.8662  0.0122\n",
      "     43       \u001b[36m23.0939\u001b[0m       26.8608  0.0121\n",
      "     44       \u001b[36m23.0897\u001b[0m       26.8560  0.0142\n",
      "     45       \u001b[36m23.0859\u001b[0m       26.8500  0.0119\n",
      "     46       \u001b[36m23.0822\u001b[0m       26.8436  0.0122\n",
      "     47       \u001b[36m23.0788\u001b[0m       26.8393  0.0121\n",
      "     48       \u001b[36m23.0754\u001b[0m       26.8346  0.0120\n",
      "     49       \u001b[36m23.0724\u001b[0m       26.8293  0.0116\n",
      "     50       \u001b[36m23.0693\u001b[0m       26.8246  0.0114\n",
      "     51       \u001b[36m23.0673\u001b[0m       26.8230  0.0118\n",
      "     52       \u001b[36m23.0656\u001b[0m       26.8173  0.0114\n",
      "     53       23.0691       26.8238  0.0117\n",
      "     54       23.0751       26.8182  0.0117\n",
      "     55       23.0908       26.8278  0.0115\n",
      "     56       23.0989       26.8005  0.0123\n",
      "     57       23.0862       26.7949  0.0185\n",
      "     58       \u001b[36m23.0618\u001b[0m       26.7961  0.0135\n",
      "     59       \u001b[36m23.0501\u001b[0m       26.7900  0.0124\n",
      "     60       23.0503       26.7905  0.0131\n",
      "     61       \u001b[36m23.0481\u001b[0m       26.7829  0.0135\n",
      "     62       \u001b[36m23.0454\u001b[0m       26.7791  0.0146\n",
      "     63       \u001b[36m23.0414\u001b[0m       26.7784  0.0132\n",
      "     64       \u001b[36m23.0393\u001b[0m       26.7740  0.0122\n",
      "     65       \u001b[36m23.0382\u001b[0m       26.7695  0.0119\n",
      "     66       \u001b[36m23.0360\u001b[0m       26.7630  0.0121\n",
      "     67       \u001b[36m23.0345\u001b[0m       26.7582  0.0126\n",
      "     68       \u001b[36m23.0325\u001b[0m       26.7561  0.0129\n",
      "     69       \u001b[36m23.0309\u001b[0m       26.7527  0.0120\n",
      "     70       \u001b[36m23.0294\u001b[0m       26.7496  0.0119\n",
      "     71       \u001b[36m23.0278\u001b[0m       26.7460  0.0122\n",
      "     72       \u001b[36m23.0264\u001b[0m       26.7427  0.0120\n",
      "     73       \u001b[36m23.0249\u001b[0m       26.7401  0.0119\n",
      "     74       \u001b[36m23.0235\u001b[0m       26.7373  0.0119\n",
      "     75       \u001b[36m23.0221\u001b[0m       26.7346  0.0118\n",
      "     76       \u001b[36m23.0208\u001b[0m       26.7334  0.0124\n",
      "     77       \u001b[36m23.0194\u001b[0m       26.7311  0.0127\n",
      "     78       \u001b[36m23.0181\u001b[0m       26.7287  0.0122\n",
      "     79       \u001b[36m23.0168\u001b[0m       26.7272  0.0117\n",
      "     80       \u001b[36m23.0155\u001b[0m       26.7258  0.0119\n",
      "     81       \u001b[36m23.0143\u001b[0m       26.7223  0.0125\n",
      "     82       \u001b[36m23.0131\u001b[0m       26.7192  0.0131\n",
      "     83       \u001b[36m23.0119\u001b[0m       26.7172  0.0133\n",
      "     84       \u001b[36m23.0107\u001b[0m       26.7144  0.0120\n",
      "     85       \u001b[36m23.0096\u001b[0m       26.7125  0.0117\n",
      "     86       \u001b[36m23.0084\u001b[0m       26.7119  0.0126\n",
      "     87       \u001b[36m23.0073\u001b[0m       26.7105  0.0120\n",
      "     88       \u001b[36m23.0062\u001b[0m       26.7076  0.0125\n",
      "     89       \u001b[36m23.0051\u001b[0m       26.7059  0.0125\n",
      "     90       \u001b[36m23.0041\u001b[0m       26.7062  0.0118\n",
      "     91       \u001b[36m23.0030\u001b[0m       26.7056  0.0122\n",
      "     92       \u001b[36m23.0020\u001b[0m       26.7038  0.0124\n",
      "     93       \u001b[36m23.0010\u001b[0m       26.7019  0.0122\n",
      "     94       \u001b[36m23.0000\u001b[0m       26.7008  0.0118\n",
      "     95       \u001b[36m22.9990\u001b[0m       26.6982  0.0129\n",
      "     96       \u001b[36m22.9981\u001b[0m       26.6970  0.0127\n",
      "     97       \u001b[36m22.9971\u001b[0m       26.6960  0.0120\n",
      "     98       \u001b[36m22.9962\u001b[0m       26.6939  0.0128\n",
      "     99       \u001b[36m22.9953\u001b[0m       26.6934  0.0118\n",
      "    100       \u001b[36m22.9944\u001b[0m       26.6927  0.0119\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m40.7876\u001b[0m       \u001b[32m31.7313\u001b[0m  0.0126\n",
      "      2       \u001b[36m39.0645\u001b[0m       \u001b[32m30.5532\u001b[0m  0.0123\n",
      "      3       \u001b[36m37.3120\u001b[0m       \u001b[32m29.2943\u001b[0m  0.0120\n",
      "      4       \u001b[36m35.3841\u001b[0m       \u001b[32m28.0295\u001b[0m  0.0123\n",
      "      5       \u001b[36m33.4248\u001b[0m       \u001b[32m27.0573\u001b[0m  0.0122\n",
      "      6       \u001b[36m31.6639\u001b[0m       \u001b[32m26.8178\u001b[0m  0.0120\n",
      "      7       \u001b[36m30.5494\u001b[0m       27.6069  0.0138\n",
      "      8       \u001b[36m30.3499\u001b[0m       28.5720  0.0118\n",
      "      9       30.3971       28.4862  0.0123\n",
      "     10       \u001b[36m30.0926\u001b[0m       27.7851  0.0126\n",
      "     11       \u001b[36m29.7454\u001b[0m       27.2591  0.0119\n",
      "     12       \u001b[36m29.5527\u001b[0m       27.0573  0.0122\n",
      "     13       \u001b[36m29.4155\u001b[0m       27.0931  0.0120\n",
      "     14       \u001b[36m29.2920\u001b[0m       27.2845  0.0121\n",
      "     15       \u001b[36m29.2040\u001b[0m       27.5099  0.0120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     16       \u001b[36m29.1435\u001b[0m       27.6466  0.0120\n",
      "     17       \u001b[36m29.0851\u001b[0m       27.6687  0.0122\n",
      "     18       \u001b[36m29.0204\u001b[0m       27.6271  0.0122\n",
      "     19       \u001b[36m28.9580\u001b[0m       27.5787  0.0120\n",
      "     20       \u001b[36m28.9051\u001b[0m       27.5498  0.0120\n",
      "     21       \u001b[36m28.8602\u001b[0m       27.5476  0.0124\n",
      "     22       \u001b[36m28.8221\u001b[0m       27.5684  0.0126\n",
      "     23       \u001b[36m28.7893\u001b[0m       27.5941  0.0120\n",
      "     24       \u001b[36m28.7600\u001b[0m       27.6067  0.0122\n",
      "     25       \u001b[36m28.7323\u001b[0m       27.6104  0.0117\n",
      "     26       \u001b[36m28.7063\u001b[0m       27.6150  0.0122\n",
      "     27       \u001b[36m28.6828\u001b[0m       27.6204  0.0124\n",
      "     28       \u001b[36m28.6617\u001b[0m       27.6269  0.0124\n",
      "     29       \u001b[36m28.6426\u001b[0m       27.6347  0.0130\n",
      "     30       \u001b[36m28.6256\u001b[0m       27.6424  0.0122\n",
      "     31       \u001b[36m28.6101\u001b[0m       27.6462  0.0120\n",
      "     32       \u001b[36m28.5958\u001b[0m       27.6494  0.0129\n",
      "     33       \u001b[36m28.5828\u001b[0m       27.6530  0.0195\n",
      "     34       \u001b[36m28.5707\u001b[0m       27.6553  0.0211\n",
      "     35       \u001b[36m28.5596\u001b[0m       27.6575  0.0201\n",
      "     36       \u001b[36m28.5496\u001b[0m       27.6596  0.0150\n",
      "     37       \u001b[36m28.5405\u001b[0m       27.6590  0.0129\n",
      "     38       \u001b[36m28.5320\u001b[0m       27.6557  0.0182\n",
      "     39       \u001b[36m28.5241\u001b[0m       27.6509  0.0120\n",
      "     40       \u001b[36m28.5167\u001b[0m       27.6475  0.0118\n",
      "     41       \u001b[36m28.5098\u001b[0m       27.6450  0.0117\n",
      "     42       \u001b[36m28.5034\u001b[0m       27.6433  0.0117\n",
      "     43       \u001b[36m28.4976\u001b[0m       27.6424  0.0120\n",
      "     44       \u001b[36m28.4921\u001b[0m       27.6394  0.0120\n",
      "     45       \u001b[36m28.4869\u001b[0m       27.6367  0.0128\n",
      "     46       \u001b[36m28.4819\u001b[0m       27.6389  0.0119\n",
      "     47       \u001b[36m28.4776\u001b[0m       27.6390  0.0118\n",
      "     48       \u001b[36m28.4731\u001b[0m       27.6362  0.0121\n",
      "     49       \u001b[36m28.4692\u001b[0m       27.6373  0.0116\n",
      "     50       \u001b[36m28.4651\u001b[0m       27.6389  0.0116\n",
      "     51       \u001b[36m28.4620\u001b[0m       27.6404  0.0115\n",
      "     52       \u001b[36m28.4578\u001b[0m       27.6423  0.0114\n",
      "     53       \u001b[36m28.4557\u001b[0m       27.6425  0.0118\n",
      "     54       \u001b[36m28.4514\u001b[0m       27.6424  0.0115\n",
      "     55       \u001b[36m28.4508\u001b[0m       27.6485  0.0119\n",
      "     56       \u001b[36m28.4463\u001b[0m       27.6494  0.0116\n",
      "     57       28.4476       27.6523  0.0119\n",
      "     58       \u001b[36m28.4425\u001b[0m       27.6519  0.0123\n",
      "     59       28.4467       27.6539  0.0116\n",
      "     60       \u001b[36m28.4421\u001b[0m       27.6583  0.0117\n",
      "     61       28.4544       27.6596  0.0115\n",
      "     62       28.4518       27.6693  0.0115\n",
      "     63       28.4772       27.6542  0.0114\n",
      "     64       28.4586       27.6553  0.0112\n",
      "     65       28.4620       27.6669  0.0113\n",
      "     66       \u001b[36m28.4341\u001b[0m       27.6513  0.0119\n",
      "     67       \u001b[36m28.4263\u001b[0m       27.6627  0.0117\n",
      "     68       28.4279       27.6588  0.0116\n",
      "     69       \u001b[36m28.4211\u001b[0m       27.6665  0.0113\n",
      "     70       28.4216       27.6727  0.0114\n",
      "     71       \u001b[36m28.4172\u001b[0m       27.6707  0.0124\n",
      "     72       \u001b[36m28.4155\u001b[0m       27.6730  0.0116\n",
      "     73       \u001b[36m28.4151\u001b[0m       27.6756  0.0114\n",
      "     74       \u001b[36m28.4125\u001b[0m       27.6773  0.0115\n",
      "     75       \u001b[36m28.4122\u001b[0m       27.6759  0.0114\n",
      "     76       \u001b[36m28.4102\u001b[0m       27.6747  0.0125\n",
      "     77       \u001b[36m28.4091\u001b[0m       27.6785  0.0115\n",
      "     78       \u001b[36m28.4082\u001b[0m       27.6808  0.0119\n",
      "     79       \u001b[36m28.4069\u001b[0m       27.6797  0.0114\n",
      "     80       \u001b[36m28.4060\u001b[0m       27.6773  0.0112\n",
      "     81       \u001b[36m28.4047\u001b[0m       27.6795  0.0119\n",
      "     82       \u001b[36m28.4039\u001b[0m       27.6833  0.0115\n",
      "     83       \u001b[36m28.4029\u001b[0m       27.6823  0.0117\n",
      "     84       \u001b[36m28.4019\u001b[0m       27.6812  0.0115\n",
      "     85       \u001b[36m28.4010\u001b[0m       27.6843  0.0119\n",
      "     86       \u001b[36m28.4001\u001b[0m       27.6848  0.0118\n",
      "     87       \u001b[36m28.3992\u001b[0m       27.6847  0.0116\n",
      "     88       \u001b[36m28.3983\u001b[0m       27.6893  0.0113\n",
      "     89       \u001b[36m28.3975\u001b[0m       27.6898  0.0113\n",
      "     90       \u001b[36m28.3966\u001b[0m       27.6883  0.0112\n",
      "     91       \u001b[36m28.3958\u001b[0m       27.6918  0.0117\n",
      "     92       \u001b[36m28.3951\u001b[0m       27.6932  0.0115\n",
      "     93       \u001b[36m28.3942\u001b[0m       27.6949  0.0119\n",
      "     94       \u001b[36m28.3935\u001b[0m       27.6947  0.0114\n",
      "     95       \u001b[36m28.3927\u001b[0m       27.6959  0.0113\n",
      "     96       \u001b[36m28.3920\u001b[0m       27.6964  0.0117\n",
      "     97       \u001b[36m28.3912\u001b[0m       27.6979  0.0113\n",
      "     98       \u001b[36m28.3906\u001b[0m       27.7005  0.0117\n",
      "     99       \u001b[36m28.3899\u001b[0m       27.7008  0.0111\n",
      "    100       \u001b[36m28.3892\u001b[0m       27.7017  0.0112\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m43.2149\u001b[0m       \u001b[32m45.1698\u001b[0m  0.0112\n",
      "      2       \u001b[36m42.7687\u001b[0m       \u001b[32m44.6199\u001b[0m  0.0108\n",
      "      3       \u001b[36m42.3404\u001b[0m       \u001b[32m44.0880\u001b[0m  0.0108\n",
      "      4       \u001b[36m41.9261\u001b[0m       \u001b[32m43.5698\u001b[0m  0.0108\n",
      "      5       \u001b[36m41.5236\u001b[0m       \u001b[32m43.0627\u001b[0m  0.0106\n",
      "      6       \u001b[36m41.1295\u001b[0m       \u001b[32m42.5632\u001b[0m  0.0109\n",
      "      7       \u001b[36m40.7424\u001b[0m       \u001b[32m42.0709\u001b[0m  0.0108\n",
      "      8       \u001b[36m40.3617\u001b[0m       \u001b[32m41.5848\u001b[0m  0.0110\n",
      "      9       \u001b[36m39.9873\u001b[0m       \u001b[32m41.1039\u001b[0m  0.0108\n",
      "     10       \u001b[36m39.6189\u001b[0m       \u001b[32m40.6285\u001b[0m  0.0106\n",
      "     11       \u001b[36m39.2566\u001b[0m       \u001b[32m40.1595\u001b[0m  0.0150\n",
      "     12       \u001b[36m38.9008\u001b[0m       \u001b[32m39.6972\u001b[0m  0.0133\n",
      "     13       \u001b[36m38.5520\u001b[0m       \u001b[32m39.2423\u001b[0m  0.0119\n",
      "     14       \u001b[36m38.2112\u001b[0m       \u001b[32m38.7956\u001b[0m  0.0115\n",
      "     15       \u001b[36m37.8792\u001b[0m       \u001b[32m38.3579\u001b[0m  0.0113\n",
      "     16       \u001b[36m37.5563\u001b[0m       \u001b[32m37.9300\u001b[0m  0.0110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m37.2434\u001b[0m       \u001b[32m37.5126\u001b[0m  0.0139\n",
      "     18       \u001b[36m36.9409\u001b[0m       \u001b[32m37.1057\u001b[0m  0.0113\n",
      "     19       \u001b[36m36.6488\u001b[0m       \u001b[32m36.7097\u001b[0m  0.0113\n",
      "     20       \u001b[36m36.3676\u001b[0m       \u001b[32m36.3247\u001b[0m  0.0110\n",
      "     21       \u001b[36m36.0974\u001b[0m       \u001b[32m35.9509\u001b[0m  0.0110\n",
      "     22       \u001b[36m35.8381\u001b[0m       \u001b[32m35.5883\u001b[0m  0.0111\n",
      "     23       \u001b[36m35.5898\u001b[0m       \u001b[32m35.2363\u001b[0m  0.0110\n",
      "     24       \u001b[36m35.3522\u001b[0m       \u001b[32m34.8957\u001b[0m  0.0109\n",
      "     25       \u001b[36m35.1259\u001b[0m       \u001b[32m34.5673\u001b[0m  0.0108\n",
      "     26       \u001b[36m34.9110\u001b[0m       \u001b[32m34.2510\u001b[0m  0.0108\n",
      "     27       \u001b[36m34.7076\u001b[0m       \u001b[32m33.9476\u001b[0m  0.0108\n",
      "     28       \u001b[36m34.5159\u001b[0m       \u001b[32m33.6575\u001b[0m  0.0109\n",
      "     29       \u001b[36m34.3361\u001b[0m       \u001b[32m33.3812\u001b[0m  0.0110\n",
      "     30       \u001b[36m34.1683\u001b[0m       \u001b[32m33.1192\u001b[0m  0.0108\n",
      "     31       \u001b[36m34.0126\u001b[0m       \u001b[32m32.8717\u001b[0m  0.0115\n",
      "     32       \u001b[36m33.8689\u001b[0m       \u001b[32m32.6395\u001b[0m  0.0109\n",
      "     33       \u001b[36m33.7370\u001b[0m       \u001b[32m32.4220\u001b[0m  0.0106\n",
      "     34       \u001b[36m33.6163\u001b[0m       \u001b[32m32.2192\u001b[0m  0.0110\n",
      "     35       \u001b[36m33.5065\u001b[0m       \u001b[32m32.0310\u001b[0m  0.0106\n",
      "     36       \u001b[36m33.4073\u001b[0m       \u001b[32m31.8571\u001b[0m  0.0105\n",
      "     37       \u001b[36m33.3180\u001b[0m       \u001b[32m31.6971\u001b[0m  0.0108\n",
      "     38       \u001b[36m33.2379\u001b[0m       \u001b[32m31.5505\u001b[0m  0.0109\n",
      "     39       \u001b[36m33.1665\u001b[0m       \u001b[32m31.4167\u001b[0m  0.0108\n",
      "     40       \u001b[36m33.1030\u001b[0m       \u001b[32m31.2952\u001b[0m  0.0107\n",
      "     41       \u001b[36m33.0468\u001b[0m       \u001b[32m31.1849\u001b[0m  0.0106\n",
      "     42       \u001b[36m32.9969\u001b[0m       \u001b[32m31.0850\u001b[0m  0.0109\n",
      "     43       \u001b[36m32.9527\u001b[0m       \u001b[32m30.9948\u001b[0m  0.0111\n",
      "     44       \u001b[36m32.9136\u001b[0m       \u001b[32m30.9134\u001b[0m  0.0107\n",
      "     45       \u001b[36m32.8789\u001b[0m       \u001b[32m30.8400\u001b[0m  0.0107\n",
      "     46       \u001b[36m32.8481\u001b[0m       \u001b[32m30.7739\u001b[0m  0.0109\n",
      "     47       \u001b[36m32.8206\u001b[0m       \u001b[32m30.7145\u001b[0m  0.0115\n",
      "     48       \u001b[36m32.7961\u001b[0m       \u001b[32m30.6611\u001b[0m  0.0107\n",
      "     49       \u001b[36m32.7743\u001b[0m       \u001b[32m30.6129\u001b[0m  0.0110\n",
      "     50       \u001b[36m32.7547\u001b[0m       \u001b[32m30.5695\u001b[0m  0.0106\n",
      "     51       \u001b[36m32.7370\u001b[0m       \u001b[32m30.5305\u001b[0m  0.0106\n",
      "     52       \u001b[36m32.7209\u001b[0m       \u001b[32m30.4953\u001b[0m  0.0110\n",
      "     53       \u001b[36m32.7063\u001b[0m       \u001b[32m30.4634\u001b[0m  0.0108\n",
      "     54       \u001b[36m32.6929\u001b[0m       \u001b[32m30.4346\u001b[0m  0.0108\n",
      "     55       \u001b[36m32.6806\u001b[0m       \u001b[32m30.4086\u001b[0m  0.0106\n",
      "     56       \u001b[36m32.6692\u001b[0m       \u001b[32m30.3849\u001b[0m  0.0106\n",
      "     57       \u001b[36m32.6586\u001b[0m       \u001b[32m30.3634\u001b[0m  0.0107\n",
      "     58       \u001b[36m32.6487\u001b[0m       \u001b[32m30.3438\u001b[0m  0.0110\n",
      "     59       \u001b[36m32.6395\u001b[0m       \u001b[32m30.3258\u001b[0m  0.0108\n",
      "     60       \u001b[36m32.6307\u001b[0m       \u001b[32m30.3094\u001b[0m  0.0113\n",
      "     61       \u001b[36m32.6225\u001b[0m       \u001b[32m30.2944\u001b[0m  0.0107\n",
      "     62       \u001b[36m32.6146\u001b[0m       \u001b[32m30.2805\u001b[0m  0.0114\n",
      "     63       \u001b[36m32.6071\u001b[0m       \u001b[32m30.2677\u001b[0m  0.0124\n",
      "     64       \u001b[36m32.6000\u001b[0m       \u001b[32m30.2559\u001b[0m  0.0111\n",
      "     65       \u001b[36m32.5932\u001b[0m       \u001b[32m30.2449\u001b[0m  0.0108\n",
      "     66       \u001b[36m32.5867\u001b[0m       \u001b[32m30.2347\u001b[0m  0.0109\n",
      "     67       \u001b[36m32.5805\u001b[0m       \u001b[32m30.2251\u001b[0m  0.0109\n",
      "     68       \u001b[36m32.5745\u001b[0m       \u001b[32m30.2162\u001b[0m  0.0113\n",
      "     69       \u001b[36m32.5687\u001b[0m       \u001b[32m30.2079\u001b[0m  0.0113\n",
      "     70       \u001b[36m32.5631\u001b[0m       \u001b[32m30.2000\u001b[0m  0.0105\n",
      "     71       \u001b[36m32.5577\u001b[0m       \u001b[32m30.1926\u001b[0m  0.0112\n",
      "     72       \u001b[36m32.5525\u001b[0m       \u001b[32m30.1856\u001b[0m  0.0113\n",
      "     73       \u001b[36m32.5474\u001b[0m       \u001b[32m30.1790\u001b[0m  0.0110\n",
      "     74       \u001b[36m32.5425\u001b[0m       \u001b[32m30.1727\u001b[0m  0.0106\n",
      "     75       \u001b[36m32.5377\u001b[0m       \u001b[32m30.1668\u001b[0m  0.0109\n",
      "     76       \u001b[36m32.5331\u001b[0m       \u001b[32m30.1611\u001b[0m  0.0105\n",
      "     77       \u001b[36m32.5287\u001b[0m       \u001b[32m30.1558\u001b[0m  0.0106\n",
      "     78       \u001b[36m32.5243\u001b[0m       \u001b[32m30.1506\u001b[0m  0.0112\n",
      "     79       \u001b[36m32.5201\u001b[0m       \u001b[32m30.1457\u001b[0m  0.0106\n",
      "     80       \u001b[36m32.5159\u001b[0m       \u001b[32m30.1410\u001b[0m  0.0105\n",
      "     81       \u001b[36m32.5119\u001b[0m       \u001b[32m30.1365\u001b[0m  0.0104\n",
      "     82       \u001b[36m32.5079\u001b[0m       \u001b[32m30.1321\u001b[0m  0.0108\n",
      "     83       \u001b[36m32.5041\u001b[0m       \u001b[32m30.1279\u001b[0m  0.0110\n",
      "     84       \u001b[36m32.5004\u001b[0m       \u001b[32m30.1239\u001b[0m  0.0169\n",
      "     85       \u001b[36m32.4967\u001b[0m       \u001b[32m30.1200\u001b[0m  0.0165\n",
      "     86       \u001b[36m32.4931\u001b[0m       \u001b[32m30.1162\u001b[0m  0.0110\n",
      "     87       \u001b[36m32.4896\u001b[0m       \u001b[32m30.1126\u001b[0m  0.0111\n",
      "     88       \u001b[36m32.4862\u001b[0m       \u001b[32m30.1090\u001b[0m  0.0109\n",
      "     89       \u001b[36m32.4829\u001b[0m       \u001b[32m30.1056\u001b[0m  0.0106\n",
      "     90       \u001b[36m32.4796\u001b[0m       \u001b[32m30.1022\u001b[0m  0.0107\n",
      "     91       \u001b[36m32.4764\u001b[0m       \u001b[32m30.0990\u001b[0m  0.0111\n",
      "     92       \u001b[36m32.4732\u001b[0m       \u001b[32m30.0958\u001b[0m  0.0111\n",
      "     93       \u001b[36m32.4702\u001b[0m       \u001b[32m30.0927\u001b[0m  0.0110\n",
      "     94       \u001b[36m32.4672\u001b[0m       \u001b[32m30.0897\u001b[0m  0.0110\n",
      "     95       \u001b[36m32.4642\u001b[0m       \u001b[32m30.0868\u001b[0m  0.0115\n",
      "     96       \u001b[36m32.4613\u001b[0m       \u001b[32m30.0839\u001b[0m  0.0155\n",
      "     97       \u001b[36m32.4585\u001b[0m       \u001b[32m30.0811\u001b[0m  0.0122\n",
      "     98       \u001b[36m32.4557\u001b[0m       \u001b[32m30.0784\u001b[0m  0.0116\n",
      "     99       \u001b[36m32.4530\u001b[0m       \u001b[32m30.0757\u001b[0m  0.0117\n",
      "    100       \u001b[36m32.4504\u001b[0m       \u001b[32m30.0731\u001b[0m  0.0118\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m34.2532\u001b[0m       \u001b[32m32.7669\u001b[0m  0.0123\n",
      "      2       \u001b[36m33.9098\u001b[0m       \u001b[32m32.5023\u001b[0m  0.0118\n",
      "      3       \u001b[36m33.5726\u001b[0m       \u001b[32m32.2426\u001b[0m  0.0112\n",
      "      4       \u001b[36m33.2402\u001b[0m       \u001b[32m31.9871\u001b[0m  0.0114\n",
      "      5       \u001b[36m32.9113\u001b[0m       \u001b[32m31.7341\u001b[0m  0.0116\n",
      "      6       \u001b[36m32.5838\u001b[0m       \u001b[32m31.4832\u001b[0m  0.0114\n",
      "      7       \u001b[36m32.2567\u001b[0m       \u001b[32m31.2331\u001b[0m  0.0117\n",
      "      8       \u001b[36m31.9287\u001b[0m       \u001b[32m30.9834\u001b[0m  0.0109\n",
      "      9       \u001b[36m31.5997\u001b[0m       \u001b[32m30.7336\u001b[0m  0.0115\n",
      "     10       \u001b[36m31.2690\u001b[0m       \u001b[32m30.4841\u001b[0m  0.0114\n",
      "     11       \u001b[36m30.9365\u001b[0m       \u001b[32m30.2345\u001b[0m  0.0112\n",
      "     12       \u001b[36m30.6019\u001b[0m       \u001b[32m29.9851\u001b[0m  0.0116\n",
      "     13       \u001b[36m30.2655\u001b[0m       \u001b[32m29.7366\u001b[0m  0.0110\n",
      "     14       \u001b[36m29.9280\u001b[0m       \u001b[32m29.4892\u001b[0m  0.0112\n",
      "     15       \u001b[36m29.5893\u001b[0m       \u001b[32m29.2434\u001b[0m  0.0112\n",
      "     16       \u001b[36m29.2502\u001b[0m       \u001b[32m29.0003\u001b[0m  0.0110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m28.9119\u001b[0m       \u001b[32m28.7609\u001b[0m  0.0112\n",
      "     18       \u001b[36m28.5756\u001b[0m       \u001b[32m28.5264\u001b[0m  0.0109\n",
      "     19       \u001b[36m28.2424\u001b[0m       \u001b[32m28.2979\u001b[0m  0.0111\n",
      "     20       \u001b[36m27.9140\u001b[0m       \u001b[32m28.0772\u001b[0m  0.0109\n",
      "     21       \u001b[36m27.5918\u001b[0m       \u001b[32m27.8655\u001b[0m  0.0110\n",
      "     22       \u001b[36m27.2783\u001b[0m       \u001b[32m27.6644\u001b[0m  0.0111\n",
      "     23       \u001b[36m26.9753\u001b[0m       \u001b[32m27.4750\u001b[0m  0.0106\n",
      "     24       \u001b[36m26.6843\u001b[0m       \u001b[32m27.2988\u001b[0m  0.0110\n",
      "     25       \u001b[36m26.4066\u001b[0m       \u001b[32m27.1364\u001b[0m  0.0109\n",
      "     26       \u001b[36m26.1431\u001b[0m       \u001b[32m26.9885\u001b[0m  0.0107\n",
      "     27       \u001b[36m25.8949\u001b[0m       \u001b[32m26.8555\u001b[0m  0.0112\n",
      "     28       \u001b[36m25.6626\u001b[0m       \u001b[32m26.7377\u001b[0m  0.0109\n",
      "     29       \u001b[36m25.4466\u001b[0m       \u001b[32m26.6348\u001b[0m  0.0106\n",
      "     30       \u001b[36m25.2472\u001b[0m       \u001b[32m26.5466\u001b[0m  0.0106\n",
      "     31       \u001b[36m25.0640\u001b[0m       \u001b[32m26.4722\u001b[0m  0.0105\n",
      "     32       \u001b[36m24.8966\u001b[0m       \u001b[32m26.4109\u001b[0m  0.0107\n",
      "     33       \u001b[36m24.7443\u001b[0m       \u001b[32m26.3617\u001b[0m  0.0109\n",
      "     34       \u001b[36m24.6063\u001b[0m       \u001b[32m26.3237\u001b[0m  0.0107\n",
      "     35       \u001b[36m24.4819\u001b[0m       \u001b[32m26.2956\u001b[0m  0.0105\n",
      "     36       \u001b[36m24.3701\u001b[0m       \u001b[32m26.2763\u001b[0m  0.0105\n",
      "     37       \u001b[36m24.2701\u001b[0m       \u001b[32m26.2646\u001b[0m  0.0118\n",
      "     38       \u001b[36m24.1809\u001b[0m       \u001b[32m26.2592\u001b[0m  0.0109\n",
      "     39       \u001b[36m24.1016\u001b[0m       \u001b[32m26.2591\u001b[0m  0.0110\n",
      "     40       \u001b[36m24.0312\u001b[0m       26.2633  0.0106\n",
      "     41       \u001b[36m23.9688\u001b[0m       26.2708  0.0105\n",
      "     42       \u001b[36m23.9136\u001b[0m       26.2808  0.0107\n",
      "     43       \u001b[36m23.8646\u001b[0m       26.2926  0.0112\n",
      "     44       \u001b[36m23.8211\u001b[0m       26.3056  0.0111\n",
      "     45       \u001b[36m23.7824\u001b[0m       26.3193  0.0107\n",
      "     46       \u001b[36m23.7478\u001b[0m       26.3332  0.0104\n",
      "     47       \u001b[36m23.7169\u001b[0m       26.3470  0.0110\n",
      "     48       \u001b[36m23.6892\u001b[0m       26.3605  0.0107\n",
      "     49       \u001b[36m23.6641\u001b[0m       26.3734  0.0108\n",
      "     50       \u001b[36m23.6414\u001b[0m       26.3856  0.0108\n",
      "     51       \u001b[36m23.6208\u001b[0m       26.3972  0.0107\n",
      "     52       \u001b[36m23.6020\u001b[0m       26.4079  0.0111\n",
      "     53       \u001b[36m23.5848\u001b[0m       26.4178  0.0109\n",
      "     54       \u001b[36m23.5689\u001b[0m       26.4269  0.0108\n",
      "     55       \u001b[36m23.5543\u001b[0m       26.4352  0.0107\n",
      "     56       \u001b[36m23.5406\u001b[0m       26.4427  0.0107\n",
      "     57       \u001b[36m23.5279\u001b[0m       26.4494  0.0109\n",
      "     58       \u001b[36m23.5160\u001b[0m       26.4554  0.0107\n",
      "     59       \u001b[36m23.5048\u001b[0m       26.4609  0.0116\n",
      "     60       \u001b[36m23.4942\u001b[0m       26.4656  0.0107\n",
      "     61       \u001b[36m23.4842\u001b[0m       26.4699  0.0106\n",
      "     62       \u001b[36m23.4747\u001b[0m       26.4736  0.0112\n",
      "     63       \u001b[36m23.4656\u001b[0m       26.4768  0.0112\n",
      "     64       \u001b[36m23.4570\u001b[0m       26.4796  0.0108\n",
      "     65       \u001b[36m23.4488\u001b[0m       26.4821  0.0106\n",
      "     66       \u001b[36m23.4409\u001b[0m       26.4842  0.0109\n",
      "     67       \u001b[36m23.4333\u001b[0m       26.4859  0.0111\n",
      "     68       \u001b[36m23.4260\u001b[0m       26.4874  0.0108\n",
      "     69       \u001b[36m23.4190\u001b[0m       26.4887  0.0109\n",
      "     70       \u001b[36m23.4122\u001b[0m       26.4896  0.0106\n",
      "     71       \u001b[36m23.4057\u001b[0m       26.4904  0.0107\n",
      "     72       \u001b[36m23.3994\u001b[0m       26.4911  0.0112\n",
      "     73       \u001b[36m23.3932\u001b[0m       26.4916  0.0111\n",
      "     74       \u001b[36m23.3873\u001b[0m       26.4920  0.0107\n",
      "     75       \u001b[36m23.3816\u001b[0m       26.4923  0.0108\n",
      "     76       \u001b[36m23.3760\u001b[0m       26.4925  0.0108\n",
      "     77       \u001b[36m23.3705\u001b[0m       26.4926  0.0110\n",
      "     78       \u001b[36m23.3653\u001b[0m       26.4926  0.0110\n",
      "     79       \u001b[36m23.3601\u001b[0m       26.4925  0.0108\n",
      "     80       \u001b[36m23.3551\u001b[0m       26.4924  0.0166\n",
      "     81       \u001b[36m23.3502\u001b[0m       26.4922  0.0175\n",
      "     82       \u001b[36m23.3454\u001b[0m       26.4920  0.0187\n",
      "     83       \u001b[36m23.3407\u001b[0m       26.4917  0.0132\n",
      "     84       \u001b[36m23.3362\u001b[0m       26.4914  0.0133\n",
      "     85       \u001b[36m23.3317\u001b[0m       26.4910  0.0173\n",
      "     86       \u001b[36m23.3274\u001b[0m       26.4907  0.0145\n",
      "     87       \u001b[36m23.3232\u001b[0m       26.4904  0.0128\n",
      "     88       \u001b[36m23.3190\u001b[0m       26.4901  0.0114\n",
      "     89       \u001b[36m23.3150\u001b[0m       26.4897  0.0131\n",
      "     90       \u001b[36m23.3110\u001b[0m       26.4893  0.0143\n",
      "     91       \u001b[36m23.3071\u001b[0m       26.4889  0.0141\n",
      "     92       \u001b[36m23.3033\u001b[0m       26.4885  0.0132\n",
      "     93       \u001b[36m23.2996\u001b[0m       26.4881  0.0139\n",
      "     94       \u001b[36m23.2959\u001b[0m       26.4877  0.0114\n",
      "     95       \u001b[36m23.2924\u001b[0m       26.4873  0.0141\n",
      "     96       \u001b[36m23.2889\u001b[0m       26.4869  0.0136\n",
      "     97       \u001b[36m23.2855\u001b[0m       26.4865  0.0110\n",
      "     98       \u001b[36m23.2821\u001b[0m       26.4861  0.0110\n",
      "     99       \u001b[36m23.2789\u001b[0m       26.4857  0.0111\n",
      "    100       \u001b[36m23.2757\u001b[0m       26.4853  0.0110\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m41.6225\u001b[0m       \u001b[32m32.9495\u001b[0m  0.0111\n",
      "      2       \u001b[36m41.1634\u001b[0m       \u001b[32m32.6208\u001b[0m  0.0109\n",
      "      3       \u001b[36m40.7134\u001b[0m       \u001b[32m32.2986\u001b[0m  0.0109\n",
      "      4       \u001b[36m40.2684\u001b[0m       \u001b[32m31.9827\u001b[0m  0.0108\n",
      "      5       \u001b[36m39.8290\u001b[0m       \u001b[32m31.6710\u001b[0m  0.0110\n",
      "      6       \u001b[36m39.3920\u001b[0m       \u001b[32m31.3615\u001b[0m  0.0111\n",
      "      7       \u001b[36m38.9532\u001b[0m       \u001b[32m31.0524\u001b[0m  0.0109\n",
      "      8       \u001b[36m38.5114\u001b[0m       \u001b[32m30.7438\u001b[0m  0.0110\n",
      "      9       \u001b[36m38.0656\u001b[0m       \u001b[32m30.4350\u001b[0m  0.0109\n",
      "     10       \u001b[36m37.6148\u001b[0m       \u001b[32m30.1256\u001b[0m  0.0108\n",
      "     11       \u001b[36m37.1583\u001b[0m       \u001b[32m29.8160\u001b[0m  0.0110\n",
      "     12       \u001b[36m36.6971\u001b[0m       \u001b[32m29.5070\u001b[0m  0.0109\n",
      "     13       \u001b[36m36.2317\u001b[0m       \u001b[32m29.1995\u001b[0m  0.0107\n",
      "     14       \u001b[36m35.7633\u001b[0m       \u001b[32m28.8957\u001b[0m  0.0110\n",
      "     15       \u001b[36m35.2940\u001b[0m       \u001b[32m28.5971\u001b[0m  0.0109\n",
      "     16       \u001b[36m34.8259\u001b[0m       \u001b[32m28.3062\u001b[0m  0.0112\n",
      "     17       \u001b[36m34.3618\u001b[0m       \u001b[32m28.0259\u001b[0m  0.0110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     18       \u001b[36m33.9051\u001b[0m       \u001b[32m27.7590\u001b[0m  0.0122\n",
      "     19       \u001b[36m33.4592\u001b[0m       \u001b[32m27.5084\u001b[0m  0.0115\n",
      "     20       \u001b[36m33.0275\u001b[0m       \u001b[32m27.2769\u001b[0m  0.0115\n",
      "     21       \u001b[36m32.6132\u001b[0m       \u001b[32m27.0669\u001b[0m  0.0112\n",
      "     22       \u001b[36m32.2193\u001b[0m       \u001b[32m26.8804\u001b[0m  0.0109\n",
      "     23       \u001b[36m31.8485\u001b[0m       \u001b[32m26.7187\u001b[0m  0.0108\n",
      "     24       \u001b[36m31.5026\u001b[0m       \u001b[32m26.5826\u001b[0m  0.0113\n",
      "     25       \u001b[36m31.1830\u001b[0m       \u001b[32m26.4724\u001b[0m  0.0114\n",
      "     26       \u001b[36m30.8906\u001b[0m       \u001b[32m26.3876\u001b[0m  0.0110\n",
      "     27       \u001b[36m30.6256\u001b[0m       \u001b[32m26.3271\u001b[0m  0.0110\n",
      "     28       \u001b[36m30.3878\u001b[0m       \u001b[32m26.2895\u001b[0m  0.0108\n",
      "     29       \u001b[36m30.1765\u001b[0m       \u001b[32m26.2727\u001b[0m  0.0111\n",
      "     30       \u001b[36m29.9903\u001b[0m       26.2740  0.0109\n",
      "     31       \u001b[36m29.8280\u001b[0m       26.2908  0.0107\n",
      "     32       \u001b[36m29.6875\u001b[0m       26.3202  0.0110\n",
      "     33       \u001b[36m29.5670\u001b[0m       26.3594  0.0106\n",
      "     34       \u001b[36m29.4643\u001b[0m       26.4059  0.0108\n",
      "     35       \u001b[36m29.3773\u001b[0m       26.4571  0.0111\n",
      "     36       \u001b[36m29.3040\u001b[0m       26.5108  0.0108\n",
      "     37       \u001b[36m29.2423\u001b[0m       26.5653  0.0114\n",
      "     38       \u001b[36m29.1904\u001b[0m       26.6192  0.0109\n",
      "     39       \u001b[36m29.1467\u001b[0m       26.6713  0.0107\n",
      "     40       \u001b[36m29.1097\u001b[0m       26.7206  0.0109\n",
      "     41       \u001b[36m29.0783\u001b[0m       26.7668  0.0110\n",
      "     42       \u001b[36m29.0514\u001b[0m       26.8095  0.0110\n",
      "     43       \u001b[36m29.0281\u001b[0m       26.8485  0.0106\n",
      "     44       \u001b[36m29.0077\u001b[0m       26.8839  0.0107\n",
      "     45       \u001b[36m28.9897\u001b[0m       26.9157  0.0112\n",
      "     46       \u001b[36m28.9735\u001b[0m       26.9440  0.0112\n",
      "     47       \u001b[36m28.9588\u001b[0m       26.9691  0.0113\n",
      "     48       \u001b[36m28.9454\u001b[0m       26.9912  0.0105\n",
      "     49       \u001b[36m28.9329\u001b[0m       27.0105  0.0106\n",
      "     50       \u001b[36m28.9212\u001b[0m       27.0274  0.0110\n",
      "     51       \u001b[36m28.9103\u001b[0m       27.0420  0.0114\n",
      "     52       \u001b[36m28.9000\u001b[0m       27.0545  0.0109\n",
      "     53       \u001b[36m28.8902\u001b[0m       27.0653  0.0106\n",
      "     54       \u001b[36m28.8809\u001b[0m       27.0744  0.0106\n",
      "     55       \u001b[36m28.8719\u001b[0m       27.0823  0.0119\n",
      "     56       \u001b[36m28.8632\u001b[0m       27.0889  0.0112\n",
      "     57       \u001b[36m28.8549\u001b[0m       27.0944  0.0108\n",
      "     58       \u001b[36m28.8468\u001b[0m       27.0991  0.0111\n",
      "     59       \u001b[36m28.8390\u001b[0m       27.1029  0.0109\n",
      "     60       \u001b[36m28.8315\u001b[0m       27.1060  0.0118\n",
      "     61       \u001b[36m28.8241\u001b[0m       27.1086  0.0188\n",
      "     62       \u001b[36m28.8170\u001b[0m       27.1106  0.0122\n",
      "     63       \u001b[36m28.8100\u001b[0m       27.1122  0.0119\n",
      "     64       \u001b[36m28.8033\u001b[0m       27.1134  0.0126\n",
      "     65       \u001b[36m28.7967\u001b[0m       27.1142  0.0187\n",
      "     66       \u001b[36m28.7903\u001b[0m       27.1148  0.0124\n",
      "     67       \u001b[36m28.7841\u001b[0m       27.1153  0.0123\n",
      "     68       \u001b[36m28.7781\u001b[0m       27.1157  0.0115\n",
      "     69       \u001b[36m28.7723\u001b[0m       27.1159  0.0112\n",
      "     70       \u001b[36m28.7667\u001b[0m       27.1158  0.0120\n",
      "     71       \u001b[36m28.7611\u001b[0m       27.1155  0.0115\n",
      "     72       \u001b[36m28.7557\u001b[0m       27.1151  0.0114\n",
      "     73       \u001b[36m28.7504\u001b[0m       27.1145  0.0112\n",
      "     74       \u001b[36m28.7453\u001b[0m       27.1139  0.0113\n",
      "     75       \u001b[36m28.7402\u001b[0m       27.1132  0.0127\n",
      "     76       \u001b[36m28.7353\u001b[0m       27.1124  0.0125\n",
      "     77       \u001b[36m28.7305\u001b[0m       27.1116  0.0124\n",
      "     78       \u001b[36m28.7257\u001b[0m       27.1107  0.0122\n",
      "     79       \u001b[36m28.7211\u001b[0m       27.1098  0.0109\n",
      "     80       \u001b[36m28.7166\u001b[0m       27.1090  0.0111\n",
      "     81       \u001b[36m28.7123\u001b[0m       27.1082  0.0110\n",
      "     82       \u001b[36m28.7080\u001b[0m       27.1074  0.0113\n",
      "     83       \u001b[36m28.7039\u001b[0m       27.1066  0.0112\n",
      "     84       \u001b[36m28.6999\u001b[0m       27.1059  0.0112\n",
      "     85       \u001b[36m28.6959\u001b[0m       27.1050  0.0113\n",
      "     86       \u001b[36m28.6921\u001b[0m       27.1043  0.0108\n",
      "     87       \u001b[36m28.6883\u001b[0m       27.1035  0.0112\n",
      "     88       \u001b[36m28.6846\u001b[0m       27.1027  0.0113\n",
      "     89       \u001b[36m28.6810\u001b[0m       27.1019  0.0116\n",
      "     90       \u001b[36m28.6776\u001b[0m       27.1012  0.0121\n",
      "     91       \u001b[36m28.6742\u001b[0m       27.1004  0.0115\n",
      "     92       \u001b[36m28.6709\u001b[0m       27.0997  0.0115\n",
      "     93       \u001b[36m28.6676\u001b[0m       27.0990  0.0112\n",
      "     94       \u001b[36m28.6645\u001b[0m       27.0982  0.0114\n",
      "     95       \u001b[36m28.6613\u001b[0m       27.0974  0.0121\n",
      "     96       \u001b[36m28.6583\u001b[0m       27.0967  0.0115\n",
      "     97       \u001b[36m28.6553\u001b[0m       27.0960  0.0117\n",
      "     98       \u001b[36m28.6523\u001b[0m       27.0953  0.0117\n",
      "     99       \u001b[36m28.6494\u001b[0m       27.0945  0.0108\n",
      "    100       \u001b[36m28.6466\u001b[0m       27.0939  0.0118\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m41.5837\u001b[0m       \u001b[32m41.9822\u001b[0m  0.0136\n",
      "      2       \u001b[36m40.0510\u001b[0m       \u001b[32m40.2571\u001b[0m  0.0121\n",
      "      3       \u001b[36m38.6603\u001b[0m       \u001b[32m38.0154\u001b[0m  0.0116\n",
      "      4       \u001b[36m36.8352\u001b[0m       \u001b[32m35.1163\u001b[0m  0.0120\n",
      "      5       \u001b[36m34.8566\u001b[0m       \u001b[32m32.3701\u001b[0m  0.0130\n",
      "      6       \u001b[36m33.6201\u001b[0m       \u001b[32m30.9287\u001b[0m  0.0115\n",
      "      7       \u001b[36m33.5440\u001b[0m       \u001b[32m30.6473\u001b[0m  0.0124\n",
      "      8       \u001b[36m33.4545\u001b[0m       30.6627  0.0120\n",
      "      9       \u001b[36m33.1055\u001b[0m       30.9558  0.0123\n",
      "     10       \u001b[36m32.9609\u001b[0m       31.1249  0.0123\n",
      "     11       \u001b[36m32.8919\u001b[0m       30.9991  0.0121\n",
      "     12       \u001b[36m32.7936\u001b[0m       30.7479  0.0124\n",
      "     13       \u001b[36m32.7053\u001b[0m       \u001b[32m30.5436\u001b[0m  0.0125\n",
      "     14       \u001b[36m32.6490\u001b[0m       \u001b[32m30.4598\u001b[0m  0.0126\n",
      "     15       \u001b[36m32.6087\u001b[0m       \u001b[32m30.4538\u001b[0m  0.0123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     16       \u001b[36m32.5640\u001b[0m       30.4720  0.0131\n",
      "     17       \u001b[36m32.5237\u001b[0m       30.4916  0.0128\n",
      "     18       \u001b[36m32.4899\u001b[0m       30.4879  0.0123\n",
      "     19       \u001b[36m32.4573\u001b[0m       \u001b[32m30.4527\u001b[0m  0.0124\n",
      "     20       \u001b[36m32.4280\u001b[0m       \u001b[32m30.4084\u001b[0m  0.0139\n",
      "     21       \u001b[36m32.4032\u001b[0m       \u001b[32m30.3892\u001b[0m  0.0125\n",
      "     22       \u001b[36m32.3821\u001b[0m       30.3935  0.0119\n",
      "     23       \u001b[36m32.3628\u001b[0m       30.3949  0.0121\n",
      "     24       \u001b[36m32.3452\u001b[0m       30.3903  0.0136\n",
      "     25       \u001b[36m32.3296\u001b[0m       \u001b[32m30.3828\u001b[0m  0.0137\n",
      "     26       \u001b[36m32.3154\u001b[0m       \u001b[32m30.3697\u001b[0m  0.0117\n",
      "     27       \u001b[36m32.3026\u001b[0m       \u001b[32m30.3579\u001b[0m  0.0132\n",
      "     28       \u001b[36m32.2914\u001b[0m       \u001b[32m30.3531\u001b[0m  0.0128\n",
      "     29       \u001b[36m32.2811\u001b[0m       \u001b[32m30.3494\u001b[0m  0.0127\n",
      "     30       \u001b[36m32.2715\u001b[0m       \u001b[32m30.3440\u001b[0m  0.0123\n",
      "     31       \u001b[36m32.2627\u001b[0m       \u001b[32m30.3382\u001b[0m  0.0125\n",
      "     32       \u001b[36m32.2545\u001b[0m       \u001b[32m30.3307\u001b[0m  0.0136\n",
      "     33       \u001b[36m32.2470\u001b[0m       \u001b[32m30.3254\u001b[0m  0.0118\n",
      "     34       \u001b[36m32.2400\u001b[0m       \u001b[32m30.3213\u001b[0m  0.0122\n",
      "     35       \u001b[36m32.2336\u001b[0m       \u001b[32m30.3191\u001b[0m  0.0131\n",
      "     36       \u001b[36m32.2275\u001b[0m       \u001b[32m30.3148\u001b[0m  0.0142\n",
      "     37       \u001b[36m32.2217\u001b[0m       \u001b[32m30.3102\u001b[0m  0.0136\n",
      "     38       \u001b[36m32.2162\u001b[0m       \u001b[32m30.3066\u001b[0m  0.0158\n",
      "     39       \u001b[36m32.2111\u001b[0m       \u001b[32m30.3037\u001b[0m  0.0182\n",
      "     40       \u001b[36m32.2061\u001b[0m       \u001b[32m30.2999\u001b[0m  0.0155\n",
      "     41       \u001b[36m32.2014\u001b[0m       \u001b[32m30.2961\u001b[0m  0.0157\n",
      "     42       \u001b[36m32.1970\u001b[0m       \u001b[32m30.2920\u001b[0m  0.0168\n",
      "     43       \u001b[36m32.1927\u001b[0m       \u001b[32m30.2892\u001b[0m  0.0133\n",
      "     44       \u001b[36m32.1887\u001b[0m       \u001b[32m30.2841\u001b[0m  0.0123\n",
      "     45       \u001b[36m32.1849\u001b[0m       \u001b[32m30.2802\u001b[0m  0.0125\n",
      "     46       \u001b[36m32.1813\u001b[0m       \u001b[32m30.2762\u001b[0m  0.0122\n",
      "     47       \u001b[36m32.1778\u001b[0m       \u001b[32m30.2737\u001b[0m  0.0123\n",
      "     48       \u001b[36m32.1745\u001b[0m       \u001b[32m30.2696\u001b[0m  0.0164\n",
      "     49       \u001b[36m32.1712\u001b[0m       \u001b[32m30.2673\u001b[0m  0.0124\n",
      "     50       \u001b[36m32.1680\u001b[0m       \u001b[32m30.2633\u001b[0m  0.0120\n",
      "     51       \u001b[36m32.1650\u001b[0m       \u001b[32m30.2609\u001b[0m  0.0119\n",
      "     52       \u001b[36m32.1621\u001b[0m       \u001b[32m30.2569\u001b[0m  0.0119\n",
      "     53       \u001b[36m32.1592\u001b[0m       \u001b[32m30.2555\u001b[0m  0.0122\n",
      "     54       \u001b[36m32.1564\u001b[0m       \u001b[32m30.2514\u001b[0m  0.0120\n",
      "     55       \u001b[36m32.1538\u001b[0m       \u001b[32m30.2510\u001b[0m  0.0132\n",
      "     56       \u001b[36m32.1512\u001b[0m       \u001b[32m30.2460\u001b[0m  0.0116\n",
      "     57       \u001b[36m32.1487\u001b[0m       \u001b[32m30.2459\u001b[0m  0.0116\n",
      "     58       \u001b[36m32.1462\u001b[0m       \u001b[32m30.2392\u001b[0m  0.0164\n",
      "     59       \u001b[36m32.1439\u001b[0m       30.2423  0.0127\n",
      "     60       \u001b[36m32.1416\u001b[0m       \u001b[32m30.2329\u001b[0m  0.0130\n",
      "     61       \u001b[36m32.1394\u001b[0m       30.2401  0.0130\n",
      "     62       \u001b[36m32.1372\u001b[0m       \u001b[32m30.2242\u001b[0m  0.0121\n",
      "     63       \u001b[36m32.1352\u001b[0m       30.2419  0.0119\n",
      "     64       \u001b[36m32.1333\u001b[0m       \u001b[32m30.2148\u001b[0m  0.0118\n",
      "     65       \u001b[36m32.1316\u001b[0m       30.2512  0.0119\n",
      "     66       \u001b[36m32.1303\u001b[0m       \u001b[32m30.2001\u001b[0m  0.0118\n",
      "     67       \u001b[36m32.1298\u001b[0m       30.2732  0.0120\n",
      "     68       32.1307       \u001b[32m30.1850\u001b[0m  0.0119\n",
      "     69       32.1332       30.3061  0.0120\n",
      "     70       32.1371       \u001b[32m30.1739\u001b[0m  0.0120\n",
      "     71       32.1402       30.2722  0.0120\n",
      "     72       32.1351       30.1763  0.0131\n",
      "     73       \u001b[36m32.1251\u001b[0m       30.1984  0.0125\n",
      "     74       \u001b[36m32.1187\u001b[0m       30.2081  0.0120\n",
      "     75       \u001b[36m32.1136\u001b[0m       30.1776  0.0119\n",
      "     76       \u001b[36m32.1127\u001b[0m       30.2116  0.0120\n",
      "     77       \u001b[36m32.1123\u001b[0m       30.1866  0.0119\n",
      "     78       \u001b[36m32.1092\u001b[0m       30.1987  0.0128\n",
      "     79       \u001b[36m32.1082\u001b[0m       30.1993  0.0120\n",
      "     80       \u001b[36m32.1059\u001b[0m       30.1939  0.0124\n",
      "     81       \u001b[36m32.1047\u001b[0m       30.2037  0.0118\n",
      "     82       \u001b[36m32.1034\u001b[0m       30.1964  0.0120\n",
      "     83       \u001b[36m32.1018\u001b[0m       30.2035  0.0123\n",
      "     84       \u001b[36m32.1006\u001b[0m       30.1991  0.0119\n",
      "     85       \u001b[36m32.0990\u001b[0m       30.2024  0.0118\n",
      "     86       \u001b[36m32.0979\u001b[0m       30.2021  0.0119\n",
      "     87       \u001b[36m32.0964\u001b[0m       30.2031  0.0116\n",
      "     88       \u001b[36m32.0952\u001b[0m       30.2039  0.0121\n",
      "     89       \u001b[36m32.0938\u001b[0m       30.2038  0.0121\n",
      "     90       \u001b[36m32.0926\u001b[0m       30.2049  0.0118\n",
      "     91       \u001b[36m32.0913\u001b[0m       30.2056  0.0116\n",
      "     92       \u001b[36m32.0901\u001b[0m       30.2066  0.0116\n",
      "     93       \u001b[36m32.0889\u001b[0m       30.2068  0.0128\n",
      "     94       \u001b[36m32.0877\u001b[0m       30.2077  0.0130\n",
      "     95       \u001b[36m32.0865\u001b[0m       30.2090  0.0128\n",
      "     96       \u001b[36m32.0854\u001b[0m       30.2105  0.0132\n",
      "     97       \u001b[36m32.0842\u001b[0m       30.2106  0.0120\n",
      "     98       \u001b[36m32.0831\u001b[0m       30.2127  0.0126\n",
      "     99       \u001b[36m32.0820\u001b[0m       30.2133  0.0133\n",
      "    100       \u001b[36m32.0809\u001b[0m       30.2137  0.0126\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m34.1867\u001b[0m       \u001b[32m31.9858\u001b[0m  0.0125\n",
      "      2       \u001b[36m32.3143\u001b[0m       \u001b[32m30.6458\u001b[0m  0.0124\n",
      "      3       \u001b[36m30.4978\u001b[0m       \u001b[32m29.2492\u001b[0m  0.0128\n",
      "      4       \u001b[36m28.5504\u001b[0m       \u001b[32m27.7438\u001b[0m  0.0123\n",
      "      5       \u001b[36m26.3776\u001b[0m       \u001b[32m26.7990\u001b[0m  0.0127\n",
      "      6       \u001b[36m24.8019\u001b[0m       27.4215  0.0119\n",
      "      7       \u001b[36m24.5782\u001b[0m       28.2489  0.0121\n",
      "      8       \u001b[36m24.5400\u001b[0m       27.6273  0.0138\n",
      "      9       \u001b[36m24.1292\u001b[0m       26.8896  0.0118\n",
      "     10       \u001b[36m23.9504\u001b[0m       \u001b[32m26.6264\u001b[0m  0.0117\n",
      "     11       \u001b[36m23.9071\u001b[0m       \u001b[32m26.6229\u001b[0m  0.0121\n",
      "     12       \u001b[36m23.7986\u001b[0m       26.7808  0.0150\n",
      "     13       \u001b[36m23.6783\u001b[0m       27.0380  0.0135\n",
      "     14       \u001b[36m23.6089\u001b[0m       27.2047  0.0140\n",
      "     15       \u001b[36m23.5540\u001b[0m       27.1954  0.0124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     16       \u001b[36m23.5020\u001b[0m       27.1185  0.0131\n",
      "     17       \u001b[36m23.4636\u001b[0m       27.0732  0.0145\n",
      "     18       \u001b[36m23.4287\u001b[0m       27.0787  0.0130\n",
      "     19       \u001b[36m23.3956\u001b[0m       27.1068  0.0133\n",
      "     20       \u001b[36m23.3650\u001b[0m       27.1236  0.0120\n",
      "     21       \u001b[36m23.3376\u001b[0m       27.1246  0.0123\n",
      "     22       \u001b[36m23.3136\u001b[0m       27.1181  0.0118\n",
      "     23       \u001b[36m23.2926\u001b[0m       27.1105  0.0221\n",
      "     24       \u001b[36m23.2746\u001b[0m       27.1040  0.0114\n",
      "     25       \u001b[36m23.2581\u001b[0m       27.1023  0.0134\n",
      "     26       \u001b[36m23.2427\u001b[0m       27.1028  0.0126\n",
      "     27       \u001b[36m23.2283\u001b[0m       27.0988  0.0119\n",
      "     28       \u001b[36m23.2154\u001b[0m       27.0870  0.0119\n",
      "     29       \u001b[36m23.2037\u001b[0m       27.0735  0.0115\n",
      "     30       \u001b[36m23.1929\u001b[0m       27.0645  0.0123\n",
      "     31       \u001b[36m23.1828\u001b[0m       27.0573  0.0140\n",
      "     32       \u001b[36m23.1734\u001b[0m       27.0499  0.0117\n",
      "     33       \u001b[36m23.1645\u001b[0m       27.0408  0.0116\n",
      "     34       \u001b[36m23.1562\u001b[0m       27.0282  0.0116\n",
      "     35       \u001b[36m23.1484\u001b[0m       27.0162  0.0121\n",
      "     36       \u001b[36m23.1412\u001b[0m       27.0054  0.0118\n",
      "     37       \u001b[36m23.1344\u001b[0m       26.9974  0.0118\n",
      "     38       \u001b[36m23.1279\u001b[0m       26.9901  0.0116\n",
      "     39       \u001b[36m23.1217\u001b[0m       26.9806  0.0116\n",
      "     40       \u001b[36m23.1160\u001b[0m       26.9734  0.0121\n",
      "     41       \u001b[36m23.1105\u001b[0m       26.9698  0.0117\n",
      "     42       \u001b[36m23.1053\u001b[0m       26.9644  0.0122\n",
      "     43       \u001b[36m23.1005\u001b[0m       26.9570  0.0119\n",
      "     44       \u001b[36m23.0959\u001b[0m       26.9505  0.0120\n",
      "     45       \u001b[36m23.0915\u001b[0m       26.9463  0.0126\n",
      "     46       \u001b[36m23.0872\u001b[0m       26.9435  0.0123\n",
      "     47       \u001b[36m23.0831\u001b[0m       26.9428  0.0119\n",
      "     48       \u001b[36m23.0791\u001b[0m       26.9403  0.0125\n",
      "     49       \u001b[36m23.0754\u001b[0m       26.9381  0.0122\n",
      "     50       \u001b[36m23.0718\u001b[0m       26.9368  0.0122\n",
      "     51       \u001b[36m23.0684\u001b[0m       26.9368  0.0121\n",
      "     52       \u001b[36m23.0650\u001b[0m       26.9339  0.0137\n",
      "     53       \u001b[36m23.0619\u001b[0m       26.9334  0.0129\n",
      "     54       \u001b[36m23.0587\u001b[0m       26.9287  0.0135\n",
      "     55       \u001b[36m23.0559\u001b[0m       26.9309  0.0131\n",
      "     56       \u001b[36m23.0527\u001b[0m       26.9245  0.0119\n",
      "     57       \u001b[36m23.0506\u001b[0m       26.9308  0.0119\n",
      "     58       \u001b[36m23.0478\u001b[0m       26.9161  0.0148\n",
      "     59       \u001b[36m23.0473\u001b[0m       26.9279  0.0122\n",
      "     60       \u001b[36m23.0449\u001b[0m       26.9105  0.0117\n",
      "     61       23.0457       26.9297  0.0118\n",
      "     62       \u001b[36m23.0428\u001b[0m       26.9091  0.0117\n",
      "     63       \u001b[36m23.0422\u001b[0m       26.9214  0.0115\n",
      "     64       \u001b[36m23.0372\u001b[0m       26.9089  0.0118\n",
      "     65       \u001b[36m23.0351\u001b[0m       26.9153  0.0117\n",
      "     66       \u001b[36m23.0300\u001b[0m       26.9128  0.0118\n",
      "     67       \u001b[36m23.0281\u001b[0m       26.9138  0.0124\n",
      "     68       \u001b[36m23.0250\u001b[0m       26.9144  0.0117\n",
      "     69       \u001b[36m23.0232\u001b[0m       26.9131  0.0131\n",
      "     70       \u001b[36m23.0213\u001b[0m       26.9134  0.0122\n",
      "     71       \u001b[36m23.0194\u001b[0m       26.9098  0.0129\n",
      "     72       \u001b[36m23.0178\u001b[0m       26.9107  0.0122\n",
      "     73       \u001b[36m23.0159\u001b[0m       26.9084  0.0123\n",
      "     74       \u001b[36m23.0144\u001b[0m       26.9079  0.0120\n",
      "     75       \u001b[36m23.0126\u001b[0m       26.9067  0.0119\n",
      "     76       \u001b[36m23.0112\u001b[0m       26.9079  0.0120\n",
      "     77       \u001b[36m23.0095\u001b[0m       26.9033  0.0121\n",
      "     78       \u001b[36m23.0083\u001b[0m       26.9049  0.0121\n",
      "     79       \u001b[36m23.0065\u001b[0m       26.9002  0.0122\n",
      "     80       \u001b[36m23.0056\u001b[0m       26.9021  0.0122\n",
      "     81       \u001b[36m23.0038\u001b[0m       26.8969  0.0118\n",
      "     82       \u001b[36m23.0033\u001b[0m       26.9020  0.0118\n",
      "     83       \u001b[36m23.0015\u001b[0m       26.8945  0.0131\n",
      "     84       23.0017       26.9040  0.0137\n",
      "     85       \u001b[36m23.0001\u001b[0m       26.8918  0.0126\n",
      "     86       23.0017       26.9069  0.0145\n",
      "     87       23.0011       26.8892  0.0130\n",
      "     88       23.0057       26.9123  0.0161\n",
      "     89       23.0071       26.8892  0.0143\n",
      "     90       23.0149       26.9160  0.0130\n",
      "     91       23.0143       26.8880  0.0137\n",
      "     92       23.0135       26.9013  0.0141\n",
      "     93       23.0032       26.8885  0.0141\n",
      "     94       \u001b[36m22.9963\u001b[0m       26.8904  0.0127\n",
      "     95       \u001b[36m22.9888\u001b[0m       26.8968  0.0123\n",
      "     96       \u001b[36m22.9871\u001b[0m       26.8900  0.0122\n",
      "     97       \u001b[36m22.9868\u001b[0m       26.8971  0.0120\n",
      "     98       \u001b[36m22.9850\u001b[0m       26.8934  0.0120\n",
      "     99       \u001b[36m22.9846\u001b[0m       26.8946  0.0122\n",
      "    100       \u001b[36m22.9826\u001b[0m       26.8951  0.0204\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m40.6844\u001b[0m       \u001b[32m31.7715\u001b[0m  0.0161\n",
      "      2       \u001b[36m38.9151\u001b[0m       \u001b[32m30.5523\u001b[0m  0.0127\n",
      "      3       \u001b[36m37.1560\u001b[0m       \u001b[32m29.2550\u001b[0m  0.0147\n",
      "      4       \u001b[36m35.2598\u001b[0m       \u001b[32m27.8756\u001b[0m  0.0145\n",
      "      5       \u001b[36m33.0716\u001b[0m       \u001b[32m26.7417\u001b[0m  0.0146\n",
      "      6       \u001b[36m31.0334\u001b[0m       26.7703  0.0127\n",
      "      7       \u001b[36m30.0883\u001b[0m       28.2473  0.0117\n",
      "      8       30.2602       28.8537  0.0125\n",
      "      9       \u001b[36m30.0632\u001b[0m       27.9418  0.0121\n",
      "     10       \u001b[36m29.5786\u001b[0m       27.1356  0.0124\n",
      "     11       \u001b[36m29.3422\u001b[0m       26.8412  0.0125\n",
      "     12       \u001b[36m29.2096\u001b[0m       26.8865  0.0120\n",
      "     13       \u001b[36m29.0922\u001b[0m       27.1458  0.0121\n",
      "     14       \u001b[36m29.0187\u001b[0m       27.4362  0.0134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     15       \u001b[36m28.9796\u001b[0m       27.5669  0.0139\n",
      "     16       \u001b[36m28.9261\u001b[0m       27.5206  0.0123\n",
      "     17       \u001b[36m28.8585\u001b[0m       27.4135  0.0121\n",
      "     18       \u001b[36m28.8000\u001b[0m       27.3448  0.0126\n",
      "     19       \u001b[36m28.7545\u001b[0m       27.3519  0.0114\n",
      "     20       \u001b[36m28.7191\u001b[0m       27.4042  0.0130\n",
      "     21       \u001b[36m28.6911\u001b[0m       27.4433  0.0129\n",
      "     22       \u001b[36m28.6650\u001b[0m       27.4452  0.0128\n",
      "     23       \u001b[36m28.6386\u001b[0m       27.4337  0.0116\n",
      "     24       \u001b[36m28.6146\u001b[0m       27.4284  0.0119\n",
      "     25       \u001b[36m28.5946\u001b[0m       27.4336  0.0136\n",
      "     26       \u001b[36m28.5778\u001b[0m       27.4459  0.0121\n",
      "     27       \u001b[36m28.5637\u001b[0m       27.4552  0.0125\n",
      "     28       \u001b[36m28.5510\u001b[0m       27.4567  0.0137\n",
      "     29       \u001b[36m28.5393\u001b[0m       27.4555  0.0119\n",
      "     30       \u001b[36m28.5287\u001b[0m       27.4551  0.0146\n",
      "     31       \u001b[36m28.5192\u001b[0m       27.4560  0.0133\n",
      "     32       \u001b[36m28.5107\u001b[0m       27.4584  0.0118\n",
      "     33       \u001b[36m28.5030\u001b[0m       27.4604  0.0116\n",
      "     34       \u001b[36m28.4961\u001b[0m       27.4628  0.0126\n",
      "     35       \u001b[36m28.4899\u001b[0m       27.4642  0.0129\n",
      "     36       \u001b[36m28.4839\u001b[0m       27.4639  0.0119\n",
      "     37       \u001b[36m28.4784\u001b[0m       27.4632  0.0121\n",
      "     38       \u001b[36m28.4731\u001b[0m       27.4628  0.0118\n",
      "     39       \u001b[36m28.4683\u001b[0m       27.4629  0.0115\n",
      "     40       \u001b[36m28.4638\u001b[0m       27.4620  0.0126\n",
      "     41       \u001b[36m28.4595\u001b[0m       27.4608  0.0119\n",
      "     42       \u001b[36m28.4555\u001b[0m       27.4592  0.0123\n",
      "     43       \u001b[36m28.4516\u001b[0m       27.4570  0.0123\n",
      "     44       \u001b[36m28.4479\u001b[0m       27.4566  0.0123\n",
      "     45       \u001b[36m28.4446\u001b[0m       27.4561  0.0135\n",
      "     46       \u001b[36m28.4414\u001b[0m       27.4554  0.0171\n",
      "     47       \u001b[36m28.4384\u001b[0m       27.4544  0.0125\n",
      "     48       \u001b[36m28.4357\u001b[0m       27.4547  0.0121\n",
      "     49       \u001b[36m28.4330\u001b[0m       27.4531  0.0121\n",
      "     50       \u001b[36m28.4304\u001b[0m       27.4530  0.0121\n",
      "     51       \u001b[36m28.4280\u001b[0m       27.4553  0.0120\n",
      "     52       \u001b[36m28.4259\u001b[0m       27.4562  0.0119\n",
      "     53       \u001b[36m28.4238\u001b[0m       27.4536  0.0124\n",
      "     54       \u001b[36m28.4216\u001b[0m       27.4522  0.0123\n",
      "     55       \u001b[36m28.4197\u001b[0m       27.4516  0.0121\n",
      "     56       \u001b[36m28.4179\u001b[0m       27.4492  0.0121\n",
      "     57       \u001b[36m28.4162\u001b[0m       27.4493  0.0117\n",
      "     58       \u001b[36m28.4145\u001b[0m       27.4478  0.0117\n",
      "     59       \u001b[36m28.4129\u001b[0m       27.4459  0.0123\n",
      "     60       \u001b[36m28.4114\u001b[0m       27.4467  0.0136\n",
      "     61       \u001b[36m28.4101\u001b[0m       27.4457  0.0156\n",
      "     62       \u001b[36m28.4086\u001b[0m       27.4429  0.0148\n",
      "     63       \u001b[36m28.4072\u001b[0m       27.4420  0.0123\n",
      "     64       \u001b[36m28.4059\u001b[0m       27.4432  0.0130\n",
      "     65       \u001b[36m28.4048\u001b[0m       27.4434  0.0129\n",
      "     66       \u001b[36m28.4036\u001b[0m       27.4418  0.0137\n",
      "     67       \u001b[36m28.4023\u001b[0m       27.4409  0.0121\n",
      "     68       \u001b[36m28.4012\u001b[0m       27.4423  0.0125\n",
      "     69       \u001b[36m28.4005\u001b[0m       27.4409  0.0122\n",
      "     70       \u001b[36m28.3992\u001b[0m       27.4389  0.0121\n",
      "     71       \u001b[36m28.3982\u001b[0m       27.4400  0.0122\n",
      "     72       28.3988       27.4423  0.0116\n",
      "     73       28.3987       27.4362  0.0119\n",
      "     74       \u001b[36m28.3977\u001b[0m       27.4364  0.0126\n",
      "     75       28.4026       27.4440  0.0122\n",
      "     76       28.4071       27.4398  0.0237\n",
      "     77       28.4053       27.4296  0.0123\n",
      "     78       \u001b[36m28.3954\u001b[0m       27.4324  0.0116\n",
      "     79       \u001b[36m28.3941\u001b[0m       27.4255  0.0116\n",
      "     80       28.3941       27.4247  0.0120\n",
      "     81       \u001b[36m28.3897\u001b[0m       27.4334  0.0121\n",
      "     82       28.3898       27.4278  0.0121\n",
      "     83       \u001b[36m28.3886\u001b[0m       27.4250  0.0122\n",
      "     84       28.3897       27.4253  0.0119\n",
      "     85       \u001b[36m28.3868\u001b[0m       27.4279  0.0120\n",
      "     86       \u001b[36m28.3863\u001b[0m       27.4251  0.0119\n",
      "     87       \u001b[36m28.3853\u001b[0m       27.4248  0.0122\n",
      "     88       \u001b[36m28.3850\u001b[0m       27.4249  0.0118\n",
      "     89       \u001b[36m28.3840\u001b[0m       27.4263  0.0123\n",
      "     90       \u001b[36m28.3835\u001b[0m       27.4258  0.0120\n",
      "     91       \u001b[36m28.3829\u001b[0m       27.4246  0.0120\n",
      "     92       \u001b[36m28.3822\u001b[0m       27.4241  0.0121\n",
      "     93       \u001b[36m28.3818\u001b[0m       27.4254  0.0124\n",
      "     94       \u001b[36m28.3811\u001b[0m       27.4242  0.0123\n",
      "     95       \u001b[36m28.3806\u001b[0m       27.4222  0.0126\n",
      "     96       \u001b[36m28.3801\u001b[0m       27.4238  0.0121\n",
      "     97       \u001b[36m28.3795\u001b[0m       27.4240  0.0125\n",
      "     98       \u001b[36m28.3791\u001b[0m       27.4222  0.0121\n",
      "     99       \u001b[36m28.3785\u001b[0m       27.4209  0.0123\n",
      "    100       \u001b[36m28.3779\u001b[0m       27.4208  0.0123\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m44.3887\u001b[0m       \u001b[32m46.5541\u001b[0m  0.0115\n",
      "      2       \u001b[36m43.9068\u001b[0m       \u001b[32m45.9745\u001b[0m  0.0114\n",
      "      3       \u001b[36m43.4454\u001b[0m       \u001b[32m45.4134\u001b[0m  0.0116\n",
      "      4       \u001b[36m42.9985\u001b[0m       \u001b[32m44.8649\u001b[0m  0.0115\n",
      "      5       \u001b[36m42.5621\u001b[0m       \u001b[32m44.3240\u001b[0m  0.0114\n",
      "      6       \u001b[36m42.1323\u001b[0m       \u001b[32m43.7866\u001b[0m  0.0108\n",
      "      7       \u001b[36m41.7060\u001b[0m       \u001b[32m43.2511\u001b[0m  0.0114\n",
      "      8       \u001b[36m41.2830\u001b[0m       \u001b[32m42.7157\u001b[0m  0.0112\n",
      "      9       \u001b[36m40.8621\u001b[0m       \u001b[32m42.1791\u001b[0m  0.0110\n",
      "     10       \u001b[36m40.4423\u001b[0m       \u001b[32m41.6412\u001b[0m  0.0110\n",
      "     11       \u001b[36m40.0237\u001b[0m       \u001b[32m41.1021\u001b[0m  0.0115\n",
      "     12       \u001b[36m39.6061\u001b[0m       \u001b[32m40.5612\u001b[0m  0.0119\n",
      "     13       \u001b[36m39.1901\u001b[0m       \u001b[32m40.0204\u001b[0m  0.0123\n",
      "     14       \u001b[36m38.7772\u001b[0m       \u001b[32m39.4808\u001b[0m  0.0115\n",
      "     15       \u001b[36m38.3687\u001b[0m       \u001b[32m38.9444\u001b[0m  0.0111\n",
      "     16       \u001b[36m37.9661\u001b[0m       \u001b[32m38.4131\u001b[0m  0.0110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m37.5715\u001b[0m       \u001b[32m37.8897\u001b[0m  0.0121\n",
      "     18       \u001b[36m37.1869\u001b[0m       \u001b[32m37.3762\u001b[0m  0.0121\n",
      "     19       \u001b[36m36.8141\u001b[0m       \u001b[32m36.8750\u001b[0m  0.0113\n",
      "     20       \u001b[36m36.4550\u001b[0m       \u001b[32m36.3886\u001b[0m  0.0111\n",
      "     21       \u001b[36m36.1114\u001b[0m       \u001b[32m35.9188\u001b[0m  0.0113\n",
      "     22       \u001b[36m35.7850\u001b[0m       \u001b[32m35.4672\u001b[0m  0.0121\n",
      "     23       \u001b[36m35.4769\u001b[0m       \u001b[32m35.0357\u001b[0m  0.0111\n",
      "     24       \u001b[36m35.1883\u001b[0m       \u001b[32m34.6255\u001b[0m  0.0111\n",
      "     25       \u001b[36m34.9197\u001b[0m       \u001b[32m34.2378\u001b[0m  0.0111\n",
      "     26       \u001b[36m34.6718\u001b[0m       \u001b[32m33.8733\u001b[0m  0.0113\n",
      "     27       \u001b[36m34.4446\u001b[0m       \u001b[32m33.5329\u001b[0m  0.0121\n",
      "     28       \u001b[36m34.2380\u001b[0m       \u001b[32m33.2166\u001b[0m  0.0118\n",
      "     29       \u001b[36m34.0518\u001b[0m       \u001b[32m32.9245\u001b[0m  0.0115\n",
      "     30       \u001b[36m33.8850\u001b[0m       \u001b[32m32.6562\u001b[0m  0.0113\n",
      "     31       \u001b[36m33.7367\u001b[0m       \u001b[32m32.4110\u001b[0m  0.0111\n",
      "     32       \u001b[36m33.6055\u001b[0m       \u001b[32m32.1882\u001b[0m  0.0123\n",
      "     33       \u001b[36m33.4904\u001b[0m       \u001b[32m31.9864\u001b[0m  0.0117\n",
      "     34       \u001b[36m33.3897\u001b[0m       \u001b[32m31.8044\u001b[0m  0.0115\n",
      "     35       \u001b[36m33.3020\u001b[0m       \u001b[32m31.6410\u001b[0m  0.0114\n",
      "     36       \u001b[36m33.2260\u001b[0m       \u001b[32m31.4948\u001b[0m  0.0116\n",
      "     37       \u001b[36m33.1601\u001b[0m       \u001b[32m31.3642\u001b[0m  0.0116\n",
      "     38       \u001b[36m33.1031\u001b[0m       \u001b[32m31.2480\u001b[0m  0.0115\n",
      "     39       \u001b[36m33.0538\u001b[0m       \u001b[32m31.1447\u001b[0m  0.0176\n",
      "     40       \u001b[36m33.0111\u001b[0m       \u001b[32m31.0530\u001b[0m  0.0133\n",
      "     41       \u001b[36m32.9740\u001b[0m       \u001b[32m30.9715\u001b[0m  0.0116\n",
      "     42       \u001b[36m32.9418\u001b[0m       \u001b[32m30.8991\u001b[0m  0.0124\n",
      "     43       \u001b[36m32.9134\u001b[0m       \u001b[32m30.8346\u001b[0m  0.0121\n",
      "     44       \u001b[36m32.8883\u001b[0m       \u001b[32m30.7773\u001b[0m  0.0126\n",
      "     45       \u001b[36m32.8659\u001b[0m       \u001b[32m30.7261\u001b[0m  0.0123\n",
      "     46       \u001b[36m32.8458\u001b[0m       \u001b[32m30.6804\u001b[0m  0.0123\n",
      "     47       \u001b[36m32.8276\u001b[0m       \u001b[32m30.6393\u001b[0m  0.0116\n",
      "     48       \u001b[36m32.8110\u001b[0m       \u001b[32m30.6025\u001b[0m  0.0115\n",
      "     49       \u001b[36m32.7957\u001b[0m       \u001b[32m30.5694\u001b[0m  0.0113\n",
      "     50       \u001b[36m32.7816\u001b[0m       \u001b[32m30.5394\u001b[0m  0.0114\n",
      "     51       \u001b[36m32.7684\u001b[0m       \u001b[32m30.5123\u001b[0m  0.0115\n",
      "     52       \u001b[36m32.7560\u001b[0m       \u001b[32m30.4876\u001b[0m  0.0119\n",
      "     53       \u001b[36m32.7443\u001b[0m       \u001b[32m30.4651\u001b[0m  0.0113\n",
      "     54       \u001b[36m32.7332\u001b[0m       \u001b[32m30.4445\u001b[0m  0.0114\n",
      "     55       \u001b[36m32.7227\u001b[0m       \u001b[32m30.4257\u001b[0m  0.0113\n",
      "     56       \u001b[36m32.7126\u001b[0m       \u001b[32m30.4083\u001b[0m  0.0177\n",
      "     57       \u001b[36m32.7030\u001b[0m       \u001b[32m30.3924\u001b[0m  0.0160\n",
      "     58       \u001b[36m32.6938\u001b[0m       \u001b[32m30.3776\u001b[0m  0.0110\n",
      "     59       \u001b[36m32.6849\u001b[0m       \u001b[32m30.3640\u001b[0m  0.0110\n",
      "     60       \u001b[36m32.6764\u001b[0m       \u001b[32m30.3512\u001b[0m  0.0111\n",
      "     61       \u001b[36m32.6681\u001b[0m       \u001b[32m30.3394\u001b[0m  0.0117\n",
      "     62       \u001b[36m32.6602\u001b[0m       \u001b[32m30.3282\u001b[0m  0.0113\n",
      "     63       \u001b[36m32.6525\u001b[0m       \u001b[32m30.3177\u001b[0m  0.0115\n",
      "     64       \u001b[36m32.6451\u001b[0m       \u001b[32m30.3078\u001b[0m  0.0111\n",
      "     65       \u001b[36m32.6379\u001b[0m       \u001b[32m30.2985\u001b[0m  0.0111\n",
      "     66       \u001b[36m32.6309\u001b[0m       \u001b[32m30.2897\u001b[0m  0.0115\n",
      "     67       \u001b[36m32.6241\u001b[0m       \u001b[32m30.2813\u001b[0m  0.0123\n",
      "     68       \u001b[36m32.6175\u001b[0m       \u001b[32m30.2733\u001b[0m  0.0111\n",
      "     69       \u001b[36m32.6110\u001b[0m       \u001b[32m30.2657\u001b[0m  0.0116\n",
      "     70       \u001b[36m32.6048\u001b[0m       \u001b[32m30.2585\u001b[0m  0.0112\n",
      "     71       \u001b[36m32.5987\u001b[0m       \u001b[32m30.2515\u001b[0m  0.0114\n",
      "     72       \u001b[36m32.5927\u001b[0m       \u001b[32m30.2448\u001b[0m  0.0110\n",
      "     73       \u001b[36m32.5870\u001b[0m       \u001b[32m30.2384\u001b[0m  0.0111\n",
      "     74       \u001b[36m32.5813\u001b[0m       \u001b[32m30.2322\u001b[0m  0.0112\n",
      "     75       \u001b[36m32.5758\u001b[0m       \u001b[32m30.2263\u001b[0m  0.0116\n",
      "     76       \u001b[36m32.5705\u001b[0m       \u001b[32m30.2205\u001b[0m  0.0113\n",
      "     77       \u001b[36m32.5652\u001b[0m       \u001b[32m30.2149\u001b[0m  0.0111\n",
      "     78       \u001b[36m32.5601\u001b[0m       \u001b[32m30.2095\u001b[0m  0.0115\n",
      "     79       \u001b[36m32.5551\u001b[0m       \u001b[32m30.2042\u001b[0m  0.0114\n",
      "     80       \u001b[36m32.5502\u001b[0m       \u001b[32m30.1991\u001b[0m  0.0121\n",
      "     81       \u001b[36m32.5454\u001b[0m       \u001b[32m30.1942\u001b[0m  0.0113\n",
      "     82       \u001b[36m32.5408\u001b[0m       \u001b[32m30.1893\u001b[0m  0.0113\n",
      "     83       \u001b[36m32.5362\u001b[0m       \u001b[32m30.1846\u001b[0m  0.0114\n",
      "     84       \u001b[36m32.5317\u001b[0m       \u001b[32m30.1800\u001b[0m  0.0113\n",
      "     85       \u001b[36m32.5273\u001b[0m       \u001b[32m30.1756\u001b[0m  0.0111\n",
      "     86       \u001b[36m32.5231\u001b[0m       \u001b[32m30.1712\u001b[0m  0.0118\n",
      "     87       \u001b[36m32.5189\u001b[0m       \u001b[32m30.1670\u001b[0m  0.0111\n",
      "     88       \u001b[36m32.5148\u001b[0m       \u001b[32m30.1628\u001b[0m  0.0129\n",
      "     89       \u001b[36m32.5107\u001b[0m       \u001b[32m30.1588\u001b[0m  0.0108\n",
      "     90       \u001b[36m32.5068\u001b[0m       \u001b[32m30.1548\u001b[0m  0.0108\n",
      "     91       \u001b[36m32.5029\u001b[0m       \u001b[32m30.1509\u001b[0m  0.0114\n",
      "     92       \u001b[36m32.4992\u001b[0m       \u001b[32m30.1471\u001b[0m  0.0119\n",
      "     93       \u001b[36m32.4954\u001b[0m       \u001b[32m30.1434\u001b[0m  0.0116\n",
      "     94       \u001b[36m32.4918\u001b[0m       \u001b[32m30.1398\u001b[0m  0.0110\n",
      "     95       \u001b[36m32.4882\u001b[0m       \u001b[32m30.1362\u001b[0m  0.0112\n",
      "     96       \u001b[36m32.4847\u001b[0m       \u001b[32m30.1328\u001b[0m  0.0113\n",
      "     97       \u001b[36m32.4813\u001b[0m       \u001b[32m30.1294\u001b[0m  0.0115\n",
      "     98       \u001b[36m32.4779\u001b[0m       \u001b[32m30.1261\u001b[0m  0.0116\n",
      "     99       \u001b[36m32.4746\u001b[0m       \u001b[32m30.1229\u001b[0m  0.0115\n",
      "    100       \u001b[36m32.4714\u001b[0m       \u001b[32m30.1198\u001b[0m  0.0115\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m34.9105\u001b[0m       \u001b[32m33.2235\u001b[0m  0.0117\n",
      "      2       \u001b[36m34.4067\u001b[0m       \u001b[32m32.8409\u001b[0m  0.0111\n",
      "      3       \u001b[36m33.9265\u001b[0m       \u001b[32m32.4755\u001b[0m  0.0110\n",
      "      4       \u001b[36m33.4639\u001b[0m       \u001b[32m32.1236\u001b[0m  0.0111\n",
      "      5       \u001b[36m33.0148\u001b[0m       \u001b[32m31.7820\u001b[0m  0.0110\n",
      "      6       \u001b[36m32.5754\u001b[0m       \u001b[32m31.4478\u001b[0m  0.0109\n",
      "      7       \u001b[36m32.1433\u001b[0m       \u001b[32m31.1194\u001b[0m  0.0134\n",
      "      8       \u001b[36m31.7166\u001b[0m       \u001b[32m30.7956\u001b[0m  0.0110\n",
      "      9       \u001b[36m31.2940\u001b[0m       \u001b[32m30.4762\u001b[0m  0.0111\n",
      "     10       \u001b[36m30.8746\u001b[0m       \u001b[32m30.1612\u001b[0m  0.0109\n",
      "     11       \u001b[36m30.4582\u001b[0m       \u001b[32m29.8513\u001b[0m  0.0111\n",
      "     12       \u001b[36m30.0458\u001b[0m       \u001b[32m29.5473\u001b[0m  0.0110\n",
      "     13       \u001b[36m29.6384\u001b[0m       \u001b[32m29.2505\u001b[0m  0.0111\n",
      "     14       \u001b[36m29.2373\u001b[0m       \u001b[32m28.9624\u001b[0m  0.0111\n",
      "     15       \u001b[36m28.8438\u001b[0m       \u001b[32m28.6842\u001b[0m  0.0115\n",
      "     16       \u001b[36m28.4595\u001b[0m       \u001b[32m28.4179\u001b[0m  0.0121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m28.0861\u001b[0m       \u001b[32m28.1650\u001b[0m  0.0127\n",
      "     18       \u001b[36m27.7251\u001b[0m       \u001b[32m27.9266\u001b[0m  0.0116\n",
      "     19       \u001b[36m27.3781\u001b[0m       \u001b[32m27.7040\u001b[0m  0.0175\n",
      "     20       \u001b[36m27.0462\u001b[0m       \u001b[32m27.4983\u001b[0m  0.0218\n",
      "     21       \u001b[36m26.7305\u001b[0m       \u001b[32m27.3103\u001b[0m  0.0207\n",
      "     22       \u001b[36m26.4316\u001b[0m       \u001b[32m27.1404\u001b[0m  0.0201\n",
      "     23       \u001b[36m26.1505\u001b[0m       \u001b[32m26.9888\u001b[0m  0.0189\n",
      "     24       \u001b[36m25.8872\u001b[0m       \u001b[32m26.8554\u001b[0m  0.0137\n",
      "     25       \u001b[36m25.6423\u001b[0m       \u001b[32m26.7403\u001b[0m  0.0114\n",
      "     26       \u001b[36m25.4159\u001b[0m       \u001b[32m26.6427\u001b[0m  0.0114\n",
      "     27       \u001b[36m25.2077\u001b[0m       \u001b[32m26.5616\u001b[0m  0.0113\n",
      "     28       \u001b[36m25.0171\u001b[0m       \u001b[32m26.4962\u001b[0m  0.0112\n",
      "     29       \u001b[36m24.8438\u001b[0m       \u001b[32m26.4452\u001b[0m  0.0112\n",
      "     30       \u001b[36m24.6873\u001b[0m       \u001b[32m26.4073\u001b[0m  0.0112\n",
      "     31       \u001b[36m24.5467\u001b[0m       \u001b[32m26.3813\u001b[0m  0.0109\n",
      "     32       \u001b[36m24.4213\u001b[0m       \u001b[32m26.3659\u001b[0m  0.0110\n",
      "     33       \u001b[36m24.3099\u001b[0m       \u001b[32m26.3589\u001b[0m  0.0112\n",
      "     34       \u001b[36m24.2113\u001b[0m       26.3591  0.0120\n",
      "     35       \u001b[36m24.1246\u001b[0m       26.3649  0.0112\n",
      "     36       \u001b[36m24.0482\u001b[0m       26.3749  0.0114\n",
      "     37       \u001b[36m23.9812\u001b[0m       26.3880  0.0111\n",
      "     38       \u001b[36m23.9224\u001b[0m       26.4033  0.0108\n",
      "     39       \u001b[36m23.8709\u001b[0m       26.4200  0.0108\n",
      "     40       \u001b[36m23.8257\u001b[0m       26.4372  0.0112\n",
      "     41       \u001b[36m23.7859\u001b[0m       26.4545  0.0113\n",
      "     42       \u001b[36m23.7509\u001b[0m       26.4714  0.0107\n",
      "     43       \u001b[36m23.7199\u001b[0m       26.4876  0.0106\n",
      "     44       \u001b[36m23.6924\u001b[0m       26.5028  0.0110\n",
      "     45       \u001b[36m23.6679\u001b[0m       26.5169  0.0110\n",
      "     46       \u001b[36m23.6460\u001b[0m       26.5298  0.0110\n",
      "     47       \u001b[36m23.6264\u001b[0m       26.5416  0.0107\n",
      "     48       \u001b[36m23.6086\u001b[0m       26.5521  0.0107\n",
      "     49       \u001b[36m23.5925\u001b[0m       26.5615  0.0107\n",
      "     50       \u001b[36m23.5778\u001b[0m       26.5697  0.0110\n",
      "     51       \u001b[36m23.5643\u001b[0m       26.5768  0.0109\n",
      "     52       \u001b[36m23.5518\u001b[0m       26.5830  0.0108\n",
      "     53       \u001b[36m23.5402\u001b[0m       26.5882  0.0105\n",
      "     54       \u001b[36m23.5294\u001b[0m       26.5926  0.0109\n",
      "     55       \u001b[36m23.5193\u001b[0m       26.5962  0.0106\n",
      "     56       \u001b[36m23.5098\u001b[0m       26.5991  0.0108\n",
      "     57       \u001b[36m23.5008\u001b[0m       26.6015  0.0107\n",
      "     58       \u001b[36m23.4922\u001b[0m       26.6032  0.0108\n",
      "     59       \u001b[36m23.4841\u001b[0m       26.6046  0.0114\n",
      "     60       \u001b[36m23.4763\u001b[0m       26.6054  0.0114\n",
      "     61       \u001b[36m23.4689\u001b[0m       26.6059  0.0111\n",
      "     62       \u001b[36m23.4617\u001b[0m       26.6060  0.0109\n",
      "     63       \u001b[36m23.4549\u001b[0m       26.6060  0.0107\n",
      "     64       \u001b[36m23.4483\u001b[0m       26.6058  0.0113\n",
      "     65       \u001b[36m23.4419\u001b[0m       26.6053  0.0110\n",
      "     66       \u001b[36m23.4357\u001b[0m       26.6045  0.0109\n",
      "     67       \u001b[36m23.4298\u001b[0m       26.6037  0.0106\n",
      "     68       \u001b[36m23.4240\u001b[0m       26.6028  0.0109\n",
      "     69       \u001b[36m23.4184\u001b[0m       26.6018  0.0111\n",
      "     70       \u001b[36m23.4130\u001b[0m       26.6007  0.0110\n",
      "     71       \u001b[36m23.4077\u001b[0m       26.5995  0.0107\n",
      "     72       \u001b[36m23.4025\u001b[0m       26.5982  0.0107\n",
      "     73       \u001b[36m23.3975\u001b[0m       26.5968  0.0106\n",
      "     74       \u001b[36m23.3926\u001b[0m       26.5955  0.0111\n",
      "     75       \u001b[36m23.3879\u001b[0m       26.5941  0.0113\n",
      "     76       \u001b[36m23.3832\u001b[0m       26.5928  0.0110\n",
      "     77       \u001b[36m23.3787\u001b[0m       26.5913  0.0105\n",
      "     78       \u001b[36m23.3743\u001b[0m       26.5899  0.0107\n",
      "     79       \u001b[36m23.3700\u001b[0m       26.5885  0.0112\n",
      "     80       \u001b[36m23.3658\u001b[0m       26.5870  0.0110\n",
      "     81       \u001b[36m23.3617\u001b[0m       26.5857  0.0108\n",
      "     82       \u001b[36m23.3577\u001b[0m       26.5843  0.0107\n",
      "     83       \u001b[36m23.3537\u001b[0m       26.5830  0.0106\n",
      "     84       \u001b[36m23.3499\u001b[0m       26.5815  0.0109\n",
      "     85       \u001b[36m23.3461\u001b[0m       26.5801  0.0108\n",
      "     86       \u001b[36m23.3424\u001b[0m       26.5788  0.0110\n",
      "     87       \u001b[36m23.3388\u001b[0m       26.5774  0.0107\n",
      "     88       \u001b[36m23.3353\u001b[0m       26.5762  0.0106\n",
      "     89       \u001b[36m23.3318\u001b[0m       26.5748  0.0111\n",
      "     90       \u001b[36m23.3284\u001b[0m       26.5735  0.0108\n",
      "     91       \u001b[36m23.3250\u001b[0m       26.5722  0.0110\n",
      "     92       \u001b[36m23.3218\u001b[0m       26.5710  0.0107\n",
      "     93       \u001b[36m23.3185\u001b[0m       26.5698  0.0106\n",
      "     94       \u001b[36m23.3154\u001b[0m       26.5685  0.0111\n",
      "     95       \u001b[36m23.3123\u001b[0m       26.5673  0.0112\n",
      "     96       \u001b[36m23.3092\u001b[0m       26.5662  0.0110\n",
      "     97       \u001b[36m23.3062\u001b[0m       26.5650  0.0107\n",
      "     98       \u001b[36m23.3032\u001b[0m       26.5639  0.0107\n",
      "     99       \u001b[36m23.3003\u001b[0m       26.5628  0.0113\n",
      "    100       \u001b[36m23.2975\u001b[0m       26.5617  0.0109\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m39.9822\u001b[0m       \u001b[32m31.7595\u001b[0m  0.0149\n",
      "      2       \u001b[36m39.4819\u001b[0m       \u001b[32m31.4162\u001b[0m  0.0169\n",
      "      3       \u001b[36m39.0005\u001b[0m       \u001b[32m31.0867\u001b[0m  0.0122\n",
      "      4       \u001b[36m38.5340\u001b[0m       \u001b[32m30.7688\u001b[0m  0.0113\n",
      "      5       \u001b[36m38.0803\u001b[0m       \u001b[32m30.4611\u001b[0m  0.0117\n",
      "      6       \u001b[36m37.6379\u001b[0m       \u001b[32m30.1630\u001b[0m  0.0124\n",
      "      7       \u001b[36m37.2050\u001b[0m       \u001b[32m29.8738\u001b[0m  0.0123\n",
      "      8       \u001b[36m36.7809\u001b[0m       \u001b[32m29.5937\u001b[0m  0.0116\n",
      "      9       \u001b[36m36.3657\u001b[0m       \u001b[32m29.3230\u001b[0m  0.0113\n",
      "     10       \u001b[36m35.9599\u001b[0m       \u001b[32m29.0623\u001b[0m  0.0111\n",
      "     11       \u001b[36m35.5646\u001b[0m       \u001b[32m28.8119\u001b[0m  0.0109\n",
      "     12       \u001b[36m35.1802\u001b[0m       \u001b[32m28.5725\u001b[0m  0.0109\n",
      "     13       \u001b[36m34.8072\u001b[0m       \u001b[32m28.3453\u001b[0m  0.0110\n",
      "     14       \u001b[36m34.4467\u001b[0m       \u001b[32m28.1308\u001b[0m  0.0107\n",
      "     15       \u001b[36m34.0989\u001b[0m       \u001b[32m27.9294\u001b[0m  0.0109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     16       \u001b[36m33.7637\u001b[0m       \u001b[32m27.7412\u001b[0m  0.0109\n",
      "     17       \u001b[36m33.4409\u001b[0m       \u001b[32m27.5666\u001b[0m  0.0109\n",
      "     18       \u001b[36m33.1301\u001b[0m       \u001b[32m27.4059\u001b[0m  0.0109\n",
      "     19       \u001b[36m32.8313\u001b[0m       \u001b[32m27.2590\u001b[0m  0.0112\n",
      "     20       \u001b[36m32.5440\u001b[0m       \u001b[32m27.1262\u001b[0m  0.0109\n",
      "     21       \u001b[36m32.2681\u001b[0m       \u001b[32m27.0075\u001b[0m  0.0106\n",
      "     22       \u001b[36m32.0035\u001b[0m       \u001b[32m26.9031\u001b[0m  0.0107\n",
      "     23       \u001b[36m31.7506\u001b[0m       \u001b[32m26.8131\u001b[0m  0.0110\n",
      "     24       \u001b[36m31.5096\u001b[0m       \u001b[32m26.7376\u001b[0m  0.0109\n",
      "     25       \u001b[36m31.2804\u001b[0m       \u001b[32m26.6760\u001b[0m  0.0110\n",
      "     26       \u001b[36m31.0636\u001b[0m       \u001b[32m26.6283\u001b[0m  0.0108\n",
      "     27       \u001b[36m30.8596\u001b[0m       \u001b[32m26.5940\u001b[0m  0.0108\n",
      "     28       \u001b[36m30.6688\u001b[0m       \u001b[32m26.5721\u001b[0m  0.0110\n",
      "     29       \u001b[36m30.4911\u001b[0m       \u001b[32m26.5619\u001b[0m  0.0110\n",
      "     30       \u001b[36m30.3263\u001b[0m       26.5624  0.0106\n",
      "     31       \u001b[36m30.1743\u001b[0m       26.5726  0.0107\n",
      "     32       \u001b[36m30.0351\u001b[0m       26.5913  0.0106\n",
      "     33       \u001b[36m29.9082\u001b[0m       26.6171  0.0110\n",
      "     34       \u001b[36m29.7932\u001b[0m       26.6485  0.0109\n",
      "     35       \u001b[36m29.6893\u001b[0m       26.6845  0.0111\n",
      "     36       \u001b[36m29.5961\u001b[0m       26.7238  0.0106\n",
      "     37       \u001b[36m29.5125\u001b[0m       26.7652  0.0106\n",
      "     38       \u001b[36m29.4377\u001b[0m       26.8075  0.0111\n",
      "     39       \u001b[36m29.3710\u001b[0m       26.8497  0.0108\n",
      "     40       \u001b[36m29.3115\u001b[0m       26.8909  0.0108\n",
      "     41       \u001b[36m29.2586\u001b[0m       26.9309  0.0109\n",
      "     42       \u001b[36m29.2113\u001b[0m       26.9690  0.0106\n",
      "     43       \u001b[36m29.1690\u001b[0m       27.0047  0.0109\n",
      "     44       \u001b[36m29.1310\u001b[0m       27.0379  0.0108\n",
      "     45       \u001b[36m29.0969\u001b[0m       27.0683  0.0113\n",
      "     46       \u001b[36m29.0663\u001b[0m       27.0958  0.0107\n",
      "     47       \u001b[36m29.0385\u001b[0m       27.1213  0.0106\n",
      "     48       \u001b[36m29.0134\u001b[0m       27.1440  0.0111\n",
      "     49       \u001b[36m28.9904\u001b[0m       27.1639  0.0112\n",
      "     50       \u001b[36m28.9693\u001b[0m       27.1812  0.0112\n",
      "     51       \u001b[36m28.9497\u001b[0m       27.1961  0.0108\n",
      "     52       \u001b[36m28.9315\u001b[0m       27.2087  0.0107\n",
      "     53       \u001b[36m28.9145\u001b[0m       27.2193  0.0110\n",
      "     54       \u001b[36m28.8986\u001b[0m       27.2283  0.0110\n",
      "     55       \u001b[36m28.8837\u001b[0m       27.2358  0.0111\n",
      "     56       \u001b[36m28.8697\u001b[0m       27.2421  0.0108\n",
      "     57       \u001b[36m28.8566\u001b[0m       27.2480  0.0108\n",
      "     58       \u001b[36m28.8444\u001b[0m       27.2526  0.0110\n",
      "     59       \u001b[36m28.8327\u001b[0m       27.2560  0.0110\n",
      "     60       \u001b[36m28.8217\u001b[0m       27.2583  0.0109\n",
      "     61       \u001b[36m28.8112\u001b[0m       27.2598  0.0108\n",
      "     62       \u001b[36m28.8012\u001b[0m       27.2604  0.0108\n",
      "     63       \u001b[36m28.7916\u001b[0m       27.2603  0.0111\n",
      "     64       \u001b[36m28.7824\u001b[0m       27.2596  0.0110\n",
      "     65       \u001b[36m28.7735\u001b[0m       27.2583  0.0109\n",
      "     66       \u001b[36m28.7649\u001b[0m       27.2571  0.0103\n",
      "     67       \u001b[36m28.7568\u001b[0m       27.2553  0.0108\n",
      "     68       \u001b[36m28.7489\u001b[0m       27.2532  0.0114\n",
      "     69       \u001b[36m28.7413\u001b[0m       27.2509  0.0109\n",
      "     70       \u001b[36m28.7339\u001b[0m       27.2482  0.0107\n",
      "     71       \u001b[36m28.7268\u001b[0m       27.2455  0.0107\n",
      "     72       \u001b[36m28.7199\u001b[0m       27.2425  0.0106\n",
      "     73       \u001b[36m28.7133\u001b[0m       27.2392  0.0110\n",
      "     74       \u001b[36m28.7068\u001b[0m       27.2360  0.0111\n",
      "     75       \u001b[36m28.7006\u001b[0m       27.2329  0.0112\n",
      "     76       \u001b[36m28.6947\u001b[0m       27.2297  0.0105\n",
      "     77       \u001b[36m28.6889\u001b[0m       27.2266  0.0108\n",
      "     78       \u001b[36m28.6833\u001b[0m       27.2233  0.0109\n",
      "     79       \u001b[36m28.6779\u001b[0m       27.2201  0.0107\n",
      "     80       \u001b[36m28.6726\u001b[0m       27.2170  0.0109\n",
      "     81       \u001b[36m28.6675\u001b[0m       27.2138  0.0108\n",
      "     82       \u001b[36m28.6626\u001b[0m       27.2107  0.0107\n",
      "     83       \u001b[36m28.6578\u001b[0m       27.2078  0.0114\n",
      "     84       \u001b[36m28.6533\u001b[0m       27.2050  0.0109\n",
      "     85       \u001b[36m28.6489\u001b[0m       27.2022  0.0111\n",
      "     86       \u001b[36m28.6446\u001b[0m       27.1994  0.0136\n",
      "     87       \u001b[36m28.6404\u001b[0m       27.1966  0.0157\n",
      "     88       \u001b[36m28.6363\u001b[0m       27.1939  0.0132\n",
      "     89       \u001b[36m28.6323\u001b[0m       27.1913  0.0117\n",
      "     90       \u001b[36m28.6284\u001b[0m       27.1886  0.0132\n",
      "     91       \u001b[36m28.6246\u001b[0m       27.1860  0.0137\n",
      "     92       \u001b[36m28.6210\u001b[0m       27.1835  0.0147\n",
      "     93       \u001b[36m28.6174\u001b[0m       27.1810  0.0120\n",
      "     94       \u001b[36m28.6139\u001b[0m       27.1786  0.0127\n",
      "     95       \u001b[36m28.6105\u001b[0m       27.1763  0.0113\n",
      "     96       \u001b[36m28.6072\u001b[0m       27.1740  0.0114\n",
      "     97       \u001b[36m28.6039\u001b[0m       27.1718  0.0114\n",
      "     98       \u001b[36m28.6008\u001b[0m       27.1698  0.0112\n",
      "     99       \u001b[36m28.5977\u001b[0m       27.1677  0.0113\n",
      "    100       \u001b[36m28.5948\u001b[0m       27.1658  0.0110\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m41.7559\u001b[0m       \u001b[32m41.7685\u001b[0m  0.0119\n",
      "      2       \u001b[36m39.6919\u001b[0m       \u001b[32m39.1775\u001b[0m  0.0131\n",
      "      3       \u001b[36m37.1834\u001b[0m       \u001b[32m34.6548\u001b[0m  0.0123\n",
      "      4       \u001b[36m34.4568\u001b[0m       \u001b[32m31.7539\u001b[0m  0.0119\n",
      "      5       \u001b[36m34.3106\u001b[0m       \u001b[32m31.5030\u001b[0m  0.0116\n",
      "      6       \u001b[36m33.9308\u001b[0m       31.6407  0.0116\n",
      "      7       \u001b[36m33.4459\u001b[0m       31.8692  0.0121\n",
      "      8       \u001b[36m33.2852\u001b[0m       \u001b[32m31.4388\u001b[0m  0.0120\n",
      "      9       \u001b[36m33.0156\u001b[0m       \u001b[32m30.7790\u001b[0m  0.0121\n",
      "     10       \u001b[36m32.8404\u001b[0m       \u001b[32m30.4229\u001b[0m  0.0118\n",
      "     11       \u001b[36m32.7606\u001b[0m       \u001b[32m30.3968\u001b[0m  0.0116\n",
      "     12       \u001b[36m32.6550\u001b[0m       30.4780  0.0120\n",
      "     13       \u001b[36m32.5818\u001b[0m       30.4314  0.0124\n",
      "     14       \u001b[36m32.5127\u001b[0m       \u001b[32m30.2601\u001b[0m  0.0122\n",
      "     15       \u001b[36m32.4564\u001b[0m       \u001b[32m30.1468\u001b[0m  0.0121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     16       \u001b[36m32.4157\u001b[0m       \u001b[32m30.1462\u001b[0m  0.0124\n",
      "     17       \u001b[36m32.3780\u001b[0m       30.1520  0.0124\n",
      "     18       \u001b[36m32.3465\u001b[0m       \u001b[32m30.1216\u001b[0m  0.0122\n",
      "     19       \u001b[36m32.3188\u001b[0m       \u001b[32m30.0659\u001b[0m  0.0134\n",
      "     20       \u001b[36m32.2952\u001b[0m       \u001b[32m30.0434\u001b[0m  0.0144\n",
      "     21       \u001b[36m32.2778\u001b[0m       \u001b[32m30.0419\u001b[0m  0.0127\n",
      "     22       \u001b[36m32.2623\u001b[0m       \u001b[32m30.0402\u001b[0m  0.0123\n",
      "     23       \u001b[36m32.2498\u001b[0m       \u001b[32m30.0284\u001b[0m  0.0120\n",
      "     24       \u001b[36m32.2383\u001b[0m       \u001b[32m30.0150\u001b[0m  0.0125\n",
      "     25       \u001b[36m32.2290\u001b[0m       \u001b[32m30.0109\u001b[0m  0.0124\n",
      "     26       \u001b[36m32.2211\u001b[0m       30.0147  0.0123\n",
      "     27       \u001b[36m32.2141\u001b[0m       30.0148  0.0118\n",
      "     28       \u001b[36m32.2074\u001b[0m       \u001b[32m30.0077\u001b[0m  0.0120\n",
      "     29       \u001b[36m32.2015\u001b[0m       \u001b[32m30.0039\u001b[0m  0.0121\n",
      "     30       \u001b[36m32.1964\u001b[0m       30.0044  0.0132\n",
      "     31       \u001b[36m32.1916\u001b[0m       30.0051  0.0129\n",
      "     32       \u001b[36m32.1871\u001b[0m       \u001b[32m30.0009\u001b[0m  0.0132\n",
      "     33       \u001b[36m32.1830\u001b[0m       \u001b[32m29.9976\u001b[0m  0.0126\n",
      "     34       \u001b[36m32.1796\u001b[0m       \u001b[32m29.9955\u001b[0m  0.0125\n",
      "     35       \u001b[36m32.1769\u001b[0m       29.9967  0.0138\n",
      "     36       \u001b[36m32.1742\u001b[0m       \u001b[32m29.9917\u001b[0m  0.0120\n",
      "     37       32.1742       30.0028  0.0120\n",
      "     38       \u001b[36m32.1732\u001b[0m       \u001b[32m29.9889\u001b[0m  0.0133\n",
      "     39       32.1767       30.0088  0.0131\n",
      "     40       32.1732       \u001b[32m29.9571\u001b[0m  0.0125\n",
      "     41       \u001b[36m32.1648\u001b[0m       29.9607  0.0125\n",
      "     42       \u001b[36m32.1621\u001b[0m       \u001b[32m29.9527\u001b[0m  0.0119\n",
      "     43       \u001b[36m32.1511\u001b[0m       \u001b[32m29.9484\u001b[0m  0.0121\n",
      "     44       \u001b[36m32.1497\u001b[0m       29.9596  0.0123\n",
      "     45       \u001b[36m32.1461\u001b[0m       \u001b[32m29.9466\u001b[0m  0.0136\n",
      "     46       \u001b[36m32.1434\u001b[0m       29.9543  0.0128\n",
      "     47       \u001b[36m32.1418\u001b[0m       \u001b[32m29.9462\u001b[0m  0.0135\n",
      "     48       \u001b[36m32.1381\u001b[0m       29.9472  0.0126\n",
      "     49       \u001b[36m32.1368\u001b[0m       \u001b[32m29.9431\u001b[0m  0.0119\n",
      "     50       \u001b[36m32.1335\u001b[0m       29.9439  0.0132\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m31.8736\u001b[0m       \u001b[32m29.8853\u001b[0m  0.0121\n",
      "      2       \u001b[36m29.1738\u001b[0m       \u001b[32m27.8030\u001b[0m  0.0129\n",
      "      3       \u001b[36m26.1683\u001b[0m       \u001b[32m27.2185\u001b[0m  0.0124\n",
      "      4       \u001b[36m24.9200\u001b[0m       28.6709  0.0125\n",
      "      5       \u001b[36m24.6221\u001b[0m       \u001b[32m26.9425\u001b[0m  0.0121\n",
      "      6       \u001b[36m24.0296\u001b[0m       \u001b[32m26.4157\u001b[0m  0.0120\n",
      "      7       \u001b[36m23.9267\u001b[0m       \u001b[32m26.3672\u001b[0m  0.0127\n",
      "      8       \u001b[36m23.6862\u001b[0m       26.7191  0.0128\n",
      "      9       \u001b[36m23.5108\u001b[0m       27.1366  0.0124\n",
      "     10       \u001b[36m23.4202\u001b[0m       26.9902  0.0128\n",
      "     11       \u001b[36m23.3320\u001b[0m       26.6962  0.0130\n",
      "     12       \u001b[36m23.3037\u001b[0m       26.6169  0.0197\n",
      "     13       \u001b[36m23.2591\u001b[0m       26.7170  0.0132\n",
      "     14       \u001b[36m23.2172\u001b[0m       26.7795  0.0165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     15       \u001b[36m23.1890\u001b[0m       26.7265  0.0132\n",
      "     16       \u001b[36m23.1673\u001b[0m       26.6606  0.0145\n",
      "     17       \u001b[36m23.1546\u001b[0m       26.6465  0.0130\n",
      "     18       \u001b[36m23.1392\u001b[0m       26.6610  0.0140\n",
      "     19       \u001b[36m23.1250\u001b[0m       26.6372  0.0127\n",
      "     20       \u001b[36m23.1142\u001b[0m       26.6079  0.0123\n",
      "     21       \u001b[36m23.1056\u001b[0m       26.5878  0.0125\n",
      "     22       \u001b[36m23.0984\u001b[0m       26.5924  0.0123\n",
      "     23       \u001b[36m23.0902\u001b[0m       26.5858  0.0119\n",
      "     24       \u001b[36m23.0838\u001b[0m       26.5862  0.0120\n",
      "     25       \u001b[36m23.0776\u001b[0m       26.5739  0.0130\n",
      "     26       \u001b[36m23.0734\u001b[0m       26.5850  0.0124\n",
      "     27       \u001b[36m23.0679\u001b[0m       26.5766  0.0127\n",
      "     28       \u001b[36m23.0644\u001b[0m       26.5907  0.0123\n",
      "     29       \u001b[36m23.0593\u001b[0m       26.5688  0.0113\n",
      "     30       \u001b[36m23.0569\u001b[0m       26.5804  0.0124\n",
      "     31       \u001b[36m23.0521\u001b[0m       26.5575  0.0124\n",
      "     32       \u001b[36m23.0503\u001b[0m       26.5791  0.0126\n",
      "     33       \u001b[36m23.0456\u001b[0m       26.5483  0.0121\n",
      "     34       23.0457       26.5742  0.0118\n",
      "     35       \u001b[36m23.0412\u001b[0m       26.5389  0.0126\n",
      "     36       23.0426       26.5863  0.0132\n",
      "     37       \u001b[36m23.0382\u001b[0m       26.5385  0.0125\n",
      "     38       23.0408       26.5995  0.0118\n",
      "     39       \u001b[36m23.0361\u001b[0m       26.5390  0.0117\n",
      "     40       23.0391       26.6063  0.0120\n",
      "     41       \u001b[36m23.0343\u001b[0m       26.5346  0.0118\n",
      "     42       23.0408       26.6227  0.0120\n",
      "     43       23.0371       26.5349  0.0120\n",
      "     44       23.0457       26.6391  0.0120\n",
      "     45       23.0423       26.5265  0.0122\n",
      "     46       23.0537       26.6545  0.0121\n",
      "     47       23.0502       26.5348  0.0122\n",
      "     48       23.0593       26.6486  0.0121\n",
      "     49       23.0517       26.5419  0.0121\n",
      "     50       23.0500       26.6230  0.0123\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m39.8273\u001b[0m       \u001b[32m31.0110\u001b[0m  0.0126\n",
      "      2       \u001b[36m37.5134\u001b[0m       \u001b[32m29.2305\u001b[0m  0.0130\n",
      "      3       \u001b[36m34.6429\u001b[0m       \u001b[32m27.1450\u001b[0m  0.0123\n",
      "      4       \u001b[36m31.1397\u001b[0m       28.2218  0.0126\n",
      "      5       \u001b[36m30.9627\u001b[0m       29.9269  0.0120\n",
      "      6       \u001b[36m30.6088\u001b[0m       27.4717  0.0119\n",
      "      7       \u001b[36m29.8032\u001b[0m       \u001b[32m26.6908\u001b[0m  0.0122\n",
      "      8       \u001b[36m29.6990\u001b[0m       26.7809  0.0128\n",
      "      9       \u001b[36m29.3764\u001b[0m       27.4638  0.0125\n",
      "     10       \u001b[36m29.2896\u001b[0m       28.0145  0.0130\n",
      "     11       \u001b[36m29.2155\u001b[0m       27.6792  0.0123\n",
      "     12       \u001b[36m29.0284\u001b[0m       27.3147  0.0125\n",
      "     13       \u001b[36m28.9096\u001b[0m       27.3107  0.0124\n",
      "     14       \u001b[36m28.8359\u001b[0m       27.4709  0.0120\n",
      "     15       \u001b[36m28.7918\u001b[0m       27.5496  0.0120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     16       \u001b[36m28.7444\u001b[0m       27.4588  0.0117\n",
      "     17       \u001b[36m28.6903\u001b[0m       27.3691  0.0128\n",
      "     18       \u001b[36m28.6481\u001b[0m       27.4109  0.0128\n",
      "     19       \u001b[36m28.6229\u001b[0m       27.4521  0.0124\n",
      "     20       \u001b[36m28.6009\u001b[0m       27.4334  0.0133\n",
      "     21       \u001b[36m28.5773\u001b[0m       27.4140  0.0116\n",
      "     22       \u001b[36m28.5571\u001b[0m       27.3917  0.0121\n",
      "     23       \u001b[36m28.5408\u001b[0m       27.3706  0.0131\n",
      "     24       \u001b[36m28.5273\u001b[0m       27.3521  0.0128\n",
      "     25       \u001b[36m28.5162\u001b[0m       27.3377  0.0125\n",
      "     26       \u001b[36m28.5060\u001b[0m       27.3325  0.0118\n",
      "     27       \u001b[36m28.4971\u001b[0m       27.3242  0.0117\n",
      "     28       \u001b[36m28.4885\u001b[0m       27.3205  0.0125\n",
      "     29       \u001b[36m28.4819\u001b[0m       27.3196  0.0120\n",
      "     30       \u001b[36m28.4754\u001b[0m       27.3161  0.0124\n",
      "     31       \u001b[36m28.4693\u001b[0m       27.3165  0.0119\n",
      "     32       \u001b[36m28.4643\u001b[0m       27.3246  0.0118\n",
      "     33       \u001b[36m28.4596\u001b[0m       27.3247  0.0131\n",
      "     34       \u001b[36m28.4550\u001b[0m       27.3236  0.0124\n",
      "     35       \u001b[36m28.4509\u001b[0m       27.3296  0.0123\n",
      "     36       \u001b[36m28.4475\u001b[0m       27.3330  0.0127\n",
      "     37       \u001b[36m28.4440\u001b[0m       27.3345  0.0219\n",
      "     38       \u001b[36m28.4408\u001b[0m       27.3394  0.0127\n",
      "     39       \u001b[36m28.4378\u001b[0m       27.3424  0.0128\n",
      "     40       \u001b[36m28.4353\u001b[0m       27.3413  0.0129\n",
      "     41       \u001b[36m28.4326\u001b[0m       27.3431  0.0141\n",
      "     42       \u001b[36m28.4300\u001b[0m       27.3424  0.0163\n",
      "     43       \u001b[36m28.4274\u001b[0m       27.3448  0.0137\n",
      "     44       \u001b[36m28.4258\u001b[0m       27.3458  0.0128\n",
      "     45       \u001b[36m28.4230\u001b[0m       27.3501  0.0125\n",
      "     46       \u001b[36m28.4216\u001b[0m       27.3510  0.0125\n",
      "     47       \u001b[36m28.4195\u001b[0m       27.3549  0.0124\n",
      "     48       \u001b[36m28.4182\u001b[0m       27.3604  0.0127\n",
      "     49       \u001b[36m28.4164\u001b[0m       27.3560  0.0120\n",
      "     50       \u001b[36m28.4144\u001b[0m       27.3635  0.0123\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m42.2178\u001b[0m       \u001b[32m43.2303\u001b[0m  0.0112\n",
      "      2       \u001b[36m40.9961\u001b[0m       \u001b[32m41.8126\u001b[0m  0.0127\n",
      "      3       \u001b[36m39.8812\u001b[0m       \u001b[32m40.4636\u001b[0m  0.0119\n",
      "      4       \u001b[36m38.8299\u001b[0m       \u001b[32m39.1543\u001b[0m  0.0111\n",
      "      5       \u001b[36m37.8226\u001b[0m       \u001b[32m37.8620\u001b[0m  0.0114\n",
      "      6       \u001b[36m36.8519\u001b[0m       \u001b[32m36.5914\u001b[0m  0.0110\n",
      "      7       \u001b[36m35.9313\u001b[0m       \u001b[32m35.3705\u001b[0m  0.0111\n",
      "      8       \u001b[36m35.0872\u001b[0m       \u001b[32m34.2412\u001b[0m  0.0117\n",
      "      9       \u001b[36m34.3519\u001b[0m       \u001b[32m33.2456\u001b[0m  0.0126\n",
      "     10       \u001b[36m33.7516\u001b[0m       \u001b[32m32.4126\u001b[0m  0.0112\n",
      "     11       \u001b[36m33.2977\u001b[0m       \u001b[32m31.7545\u001b[0m  0.0115\n",
      "     12       \u001b[36m32.9797\u001b[0m       \u001b[32m31.2593\u001b[0m  0.0118\n",
      "     13       \u001b[36m32.7718\u001b[0m       \u001b[32m30.9002\u001b[0m  0.0117\n",
      "     14       \u001b[36m32.6424\u001b[0m       \u001b[32m30.6467\u001b[0m  0.0119\n",
      "     15       \u001b[36m32.5637\u001b[0m       \u001b[32m30.4700\u001b[0m  0.0128\n",
      "     16       \u001b[36m32.5156\u001b[0m       \u001b[32m30.3473\u001b[0m  0.0108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m32.4844\u001b[0m       \u001b[32m30.2603\u001b[0m  0.0129\n",
      "     18       \u001b[36m32.4622\u001b[0m       \u001b[32m30.1975\u001b[0m  0.0113\n",
      "     19       \u001b[36m32.4450\u001b[0m       \u001b[32m30.1508\u001b[0m  0.0128\n",
      "     20       \u001b[36m32.4308\u001b[0m       \u001b[32m30.1151\u001b[0m  0.0118\n",
      "     21       \u001b[36m32.4186\u001b[0m       \u001b[32m30.0871\u001b[0m  0.0113\n",
      "     22       \u001b[36m32.4079\u001b[0m       \u001b[32m30.0646\u001b[0m  0.0120\n",
      "     23       \u001b[36m32.3982\u001b[0m       \u001b[32m30.0460\u001b[0m  0.0114\n",
      "     24       \u001b[36m32.3894\u001b[0m       \u001b[32m30.0301\u001b[0m  0.0113\n",
      "     25       \u001b[36m32.3813\u001b[0m       \u001b[32m30.0161\u001b[0m  0.0113\n",
      "     26       \u001b[36m32.3738\u001b[0m       \u001b[32m30.0038\u001b[0m  0.0111\n",
      "     27       \u001b[36m32.3667\u001b[0m       \u001b[32m29.9929\u001b[0m  0.0114\n",
      "     28       \u001b[36m32.3602\u001b[0m       \u001b[32m29.9830\u001b[0m  0.0112\n",
      "     29       \u001b[36m32.3542\u001b[0m       \u001b[32m29.9739\u001b[0m  0.0113\n",
      "     30       \u001b[36m32.3485\u001b[0m       \u001b[32m29.9655\u001b[0m  0.0115\n",
      "     31       \u001b[36m32.3432\u001b[0m       \u001b[32m29.9579\u001b[0m  0.0110\n",
      "     32       \u001b[36m32.3382\u001b[0m       \u001b[32m29.9509\u001b[0m  0.0116\n",
      "     33       \u001b[36m32.3335\u001b[0m       \u001b[32m29.9443\u001b[0m  0.0113\n",
      "     34       \u001b[36m32.3291\u001b[0m       \u001b[32m29.9380\u001b[0m  0.0123\n",
      "     35       \u001b[36m32.3251\u001b[0m       \u001b[32m29.9322\u001b[0m  0.0115\n",
      "     36       \u001b[36m32.3212\u001b[0m       \u001b[32m29.9267\u001b[0m  0.0114\n",
      "     37       \u001b[36m32.3177\u001b[0m       \u001b[32m29.9215\u001b[0m  0.0117\n",
      "     38       \u001b[36m32.3143\u001b[0m       \u001b[32m29.9166\u001b[0m  0.0119\n",
      "     39       \u001b[36m32.3110\u001b[0m       \u001b[32m29.9119\u001b[0m  0.0119\n",
      "     40       \u001b[36m32.3079\u001b[0m       \u001b[32m29.9075\u001b[0m  0.0112\n",
      "     41       \u001b[36m32.3050\u001b[0m       \u001b[32m29.9035\u001b[0m  0.0114\n",
      "     42       \u001b[36m32.3022\u001b[0m       \u001b[32m29.8997\u001b[0m  0.0115\n",
      "     43       \u001b[36m32.2995\u001b[0m       \u001b[32m29.8962\u001b[0m  0.0120\n",
      "     44       \u001b[36m32.2969\u001b[0m       \u001b[32m29.8926\u001b[0m  0.0117\n",
      "     45       \u001b[36m32.2945\u001b[0m       \u001b[32m29.8892\u001b[0m  0.0117\n",
      "     46       \u001b[36m32.2922\u001b[0m       \u001b[32m29.8860\u001b[0m  0.0113\n",
      "     47       \u001b[36m32.2900\u001b[0m       \u001b[32m29.8829\u001b[0m  0.0132\n",
      "     48       \u001b[36m32.2879\u001b[0m       \u001b[32m29.8800\u001b[0m  0.0113\n",
      "     49       \u001b[36m32.2858\u001b[0m       \u001b[32m29.8774\u001b[0m  0.0116\n",
      "     50       \u001b[36m32.2838\u001b[0m       \u001b[32m29.8749\u001b[0m  0.0112\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m34.7613\u001b[0m       \u001b[32m32.4947\u001b[0m  0.0115\n",
      "      2       \u001b[36m33.1667\u001b[0m       \u001b[32m31.3580\u001b[0m  0.0120\n",
      "      3       \u001b[36m31.6917\u001b[0m       \u001b[32m30.3033\u001b[0m  0.0113\n",
      "      4       \u001b[36m30.2788\u001b[0m       \u001b[32m29.3075\u001b[0m  0.0116\n",
      "      5       \u001b[36m28.9109\u001b[0m       \u001b[32m28.3797\u001b[0m  0.0110\n",
      "      6       \u001b[36m27.6103\u001b[0m       \u001b[32m27.5603\u001b[0m  0.0113\n",
      "      7       \u001b[36m26.4305\u001b[0m       \u001b[32m26.9005\u001b[0m  0.0114\n",
      "      8       \u001b[36m25.4299\u001b[0m       \u001b[32m26.4413\u001b[0m  0.0112\n",
      "      9       \u001b[36m24.6485\u001b[0m       \u001b[32m26.1916\u001b[0m  0.0112\n",
      "     10       \u001b[36m24.0965\u001b[0m       \u001b[32m26.1171\u001b[0m  0.0112\n",
      "     11       \u001b[36m23.7478\u001b[0m       26.1511  0.0113\n",
      "     12       \u001b[36m23.5473\u001b[0m       26.2260  0.0111\n",
      "     13       \u001b[36m23.4371\u001b[0m       26.2988  0.0113\n",
      "     14       \u001b[36m23.3747\u001b[0m       26.3540  0.0113\n",
      "     15       \u001b[36m23.3361\u001b[0m       26.3911  0.0140\n",
      "     16       \u001b[36m23.3093\u001b[0m       26.4141  0.0165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m23.2890\u001b[0m       26.4277  0.0140\n",
      "     18       \u001b[36m23.2723\u001b[0m       26.4348  0.0131\n",
      "     19       \u001b[36m23.2583\u001b[0m       26.4380  0.0138\n",
      "     20       \u001b[36m23.2460\u001b[0m       26.4391  0.0124\n",
      "     21       \u001b[36m23.2349\u001b[0m       26.4387  0.0135\n",
      "     22       \u001b[36m23.2248\u001b[0m       26.4376  0.0126\n",
      "     23       \u001b[36m23.2154\u001b[0m       26.4362  0.0118\n",
      "     24       \u001b[36m23.2068\u001b[0m       26.4346  0.0117\n",
      "     25       \u001b[36m23.1989\u001b[0m       26.4332  0.0116\n",
      "     26       \u001b[36m23.1915\u001b[0m       26.4316  0.0131\n",
      "     27       \u001b[36m23.1848\u001b[0m       26.4303  0.0112\n",
      "     28       \u001b[36m23.1786\u001b[0m       26.4292  0.0114\n",
      "     29       \u001b[36m23.1729\u001b[0m       26.4280  0.0112\n",
      "     30       \u001b[36m23.1676\u001b[0m       26.4269  0.0113\n",
      "     31       \u001b[36m23.1626\u001b[0m       26.4258  0.0116\n",
      "     32       \u001b[36m23.1579\u001b[0m       26.4248  0.0112\n",
      "     33       \u001b[36m23.1536\u001b[0m       26.4241  0.0112\n",
      "     34       \u001b[36m23.1496\u001b[0m       26.4233  0.0108\n",
      "     35       \u001b[36m23.1458\u001b[0m       26.4227  0.0114\n",
      "     36       \u001b[36m23.1422\u001b[0m       26.4220  0.0128\n",
      "     37       \u001b[36m23.1388\u001b[0m       26.4213  0.0205\n",
      "     38       \u001b[36m23.1357\u001b[0m       26.4208  0.0110\n",
      "     39       \u001b[36m23.1327\u001b[0m       26.4204  0.0117\n",
      "     40       \u001b[36m23.1299\u001b[0m       26.4200  0.0107\n",
      "     41       \u001b[36m23.1273\u001b[0m       26.4196  0.0117\n",
      "     42       \u001b[36m23.1248\u001b[0m       26.4193  0.0126\n",
      "     43       \u001b[36m23.1225\u001b[0m       26.4190  0.0114\n",
      "     44       \u001b[36m23.1203\u001b[0m       26.4186  0.0106\n",
      "     45       \u001b[36m23.1182\u001b[0m       26.4181  0.0111\n",
      "     46       \u001b[36m23.1162\u001b[0m       26.4178  0.0115\n",
      "     47       \u001b[36m23.1143\u001b[0m       26.4175  0.0117\n",
      "     48       \u001b[36m23.1125\u001b[0m       26.4171  0.0111\n",
      "     49       \u001b[36m23.1108\u001b[0m       26.4168  0.0111\n",
      "     50       \u001b[36m23.1091\u001b[0m       26.4165  0.0115\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m42.2924\u001b[0m       \u001b[32m32.6656\u001b[0m  0.0114\n",
      "      2       \u001b[36m40.2555\u001b[0m       \u001b[32m31.3097\u001b[0m  0.0113\n",
      "      3       \u001b[36m38.3809\u001b[0m       \u001b[32m30.0495\u001b[0m  0.0114\n",
      "      4       \u001b[36m36.5430\u001b[0m       \u001b[32m28.8618\u001b[0m  0.0112\n",
      "      5       \u001b[36m34.7388\u001b[0m       \u001b[32m27.7956\u001b[0m  0.0111\n",
      "      6       \u001b[36m33.0428\u001b[0m       \u001b[32m26.9399\u001b[0m  0.0110\n",
      "      7       \u001b[36m31.5577\u001b[0m       \u001b[32m26.3824\u001b[0m  0.0113\n",
      "      8       \u001b[36m30.3683\u001b[0m       \u001b[32m26.1741\u001b[0m  0.0112\n",
      "      9       \u001b[36m29.5356\u001b[0m       26.2759  0.0110\n",
      "     10       \u001b[36m29.0581\u001b[0m       26.5390  0.0110\n",
      "     11       \u001b[36m28.8384\u001b[0m       26.8026  0.0109\n",
      "     12       \u001b[36m28.7515\u001b[0m       26.9908  0.0112\n",
      "     13       \u001b[36m28.7142\u001b[0m       27.1021  0.0111\n",
      "     14       \u001b[36m28.6911\u001b[0m       27.1613  0.0110\n",
      "     15       \u001b[36m28.6719\u001b[0m       27.1904  0.0112\n",
      "     16       \u001b[36m28.6540\u001b[0m       27.2030  0.0114\n",
      "     17       \u001b[36m28.6371\u001b[0m       27.2073  0.0113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     18       \u001b[36m28.6213\u001b[0m       27.2080  0.0119\n",
      "     19       \u001b[36m28.6069\u001b[0m       27.2069  0.0113\n",
      "     20       \u001b[36m28.5935\u001b[0m       27.2046  0.0111\n",
      "     21       \u001b[36m28.5812\u001b[0m       27.2027  0.0109\n",
      "     22       \u001b[36m28.5702\u001b[0m       27.2012  0.0110\n",
      "     23       \u001b[36m28.5600\u001b[0m       27.2000  0.0114\n",
      "     24       \u001b[36m28.5508\u001b[0m       27.1988  0.0114\n",
      "     25       \u001b[36m28.5423\u001b[0m       27.1978  0.0119\n",
      "     26       \u001b[36m28.5346\u001b[0m       27.1966  0.0114\n",
      "     27       \u001b[36m28.5276\u001b[0m       27.1960  0.0113\n",
      "     28       \u001b[36m28.5212\u001b[0m       27.1951  0.0111\n",
      "     29       \u001b[36m28.5151\u001b[0m       27.1946  0.0112\n",
      "     30       \u001b[36m28.5096\u001b[0m       27.1950  0.0112\n",
      "     31       \u001b[36m28.5044\u001b[0m       27.1952  0.0113\n",
      "     32       \u001b[36m28.4996\u001b[0m       27.1949  0.0108\n",
      "     33       \u001b[36m28.4950\u001b[0m       27.1947  0.0119\n",
      "     34       \u001b[36m28.4908\u001b[0m       27.1945  0.0111\n",
      "     35       \u001b[36m28.4870\u001b[0m       27.1939  0.0112\n",
      "     36       \u001b[36m28.4833\u001b[0m       27.1940  0.0119\n",
      "     37       \u001b[36m28.4799\u001b[0m       27.1926  0.0111\n",
      "     38       \u001b[36m28.4765\u001b[0m       27.1913  0.0119\n",
      "     39       \u001b[36m28.4732\u001b[0m       27.1906  0.0116\n",
      "     40       \u001b[36m28.4703\u001b[0m       27.1906  0.0121\n",
      "     41       \u001b[36m28.4678\u001b[0m       27.1906  0.0121\n",
      "     42       \u001b[36m28.4653\u001b[0m       27.1906  0.0114\n",
      "     43       \u001b[36m28.4630\u001b[0m       27.1901  0.0116\n",
      "     44       \u001b[36m28.4607\u001b[0m       27.1899  0.0114\n",
      "     45       \u001b[36m28.4586\u001b[0m       27.1896  0.0145\n",
      "     46       \u001b[36m28.4566\u001b[0m       27.1892  0.0129\n",
      "     47       \u001b[36m28.4547\u001b[0m       27.1893  0.0118\n",
      "     48       \u001b[36m28.4529\u001b[0m       27.1897  0.0123\n",
      "     49       \u001b[36m28.4513\u001b[0m       27.1902  0.0118\n",
      "     50       \u001b[36m28.4497\u001b[0m       27.1905  0.0115\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m42.7228\u001b[0m       \u001b[32m42.8480\u001b[0m  0.0137\n",
      "      2       \u001b[36m40.2137\u001b[0m       \u001b[32m39.3722\u001b[0m  0.0146\n",
      "      3       \u001b[36m37.3401\u001b[0m       \u001b[32m34.1836\u001b[0m  0.0120\n",
      "      4       \u001b[36m34.6452\u001b[0m       \u001b[32m31.9147\u001b[0m  0.0138\n",
      "      5       34.6940       \u001b[32m31.5591\u001b[0m  0.0122\n",
      "      6       \u001b[36m33.8115\u001b[0m       32.0471  0.0118\n",
      "      7       \u001b[36m33.5861\u001b[0m       31.9963  0.0122\n",
      "      8       \u001b[36m33.3025\u001b[0m       \u001b[32m31.0895\u001b[0m  0.0118\n",
      "      9       \u001b[36m33.0159\u001b[0m       \u001b[32m30.5445\u001b[0m  0.0119\n",
      "     10       \u001b[36m32.8928\u001b[0m       \u001b[32m30.4914\u001b[0m  0.0125\n",
      "     11       \u001b[36m32.7484\u001b[0m       30.6170  0.0130\n",
      "     12       \u001b[36m32.6581\u001b[0m       \u001b[32m30.4794\u001b[0m  0.0121\n",
      "     13       \u001b[36m32.5652\u001b[0m       \u001b[32m30.2827\u001b[0m  0.0119\n",
      "     14       \u001b[36m32.4939\u001b[0m       \u001b[32m30.2478\u001b[0m  0.0120\n",
      "     15       \u001b[36m32.4417\u001b[0m       \u001b[32m30.2218\u001b[0m  0.0122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     16       \u001b[36m32.3949\u001b[0m       30.2401  0.0122\n",
      "     17       \u001b[36m32.3571\u001b[0m       \u001b[32m30.2050\u001b[0m  0.0231\n",
      "     18       \u001b[36m32.3234\u001b[0m       \u001b[32m30.1179\u001b[0m  0.0119\n",
      "     19       \u001b[36m32.3016\u001b[0m       30.1322  0.0116\n",
      "     20       \u001b[36m32.2845\u001b[0m       30.1181  0.0119\n",
      "     21       \u001b[36m32.2679\u001b[0m       \u001b[32m30.1043\u001b[0m  0.0120\n",
      "     22       \u001b[36m32.2541\u001b[0m       \u001b[32m30.0664\u001b[0m  0.0119\n",
      "     23       \u001b[36m32.2423\u001b[0m       \u001b[32m30.0616\u001b[0m  0.0118\n",
      "     24       \u001b[36m32.2339\u001b[0m       \u001b[32m30.0558\u001b[0m  0.0137\n",
      "     25       \u001b[36m32.2251\u001b[0m       \u001b[32m30.0490\u001b[0m  0.0118\n",
      "     26       \u001b[36m32.2177\u001b[0m       \u001b[32m30.0324\u001b[0m  0.0124\n",
      "     27       \u001b[36m32.2111\u001b[0m       \u001b[32m30.0317\u001b[0m  0.0125\n",
      "     28       \u001b[36m32.2056\u001b[0m       \u001b[32m30.0209\u001b[0m  0.0122\n",
      "     29       \u001b[36m32.2001\u001b[0m       \u001b[32m30.0193\u001b[0m  0.0128\n",
      "     30       \u001b[36m32.1948\u001b[0m       \u001b[32m30.0068\u001b[0m  0.0119\n",
      "     31       \u001b[36m32.1899\u001b[0m       \u001b[32m30.0065\u001b[0m  0.0127\n",
      "     32       \u001b[36m32.1857\u001b[0m       \u001b[32m29.9916\u001b[0m  0.0120\n",
      "     33       \u001b[36m32.1814\u001b[0m       29.9960  0.0121\n",
      "     34       \u001b[36m32.1776\u001b[0m       \u001b[32m29.9863\u001b[0m  0.0121\n",
      "     35       \u001b[36m32.1744\u001b[0m       29.9935  0.0120\n",
      "     36       \u001b[36m32.1710\u001b[0m       \u001b[32m29.9814\u001b[0m  0.0124\n",
      "     37       \u001b[36m32.1683\u001b[0m       29.9891  0.0125\n",
      "     38       \u001b[36m32.1650\u001b[0m       \u001b[32m29.9772\u001b[0m  0.0123\n",
      "     39       \u001b[36m32.1629\u001b[0m       29.9825  0.0119\n",
      "     40       \u001b[36m32.1594\u001b[0m       \u001b[32m29.9714\u001b[0m  0.0121\n",
      "     41       \u001b[36m32.1578\u001b[0m       29.9762  0.0118\n",
      "     42       \u001b[36m32.1543\u001b[0m       \u001b[32m29.9668\u001b[0m  0.0115\n",
      "     43       \u001b[36m32.1527\u001b[0m       29.9695  0.0118\n",
      "     44       \u001b[36m32.1495\u001b[0m       \u001b[32m29.9668\u001b[0m  0.0125\n",
      "     45       \u001b[36m32.1479\u001b[0m       \u001b[32m29.9662\u001b[0m  0.0119\n",
      "     46       \u001b[36m32.1452\u001b[0m       \u001b[32m29.9653\u001b[0m  0.0117\n",
      "     47       \u001b[36m32.1435\u001b[0m       \u001b[32m29.9648\u001b[0m  0.0119\n",
      "     48       \u001b[36m32.1414\u001b[0m       \u001b[32m29.9631\u001b[0m  0.0122\n",
      "     49       \u001b[36m32.1393\u001b[0m       \u001b[32m29.9622\u001b[0m  0.0125\n",
      "     50       \u001b[36m32.1377\u001b[0m       \u001b[32m29.9595\u001b[0m  0.0129\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m34.3341\u001b[0m       \u001b[32m31.7794\u001b[0m  0.0124\n",
      "      2       \u001b[36m31.8960\u001b[0m       \u001b[32m29.4854\u001b[0m  0.0120\n",
      "      3       \u001b[36m28.8335\u001b[0m       \u001b[32m27.5394\u001b[0m  0.0119\n",
      "      4       \u001b[36m25.7510\u001b[0m       29.4173  0.0129\n",
      "      5       \u001b[36m25.4955\u001b[0m       28.0899  0.0130\n",
      "      6       \u001b[36m24.6022\u001b[0m       \u001b[32m26.6506\u001b[0m  0.0117\n",
      "      7       \u001b[36m24.3602\u001b[0m       \u001b[32m26.4494\u001b[0m  0.0120\n",
      "      8       \u001b[36m24.1116\u001b[0m       26.6343  0.0117\n",
      "      9       \u001b[36m23.8355\u001b[0m       27.0800  0.0136\n",
      "     10       \u001b[36m23.6988\u001b[0m       27.0638  0.0133\n",
      "     11       \u001b[36m23.5649\u001b[0m       26.9361  0.0116\n",
      "     12       \u001b[36m23.5094\u001b[0m       26.9378  0.0118\n",
      "     13       \u001b[36m23.4399\u001b[0m       26.9952  0.0129\n",
      "     14       \u001b[36m23.3752\u001b[0m       26.9501  0.0122\n",
      "     15       \u001b[36m23.3269\u001b[0m       26.8261  0.0136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     16       \u001b[36m23.2893\u001b[0m       26.7613  0.0128\n",
      "     17       \u001b[36m23.2619\u001b[0m       26.7892  0.0122\n",
      "     18       \u001b[36m23.2351\u001b[0m       26.8447  0.0139\n",
      "     19       \u001b[36m23.2115\u001b[0m       26.8519  0.0123\n",
      "     20       \u001b[36m23.1927\u001b[0m       26.8260  0.0149\n",
      "     21       \u001b[36m23.1771\u001b[0m       26.8085  0.0157\n",
      "     22       \u001b[36m23.1647\u001b[0m       26.8104  0.0125\n",
      "     23       \u001b[36m23.1528\u001b[0m       26.8121  0.0158\n",
      "     24       \u001b[36m23.1425\u001b[0m       26.8054  0.0152\n",
      "     25       \u001b[36m23.1343\u001b[0m       26.8015  0.0160\n",
      "     26       \u001b[36m23.1265\u001b[0m       26.8052  0.0134\n",
      "     27       \u001b[36m23.1196\u001b[0m       26.8104  0.0126\n",
      "     28       \u001b[36m23.1133\u001b[0m       26.8044  0.0123\n",
      "     29       \u001b[36m23.1076\u001b[0m       26.8008  0.0123\n",
      "     30       \u001b[36m23.1020\u001b[0m       26.8017  0.0136\n",
      "     31       \u001b[36m23.0972\u001b[0m       26.8048  0.0122\n",
      "     32       \u001b[36m23.0926\u001b[0m       26.8073  0.0125\n",
      "     33       \u001b[36m23.0881\u001b[0m       26.8079  0.0124\n",
      "     34       \u001b[36m23.0840\u001b[0m       26.8077  0.0120\n",
      "     35       \u001b[36m23.0800\u001b[0m       26.8051  0.0127\n",
      "     36       \u001b[36m23.0765\u001b[0m       26.8014  0.0123\n",
      "     37       \u001b[36m23.0732\u001b[0m       26.7998  0.0117\n",
      "     38       \u001b[36m23.0699\u001b[0m       26.8008  0.0124\n",
      "     39       \u001b[36m23.0668\u001b[0m       26.7982  0.0122\n",
      "     40       \u001b[36m23.0641\u001b[0m       26.7924  0.0137\n",
      "     41       \u001b[36m23.0614\u001b[0m       26.7883  0.0130\n",
      "     42       \u001b[36m23.0587\u001b[0m       26.7864  0.0140\n",
      "     43       \u001b[36m23.0561\u001b[0m       26.7854  0.0225\n",
      "     44       \u001b[36m23.0538\u001b[0m       26.7844  0.0120\n",
      "     45       \u001b[36m23.0514\u001b[0m       26.7849  0.0120\n",
      "     46       \u001b[36m23.0492\u001b[0m       26.7813  0.0122\n",
      "     47       \u001b[36m23.0470\u001b[0m       26.7797  0.0126\n",
      "     48       \u001b[36m23.0447\u001b[0m       26.7762  0.0121\n",
      "     49       \u001b[36m23.0428\u001b[0m       26.7735  0.0124\n",
      "     50       \u001b[36m23.0406\u001b[0m       26.7717  0.0122\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m39.5822\u001b[0m       \u001b[32m30.8194\u001b[0m  0.0124\n",
      "      2       \u001b[36m37.4503\u001b[0m       \u001b[32m29.2861\u001b[0m  0.0119\n",
      "      3       \u001b[36m34.8855\u001b[0m       \u001b[32m27.1926\u001b[0m  0.0116\n",
      "      4       \u001b[36m31.3542\u001b[0m       27.9096  0.0120\n",
      "      5       \u001b[36m30.6775\u001b[0m       30.2425  0.0119\n",
      "      6       \u001b[36m30.4674\u001b[0m       27.4853  0.0117\n",
      "      7       \u001b[36m29.5666\u001b[0m       \u001b[32m26.5609\u001b[0m  0.0119\n",
      "      8       \u001b[36m29.3511\u001b[0m       26.6610  0.0118\n",
      "      9       \u001b[36m29.1023\u001b[0m       27.3942  0.0121\n",
      "     10       \u001b[36m29.0709\u001b[0m       27.9015  0.0117\n",
      "     11       \u001b[36m28.9870\u001b[0m       27.5458  0.0118\n",
      "     12       \u001b[36m28.8213\u001b[0m       27.2039  0.0116\n",
      "     13       \u001b[36m28.7313\u001b[0m       27.2575  0.0117\n",
      "     14       \u001b[36m28.6801\u001b[0m       27.4695  0.0123\n",
      "     15       \u001b[36m28.6547\u001b[0m       27.4482  0.0117\n",
      "     16       \u001b[36m28.6022\u001b[0m       27.3210  0.0124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m28.5578\u001b[0m       27.3122  0.0118\n",
      "     18       \u001b[36m28.5340\u001b[0m       27.3752  0.0117\n",
      "     19       \u001b[36m28.5200\u001b[0m       27.3790  0.0121\n",
      "     20       \u001b[36m28.5023\u001b[0m       27.3459  0.0119\n",
      "     21       \u001b[36m28.4872\u001b[0m       27.3284  0.0116\n",
      "     22       \u001b[36m28.4764\u001b[0m       27.3416  0.0113\n",
      "     23       \u001b[36m28.4695\u001b[0m       27.3303  0.0115\n",
      "     24       \u001b[36m28.4598\u001b[0m       27.3123  0.0117\n",
      "     25       \u001b[36m28.4521\u001b[0m       27.2940  0.0115\n",
      "     26       \u001b[36m28.4456\u001b[0m       27.2952  0.0114\n",
      "     27       \u001b[36m28.4413\u001b[0m       27.2760  0.0114\n",
      "     28       \u001b[36m28.4350\u001b[0m       27.2670  0.0113\n",
      "     29       \u001b[36m28.4305\u001b[0m       27.2579  0.0117\n",
      "     30       \u001b[36m28.4270\u001b[0m       27.2485  0.0120\n",
      "     31       \u001b[36m28.4232\u001b[0m       27.2418  0.0116\n",
      "     32       \u001b[36m28.4204\u001b[0m       27.2307  0.0115\n",
      "     33       \u001b[36m28.4170\u001b[0m       27.2306  0.0114\n",
      "     34       \u001b[36m28.4147\u001b[0m       27.2222  0.0120\n",
      "     35       \u001b[36m28.4138\u001b[0m       27.1986  0.0116\n",
      "     36       \u001b[36m28.4106\u001b[0m       27.2192  0.0115\n",
      "     37       \u001b[36m28.4106\u001b[0m       27.1993  0.0117\n",
      "     38       28.4106       27.1913  0.0118\n",
      "     39       \u001b[36m28.4074\u001b[0m       27.2284  0.0119\n",
      "     40       28.4138       27.1731  0.0117\n",
      "     41       \u001b[36m28.4032\u001b[0m       27.2133  0.0115\n",
      "     42       \u001b[36m28.4008\u001b[0m       27.1864  0.0117\n",
      "     43       \u001b[36m28.3997\u001b[0m       27.2005  0.0119\n",
      "     44       \u001b[36m28.3978\u001b[0m       27.1837  0.0114\n",
      "     45       \u001b[36m28.3958\u001b[0m       27.1910  0.0115\n",
      "     46       \u001b[36m28.3951\u001b[0m       27.1914  0.0194\n",
      "     47       \u001b[36m28.3947\u001b[0m       27.1878  0.0131\n",
      "     48       \u001b[36m28.3922\u001b[0m       27.1863  0.0125\n",
      "     49       \u001b[36m28.3917\u001b[0m       27.1940  0.0123\n",
      "     50       \u001b[36m28.3909\u001b[0m       27.1978  0.0127\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m44.1707\u001b[0m       \u001b[32m45.0740\u001b[0m  0.0122\n",
      "      2       \u001b[36m42.2169\u001b[0m       \u001b[32m42.7923\u001b[0m  0.0124\n",
      "      3       \u001b[36m40.4177\u001b[0m       \u001b[32m40.5681\u001b[0m  0.0115\n",
      "      4       \u001b[36m38.6796\u001b[0m       \u001b[32m38.3627\u001b[0m  0.0114\n",
      "      5       \u001b[36m37.0170\u001b[0m       \u001b[32m36.2476\u001b[0m  0.0114\n",
      "      6       \u001b[36m35.5260\u001b[0m       \u001b[32m34.3608\u001b[0m  0.0112\n",
      "      7       \u001b[36m34.3248\u001b[0m       \u001b[32m32.8417\u001b[0m  0.0113\n",
      "      8       \u001b[36m33.4866\u001b[0m       \u001b[32m31.7665\u001b[0m  0.0112\n",
      "      9       \u001b[36m32.9967\u001b[0m       \u001b[32m31.0923\u001b[0m  0.0113\n",
      "     10       \u001b[36m32.7522\u001b[0m       \u001b[32m30.7045\u001b[0m  0.0111\n",
      "     11       \u001b[36m32.6406\u001b[0m       \u001b[32m30.4885\u001b[0m  0.0110\n",
      "     12       \u001b[36m32.5859\u001b[0m       \u001b[32m30.3650\u001b[0m  0.0115\n",
      "     13       \u001b[36m32.5536\u001b[0m       \u001b[32m30.2903\u001b[0m  0.0129\n",
      "     14       \u001b[36m32.5297\u001b[0m       \u001b[32m30.2415\u001b[0m  0.0113\n",
      "     15       \u001b[36m32.5098\u001b[0m       \u001b[32m30.2059\u001b[0m  0.0113\n",
      "     16       \u001b[36m32.4921\u001b[0m       \u001b[32m30.1774\u001b[0m  0.0112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m32.4765\u001b[0m       \u001b[32m30.1542\u001b[0m  0.0119\n",
      "     18       \u001b[36m32.4625\u001b[0m       \u001b[32m30.1348\u001b[0m  0.0112\n",
      "     19       \u001b[36m32.4498\u001b[0m       \u001b[32m30.1182\u001b[0m  0.0114\n",
      "     20       \u001b[36m32.4377\u001b[0m       \u001b[32m30.1027\u001b[0m  0.0113\n",
      "     21       \u001b[36m32.4268\u001b[0m       \u001b[32m30.0884\u001b[0m  0.0111\n",
      "     22       \u001b[36m32.4167\u001b[0m       \u001b[32m30.0756\u001b[0m  0.0119\n",
      "     23       \u001b[36m32.4073\u001b[0m       \u001b[32m30.0640\u001b[0m  0.0116\n",
      "     24       \u001b[36m32.3985\u001b[0m       \u001b[32m30.0536\u001b[0m  0.0118\n",
      "     25       \u001b[36m32.3902\u001b[0m       \u001b[32m30.0440\u001b[0m  0.0112\n",
      "     26       \u001b[36m32.3827\u001b[0m       \u001b[32m30.0350\u001b[0m  0.0109\n",
      "     27       \u001b[36m32.3756\u001b[0m       \u001b[32m30.0264\u001b[0m  0.0113\n",
      "     28       \u001b[36m32.3693\u001b[0m       \u001b[32m30.0185\u001b[0m  0.0112\n",
      "     29       \u001b[36m32.3636\u001b[0m       \u001b[32m30.0109\u001b[0m  0.0113\n",
      "     30       \u001b[36m32.3581\u001b[0m       \u001b[32m30.0037\u001b[0m  0.0111\n",
      "     31       \u001b[36m32.3531\u001b[0m       \u001b[32m29.9974\u001b[0m  0.0110\n",
      "     32       \u001b[36m32.3481\u001b[0m       \u001b[32m29.9911\u001b[0m  0.0109\n",
      "     33       \u001b[36m32.3435\u001b[0m       \u001b[32m29.9851\u001b[0m  0.0115\n",
      "     34       \u001b[36m32.3391\u001b[0m       \u001b[32m29.9799\u001b[0m  0.0115\n",
      "     35       \u001b[36m32.3349\u001b[0m       \u001b[32m29.9747\u001b[0m  0.0111\n",
      "     36       \u001b[36m32.3309\u001b[0m       \u001b[32m29.9694\u001b[0m  0.0107\n",
      "     37       \u001b[36m32.3270\u001b[0m       \u001b[32m29.9648\u001b[0m  0.0117\n",
      "     38       \u001b[36m32.3233\u001b[0m       \u001b[32m29.9600\u001b[0m  0.0112\n",
      "     39       \u001b[36m32.3197\u001b[0m       \u001b[32m29.9557\u001b[0m  0.0115\n",
      "     40       \u001b[36m32.3162\u001b[0m       \u001b[32m29.9513\u001b[0m  0.0107\n",
      "     41       \u001b[36m32.3129\u001b[0m       \u001b[32m29.9475\u001b[0m  0.0109\n",
      "     42       \u001b[36m32.3097\u001b[0m       \u001b[32m29.9437\u001b[0m  0.0115\n",
      "     43       \u001b[36m32.3067\u001b[0m       \u001b[32m29.9403\u001b[0m  0.0135\n",
      "     44       \u001b[36m32.3036\u001b[0m       \u001b[32m29.9371\u001b[0m  0.0110\n",
      "     45       \u001b[36m32.3008\u001b[0m       \u001b[32m29.9337\u001b[0m  0.0108\n",
      "     46       \u001b[36m32.2980\u001b[0m       \u001b[32m29.9305\u001b[0m  0.0107\n",
      "     47       \u001b[36m32.2953\u001b[0m       \u001b[32m29.9278\u001b[0m  0.0113\n",
      "     48       \u001b[36m32.2928\u001b[0m       \u001b[32m29.9247\u001b[0m  0.0121\n",
      "     49       \u001b[36m32.2903\u001b[0m       \u001b[32m29.9224\u001b[0m  0.0109\n",
      "     50       \u001b[36m32.2879\u001b[0m       \u001b[32m29.9197\u001b[0m  0.0107\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m34.3956\u001b[0m       \u001b[32m32.2681\u001b[0m  0.0108\n",
      "      2       \u001b[36m32.9180\u001b[0m       \u001b[32m31.2118\u001b[0m  0.0112\n",
      "      3       \u001b[36m31.5734\u001b[0m       \u001b[32m30.2475\u001b[0m  0.0109\n",
      "      4       \u001b[36m30.3062\u001b[0m       \u001b[32m29.3498\u001b[0m  0.0107\n",
      "      5       \u001b[36m29.0914\u001b[0m       \u001b[32m28.5127\u001b[0m  0.0107\n",
      "      6       \u001b[36m27.9204\u001b[0m       \u001b[32m27.7460\u001b[0m  0.0106\n",
      "      7       \u001b[36m26.8104\u001b[0m       \u001b[32m27.0833\u001b[0m  0.0111\n",
      "      8       \u001b[36m25.8086\u001b[0m       \u001b[32m26.5737\u001b[0m  0.0114\n",
      "      9       \u001b[36m24.9705\u001b[0m       \u001b[32m26.2482\u001b[0m  0.0112\n",
      "     10       \u001b[36m24.3332\u001b[0m       \u001b[32m26.1034\u001b[0m  0.0112\n",
      "     11       \u001b[36m23.9025\u001b[0m       \u001b[32m26.0927\u001b[0m  0.0107\n",
      "     12       \u001b[36m23.6407\u001b[0m       26.1499  0.0119\n",
      "     13       \u001b[36m23.4918\u001b[0m       26.2226  0.0117\n",
      "     14       \u001b[36m23.4077\u001b[0m       26.2854  0.0111\n",
      "     15       \u001b[36m23.3576\u001b[0m       26.3314  0.0109\n",
      "     16       \u001b[36m23.3247\u001b[0m       26.3614  0.0109\n",
      "     17       \u001b[36m23.3005\u001b[0m       26.3799  0.0112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     18       \u001b[36m23.2813\u001b[0m       26.3907  0.0115\n",
      "     19       \u001b[36m23.2651\u001b[0m       26.3966  0.0109\n",
      "     20       \u001b[36m23.2511\u001b[0m       26.3996  0.0110\n",
      "     21       \u001b[36m23.2389\u001b[0m       26.4011  0.0107\n",
      "     22       \u001b[36m23.2280\u001b[0m       26.4014  0.0108\n",
      "     23       \u001b[36m23.2182\u001b[0m       26.4012  0.0111\n",
      "     24       \u001b[36m23.2093\u001b[0m       26.4007  0.0114\n",
      "     25       \u001b[36m23.2012\u001b[0m       26.4000  0.0115\n",
      "     26       \u001b[36m23.1936\u001b[0m       26.3993  0.0119\n",
      "     27       \u001b[36m23.1867\u001b[0m       26.3988  0.0125\n",
      "     28       \u001b[36m23.1803\u001b[0m       26.3983  0.0157\n",
      "     29       \u001b[36m23.1743\u001b[0m       26.3977  0.0137\n",
      "     30       \u001b[36m23.1688\u001b[0m       26.3972  0.0124\n",
      "     31       \u001b[36m23.1637\u001b[0m       26.3966  0.0122\n",
      "     32       \u001b[36m23.1588\u001b[0m       26.3960  0.0125\n",
      "     33       \u001b[36m23.1543\u001b[0m       26.3954  0.0133\n",
      "     34       \u001b[36m23.1501\u001b[0m       26.3949  0.0116\n",
      "     35       \u001b[36m23.1460\u001b[0m       26.3943  0.0127\n",
      "     36       \u001b[36m23.1422\u001b[0m       26.3938  0.0113\n",
      "     37       \u001b[36m23.1386\u001b[0m       26.3934  0.0111\n",
      "     38       \u001b[36m23.1352\u001b[0m       26.3931  0.0128\n",
      "     39       \u001b[36m23.1320\u001b[0m       26.3928  0.0117\n",
      "     40       \u001b[36m23.1290\u001b[0m       26.3925  0.0112\n",
      "     41       \u001b[36m23.1261\u001b[0m       26.3923  0.0111\n",
      "     42       \u001b[36m23.1234\u001b[0m       26.3922  0.0110\n",
      "     43       \u001b[36m23.1208\u001b[0m       26.3921  0.0113\n",
      "     44       \u001b[36m23.1183\u001b[0m       26.3920  0.0114\n",
      "     45       \u001b[36m23.1160\u001b[0m       26.3919  0.0112\n",
      "     46       \u001b[36m23.1137\u001b[0m       26.3918  0.0109\n",
      "     47       \u001b[36m23.1116\u001b[0m       26.3918  0.0110\n",
      "     48       \u001b[36m23.1096\u001b[0m       26.3919  0.0115\n",
      "     49       \u001b[36m23.1076\u001b[0m       26.3919  0.0111\n",
      "     50       \u001b[36m23.1058\u001b[0m       26.3919  0.0112\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m40.3030\u001b[0m       \u001b[32m31.3562\u001b[0m  0.0114\n",
      "      2       \u001b[36m38.5350\u001b[0m       \u001b[32m30.2300\u001b[0m  0.0112\n",
      "      3       \u001b[36m36.9212\u001b[0m       \u001b[32m29.2133\u001b[0m  0.0123\n",
      "      4       \u001b[36m35.4067\u001b[0m       \u001b[32m28.3005\u001b[0m  0.0117\n",
      "      5       \u001b[36m34.0002\u001b[0m       \u001b[32m27.5136\u001b[0m  0.0114\n",
      "      6       \u001b[36m32.7302\u001b[0m       \u001b[32m26.8805\u001b[0m  0.0113\n",
      "      7       \u001b[36m31.6191\u001b[0m       \u001b[32m26.4281\u001b[0m  0.0112\n",
      "      8       \u001b[36m30.6766\u001b[0m       \u001b[32m26.1785\u001b[0m  0.0116\n",
      "      9       \u001b[36m29.9158\u001b[0m       \u001b[32m26.1397\u001b[0m  0.0113\n",
      "     10       \u001b[36m29.3582\u001b[0m       26.2780  0.0117\n",
      "     11       \u001b[36m29.0033\u001b[0m       26.5100  0.0110\n",
      "     12       \u001b[36m28.8074\u001b[0m       26.7479  0.0111\n",
      "     13       \u001b[36m28.7096\u001b[0m       26.9396  0.0115\n",
      "     14       \u001b[36m28.6619\u001b[0m       27.0717  0.0114\n",
      "     15       \u001b[36m28.6353\u001b[0m       27.1542  0.0111\n",
      "     16       \u001b[36m28.6168\u001b[0m       27.2018  0.0109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m28.6014\u001b[0m       27.2263  0.0138\n",
      "     18       \u001b[36m28.5874\u001b[0m       27.2393  0.0121\n",
      "     19       \u001b[36m28.5748\u001b[0m       27.2444  0.0112\n",
      "     20       \u001b[36m28.5633\u001b[0m       27.2452  0.0112\n",
      "     21       \u001b[36m28.5527\u001b[0m       27.2432  0.0112\n",
      "     22       \u001b[36m28.5431\u001b[0m       27.2401  0.0110\n",
      "     23       \u001b[36m28.5342\u001b[0m       27.2366  0.0114\n",
      "     24       \u001b[36m28.5262\u001b[0m       27.2332  0.0116\n",
      "     25       \u001b[36m28.5188\u001b[0m       27.2305  0.0120\n",
      "     26       \u001b[36m28.5122\u001b[0m       27.2280  0.0112\n",
      "     27       \u001b[36m28.5061\u001b[0m       27.2255  0.0110\n",
      "     28       \u001b[36m28.5005\u001b[0m       27.2238  0.0115\n",
      "     29       \u001b[36m28.4954\u001b[0m       27.2220  0.0112\n",
      "     30       \u001b[36m28.4909\u001b[0m       27.2204  0.0113\n",
      "     31       \u001b[36m28.4867\u001b[0m       27.2191  0.0112\n",
      "     32       \u001b[36m28.4830\u001b[0m       27.2174  0.0112\n",
      "     33       \u001b[36m28.4794\u001b[0m       27.2156  0.0120\n",
      "     34       \u001b[36m28.4761\u001b[0m       27.2142  0.0112\n",
      "     35       \u001b[36m28.4730\u001b[0m       27.2128  0.0115\n",
      "     36       \u001b[36m28.4701\u001b[0m       27.2113  0.0109\n",
      "     37       \u001b[36m28.4675\u001b[0m       27.2099  0.0112\n",
      "     38       \u001b[36m28.4649\u001b[0m       27.2086  0.0119\n",
      "     39       \u001b[36m28.4626\u001b[0m       27.2077  0.0118\n",
      "     40       \u001b[36m28.4604\u001b[0m       27.2073  0.0118\n",
      "     41       \u001b[36m28.4583\u001b[0m       27.2068  0.0110\n",
      "     42       \u001b[36m28.4564\u001b[0m       27.2065  0.0117\n",
      "     43       \u001b[36m28.4545\u001b[0m       27.2062  0.0125\n",
      "     44       \u001b[36m28.4528\u001b[0m       27.2057  0.0113\n",
      "     45       \u001b[36m28.4511\u001b[0m       27.2058  0.0119\n",
      "     46       \u001b[36m28.4495\u001b[0m       27.2053  0.0113\n",
      "     47       \u001b[36m28.4480\u001b[0m       27.2048  0.0113\n",
      "     48       \u001b[36m28.4466\u001b[0m       27.2044  0.0114\n",
      "     49       \u001b[36m28.4452\u001b[0m       27.2037  0.0117\n",
      "     50       \u001b[36m28.4438\u001b[0m       27.2033  0.0115\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m42.3687\u001b[0m       \u001b[32m40.9280\u001b[0m  0.0120\n",
      "      2       \u001b[36m38.1815\u001b[0m       \u001b[32m34.1501\u001b[0m  0.0123\n",
      "      3       \u001b[36m34.1850\u001b[0m       \u001b[32m31.2073\u001b[0m  0.0133\n",
      "      4       34.2824       \u001b[32m30.8358\u001b[0m  0.0123\n",
      "      5       \u001b[36m33.2358\u001b[0m       31.6594  0.0120\n",
      "      6       \u001b[36m33.1460\u001b[0m       31.1526  0.0121\n",
      "      7       \u001b[36m32.8228\u001b[0m       \u001b[32m30.4080\u001b[0m  0.0160\n",
      "      8       \u001b[36m32.7392\u001b[0m       \u001b[32m30.3327\u001b[0m  0.0217\n",
      "      9       \u001b[36m32.6497\u001b[0m       30.5377  0.0173\n",
      "     10       \u001b[36m32.5715\u001b[0m       30.4699  0.0149\n",
      "     11       \u001b[36m32.4914\u001b[0m       \u001b[32m30.2790\u001b[0m  0.0149\n",
      "     12       \u001b[36m32.4338\u001b[0m       30.3028  0.0193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     13       \u001b[36m32.4016\u001b[0m       30.3397  0.0160\n",
      "     14       \u001b[36m32.3682\u001b[0m       30.3648  0.0124\n",
      "     15       \u001b[36m32.3420\u001b[0m       30.3381  0.0120\n",
      "     16       \u001b[36m32.3185\u001b[0m       \u001b[32m30.2744\u001b[0m  0.0117\n",
      "     17       \u001b[36m32.3032\u001b[0m       30.2769  0.0117\n",
      "     18       \u001b[36m32.2913\u001b[0m       30.3050  0.0119\n",
      "     19       \u001b[36m32.2798\u001b[0m       \u001b[32m30.2636\u001b[0m  0.0120\n",
      "     20       \u001b[36m32.2679\u001b[0m       \u001b[32m30.2495\u001b[0m  0.0118\n",
      "     21       \u001b[36m32.2611\u001b[0m       \u001b[32m30.2493\u001b[0m  0.0120\n",
      "     22       \u001b[36m32.2525\u001b[0m       \u001b[32m30.2397\u001b[0m  0.0119\n",
      "     23       \u001b[36m32.2451\u001b[0m       \u001b[32m30.2378\u001b[0m  0.0126\n",
      "     24       \u001b[36m32.2383\u001b[0m       \u001b[32m30.2243\u001b[0m  0.0124\n",
      "     25       \u001b[36m32.2317\u001b[0m       30.2315  0.0118\n",
      "     26       \u001b[36m32.2264\u001b[0m       30.2282  0.0115\n",
      "     27       \u001b[36m32.2202\u001b[0m       30.2290  0.0116\n",
      "     28       \u001b[36m32.2154\u001b[0m       30.2263  0.0141\n",
      "     29       \u001b[36m32.2104\u001b[0m       30.2248  0.0127\n",
      "     30       \u001b[36m32.2060\u001b[0m       \u001b[32m30.2236\u001b[0m  0.0120\n",
      "     31       \u001b[36m32.2016\u001b[0m       30.2258  0.0118\n",
      "     32       \u001b[36m32.1971\u001b[0m       \u001b[32m30.2225\u001b[0m  0.0118\n",
      "     33       \u001b[36m32.1929\u001b[0m       30.2245  0.0117\n",
      "     34       \u001b[36m32.1888\u001b[0m       30.2232  0.0116\n",
      "     35       \u001b[36m32.1846\u001b[0m       30.2279  0.0116\n",
      "     36       \u001b[36m32.1811\u001b[0m       30.2248  0.0115\n",
      "     37       \u001b[36m32.1768\u001b[0m       30.2318  0.0116\n",
      "     38       \u001b[36m32.1738\u001b[0m       30.2378  0.0119\n",
      "     39       \u001b[36m32.1703\u001b[0m       30.2408  0.0114\n",
      "     40       \u001b[36m32.1667\u001b[0m       30.2312  0.0114\n",
      "     41       \u001b[36m32.1633\u001b[0m       30.2458  0.0115\n",
      "     42       \u001b[36m32.1606\u001b[0m       30.2329  0.0117\n",
      "     43       \u001b[36m32.1573\u001b[0m       30.2439  0.0128\n",
      "     44       \u001b[36m32.1546\u001b[0m       30.2315  0.0118\n",
      "     45       \u001b[36m32.1513\u001b[0m       30.2541  0.0120\n",
      "     46       \u001b[36m32.1486\u001b[0m       30.2326  0.0120\n",
      "     47       \u001b[36m32.1460\u001b[0m       30.2828  0.0122\n",
      "     48       \u001b[36m32.1448\u001b[0m       \u001b[32m30.2187\u001b[0m  0.0122\n",
      "     49       \u001b[36m32.1425\u001b[0m       30.3246  0.0114\n",
      "     50       32.1438       \u001b[32m30.1931\u001b[0m  0.0115\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m33.1621\u001b[0m       \u001b[32m29.8829\u001b[0m  0.0120\n",
      "      2       \u001b[36m28.2932\u001b[0m       \u001b[32m27.1422\u001b[0m  0.0115\n",
      "      3       \u001b[36m25.6384\u001b[0m       29.1554  0.0116\n",
      "      4       \u001b[36m25.0137\u001b[0m       \u001b[32m26.9110\u001b[0m  0.0118\n",
      "      5       \u001b[36m24.1942\u001b[0m       \u001b[32m26.3539\u001b[0m  0.0119\n",
      "      6       \u001b[36m24.0995\u001b[0m       26.5264  0.0117\n",
      "      7       \u001b[36m23.6770\u001b[0m       27.3472  0.0115\n",
      "      8       \u001b[36m23.5413\u001b[0m       27.3376  0.0115\n",
      "      9       \u001b[36m23.3957\u001b[0m       26.7931  0.0118\n",
      "     10       \u001b[36m23.3535\u001b[0m       26.7215  0.0119\n",
      "     11       \u001b[36m23.3090\u001b[0m       26.9455  0.0117\n",
      "     12       \u001b[36m23.2513\u001b[0m       27.0200  0.0115\n",
      "     13       \u001b[36m23.2188\u001b[0m       26.8594  0.0115\n",
      "     14       \u001b[36m23.1931\u001b[0m       26.7447  0.0120\n",
      "     15       \u001b[36m23.1766\u001b[0m       26.7323  0.0121\n",
      "     16       \u001b[36m23.1583\u001b[0m       26.7665  0.0124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m23.1430\u001b[0m       26.7738  0.0124\n",
      "     18       \u001b[36m23.1283\u001b[0m       26.7210  0.0121\n",
      "     19       \u001b[36m23.1204\u001b[0m       26.6883  0.0125\n",
      "     20       \u001b[36m23.1134\u001b[0m       26.7033  0.0122\n",
      "     21       \u001b[36m23.1042\u001b[0m       26.7065  0.0124\n",
      "     22       \u001b[36m23.0974\u001b[0m       26.6894  0.0116\n",
      "     23       \u001b[36m23.0919\u001b[0m       26.6846  0.0117\n",
      "     24       \u001b[36m23.0868\u001b[0m       26.6843  0.0121\n",
      "     25       \u001b[36m23.0822\u001b[0m       26.6834  0.0118\n",
      "     26       \u001b[36m23.0777\u001b[0m       26.6767  0.0118\n",
      "     27       \u001b[36m23.0737\u001b[0m       26.6727  0.0118\n",
      "     28       \u001b[36m23.0703\u001b[0m       26.6765  0.0118\n",
      "     29       \u001b[36m23.0666\u001b[0m       26.6756  0.0122\n",
      "     30       \u001b[36m23.0635\u001b[0m       26.6715  0.0116\n",
      "     31       \u001b[36m23.0605\u001b[0m       26.6742  0.0119\n",
      "     32       \u001b[36m23.0573\u001b[0m       26.6735  0.0118\n",
      "     33       \u001b[36m23.0546\u001b[0m       26.6724  0.0135\n",
      "     34       \u001b[36m23.0516\u001b[0m       26.6754  0.0184\n",
      "     35       \u001b[36m23.0489\u001b[0m       26.6747  0.0197\n",
      "     36       \u001b[36m23.0464\u001b[0m       26.6772  0.0201\n",
      "     37       \u001b[36m23.0443\u001b[0m       26.6772  0.0222\n",
      "     38       \u001b[36m23.0416\u001b[0m       26.6795  0.0194\n",
      "     39       \u001b[36m23.0398\u001b[0m       26.6817  0.0129\n",
      "     40       \u001b[36m23.0374\u001b[0m       26.6814  0.0128\n",
      "     41       \u001b[36m23.0358\u001b[0m       26.6795  0.0121\n",
      "     42       \u001b[36m23.0332\u001b[0m       26.6683  0.0119\n",
      "     43       \u001b[36m23.0323\u001b[0m       26.6796  0.0118\n",
      "     44       \u001b[36m23.0292\u001b[0m       26.6769  0.0121\n",
      "     45       \u001b[36m23.0288\u001b[0m       26.6819  0.0118\n",
      "     46       \u001b[36m23.0258\u001b[0m       26.6746  0.0119\n",
      "     47       23.0261       26.6896  0.0121\n",
      "     48       \u001b[36m23.0225\u001b[0m       26.6726  0.0119\n",
      "     49       23.0244       26.6950  0.0119\n",
      "     50       \u001b[36m23.0194\u001b[0m       26.6710  0.0117\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m39.2249\u001b[0m       \u001b[32m29.5687\u001b[0m  0.0117\n",
      "      2       \u001b[36m34.2766\u001b[0m       \u001b[32m26.5686\u001b[0m  0.0121\n",
      "      3       \u001b[36m30.3802\u001b[0m       30.9814  0.0117\n",
      "      4       \u001b[36m30.3661\u001b[0m       27.1540  0.0117\n",
      "      5       \u001b[36m29.3115\u001b[0m       \u001b[32m26.4872\u001b[0m  0.0119\n",
      "      6       \u001b[36m29.0926\u001b[0m       27.1335  0.0119\n",
      "      7       \u001b[36m28.9551\u001b[0m       28.2564  0.0114\n",
      "      8       28.9579       27.6486  0.0119\n",
      "      9       \u001b[36m28.7294\u001b[0m       27.0774  0.0118\n",
      "     10       \u001b[36m28.6499\u001b[0m       27.2104  0.0118\n",
      "     11       \u001b[36m28.6301\u001b[0m       27.4138  0.0120\n",
      "     12       \u001b[36m28.6037\u001b[0m       27.2200  0.0116\n",
      "     13       \u001b[36m28.5443\u001b[0m       27.1079  0.0118\n",
      "     14       \u001b[36m28.5168\u001b[0m       27.2683  0.0118\n",
      "     15       \u001b[36m28.5149\u001b[0m       27.3044  0.0117\n",
      "     16       \u001b[36m28.4969\u001b[0m       27.1609  0.0115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m28.4741\u001b[0m       27.1506  0.0117\n",
      "     18       \u001b[36m28.4679\u001b[0m       27.1996  0.0118\n",
      "     19       \u001b[36m28.4624\u001b[0m       27.1584  0.0174\n",
      "     20       \u001b[36m28.4493\u001b[0m       27.1273  0.0191\n",
      "     21       \u001b[36m28.4412\u001b[0m       27.1585  0.0136\n",
      "     22       \u001b[36m28.4379\u001b[0m       27.1539  0.0123\n",
      "     23       \u001b[36m28.4311\u001b[0m       27.1314  0.0118\n",
      "     24       \u001b[36m28.4253\u001b[0m       27.1366  0.0147\n",
      "     25       \u001b[36m28.4224\u001b[0m       27.1414  0.0119\n",
      "     26       \u001b[36m28.4187\u001b[0m       27.1336  0.0121\n",
      "     27       \u001b[36m28.4148\u001b[0m       27.1340  0.0121\n",
      "     28       \u001b[36m28.4124\u001b[0m       27.1353  0.0124\n",
      "     29       \u001b[36m28.4105\u001b[0m       27.1308  0.0121\n",
      "     30       \u001b[36m28.4075\u001b[0m       27.1328  0.0116\n",
      "     31       \u001b[36m28.4074\u001b[0m       27.1411  0.0124\n",
      "     32       28.4100       27.1244  0.0118\n",
      "     33       28.4097       27.1398  0.0113\n",
      "     34       28.4183       27.1821  0.0122\n",
      "     35       28.4420       27.1526  0.0122\n",
      "     36       28.4604       27.0978  0.0124\n",
      "     37       28.4440       27.1961  0.0123\n",
      "     38       28.4288       27.1355  0.0119\n",
      "     39       28.4159       27.0244  0.0140\n",
      "     40       \u001b[36m28.4020\u001b[0m       27.0887  0.0119\n",
      "     41       28.4022       27.1715  0.0118\n",
      "     42       \u001b[36m28.4003\u001b[0m       27.0921  0.0121\n",
      "     43       \u001b[36m28.3930\u001b[0m       27.0871  0.0116\n",
      "     44       28.3943       27.1162  0.0118\n",
      "     45       \u001b[36m28.3920\u001b[0m       27.1029  0.0118\n",
      "     46       \u001b[36m28.3895\u001b[0m       27.1122  0.0114\n",
      "     47       \u001b[36m28.3893\u001b[0m       27.1152  0.0121\n",
      "     48       \u001b[36m28.3880\u001b[0m       27.0998  0.0120\n",
      "     49       \u001b[36m28.3862\u001b[0m       27.1084  0.0121\n",
      "     50       \u001b[36m28.3859\u001b[0m       27.1191  0.0132\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m43.1638\u001b[0m       \u001b[32m43.5552\u001b[0m  0.0107\n",
      "      2       \u001b[36m41.0204\u001b[0m       \u001b[32m41.1545\u001b[0m  0.0109\n",
      "      3       \u001b[36m39.2276\u001b[0m       \u001b[32m39.0632\u001b[0m  0.0110\n",
      "      4       \u001b[36m37.6940\u001b[0m       \u001b[32m37.2481\u001b[0m  0.0108\n",
      "      5       \u001b[36m36.3980\u001b[0m       \u001b[32m35.6776\u001b[0m  0.0107\n",
      "      6       \u001b[36m35.3160\u001b[0m       \u001b[32m34.3104\u001b[0m  0.0107\n",
      "      7       \u001b[36m34.4268\u001b[0m       \u001b[32m33.1371\u001b[0m  0.0126\n",
      "      8       \u001b[36m33.7305\u001b[0m       \u001b[32m32.1930\u001b[0m  0.0144\n",
      "      9       \u001b[36m33.2361\u001b[0m       \u001b[32m31.4874\u001b[0m  0.0128\n",
      "     10       \u001b[36m32.9182\u001b[0m       \u001b[32m30.9992\u001b[0m  0.0113\n",
      "     11       \u001b[36m32.7309\u001b[0m       \u001b[32m30.6758\u001b[0m  0.0114\n",
      "     12       \u001b[36m32.6228\u001b[0m       \u001b[32m30.4664\u001b[0m  0.0118\n",
      "     13       \u001b[36m32.5576\u001b[0m       \u001b[32m30.3282\u001b[0m  0.0141\n",
      "     14       \u001b[36m32.5142\u001b[0m       \u001b[32m30.2349\u001b[0m  0.0118\n",
      "     15       \u001b[36m32.4828\u001b[0m       \u001b[32m30.1691\u001b[0m  0.0121\n",
      "     16       \u001b[36m32.4583\u001b[0m       \u001b[32m30.1214\u001b[0m  0.0110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m32.4382\u001b[0m       \u001b[32m30.0857\u001b[0m  0.0114\n",
      "     18       \u001b[36m32.4212\u001b[0m       \u001b[32m30.0577\u001b[0m  0.0111\n",
      "     19       \u001b[36m32.4066\u001b[0m       \u001b[32m30.0350\u001b[0m  0.0112\n",
      "     20       \u001b[36m32.3939\u001b[0m       \u001b[32m30.0164\u001b[0m  0.0112\n",
      "     21       \u001b[36m32.3828\u001b[0m       \u001b[32m30.0010\u001b[0m  0.0111\n",
      "     22       \u001b[36m32.3730\u001b[0m       \u001b[32m29.9878\u001b[0m  0.0112\n",
      "     23       \u001b[36m32.3641\u001b[0m       \u001b[32m29.9763\u001b[0m  0.0112\n",
      "     24       \u001b[36m32.3561\u001b[0m       \u001b[32m29.9658\u001b[0m  0.0111\n",
      "     25       \u001b[36m32.3488\u001b[0m       \u001b[32m29.9568\u001b[0m  0.0111\n",
      "     26       \u001b[36m32.3422\u001b[0m       \u001b[32m29.9485\u001b[0m  0.0114\n",
      "     27       \u001b[36m32.3361\u001b[0m       \u001b[32m29.9413\u001b[0m  0.0120\n",
      "     28       \u001b[36m32.3305\u001b[0m       \u001b[32m29.9346\u001b[0m  0.0109\n",
      "     29       \u001b[36m32.3255\u001b[0m       \u001b[32m29.9286\u001b[0m  0.0109\n",
      "     30       \u001b[36m32.3209\u001b[0m       \u001b[32m29.9233\u001b[0m  0.0108\n",
      "     31       \u001b[36m32.3166\u001b[0m       \u001b[32m29.9184\u001b[0m  0.0109\n",
      "     32       \u001b[36m32.3126\u001b[0m       \u001b[32m29.9137\u001b[0m  0.0107\n",
      "     33       \u001b[36m32.3089\u001b[0m       \u001b[32m29.9098\u001b[0m  0.0108\n",
      "     34       \u001b[36m32.3053\u001b[0m       \u001b[32m29.9060\u001b[0m  0.0112\n",
      "     35       \u001b[36m32.3019\u001b[0m       \u001b[32m29.9021\u001b[0m  0.0107\n",
      "     36       \u001b[36m32.2988\u001b[0m       \u001b[32m29.8984\u001b[0m  0.0106\n",
      "     37       \u001b[36m32.2959\u001b[0m       \u001b[32m29.8950\u001b[0m  0.0118\n",
      "     38       \u001b[36m32.2932\u001b[0m       \u001b[32m29.8922\u001b[0m  0.0118\n",
      "     39       \u001b[36m32.2905\u001b[0m       \u001b[32m29.8891\u001b[0m  0.0108\n",
      "     40       \u001b[36m32.2880\u001b[0m       \u001b[32m29.8863\u001b[0m  0.0108\n",
      "     41       \u001b[36m32.2856\u001b[0m       \u001b[32m29.8833\u001b[0m  0.0109\n",
      "     42       \u001b[36m32.2833\u001b[0m       \u001b[32m29.8809\u001b[0m  0.0115\n",
      "     43       \u001b[36m32.2812\u001b[0m       \u001b[32m29.8784\u001b[0m  0.0111\n",
      "     44       \u001b[36m32.2791\u001b[0m       \u001b[32m29.8760\u001b[0m  0.0112\n",
      "     45       \u001b[36m32.2770\u001b[0m       \u001b[32m29.8735\u001b[0m  0.0109\n",
      "     46       \u001b[36m32.2751\u001b[0m       \u001b[32m29.8718\u001b[0m  0.0108\n",
      "     47       \u001b[36m32.2732\u001b[0m       \u001b[32m29.8695\u001b[0m  0.0117\n",
      "     48       \u001b[36m32.2714\u001b[0m       \u001b[32m29.8674\u001b[0m  0.0110\n",
      "     49       \u001b[36m32.2697\u001b[0m       \u001b[32m29.8655\u001b[0m  0.0109\n",
      "     50       \u001b[36m32.2681\u001b[0m       \u001b[32m29.8641\u001b[0m  0.0112\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m34.7063\u001b[0m       \u001b[32m32.2100\u001b[0m  0.0109\n",
      "      2       \u001b[36m32.5505\u001b[0m       \u001b[32m30.7679\u001b[0m  0.0110\n",
      "      3       \u001b[36m30.7452\u001b[0m       \u001b[32m29.5573\u001b[0m  0.0109\n",
      "      4       \u001b[36m29.1691\u001b[0m       \u001b[32m28.5407\u001b[0m  0.0112\n",
      "      5       \u001b[36m27.7986\u001b[0m       \u001b[32m27.7113\u001b[0m  0.0113\n",
      "      6       \u001b[36m26.6307\u001b[0m       \u001b[32m27.0634\u001b[0m  0.0110\n",
      "      7       \u001b[36m25.6560\u001b[0m       \u001b[32m26.5959\u001b[0m  0.0119\n",
      "      8       \u001b[36m24.8695\u001b[0m       \u001b[32m26.3106\u001b[0m  0.0115\n",
      "      9       \u001b[36m24.2734\u001b[0m       \u001b[32m26.1903\u001b[0m  0.0118\n",
      "     10       \u001b[36m23.8632\u001b[0m       \u001b[32m26.1881\u001b[0m  0.0111\n",
      "     11       \u001b[36m23.6085\u001b[0m       26.2468  0.0109\n",
      "     12       \u001b[36m23.4623\u001b[0m       26.3186  0.0111\n",
      "     13       \u001b[36m23.3807\u001b[0m       26.3783  0.0110\n",
      "     14       \u001b[36m23.3332\u001b[0m       26.4194  0.0110\n",
      "     15       \u001b[36m23.3026\u001b[0m       26.4443  0.0109\n",
      "     16       \u001b[36m23.2808\u001b[0m       26.4578  0.0105\n",
      "     17       \u001b[36m23.2639\u001b[0m       26.4636  0.0108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     18       \u001b[36m23.2498\u001b[0m       26.4651  0.0119\n",
      "     19       \u001b[36m23.2376\u001b[0m       26.4639  0.0111\n",
      "     20       \u001b[36m23.2267\u001b[0m       26.4615  0.0111\n",
      "     21       \u001b[36m23.2170\u001b[0m       26.4587  0.0108\n",
      "     22       \u001b[36m23.2082\u001b[0m       26.4559  0.0109\n",
      "     23       \u001b[36m23.2001\u001b[0m       26.4526  0.0112\n",
      "     24       \u001b[36m23.1928\u001b[0m       26.4498  0.0109\n",
      "     25       \u001b[36m23.1860\u001b[0m       26.4471  0.0110\n",
      "     26       \u001b[36m23.1797\u001b[0m       26.4446  0.0108\n",
      "     27       \u001b[36m23.1739\u001b[0m       26.4421  0.0108\n",
      "     28       \u001b[36m23.1685\u001b[0m       26.4402  0.0112\n",
      "     29       \u001b[36m23.1634\u001b[0m       26.4385  0.0111\n",
      "     30       \u001b[36m23.1588\u001b[0m       26.4368  0.0111\n",
      "     31       \u001b[36m23.1544\u001b[0m       26.4353  0.0111\n",
      "     32       \u001b[36m23.1503\u001b[0m       26.4338  0.0113\n",
      "     33       \u001b[36m23.1464\u001b[0m       26.4323  0.0116\n",
      "     34       \u001b[36m23.1428\u001b[0m       26.4311  0.0115\n",
      "     35       \u001b[36m23.1395\u001b[0m       26.4300  0.0113\n",
      "     36       \u001b[36m23.1363\u001b[0m       26.4292  0.0116\n",
      "     37       \u001b[36m23.1333\u001b[0m       26.4282  0.0111\n",
      "     38       \u001b[36m23.1305\u001b[0m       26.4274  0.0113\n",
      "     39       \u001b[36m23.1277\u001b[0m       26.4264  0.0113\n",
      "     40       \u001b[36m23.1251\u001b[0m       26.4255  0.0107\n",
      "     41       \u001b[36m23.1227\u001b[0m       26.4247  0.0154\n",
      "     42       \u001b[36m23.1203\u001b[0m       26.4239  0.0136\n",
      "     43       \u001b[36m23.1180\u001b[0m       26.4236  0.0130\n",
      "     44       \u001b[36m23.1158\u001b[0m       26.4231  0.0120\n",
      "     45       \u001b[36m23.1138\u001b[0m       26.4225  0.0114\n",
      "     46       \u001b[36m23.1118\u001b[0m       26.4221  0.0126\n",
      "     47       \u001b[36m23.1099\u001b[0m       26.4217  0.0126\n",
      "     48       \u001b[36m23.1080\u001b[0m       26.4210  0.0124\n",
      "     49       \u001b[36m23.1063\u001b[0m       26.4207  0.0115\n",
      "     50       \u001b[36m23.1046\u001b[0m       26.4204  0.0112\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m39.3308\u001b[0m       \u001b[32m30.3246\u001b[0m  0.0111\n",
      "      2       \u001b[36m36.7763\u001b[0m       \u001b[32m28.8222\u001b[0m  0.0118\n",
      "      3       \u001b[36m34.5915\u001b[0m       \u001b[32m27.6292\u001b[0m  0.0114\n",
      "      4       \u001b[36m32.7148\u001b[0m       \u001b[32m26.7636\u001b[0m  0.0112\n",
      "      5       \u001b[36m31.1578\u001b[0m       \u001b[32m26.2941\u001b[0m  0.0113\n",
      "      6       \u001b[36m29.9975\u001b[0m       \u001b[32m26.2455\u001b[0m  0.0113\n",
      "      7       \u001b[36m29.2929\u001b[0m       26.4709  0.0117\n",
      "      8       \u001b[36m28.9552\u001b[0m       26.7531  0.0114\n",
      "      9       \u001b[36m28.8174\u001b[0m       26.9642  0.0113\n",
      "     10       \u001b[36m28.7590\u001b[0m       27.0876  0.0119\n",
      "     11       \u001b[36m28.7262\u001b[0m       27.1527  0.0112\n",
      "     12       \u001b[36m28.7019\u001b[0m       27.1813  0.0110\n",
      "     13       \u001b[36m28.6805\u001b[0m       27.1934  0.0106\n",
      "     14       \u001b[36m28.6614\u001b[0m       27.1976  0.0110\n",
      "     15       \u001b[36m28.6443\u001b[0m       27.1983  0.0106\n",
      "     16       \u001b[36m28.6290\u001b[0m       27.1978  0.0107\n",
      "     17       \u001b[36m28.6154\u001b[0m       27.1966  0.0109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     18       \u001b[36m28.6033\u001b[0m       27.1951  0.0111\n",
      "     19       \u001b[36m28.5923\u001b[0m       27.1939  0.0113\n",
      "     20       \u001b[36m28.5825\u001b[0m       27.1928  0.0111\n",
      "     21       \u001b[36m28.5736\u001b[0m       27.1919  0.0108\n",
      "     22       \u001b[36m28.5655\u001b[0m       27.1909  0.0103\n",
      "     23       \u001b[36m28.5581\u001b[0m       27.1892  0.0111\n",
      "     24       \u001b[36m28.5513\u001b[0m       27.1889  0.0110\n",
      "     25       \u001b[36m28.5449\u001b[0m       27.1879  0.0109\n",
      "     26       \u001b[36m28.5390\u001b[0m       27.1869  0.0107\n",
      "     27       \u001b[36m28.5335\u001b[0m       27.1863  0.0107\n",
      "     28       \u001b[36m28.5284\u001b[0m       27.1861  0.0114\n",
      "     29       \u001b[36m28.5237\u001b[0m       27.1857  0.0107\n",
      "     30       \u001b[36m28.5194\u001b[0m       27.1856  0.0110\n",
      "     31       \u001b[36m28.5153\u001b[0m       27.1868  0.0111\n",
      "     32       \u001b[36m28.5116\u001b[0m       27.1873  0.0106\n",
      "     33       \u001b[36m28.5081\u001b[0m       27.1877  0.0114\n",
      "     34       \u001b[36m28.5048\u001b[0m       27.1872  0.0112\n",
      "     35       \u001b[36m28.5015\u001b[0m       27.1873  0.0115\n",
      "     36       \u001b[36m28.4985\u001b[0m       27.1882  0.0107\n",
      "     37       \u001b[36m28.4958\u001b[0m       27.1886  0.0109\n",
      "     38       \u001b[36m28.4931\u001b[0m       27.1887  0.0112\n",
      "     39       \u001b[36m28.4906\u001b[0m       27.1891  0.0109\n",
      "     40       \u001b[36m28.4882\u001b[0m       27.1888  0.0111\n",
      "     41       \u001b[36m28.4858\u001b[0m       27.1887  0.0107\n",
      "     42       \u001b[36m28.4836\u001b[0m       27.1887  0.0107\n",
      "     43       \u001b[36m28.4815\u001b[0m       27.1888  0.0108\n",
      "     44       \u001b[36m28.4795\u001b[0m       27.1887  0.0109\n",
      "     45       \u001b[36m28.4776\u001b[0m       27.1888  0.0109\n",
      "     46       \u001b[36m28.4758\u001b[0m       27.1880  0.0109\n",
      "     47       \u001b[36m28.4740\u001b[0m       27.1879  0.0114\n",
      "     48       \u001b[36m28.4723\u001b[0m       27.1876  0.0117\n",
      "     49       \u001b[36m28.4706\u001b[0m       27.1875  0.0112\n",
      "     50       \u001b[36m28.4689\u001b[0m       27.1873  0.0112\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m43.7321\u001b[0m       \u001b[32m43.1799\u001b[0m  0.0119\n",
      "      2       \u001b[36m40.2170\u001b[0m       \u001b[32m38.1947\u001b[0m  0.0122\n",
      "      3       \u001b[36m35.9874\u001b[0m       \u001b[32m32.0798\u001b[0m  0.0120\n",
      "      4       \u001b[36m34.4433\u001b[0m       \u001b[32m31.4645\u001b[0m  0.0114\n",
      "      5       \u001b[36m34.1759\u001b[0m       31.6276  0.0116\n",
      "      6       \u001b[36m33.5016\u001b[0m       32.0742  0.0117\n",
      "      7       \u001b[36m33.3613\u001b[0m       \u001b[32m31.4025\u001b[0m  0.0117\n",
      "      8       \u001b[36m33.0171\u001b[0m       \u001b[32m30.6793\u001b[0m  0.0115\n",
      "      9       \u001b[36m32.8910\u001b[0m       \u001b[32m30.4484\u001b[0m  0.0117\n",
      "     10       \u001b[36m32.7776\u001b[0m       30.5370  0.0115\n",
      "     11       \u001b[36m32.6739\u001b[0m       30.6402  0.0120\n",
      "     12       \u001b[36m32.5874\u001b[0m       30.4509  0.0124\n",
      "     13       \u001b[36m32.4968\u001b[0m       \u001b[32m30.2981\u001b[0m  0.0121\n",
      "     14       \u001b[36m32.4480\u001b[0m       \u001b[32m30.2881\u001b[0m  0.0122\n",
      "     15       \u001b[36m32.4039\u001b[0m       30.3548  0.0123\n",
      "     16       \u001b[36m32.3706\u001b[0m       30.3035  0.0126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m32.3407\u001b[0m       \u001b[32m30.2604\u001b[0m  0.0122\n",
      "     18       \u001b[36m32.3178\u001b[0m       \u001b[32m30.1929\u001b[0m  0.0119\n",
      "     19       \u001b[36m32.3079\u001b[0m       30.2865  0.0115\n",
      "     20       \u001b[36m32.2933\u001b[0m       30.2249  0.0117\n",
      "     21       32.2967       30.3558  0.0120\n",
      "     22       32.2963       30.2703  0.0170\n",
      "     23       32.3430       30.4614  0.0152\n",
      "     24       32.3291       30.2374  0.0124\n",
      "     25       32.3747       \u001b[32m30.1345\u001b[0m  0.0123\n",
      "     26       \u001b[36m32.2815\u001b[0m       \u001b[32m30.1293\u001b[0m  0.0141\n",
      "     27       \u001b[36m32.2294\u001b[0m       \u001b[32m30.0406\u001b[0m  0.0196\n",
      "     28       32.2327       30.1047  0.0137\n",
      "     29       32.2314       30.0650  0.0119\n",
      "     30       \u001b[36m32.2109\u001b[0m       30.0416  0.0118\n",
      "     31       \u001b[36m32.2063\u001b[0m       30.0551  0.0122\n",
      "     32       \u001b[36m32.2007\u001b[0m       \u001b[32m30.0338\u001b[0m  0.0124\n",
      "     33       \u001b[36m32.1949\u001b[0m       30.0469  0.0122\n",
      "     34       \u001b[36m32.1920\u001b[0m       30.0385  0.0120\n",
      "     35       \u001b[36m32.1848\u001b[0m       \u001b[32m30.0295\u001b[0m  0.0121\n",
      "     36       \u001b[36m32.1819\u001b[0m       30.0421  0.0119\n",
      "     37       \u001b[36m32.1782\u001b[0m       30.0356  0.0119\n",
      "     38       \u001b[36m32.1737\u001b[0m       30.0353  0.0121\n",
      "     39       \u001b[36m32.1709\u001b[0m       30.0312  0.0124\n",
      "     40       \u001b[36m32.1666\u001b[0m       30.0332  0.0129\n",
      "     41       \u001b[36m32.1637\u001b[0m       30.0330  0.0119\n",
      "     42       \u001b[36m32.1600\u001b[0m       30.0319  0.0114\n",
      "     43       \u001b[36m32.1571\u001b[0m       30.0345  0.0119\n",
      "     44       \u001b[36m32.1539\u001b[0m       30.0349  0.0119\n",
      "     45       \u001b[36m32.1511\u001b[0m       30.0342  0.0118\n",
      "     46       \u001b[36m32.1482\u001b[0m       30.0340  0.0117\n",
      "     47       \u001b[36m32.1452\u001b[0m       30.0361  0.0116\n",
      "     48       \u001b[36m32.1425\u001b[0m       30.0381  0.0117\n",
      "     49       \u001b[36m32.1399\u001b[0m       30.0383  0.0118\n",
      "     50       \u001b[36m32.1372\u001b[0m       30.0375  0.0117\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m33.4286\u001b[0m       \u001b[32m30.5834\u001b[0m  0.0117\n",
      "      2       \u001b[36m29.8433\u001b[0m       \u001b[32m27.6918\u001b[0m  0.0117\n",
      "      3       \u001b[36m26.1405\u001b[0m       28.3624  0.0118\n",
      "      4       \u001b[36m25.3991\u001b[0m       28.0869  0.0116\n",
      "      5       \u001b[36m24.4575\u001b[0m       \u001b[32m26.5252\u001b[0m  0.0116\n",
      "      6       \u001b[36m24.2203\u001b[0m       \u001b[32m26.3984\u001b[0m  0.0114\n",
      "      7       \u001b[36m23.9550\u001b[0m       26.7881  0.0112\n",
      "      8       \u001b[36m23.6742\u001b[0m       27.4780  0.0121\n",
      "      9       \u001b[36m23.5720\u001b[0m       27.3019  0.0120\n",
      "     10       \u001b[36m23.4651\u001b[0m       27.0415  0.0115\n",
      "     11       \u001b[36m23.4071\u001b[0m       27.0449  0.0113\n",
      "     12       \u001b[36m23.3421\u001b[0m       27.1527  0.0114\n",
      "     13       \u001b[36m23.2890\u001b[0m       27.1355  0.0120\n",
      "     14       \u001b[36m23.2499\u001b[0m       27.0644  0.0119\n",
      "     15       \u001b[36m23.2222\u001b[0m       27.0204  0.0118\n",
      "     16       \u001b[36m23.2010\u001b[0m       27.0168  0.0117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m23.1776\u001b[0m       27.0117  0.0117\n",
      "     18       \u001b[36m23.1566\u001b[0m       26.9562  0.0121\n",
      "     19       \u001b[36m23.1446\u001b[0m       26.9300  0.0115\n",
      "     20       \u001b[36m23.1322\u001b[0m       26.9321  0.0118\n",
      "     21       \u001b[36m23.1205\u001b[0m       26.9211  0.0113\n",
      "     22       \u001b[36m23.1085\u001b[0m       26.8866  0.0114\n",
      "     23       \u001b[36m23.1009\u001b[0m       26.8728  0.0123\n",
      "     24       \u001b[36m23.0924\u001b[0m       26.8668  0.0116\n",
      "     25       \u001b[36m23.0862\u001b[0m       26.8638  0.0117\n",
      "     26       \u001b[36m23.0780\u001b[0m       26.8341  0.0111\n",
      "     27       \u001b[36m23.0760\u001b[0m       26.8432  0.0114\n",
      "     28       \u001b[36m23.0695\u001b[0m       26.8096  0.0118\n",
      "     29       23.0778       26.8566  0.0115\n",
      "     30       23.0798       26.7741  0.0118\n",
      "     31       23.0955       26.8801  0.0115\n",
      "     32       23.0929       26.7345  0.0116\n",
      "     33       23.0835       26.8162  0.0113\n",
      "     34       \u001b[36m23.0609\u001b[0m       26.7935  0.0117\n",
      "     35       \u001b[36m23.0487\u001b[0m       26.7662  0.0118\n",
      "     36       \u001b[36m23.0407\u001b[0m       26.8045  0.0116\n",
      "     37       \u001b[36m23.0351\u001b[0m       26.7653  0.0113\n",
      "     38       23.0360       26.7867  0.0115\n",
      "     39       \u001b[36m23.0298\u001b[0m       26.7612  0.0121\n",
      "     40       \u001b[36m23.0294\u001b[0m       26.7655  0.0120\n",
      "     41       \u001b[36m23.0249\u001b[0m       26.7622  0.0118\n",
      "     42       \u001b[36m23.0231\u001b[0m       26.7477  0.0118\n",
      "     43       \u001b[36m23.0210\u001b[0m       26.7464  0.0118\n",
      "     44       \u001b[36m23.0188\u001b[0m       26.7340  0.0124\n",
      "     45       \u001b[36m23.0173\u001b[0m       26.7291  0.0123\n",
      "     46       \u001b[36m23.0150\u001b[0m       26.7176  0.0118\n",
      "     47       \u001b[36m23.0138\u001b[0m       26.7127  0.0115\n",
      "     48       \u001b[36m23.0117\u001b[0m       26.7067  0.0115\n",
      "     49       \u001b[36m23.0105\u001b[0m       26.7038  0.0116\n",
      "     50       \u001b[36m23.0087\u001b[0m       26.6963  0.0151\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m39.1754\u001b[0m       \u001b[32m29.8687\u001b[0m  0.0147\n",
      "      2       \u001b[36m35.4689\u001b[0m       \u001b[32m26.9881\u001b[0m  0.0126\n",
      "      3       \u001b[36m30.9166\u001b[0m       30.3433  0.0141\n",
      "      4       31.3580       28.7588  0.0126\n",
      "      5       \u001b[36m29.8921\u001b[0m       \u001b[32m26.7249\u001b[0m  0.0158\n",
      "      6       \u001b[36m29.6278\u001b[0m       \u001b[32m26.6224\u001b[0m  0.0130\n",
      "      7       \u001b[36m29.2518\u001b[0m       27.4061  0.0123\n",
      "      8       \u001b[36m29.1569\u001b[0m       28.1122  0.0119\n",
      "      9       \u001b[36m29.0617\u001b[0m       27.6733  0.0118\n",
      "     10       \u001b[36m28.8419\u001b[0m       27.2891  0.0121\n",
      "     11       \u001b[36m28.7459\u001b[0m       27.3238  0.0119\n",
      "     12       \u001b[36m28.6931\u001b[0m       27.5407  0.0120\n",
      "     13       \u001b[36m28.6608\u001b[0m       27.4789  0.0117\n",
      "     14       \u001b[36m28.5968\u001b[0m       27.3672  0.0118\n",
      "     15       \u001b[36m28.5535\u001b[0m       27.4438  0.0124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     16       \u001b[36m28.5344\u001b[0m       27.4856  0.0124\n",
      "     17       \u001b[36m28.5188\u001b[0m       27.4247  0.0121\n",
      "     18       \u001b[36m28.4963\u001b[0m       27.3903  0.0117\n",
      "     19       \u001b[36m28.4836\u001b[0m       27.3874  0.0115\n",
      "     20       \u001b[36m28.4729\u001b[0m       27.3927  0.0112\n",
      "     21       \u001b[36m28.4655\u001b[0m       27.3518  0.0123\n",
      "     22       \u001b[36m28.4558\u001b[0m       27.3301  0.0116\n",
      "     23       \u001b[36m28.4484\u001b[0m       27.3294  0.0115\n",
      "     24       \u001b[36m28.4435\u001b[0m       27.3059  0.0113\n",
      "     25       \u001b[36m28.4381\u001b[0m       27.2908  0.0112\n",
      "     26       \u001b[36m28.4324\u001b[0m       27.2865  0.0118\n",
      "     27       \u001b[36m28.4306\u001b[0m       27.2617  0.0116\n",
      "     28       28.4336       27.2810  0.0116\n",
      "     29       28.4388       27.2575  0.0115\n",
      "     30       28.4506       27.2271  0.0114\n",
      "     31       28.4567       27.3298  0.0121\n",
      "     32       28.4901       27.1542  0.0118\n",
      "     33       \u001b[36m28.4159\u001b[0m       27.2668  0.0116\n",
      "     34       28.4189       27.2397  0.0114\n",
      "     35       \u001b[36m28.4149\u001b[0m       27.1871  0.0114\n",
      "     36       \u001b[36m28.4129\u001b[0m       27.1546  0.0120\n",
      "     37       \u001b[36m28.4024\u001b[0m       27.2106  0.0117\n",
      "     38       28.4047       27.1768  0.0116\n",
      "     39       \u001b[36m28.3995\u001b[0m       27.1636  0.0115\n",
      "     40       \u001b[36m28.3990\u001b[0m       27.1597  0.0114\n",
      "     41       \u001b[36m28.3966\u001b[0m       27.1674  0.0120\n",
      "     42       \u001b[36m28.3954\u001b[0m       27.1491  0.0116\n",
      "     43       \u001b[36m28.3931\u001b[0m       27.1529  0.0118\n",
      "     44       \u001b[36m28.3922\u001b[0m       27.1480  0.0114\n",
      "     45       \u001b[36m28.3907\u001b[0m       27.1488  0.0121\n",
      "     46       \u001b[36m28.3895\u001b[0m       27.1407  0.0120\n",
      "     47       \u001b[36m28.3885\u001b[0m       27.1429  0.0124\n",
      "     48       \u001b[36m28.3871\u001b[0m       27.1391  0.0114\n",
      "     49       \u001b[36m28.3864\u001b[0m       27.1382  0.0116\n",
      "     50       \u001b[36m28.3853\u001b[0m       27.1365  0.0113\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m43.0241\u001b[0m       \u001b[32m43.6595\u001b[0m  0.0115\n",
      "      2       \u001b[36m41.2044\u001b[0m       \u001b[32m41.4154\u001b[0m  0.0114\n",
      "      3       \u001b[36m39.4497\u001b[0m       \u001b[32m39.1701\u001b[0m  0.0109\n",
      "      4       \u001b[36m37.7525\u001b[0m       \u001b[32m37.0034\u001b[0m  0.0113\n",
      "      5       \u001b[36m36.2022\u001b[0m       \u001b[32m35.0418\u001b[0m  0.0110\n",
      "      6       \u001b[36m34.9066\u001b[0m       \u001b[32m33.3986\u001b[0m  0.0109\n",
      "      7       \u001b[36m33.9480\u001b[0m       \u001b[32m32.1713\u001b[0m  0.0112\n",
      "      8       \u001b[36m33.3457\u001b[0m       \u001b[32m31.3687\u001b[0m  0.0110\n",
      "      9       \u001b[36m33.0222\u001b[0m       \u001b[32m30.8951\u001b[0m  0.0108\n",
      "     10       \u001b[36m32.8611\u001b[0m       \u001b[32m30.6280\u001b[0m  0.0107\n",
      "     11       \u001b[36m32.7745\u001b[0m       \u001b[32m30.4745\u001b[0m  0.0107\n",
      "     12       \u001b[36m32.7187\u001b[0m       \u001b[32m30.3802\u001b[0m  0.0110\n",
      "     13       \u001b[36m32.6771\u001b[0m       \u001b[32m30.3174\u001b[0m  0.0109\n",
      "     14       \u001b[36m32.6425\u001b[0m       \u001b[32m30.2722\u001b[0m  0.0107\n",
      "     15       \u001b[36m32.6130\u001b[0m       \u001b[32m30.2373\u001b[0m  0.0107\n",
      "     16       \u001b[36m32.5873\u001b[0m       \u001b[32m30.2091\u001b[0m  0.0107\n",
      "     17       \u001b[36m32.5648\u001b[0m       \u001b[32m30.1853\u001b[0m  0.0109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     18       \u001b[36m32.5449\u001b[0m       \u001b[32m30.1647\u001b[0m  0.0116\n",
      "     19       \u001b[36m32.5271\u001b[0m       \u001b[32m30.1465\u001b[0m  0.0108\n",
      "     20       \u001b[36m32.5110\u001b[0m       \u001b[32m30.1297\u001b[0m  0.0111\n",
      "     21       \u001b[36m32.4964\u001b[0m       \u001b[32m30.1147\u001b[0m  0.0108\n",
      "     22       \u001b[36m32.4829\u001b[0m       \u001b[32m30.1010\u001b[0m  0.0107\n",
      "     23       \u001b[36m32.4707\u001b[0m       \u001b[32m30.0885\u001b[0m  0.0109\n",
      "     24       \u001b[36m32.4596\u001b[0m       \u001b[32m30.0770\u001b[0m  0.0111\n",
      "     25       \u001b[36m32.4492\u001b[0m       \u001b[32m30.0665\u001b[0m  0.0111\n",
      "     26       \u001b[36m32.4395\u001b[0m       \u001b[32m30.0567\u001b[0m  0.0107\n",
      "     27       \u001b[36m32.4304\u001b[0m       \u001b[32m30.0474\u001b[0m  0.0107\n",
      "     28       \u001b[36m32.4219\u001b[0m       \u001b[32m30.0387\u001b[0m  0.0111\n",
      "     29       \u001b[36m32.4139\u001b[0m       \u001b[32m30.0304\u001b[0m  0.0109\n",
      "     30       \u001b[36m32.4064\u001b[0m       \u001b[32m30.0225\u001b[0m  0.0143\n",
      "     31       \u001b[36m32.3993\u001b[0m       \u001b[32m30.0152\u001b[0m  0.0148\n",
      "     32       \u001b[36m32.3926\u001b[0m       \u001b[32m30.0083\u001b[0m  0.0114\n",
      "     33       \u001b[36m32.3862\u001b[0m       \u001b[32m30.0018\u001b[0m  0.0126\n",
      "     34       \u001b[36m32.3802\u001b[0m       \u001b[32m29.9955\u001b[0m  0.0115\n",
      "     35       \u001b[36m32.3745\u001b[0m       \u001b[32m29.9897\u001b[0m  0.0115\n",
      "     36       \u001b[36m32.3692\u001b[0m       \u001b[32m29.9839\u001b[0m  0.0131\n",
      "     37       \u001b[36m32.3642\u001b[0m       \u001b[32m29.9786\u001b[0m  0.0117\n",
      "     38       \u001b[36m32.3594\u001b[0m       \u001b[32m29.9733\u001b[0m  0.0120\n",
      "     39       \u001b[36m32.3549\u001b[0m       \u001b[32m29.9682\u001b[0m  0.0111\n",
      "     40       \u001b[36m32.3506\u001b[0m       \u001b[32m29.9637\u001b[0m  0.0111\n",
      "     41       \u001b[36m32.3464\u001b[0m       \u001b[32m29.9592\u001b[0m  0.0109\n",
      "     42       \u001b[36m32.3425\u001b[0m       \u001b[32m29.9547\u001b[0m  0.0107\n",
      "     43       \u001b[36m32.3387\u001b[0m       \u001b[32m29.9505\u001b[0m  0.0111\n",
      "     44       \u001b[36m32.3351\u001b[0m       \u001b[32m29.9463\u001b[0m  0.0111\n",
      "     45       \u001b[36m32.3316\u001b[0m       \u001b[32m29.9426\u001b[0m  0.0107\n",
      "     46       \u001b[36m32.3284\u001b[0m       \u001b[32m29.9388\u001b[0m  0.0108\n",
      "     47       \u001b[36m32.3252\u001b[0m       \u001b[32m29.9351\u001b[0m  0.0113\n",
      "     48       \u001b[36m32.3221\u001b[0m       \u001b[32m29.9316\u001b[0m  0.0115\n",
      "     49       \u001b[36m32.3191\u001b[0m       \u001b[32m29.9282\u001b[0m  0.0110\n",
      "     50       \u001b[36m32.3163\u001b[0m       \u001b[32m29.9250\u001b[0m  0.0115\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m34.6417\u001b[0m       \u001b[32m32.3680\u001b[0m  0.0116\n",
      "      2       \u001b[36m32.8372\u001b[0m       \u001b[32m31.0671\u001b[0m  0.0110\n",
      "      3       \u001b[36m31.1447\u001b[0m       \u001b[32m29.8368\u001b[0m  0.0111\n",
      "      4       \u001b[36m29.5029\u001b[0m       \u001b[32m28.6814\u001b[0m  0.0110\n",
      "      5       \u001b[36m27.9394\u001b[0m       \u001b[32m27.6676\u001b[0m  0.0108\n",
      "      6       \u001b[36m26.5458\u001b[0m       \u001b[32m26.8862\u001b[0m  0.0108\n",
      "      7       \u001b[36m25.4161\u001b[0m       \u001b[32m26.3930\u001b[0m  0.0108\n",
      "      8       \u001b[36m24.5976\u001b[0m       \u001b[32m26.1710\u001b[0m  0.0110\n",
      "      9       \u001b[36m24.0715\u001b[0m       \u001b[32m26.1369\u001b[0m  0.0111\n",
      "     10       \u001b[36m23.7668\u001b[0m       26.1894  0.0107\n",
      "     11       \u001b[36m23.6006\u001b[0m       26.2575  0.0114\n",
      "     12       \u001b[36m23.5083\u001b[0m       26.3119  0.0118\n",
      "     13       \u001b[36m23.4515\u001b[0m       26.3481  0.0113\n",
      "     14       \u001b[36m23.4123\u001b[0m       26.3702  0.0110\n",
      "     15       \u001b[36m23.3822\u001b[0m       26.3829  0.0109\n",
      "     16       \u001b[36m23.3574\u001b[0m       26.3899  0.0108\n",
      "     17       \u001b[36m23.3365\u001b[0m       26.3940  0.0113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     18       \u001b[36m23.3183\u001b[0m       26.3959  0.0116\n",
      "     19       \u001b[36m23.3022\u001b[0m       26.3969  0.0111\n",
      "     20       \u001b[36m23.2880\u001b[0m       26.3970  0.0110\n",
      "     21       \u001b[36m23.2751\u001b[0m       26.3967  0.0109\n",
      "     22       \u001b[36m23.2634\u001b[0m       26.3962  0.0108\n",
      "     23       \u001b[36m23.2528\u001b[0m       26.3955  0.0113\n",
      "     24       \u001b[36m23.2429\u001b[0m       26.3948  0.0112\n",
      "     25       \u001b[36m23.2339\u001b[0m       26.3942  0.0109\n",
      "     26       \u001b[36m23.2255\u001b[0m       26.3935  0.0109\n",
      "     27       \u001b[36m23.2177\u001b[0m       26.3930  0.0110\n",
      "     28       \u001b[36m23.2105\u001b[0m       26.3924  0.0117\n",
      "     29       \u001b[36m23.2038\u001b[0m       26.3919  0.0110\n",
      "     30       \u001b[36m23.1975\u001b[0m       26.3913  0.0113\n",
      "     31       \u001b[36m23.1917\u001b[0m       26.3908  0.0110\n",
      "     32       \u001b[36m23.1863\u001b[0m       26.3904  0.0108\n",
      "     33       \u001b[36m23.1812\u001b[0m       26.3901  0.0112\n",
      "     34       \u001b[36m23.1764\u001b[0m       26.3897  0.0112\n",
      "     35       \u001b[36m23.1719\u001b[0m       26.3894  0.0114\n",
      "     36       \u001b[36m23.1676\u001b[0m       26.3893  0.0116\n",
      "     37       \u001b[36m23.1636\u001b[0m       26.3892  0.0116\n",
      "     38       \u001b[36m23.1598\u001b[0m       26.3890  0.0116\n",
      "     39       \u001b[36m23.1562\u001b[0m       26.3889  0.0118\n",
      "     40       \u001b[36m23.1527\u001b[0m       26.3889  0.0118\n",
      "     41       \u001b[36m23.1495\u001b[0m       26.3889  0.0114\n",
      "     42       \u001b[36m23.1464\u001b[0m       26.3889  0.0114\n",
      "     43       \u001b[36m23.1435\u001b[0m       26.3889  0.0114\n",
      "     44       \u001b[36m23.1407\u001b[0m       26.3889  0.0112\n",
      "     45       \u001b[36m23.1380\u001b[0m       26.3890  0.0111\n",
      "     46       \u001b[36m23.1355\u001b[0m       26.3890  0.0112\n",
      "     47       \u001b[36m23.1331\u001b[0m       26.3891  0.0112\n",
      "     48       \u001b[36m23.1307\u001b[0m       26.3891  0.0112\n",
      "     49       \u001b[36m23.1285\u001b[0m       26.3892  0.0113\n",
      "     50       \u001b[36m23.1264\u001b[0m       26.3892  0.0113\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m41.0858\u001b[0m       \u001b[32m31.7243\u001b[0m  0.0108\n",
      "      2       \u001b[36m38.9637\u001b[0m       \u001b[32m30.2966\u001b[0m  0.0108\n",
      "      3       \u001b[36m36.9210\u001b[0m       \u001b[32m28.9310\u001b[0m  0.0113\n",
      "      4       \u001b[36m34.8627\u001b[0m       \u001b[32m27.6949\u001b[0m  0.0112\n",
      "      5       \u001b[36m32.8904\u001b[0m       \u001b[32m26.7636\u001b[0m  0.0109\n",
      "      6       \u001b[36m31.2182\u001b[0m       \u001b[32m26.3064\u001b[0m  0.0111\n",
      "      7       \u001b[36m30.0291\u001b[0m       26.3151  0.0109\n",
      "      8       \u001b[36m29.3462\u001b[0m       26.5767  0.0121\n",
      "      9       \u001b[36m29.0284\u001b[0m       26.8511  0.0130\n",
      "     10       \u001b[36m28.8951\u001b[0m       27.0349  0.0122\n",
      "     11       \u001b[36m28.8310\u001b[0m       27.1329  0.0108\n",
      "     12       \u001b[36m28.7897\u001b[0m       27.1764  0.0107\n",
      "     13       \u001b[36m28.7563\u001b[0m       27.1903  0.0153\n",
      "     14       \u001b[36m28.7275\u001b[0m       27.1913  0.0129\n",
      "     15       \u001b[36m28.7021\u001b[0m       27.1865  0.0116\n",
      "     16       \u001b[36m28.6794\u001b[0m       27.1810  0.0113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m28.6595\u001b[0m       27.1757  0.0118\n",
      "     18       \u001b[36m28.6420\u001b[0m       27.1712  0.0123\n",
      "     19       \u001b[36m28.6267\u001b[0m       27.1676  0.0136\n",
      "     20       \u001b[36m28.6132\u001b[0m       27.1655  0.0122\n",
      "     21       \u001b[36m28.6011\u001b[0m       27.1626  0.0113\n",
      "     22       \u001b[36m28.5901\u001b[0m       27.1608  0.0114\n",
      "     23       \u001b[36m28.5802\u001b[0m       27.1586  0.0112\n",
      "     24       \u001b[36m28.5711\u001b[0m       27.1576  0.0116\n",
      "     25       \u001b[36m28.5628\u001b[0m       27.1574  0.0113\n",
      "     26       \u001b[36m28.5552\u001b[0m       27.1569  0.0112\n",
      "     27       \u001b[36m28.5482\u001b[0m       27.1570  0.0113\n",
      "     28       \u001b[36m28.5418\u001b[0m       27.1571  0.0113\n",
      "     29       \u001b[36m28.5359\u001b[0m       27.1571  0.0113\n",
      "     30       \u001b[36m28.5305\u001b[0m       27.1577  0.0114\n",
      "     31       \u001b[36m28.5255\u001b[0m       27.1585  0.0112\n",
      "     32       \u001b[36m28.5208\u001b[0m       27.1592  0.0115\n",
      "     33       \u001b[36m28.5164\u001b[0m       27.1595  0.0112\n",
      "     34       \u001b[36m28.5123\u001b[0m       27.1603  0.0111\n",
      "     35       \u001b[36m28.5085\u001b[0m       27.1614  0.0112\n",
      "     36       \u001b[36m28.5048\u001b[0m       27.1623  0.0109\n",
      "     37       \u001b[36m28.5014\u001b[0m       27.1634  0.0108\n",
      "     38       \u001b[36m28.4981\u001b[0m       27.1645  0.0112\n",
      "     39       \u001b[36m28.4950\u001b[0m       27.1651  0.0126\n",
      "     40       \u001b[36m28.4920\u001b[0m       27.1660  0.0134\n",
      "     41       \u001b[36m28.4892\u001b[0m       27.1672  0.0122\n",
      "     42       \u001b[36m28.4866\u001b[0m       27.1685  0.0121\n",
      "     43       \u001b[36m28.4841\u001b[0m       27.1695  0.0118\n",
      "     44       \u001b[36m28.4817\u001b[0m       27.1702  0.0116\n",
      "     45       \u001b[36m28.4794\u001b[0m       27.1707  0.0112\n",
      "     46       \u001b[36m28.4771\u001b[0m       27.1714  0.0112\n",
      "     47       \u001b[36m28.4751\u001b[0m       27.1717  0.0112\n",
      "     48       \u001b[36m28.4730\u001b[0m       27.1727  0.0113\n",
      "     49       \u001b[36m28.4710\u001b[0m       27.1731  0.0113\n",
      "     50       \u001b[36m28.4691\u001b[0m       27.1737  0.0112\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m42.8774\u001b[0m       \u001b[32m43.1017\u001b[0m  0.0122\n",
      "      2       \u001b[36m39.5824\u001b[0m       \u001b[32m35.0252\u001b[0m  0.0120\n",
      "      3       \u001b[36m34.8833\u001b[0m       \u001b[32m31.9772\u001b[0m  0.0121\n",
      "      4       34.9049       \u001b[32m31.4049\u001b[0m  0.0119\n",
      "      5       \u001b[36m33.6738\u001b[0m       32.5764  0.0120\n",
      "      6       \u001b[36m33.6405\u001b[0m       31.7358  0.0118\n",
      "      7       \u001b[36m33.1529\u001b[0m       \u001b[32m30.7395\u001b[0m  0.0119\n",
      "      8       \u001b[36m32.9945\u001b[0m       \u001b[32m30.4770\u001b[0m  0.0119\n",
      "      9       \u001b[36m32.8193\u001b[0m       30.7189  0.0120\n",
      "     10       \u001b[36m32.7534\u001b[0m       30.6527  0.0120\n",
      "     11       \u001b[36m32.6334\u001b[0m       \u001b[32m30.3763\u001b[0m  0.0121\n",
      "     12       \u001b[36m32.5496\u001b[0m       \u001b[32m30.3291\u001b[0m  0.0131\n",
      "     13       \u001b[36m32.4982\u001b[0m       30.4159  0.0124\n",
      "     14       \u001b[36m32.4465\u001b[0m       30.3616  0.0125\n",
      "     15       \u001b[36m32.4093\u001b[0m       30.3437  0.0118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     16       \u001b[36m32.3742\u001b[0m       \u001b[32m30.2966\u001b[0m  0.0123\n",
      "     17       \u001b[36m32.3464\u001b[0m       \u001b[32m30.2382\u001b[0m  0.0117\n",
      "     18       \u001b[36m32.3258\u001b[0m       30.2786  0.0120\n",
      "     19       \u001b[36m32.3073\u001b[0m       \u001b[32m30.2206\u001b[0m  0.0117\n",
      "     20       \u001b[36m32.2878\u001b[0m       30.2220  0.0116\n",
      "     21       \u001b[36m32.2753\u001b[0m       \u001b[32m30.1776\u001b[0m  0.0115\n",
      "     22       \u001b[36m32.2631\u001b[0m       30.2398  0.0119\n",
      "     23       \u001b[36m32.2547\u001b[0m       \u001b[32m30.1412\u001b[0m  0.0116\n",
      "     24       \u001b[36m32.2476\u001b[0m       30.2718  0.0117\n",
      "     25       \u001b[36m32.2466\u001b[0m       \u001b[32m30.1096\u001b[0m  0.0119\n",
      "     26       32.2529       30.3478  0.0118\n",
      "     27       32.2607       \u001b[32m30.0929\u001b[0m  0.0118\n",
      "     28       32.2736       30.2739  0.0116\n",
      "     29       32.2592       \u001b[32m30.0592\u001b[0m  0.0115\n",
      "     30       \u001b[36m32.2265\u001b[0m       30.0944  0.0117\n",
      "     31       \u001b[36m32.2129\u001b[0m       30.1440  0.0124\n",
      "     32       \u001b[36m32.1967\u001b[0m       \u001b[32m30.0463\u001b[0m  0.0123\n",
      "     33       \u001b[36m32.1940\u001b[0m       30.1261  0.0123\n",
      "     34       \u001b[36m32.1937\u001b[0m       30.0723  0.0129\n",
      "     35       \u001b[36m32.1831\u001b[0m       30.0813  0.0130\n",
      "     36       \u001b[36m32.1819\u001b[0m       30.0890  0.0128\n",
      "     37       \u001b[36m32.1756\u001b[0m       30.0706  0.0122\n",
      "     38       \u001b[36m32.1729\u001b[0m       30.0872  0.0120\n",
      "     39       \u001b[36m32.1698\u001b[0m       30.0674  0.0123\n",
      "     40       \u001b[36m32.1658\u001b[0m       30.0804  0.0119\n",
      "     41       \u001b[36m32.1639\u001b[0m       30.0666  0.0120\n",
      "     42       \u001b[36m32.1596\u001b[0m       30.0738  0.0140\n",
      "     43       \u001b[36m32.1578\u001b[0m       30.0637  0.0134\n",
      "     44       \u001b[36m32.1541\u001b[0m       30.0723  0.0130\n",
      "     45       \u001b[36m32.1522\u001b[0m       30.0594  0.0120\n",
      "     46       \u001b[36m32.1487\u001b[0m       30.0672  0.0124\n",
      "     47       \u001b[36m32.1468\u001b[0m       30.0564  0.0186\n",
      "     48       \u001b[36m32.1438\u001b[0m       30.0668  0.0125\n",
      "     49       \u001b[36m32.1420\u001b[0m       30.0505  0.0124\n",
      "     50       \u001b[36m32.1391\u001b[0m       30.0666  0.0117\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m34.4430\u001b[0m       \u001b[32m31.0582\u001b[0m  0.0120\n",
      "      2       \u001b[36m30.2263\u001b[0m       \u001b[32m27.4403\u001b[0m  0.0120\n",
      "      3       \u001b[36m25.8260\u001b[0m       29.3818  0.0120\n",
      "      4       \u001b[36m25.6168\u001b[0m       27.6959  0.0117\n",
      "      5       \u001b[36m24.4692\u001b[0m       \u001b[32m26.5170\u001b[0m  0.0117\n",
      "      6       \u001b[36m24.3891\u001b[0m       \u001b[32m26.5022\u001b[0m  0.0113\n",
      "      7       \u001b[36m23.9342\u001b[0m       27.1870  0.0114\n",
      "      8       \u001b[36m23.7430\u001b[0m       27.6452  0.0116\n",
      "      9       \u001b[36m23.5812\u001b[0m       27.1241  0.0117\n",
      "     10       \u001b[36m23.5086\u001b[0m       26.9228  0.0114\n",
      "     11       \u001b[36m23.4288\u001b[0m       27.0734  0.0136\n",
      "     12       \u001b[36m23.3581\u001b[0m       27.1545  0.0119\n",
      "     13       \u001b[36m23.3046\u001b[0m       27.0384  0.0127\n",
      "     14       \u001b[36m23.2626\u001b[0m       26.9881  0.0120\n",
      "     15       \u001b[36m23.2390\u001b[0m       27.0055  0.0124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     16       \u001b[36m23.2100\u001b[0m       27.0097  0.0127\n",
      "     17       \u001b[36m23.1836\u001b[0m       26.9470  0.0120\n",
      "     18       \u001b[36m23.1661\u001b[0m       26.8843  0.0125\n",
      "     19       \u001b[36m23.1533\u001b[0m       26.8871  0.0121\n",
      "     20       \u001b[36m23.1378\u001b[0m       26.8631  0.0122\n",
      "     21       \u001b[36m23.1263\u001b[0m       26.8245  0.0118\n",
      "     22       \u001b[36m23.1153\u001b[0m       26.7844  0.0120\n",
      "     23       \u001b[36m23.1091\u001b[0m       26.8034  0.0120\n",
      "     24       \u001b[36m23.0987\u001b[0m       26.7792  0.0120\n",
      "     25       \u001b[36m23.0973\u001b[0m       26.7993  0.0121\n",
      "     26       \u001b[36m23.0922\u001b[0m       26.7619  0.0116\n",
      "     27       23.0988       26.8186  0.0115\n",
      "     28       23.0960       26.7358  0.0118\n",
      "     29       23.1050       26.8211  0.0116\n",
      "     30       23.1035       26.7224  0.0117\n",
      "     31       \u001b[36m23.0875\u001b[0m       26.7704  0.0115\n",
      "     32       \u001b[36m23.0660\u001b[0m       26.7678  0.0116\n",
      "     33       \u001b[36m23.0588\u001b[0m       26.7374  0.0120\n",
      "     34       \u001b[36m23.0527\u001b[0m       26.7675  0.0115\n",
      "     35       \u001b[36m23.0474\u001b[0m       26.7495  0.0122\n",
      "     36       \u001b[36m23.0470\u001b[0m       26.7644  0.0124\n",
      "     37       \u001b[36m23.0414\u001b[0m       26.7500  0.0123\n",
      "     38       \u001b[36m23.0414\u001b[0m       26.7533  0.0128\n",
      "     39       \u001b[36m23.0363\u001b[0m       26.7455  0.0128\n",
      "     40       \u001b[36m23.0354\u001b[0m       26.7490  0.0122\n",
      "     41       \u001b[36m23.0315\u001b[0m       26.7426  0.0119\n",
      "     42       \u001b[36m23.0306\u001b[0m       26.7482  0.0122\n",
      "     43       \u001b[36m23.0270\u001b[0m       26.7407  0.0122\n",
      "     44       \u001b[36m23.0264\u001b[0m       26.7442  0.0121\n",
      "     45       \u001b[36m23.0230\u001b[0m       26.7372  0.0120\n",
      "     46       \u001b[36m23.0227\u001b[0m       26.7452  0.0118\n",
      "     47       \u001b[36m23.0193\u001b[0m       26.7345  0.0117\n",
      "     48       23.0196       26.7431  0.0124\n",
      "     49       \u001b[36m23.0162\u001b[0m       26.7287  0.0120\n",
      "     50       23.0172       26.7461  0.0117\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m39.7421\u001b[0m       \u001b[32m29.7474\u001b[0m  0.0117\n",
      "      2       \u001b[36m34.2214\u001b[0m       \u001b[32m26.9972\u001b[0m  0.0118\n",
      "      3       \u001b[36m30.8824\u001b[0m       31.1488  0.0119\n",
      "      4       \u001b[36m30.6429\u001b[0m       27.1942  0.0117\n",
      "      5       \u001b[36m29.6231\u001b[0m       \u001b[32m26.5911\u001b[0m  0.0115\n",
      "      6       \u001b[36m29.2961\u001b[0m       27.4581  0.0116\n",
      "      7       \u001b[36m29.1380\u001b[0m       28.6932  0.0113\n",
      "      8       \u001b[36m29.0852\u001b[0m       27.6461  0.0123\n",
      "      9       \u001b[36m28.8020\u001b[0m       27.1447  0.0122\n",
      "     10       \u001b[36m28.7108\u001b[0m       27.4561  0.0122\n",
      "     11       \u001b[36m28.6827\u001b[0m       27.6824  0.0122\n",
      "     12       \u001b[36m28.6278\u001b[0m       27.3707  0.0125\n",
      "     13       \u001b[36m28.5556\u001b[0m       27.3362  0.0124\n",
      "     14       \u001b[36m28.5375\u001b[0m       27.5161  0.0122\n",
      "     15       \u001b[36m28.5333\u001b[0m       27.3994  0.0121\n",
      "     16       \u001b[36m28.5013\u001b[0m       27.2962  0.0116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m28.4846\u001b[0m       27.3722  0.0119\n",
      "     18       \u001b[36m28.4821\u001b[0m       27.3688  0.0122\n",
      "     19       \u001b[36m28.4673\u001b[0m       27.2946  0.0136\n",
      "     20       \u001b[36m28.4550\u001b[0m       27.3217  0.0177\n",
      "     21       \u001b[36m28.4510\u001b[0m       27.3279  0.0127\n",
      "     22       \u001b[36m28.4438\u001b[0m       27.2865  0.0130\n",
      "     23       \u001b[36m28.4358\u001b[0m       27.2978  0.0128\n",
      "     24       \u001b[36m28.4323\u001b[0m       27.3005  0.0133\n",
      "     25       \u001b[36m28.4275\u001b[0m       27.2806  0.0139\n",
      "     26       \u001b[36m28.4224\u001b[0m       27.2845  0.0132\n",
      "     27       \u001b[36m28.4194\u001b[0m       27.2837  0.0133\n",
      "     28       \u001b[36m28.4163\u001b[0m       27.2784  0.0123\n",
      "     29       \u001b[36m28.4129\u001b[0m       27.2846  0.0120\n",
      "     30       \u001b[36m28.4106\u001b[0m       27.2828  0.0148\n",
      "     31       \u001b[36m28.4094\u001b[0m       27.2798  0.0137\n",
      "     32       \u001b[36m28.4075\u001b[0m       27.2854  0.0119\n",
      "     33       28.4075       27.2818  0.0119\n",
      "     34       28.4129       27.2920  0.0117\n",
      "     35       28.4173       27.2989  0.0138\n",
      "     36       28.4291       27.2740  0.0124\n",
      "     37       28.4362       27.3013  0.0122\n",
      "     38       28.4456       27.3323  0.0117\n",
      "     39       28.4812       27.1917  0.0112\n",
      "     40       28.4180       27.3084  0.0134\n",
      "     41       \u001b[36m28.4065\u001b[0m       27.2614  0.0128\n",
      "     42       \u001b[36m28.4054\u001b[0m       27.2283  0.0124\n",
      "     43       \u001b[36m28.3921\u001b[0m       27.2912  0.0124\n",
      "     44       28.3974       27.2651  0.0113\n",
      "     45       \u001b[36m28.3885\u001b[0m       27.2672  0.0113\n",
      "     46       28.3898       27.2722  0.0131\n",
      "     47       \u001b[36m28.3872\u001b[0m       27.2625  0.0129\n",
      "     48       \u001b[36m28.3850\u001b[0m       27.2786  0.0131\n",
      "     49       \u001b[36m28.3841\u001b[0m       27.2812  0.0129\n",
      "     50       \u001b[36m28.3832\u001b[0m       27.2646  0.0119\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m42.3871\u001b[0m       \u001b[32m43.2502\u001b[0m  0.0112\n",
      "      2       \u001b[36m40.8519\u001b[0m       \u001b[32m41.4006\u001b[0m  0.0123\n",
      "      3       \u001b[36m39.4140\u001b[0m       \u001b[32m39.6038\u001b[0m  0.0115\n",
      "      4       \u001b[36m38.0523\u001b[0m       \u001b[32m37.8823\u001b[0m  0.0115\n",
      "      5       \u001b[36m36.7933\u001b[0m       \u001b[32m36.2906\u001b[0m  0.0109\n",
      "      6       \u001b[36m35.6782\u001b[0m       \u001b[32m34.8710\u001b[0m  0.0107\n",
      "      7       \u001b[36m34.7347\u001b[0m       \u001b[32m33.6393\u001b[0m  0.0118\n",
      "      8       \u001b[36m33.9714\u001b[0m       \u001b[32m32.6028\u001b[0m  0.0117\n",
      "      9       \u001b[36m33.3925\u001b[0m       \u001b[32m31.7801\u001b[0m  0.0115\n",
      "     10       \u001b[36m32.9904\u001b[0m       \u001b[32m31.1733\u001b[0m  0.0110\n",
      "     11       \u001b[36m32.7383\u001b[0m       \u001b[32m30.7576\u001b[0m  0.0107\n",
      "     12       \u001b[36m32.5939\u001b[0m       \u001b[32m30.4888\u001b[0m  0.0119\n",
      "     13       \u001b[36m32.5158\u001b[0m       \u001b[32m30.3193\u001b[0m  0.0118\n",
      "     14       \u001b[36m32.4726\u001b[0m       \u001b[32m30.2125\u001b[0m  0.0138\n",
      "     15       \u001b[36m32.4463\u001b[0m       \u001b[32m30.1440\u001b[0m  0.0115\n",
      "     16       \u001b[36m32.4282\u001b[0m       \u001b[32m30.0989\u001b[0m  0.0108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m32.4142\u001b[0m       \u001b[32m30.0678\u001b[0m  0.0124\n",
      "     18       \u001b[36m32.4025\u001b[0m       \u001b[32m30.0454\u001b[0m  0.0119\n",
      "     19       \u001b[36m32.3924\u001b[0m       \u001b[32m30.0282\u001b[0m  0.0110\n",
      "     20       \u001b[36m32.3833\u001b[0m       \u001b[32m30.0146\u001b[0m  0.0114\n",
      "     21       \u001b[36m32.3751\u001b[0m       \u001b[32m30.0034\u001b[0m  0.0115\n",
      "     22       \u001b[36m32.3675\u001b[0m       \u001b[32m29.9936\u001b[0m  0.0117\n",
      "     23       \u001b[36m32.3606\u001b[0m       \u001b[32m29.9850\u001b[0m  0.0121\n",
      "     24       \u001b[36m32.3541\u001b[0m       \u001b[32m29.9774\u001b[0m  0.0111\n",
      "     25       \u001b[36m32.3482\u001b[0m       \u001b[32m29.9705\u001b[0m  0.0118\n",
      "     26       \u001b[36m32.3427\u001b[0m       \u001b[32m29.9642\u001b[0m  0.0123\n",
      "     27       \u001b[36m32.3376\u001b[0m       \u001b[32m29.9584\u001b[0m  0.0118\n",
      "     28       \u001b[36m32.3328\u001b[0m       \u001b[32m29.9531\u001b[0m  0.0120\n",
      "     29       \u001b[36m32.3281\u001b[0m       \u001b[32m29.9479\u001b[0m  0.0116\n",
      "     30       \u001b[36m32.3239\u001b[0m       \u001b[32m29.9429\u001b[0m  0.0118\n",
      "     31       \u001b[36m32.3198\u001b[0m       \u001b[32m29.9381\u001b[0m  0.0112\n",
      "     32       \u001b[36m32.3160\u001b[0m       \u001b[32m29.9339\u001b[0m  0.0117\n",
      "     33       \u001b[36m32.3124\u001b[0m       \u001b[32m29.9298\u001b[0m  0.0122\n",
      "     34       \u001b[36m32.3090\u001b[0m       \u001b[32m29.9260\u001b[0m  0.0122\n",
      "     35       \u001b[36m32.3058\u001b[0m       \u001b[32m29.9225\u001b[0m  0.0114\n",
      "     36       \u001b[36m32.3026\u001b[0m       \u001b[32m29.9189\u001b[0m  0.0121\n",
      "     37       \u001b[36m32.2997\u001b[0m       \u001b[32m29.9155\u001b[0m  0.0116\n",
      "     38       \u001b[36m32.2969\u001b[0m       \u001b[32m29.9124\u001b[0m  0.0122\n",
      "     39       \u001b[36m32.2942\u001b[0m       \u001b[32m29.9096\u001b[0m  0.0121\n",
      "     40       \u001b[36m32.2915\u001b[0m       \u001b[32m29.9068\u001b[0m  0.0119\n",
      "     41       \u001b[36m32.2890\u001b[0m       \u001b[32m29.9042\u001b[0m  0.0136\n",
      "     42       \u001b[36m32.2865\u001b[0m       \u001b[32m29.9019\u001b[0m  0.0115\n",
      "     43       \u001b[36m32.2841\u001b[0m       \u001b[32m29.8996\u001b[0m  0.0125\n",
      "     44       \u001b[36m32.2819\u001b[0m       \u001b[32m29.8973\u001b[0m  0.0137\n",
      "     45       \u001b[36m32.2797\u001b[0m       \u001b[32m29.8951\u001b[0m  0.0127\n",
      "     46       \u001b[36m32.2777\u001b[0m       \u001b[32m29.8930\u001b[0m  0.0140\n",
      "     47       \u001b[36m32.2757\u001b[0m       \u001b[32m29.8911\u001b[0m  0.0179\n",
      "     48       \u001b[36m32.2738\u001b[0m       \u001b[32m29.8890\u001b[0m  0.0137\n",
      "     49       \u001b[36m32.2721\u001b[0m       \u001b[32m29.8872\u001b[0m  0.0122\n",
      "     50       \u001b[36m32.2703\u001b[0m       \u001b[32m29.8854\u001b[0m  0.0118\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m33.8816\u001b[0m       \u001b[32m31.7517\u001b[0m  0.0150\n",
      "      2       \u001b[36m32.2447\u001b[0m       \u001b[32m30.6288\u001b[0m  0.0149\n",
      "      3       \u001b[36m30.8375\u001b[0m       \u001b[32m29.6382\u001b[0m  0.0134\n",
      "      4       \u001b[36m29.5383\u001b[0m       \u001b[32m28.7336\u001b[0m  0.0115\n",
      "      5       \u001b[36m28.2989\u001b[0m       \u001b[32m27.9128\u001b[0m  0.0111\n",
      "      6       \u001b[36m27.1223\u001b[0m       \u001b[32m27.2180\u001b[0m  0.0110\n",
      "      7       \u001b[36m26.0587\u001b[0m       \u001b[32m26.7023\u001b[0m  0.0112\n",
      "      8       \u001b[36m25.1725\u001b[0m       \u001b[32m26.4008\u001b[0m  0.0110\n",
      "      9       \u001b[36m24.5101\u001b[0m       \u001b[32m26.2956\u001b[0m  0.0112\n",
      "     10       \u001b[36m24.0676\u001b[0m       26.3194  0.0111\n",
      "     11       \u001b[36m23.7970\u001b[0m       26.3960  0.0116\n",
      "     12       \u001b[36m23.6381\u001b[0m       26.4734  0.0112\n",
      "     13       \u001b[36m23.5429\u001b[0m       26.5299  0.0111\n",
      "     14       \u001b[36m23.4814\u001b[0m       26.5631  0.0111\n",
      "     15       \u001b[36m23.4378\u001b[0m       26.5786  0.0107\n",
      "     16       \u001b[36m23.4041\u001b[0m       26.5826  0.0119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m23.3765\u001b[0m       26.5798  0.0117\n",
      "     18       \u001b[36m23.3532\u001b[0m       26.5742  0.0115\n",
      "     19       \u001b[36m23.3331\u001b[0m       26.5662  0.0111\n",
      "     20       \u001b[36m23.3153\u001b[0m       26.5571  0.0111\n",
      "     21       \u001b[36m23.2994\u001b[0m       26.5481  0.0111\n",
      "     22       \u001b[36m23.2850\u001b[0m       26.5392  0.0113\n",
      "     23       \u001b[36m23.2719\u001b[0m       26.5311  0.0112\n",
      "     24       \u001b[36m23.2599\u001b[0m       26.5235  0.0116\n",
      "     25       \u001b[36m23.2490\u001b[0m       26.5167  0.0111\n",
      "     26       \u001b[36m23.2390\u001b[0m       26.5106  0.0111\n",
      "     27       \u001b[36m23.2298\u001b[0m       26.5051  0.0115\n",
      "     28       \u001b[36m23.2214\u001b[0m       26.4999  0.0114\n",
      "     29       \u001b[36m23.2136\u001b[0m       26.4953  0.0114\n",
      "     30       \u001b[36m23.2063\u001b[0m       26.4910  0.0111\n",
      "     31       \u001b[36m23.1996\u001b[0m       26.4873  0.0113\n",
      "     32       \u001b[36m23.1934\u001b[0m       26.4836  0.0113\n",
      "     33       \u001b[36m23.1875\u001b[0m       26.4800  0.0112\n",
      "     34       \u001b[36m23.1819\u001b[0m       26.4769  0.0110\n",
      "     35       \u001b[36m23.1768\u001b[0m       26.4742  0.0110\n",
      "     36       \u001b[36m23.1718\u001b[0m       26.4712  0.0112\n",
      "     37       \u001b[36m23.1673\u001b[0m       26.4690  0.0112\n",
      "     38       \u001b[36m23.1630\u001b[0m       26.4669  0.0113\n",
      "     39       \u001b[36m23.1589\u001b[0m       26.4649  0.0113\n",
      "     40       \u001b[36m23.1551\u001b[0m       26.4630  0.0107\n",
      "     41       \u001b[36m23.1514\u001b[0m       26.4612  0.0106\n",
      "     42       \u001b[36m23.1480\u001b[0m       26.4596  0.0109\n",
      "     43       \u001b[36m23.1448\u001b[0m       26.4581  0.0110\n",
      "     44       \u001b[36m23.1417\u001b[0m       26.4568  0.0108\n",
      "     45       \u001b[36m23.1387\u001b[0m       26.4555  0.0112\n",
      "     46       \u001b[36m23.1359\u001b[0m       26.4544  0.0107\n",
      "     47       \u001b[36m23.1332\u001b[0m       26.4530  0.0109\n",
      "     48       \u001b[36m23.1306\u001b[0m       26.4520  0.0111\n",
      "     49       \u001b[36m23.1281\u001b[0m       26.4509  0.0109\n",
      "     50       \u001b[36m23.1258\u001b[0m       26.4500  0.0108\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m41.3316\u001b[0m       \u001b[32m32.0014\u001b[0m  0.0109\n",
      "      2       \u001b[36m39.3246\u001b[0m       \u001b[32m30.6973\u001b[0m  0.0111\n",
      "      3       \u001b[36m37.4749\u001b[0m       \u001b[32m29.4980\u001b[0m  0.0111\n",
      "      4       \u001b[36m35.6919\u001b[0m       \u001b[32m28.4105\u001b[0m  0.0108\n",
      "      5       \u001b[36m34.0098\u001b[0m       \u001b[32m27.4925\u001b[0m  0.0107\n",
      "      6       \u001b[36m32.5026\u001b[0m       \u001b[32m26.7987\u001b[0m  0.0106\n",
      "      7       \u001b[36m31.2128\u001b[0m       \u001b[32m26.3739\u001b[0m  0.0117\n",
      "      8       \u001b[36m30.1811\u001b[0m       \u001b[32m26.2519\u001b[0m  0.0113\n",
      "      9       \u001b[36m29.4591\u001b[0m       26.3789  0.0113\n",
      "     10       \u001b[36m29.0392\u001b[0m       26.6185  0.0112\n",
      "     11       \u001b[36m28.8342\u001b[0m       26.8448  0.0115\n",
      "     12       \u001b[36m28.7424\u001b[0m       27.0042  0.0119\n",
      "     13       \u001b[36m28.6975\u001b[0m       27.0988  0.0117\n",
      "     14       \u001b[36m28.6697\u001b[0m       27.1488  0.0119\n",
      "     15       \u001b[36m28.6483\u001b[0m       27.1721  0.0110\n",
      "     16       \u001b[36m28.6300\u001b[0m       27.1812  0.0112\n",
      "     17       \u001b[36m28.6138\u001b[0m       27.1826  0.0113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     18       \u001b[36m28.5994\u001b[0m       27.1805  0.0113\n",
      "     19       \u001b[36m28.5865\u001b[0m       27.1769  0.0113\n",
      "     20       \u001b[36m28.5751\u001b[0m       27.1732  0.0112\n",
      "     21       \u001b[36m28.5649\u001b[0m       27.1697  0.0109\n",
      "     22       \u001b[36m28.5557\u001b[0m       27.1660  0.0110\n",
      "     23       \u001b[36m28.5473\u001b[0m       27.1627  0.0111\n",
      "     24       \u001b[36m28.5398\u001b[0m       27.1605  0.0108\n",
      "     25       \u001b[36m28.5330\u001b[0m       27.1593  0.0108\n",
      "     26       \u001b[36m28.5272\u001b[0m       27.1579  0.0107\n",
      "     27       \u001b[36m28.5217\u001b[0m       27.1567  0.0106\n",
      "     28       \u001b[36m28.5165\u001b[0m       27.1557  0.0152\n",
      "     29       \u001b[36m28.5117\u001b[0m       27.1547  0.0132\n",
      "     30       \u001b[36m28.5072\u001b[0m       27.1537  0.0114\n",
      "     31       \u001b[36m28.5029\u001b[0m       27.1527  0.0112\n",
      "     32       \u001b[36m28.4987\u001b[0m       27.1517  0.0114\n",
      "     33       \u001b[36m28.4949\u001b[0m       27.1507  0.0120\n",
      "     34       \u001b[36m28.4912\u001b[0m       27.1499  0.0131\n",
      "     35       \u001b[36m28.4877\u001b[0m       27.1489  0.0113\n",
      "     36       \u001b[36m28.4844\u001b[0m       27.1486  0.0119\n",
      "     37       \u001b[36m28.4813\u001b[0m       27.1479  0.0109\n",
      "     38       \u001b[36m28.4784\u001b[0m       27.1479  0.0112\n",
      "     39       \u001b[36m28.4756\u001b[0m       27.1472  0.0113\n",
      "     40       \u001b[36m28.4730\u001b[0m       27.1467  0.0112\n",
      "     41       \u001b[36m28.4706\u001b[0m       27.1466  0.0110\n",
      "     42       \u001b[36m28.4683\u001b[0m       27.1462  0.0109\n",
      "     43       \u001b[36m28.4660\u001b[0m       27.1459  0.0113\n",
      "     44       \u001b[36m28.4639\u001b[0m       27.1460  0.0115\n",
      "     45       \u001b[36m28.4619\u001b[0m       27.1459  0.0112\n",
      "     46       \u001b[36m28.4601\u001b[0m       27.1455  0.0111\n",
      "     47       \u001b[36m28.4582\u001b[0m       27.1456  0.0111\n",
      "     48       \u001b[36m28.4565\u001b[0m       27.1454  0.0110\n",
      "     49       \u001b[36m28.4548\u001b[0m       27.1458  0.0109\n",
      "     50       \u001b[36m28.4533\u001b[0m       27.1456  0.0115\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m41.6582\u001b[0m       \u001b[32m38.9457\u001b[0m  0.0119\n",
      "      2       \u001b[36m36.2749\u001b[0m       \u001b[32m31.8934\u001b[0m  0.0121\n",
      "      3       \u001b[36m34.6986\u001b[0m       \u001b[32m31.2382\u001b[0m  0.0128\n",
      "      4       \u001b[36m33.5855\u001b[0m       32.0854  0.0120\n",
      "      5       \u001b[36m33.3594\u001b[0m       31.3229  0.0124\n",
      "      6       \u001b[36m32.9327\u001b[0m       \u001b[32m30.4222\u001b[0m  0.0117\n",
      "      7       \u001b[36m32.7856\u001b[0m       \u001b[32m30.4083\u001b[0m  0.0118\n",
      "      8       \u001b[36m32.6541\u001b[0m       30.5989  0.0118\n",
      "      9       \u001b[36m32.5789\u001b[0m       \u001b[32m30.2991\u001b[0m  0.0119\n",
      "     10       \u001b[36m32.4828\u001b[0m       \u001b[32m30.1796\u001b[0m  0.0119\n",
      "     11       \u001b[36m32.4353\u001b[0m       30.3165  0.0117\n",
      "     12       \u001b[36m32.3905\u001b[0m       30.3003  0.0122\n",
      "     13       \u001b[36m32.3562\u001b[0m       30.2528  0.0116\n",
      "     14       \u001b[36m32.3304\u001b[0m       30.2117  0.0120\n",
      "     15       \u001b[36m32.3082\u001b[0m       30.2173  0.0115\n",
      "     16       \u001b[36m32.2945\u001b[0m       30.2533  0.0116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m32.2795\u001b[0m       30.1914  0.0120\n",
      "     18       \u001b[36m32.2652\u001b[0m       30.2003  0.0115\n",
      "     19       \u001b[36m32.2577\u001b[0m       30.1805  0.0126\n",
      "     20       \u001b[36m32.2465\u001b[0m       30.1966  0.0120\n",
      "     21       \u001b[36m32.2384\u001b[0m       \u001b[32m30.1413\u001b[0m  0.0123\n",
      "     22       \u001b[36m32.2305\u001b[0m       30.2017  0.0114\n",
      "     23       \u001b[36m32.2258\u001b[0m       \u001b[32m30.1186\u001b[0m  0.0114\n",
      "     24       \u001b[36m32.2211\u001b[0m       30.2408  0.0120\n",
      "     25       32.2212       \u001b[32m30.0798\u001b[0m  0.0115\n",
      "     26       32.2279       30.3229  0.0116\n",
      "     27       32.2298       \u001b[32m30.0446\u001b[0m  0.0113\n",
      "     28       32.2359       30.2197  0.0114\n",
      "     29       32.2285       30.0469  0.0118\n",
      "     30       \u001b[36m32.1987\u001b[0m       30.0778  0.0115\n",
      "     31       \u001b[36m32.1937\u001b[0m       30.1246  0.0119\n",
      "     32       \u001b[36m32.1788\u001b[0m       \u001b[32m30.0302\u001b[0m  0.0115\n",
      "     33       \u001b[36m32.1762\u001b[0m       30.1131  0.0119\n",
      "     34       \u001b[36m32.1753\u001b[0m       30.0657  0.0120\n",
      "     35       \u001b[36m32.1667\u001b[0m       30.0912  0.0124\n",
      "     36       \u001b[36m32.1662\u001b[0m       30.0767  0.0120\n",
      "     37       \u001b[36m32.1596\u001b[0m       30.0791  0.0123\n",
      "     38       \u001b[36m32.1581\u001b[0m       30.0807  0.0115\n",
      "     39       \u001b[36m32.1535\u001b[0m       30.0784  0.0113\n",
      "     40       \u001b[36m32.1513\u001b[0m       30.0840  0.0120\n",
      "     41       \u001b[36m32.1484\u001b[0m       30.0776  0.0120\n",
      "     42       \u001b[36m32.1457\u001b[0m       30.0834  0.0116\n",
      "     43       \u001b[36m32.1432\u001b[0m       30.0775  0.0114\n",
      "     44       \u001b[36m32.1408\u001b[0m       30.0820  0.0113\n",
      "     45       \u001b[36m32.1383\u001b[0m       30.0755  0.0117\n",
      "     46       \u001b[36m32.1360\u001b[0m       30.0822  0.0115\n",
      "     47       \u001b[36m32.1338\u001b[0m       30.0731  0.0116\n",
      "     48       \u001b[36m32.1315\u001b[0m       30.0812  0.0119\n",
      "     49       \u001b[36m32.1291\u001b[0m       30.0740  0.0117\n",
      "     50       \u001b[36m32.1272\u001b[0m       30.0830  0.0123\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m31.6731\u001b[0m       \u001b[32m27.9595\u001b[0m  0.0121\n",
      "      2       \u001b[36m26.6722\u001b[0m       28.8293  0.0122\n",
      "      3       \u001b[36m25.0978\u001b[0m       \u001b[32m27.0976\u001b[0m  0.0120\n",
      "      4       \u001b[36m24.2588\u001b[0m       \u001b[32m26.3100\u001b[0m  0.0119\n",
      "      5       \u001b[36m24.0594\u001b[0m       26.5662  0.0114\n",
      "      6       \u001b[36m23.6612\u001b[0m       27.4425  0.0114\n",
      "      7       \u001b[36m23.5228\u001b[0m       27.0144  0.0117\n",
      "      8       \u001b[36m23.4447\u001b[0m       26.7403  0.0170\n",
      "      9       \u001b[36m23.3851\u001b[0m       26.9610  0.0133\n",
      "     10       \u001b[36m23.2884\u001b[0m       27.0473  0.0123\n",
      "     11       \u001b[36m23.2321\u001b[0m       26.7618  0.0118\n",
      "     12       \u001b[36m23.2108\u001b[0m       26.7573  0.0122\n",
      "     13       \u001b[36m23.1786\u001b[0m       26.8906  0.0144\n",
      "     14       \u001b[36m23.1469\u001b[0m       26.7813  0.0121\n",
      "     15       \u001b[36m23.1304\u001b[0m       26.6578  0.0126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     16       \u001b[36m23.1210\u001b[0m       26.6846  0.0119\n",
      "     17       \u001b[36m23.1059\u001b[0m       26.6962  0.0116\n",
      "     18       \u001b[36m23.0933\u001b[0m       26.6249  0.0122\n",
      "     19       \u001b[36m23.0868\u001b[0m       26.6100  0.0119\n",
      "     20       \u001b[36m23.0786\u001b[0m       26.6305  0.0119\n",
      "     21       \u001b[36m23.0703\u001b[0m       26.6080  0.0115\n",
      "     22       \u001b[36m23.0652\u001b[0m       26.5948  0.0116\n",
      "     23       \u001b[36m23.0596\u001b[0m       26.6075  0.0123\n",
      "     24       \u001b[36m23.0535\u001b[0m       26.5988  0.0119\n",
      "     25       \u001b[36m23.0490\u001b[0m       26.5884  0.0119\n",
      "     26       \u001b[36m23.0451\u001b[0m       26.5915  0.0119\n",
      "     27       \u001b[36m23.0408\u001b[0m       26.5840  0.0116\n",
      "     28       \u001b[36m23.0374\u001b[0m       26.5782  0.0134\n",
      "     29       \u001b[36m23.0339\u001b[0m       26.5795  0.0115\n",
      "     30       \u001b[36m23.0307\u001b[0m       26.5734  0.0119\n",
      "     31       \u001b[36m23.0276\u001b[0m       26.5690  0.0114\n",
      "     32       \u001b[36m23.0249\u001b[0m       26.5696  0.0115\n",
      "     33       \u001b[36m23.0221\u001b[0m       26.5651  0.0115\n",
      "     34       \u001b[36m23.0195\u001b[0m       26.5630  0.0114\n",
      "     35       \u001b[36m23.0171\u001b[0m       26.5618  0.0117\n",
      "     36       \u001b[36m23.0146\u001b[0m       26.5564  0.0117\n",
      "     37       \u001b[36m23.0121\u001b[0m       26.5569  0.0113\n",
      "     38       \u001b[36m23.0103\u001b[0m       26.5528  0.0117\n",
      "     39       \u001b[36m23.0085\u001b[0m       26.5595  0.0116\n",
      "     40       \u001b[36m23.0069\u001b[0m       26.5490  0.0114\n",
      "     41       23.0080       26.5394  0.0113\n",
      "     42       23.0105       26.5723  0.0111\n",
      "     43       23.0126       26.5421  0.0125\n",
      "     44       23.0155       26.5220  0.0127\n",
      "     45       \u001b[36m23.0027\u001b[0m       26.5756  0.0117\n",
      "     46       \u001b[36m22.9965\u001b[0m       26.5378  0.0116\n",
      "     47       \u001b[36m22.9953\u001b[0m       26.5277  0.0122\n",
      "     48       \u001b[36m22.9928\u001b[0m       26.5452  0.0125\n",
      "     49       \u001b[36m22.9895\u001b[0m       26.5370  0.0121\n",
      "     50       \u001b[36m22.9881\u001b[0m       26.5410  0.0125\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m39.1819\u001b[0m       \u001b[32m28.5411\u001b[0m  0.0119\n",
      "      2       \u001b[36m32.4448\u001b[0m       30.2154  0.0117\n",
      "      3       \u001b[36m31.5196\u001b[0m       \u001b[32m27.9884\u001b[0m  0.0118\n",
      "      4       \u001b[36m29.6237\u001b[0m       \u001b[32m26.3598\u001b[0m  0.0118\n",
      "      5       \u001b[36m29.5949\u001b[0m       26.6964  0.0118\n",
      "      6       \u001b[36m29.0715\u001b[0m       28.4426  0.0117\n",
      "      7       29.1502       27.4989  0.0115\n",
      "      8       \u001b[36m28.8267\u001b[0m       26.9447  0.0117\n",
      "      9       \u001b[36m28.6977\u001b[0m       27.4016  0.0117\n",
      "     10       \u001b[36m28.6875\u001b[0m       27.5241  0.0116\n",
      "     11       \u001b[36m28.6395\u001b[0m       27.2468  0.0114\n",
      "     12       \u001b[36m28.5634\u001b[0m       27.2728  0.0113\n",
      "     13       \u001b[36m28.5358\u001b[0m       27.3753  0.0119\n",
      "     14       \u001b[36m28.5297\u001b[0m       27.4119  0.0116\n",
      "     15       \u001b[36m28.5097\u001b[0m       27.2780  0.0117\n",
      "     16       \u001b[36m28.4818\u001b[0m       27.3274  0.0113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       28.4824       27.3791  0.0115\n",
      "     18       \u001b[36m28.4704\u001b[0m       27.2929  0.0119\n",
      "     19       \u001b[36m28.4568\u001b[0m       27.3156  0.0120\n",
      "     20       \u001b[36m28.4528\u001b[0m       27.3327  0.0116\n",
      "     21       \u001b[36m28.4475\u001b[0m       27.2945  0.0117\n",
      "     22       \u001b[36m28.4382\u001b[0m       27.2989  0.0121\n",
      "     23       \u001b[36m28.4352\u001b[0m       27.3092  0.0123\n",
      "     24       \u001b[36m28.4306\u001b[0m       27.2964  0.0117\n",
      "     25       \u001b[36m28.4258\u001b[0m       27.2965  0.0117\n",
      "     26       \u001b[36m28.4227\u001b[0m       27.3055  0.0115\n",
      "     27       \u001b[36m28.4195\u001b[0m       27.2992  0.0114\n",
      "     28       \u001b[36m28.4160\u001b[0m       27.3045  0.0120\n",
      "     29       \u001b[36m28.4134\u001b[0m       27.3091  0.0118\n",
      "     30       \u001b[36m28.4105\u001b[0m       27.3097  0.0115\n",
      "     31       \u001b[36m28.4083\u001b[0m       27.3113  0.0114\n",
      "     32       \u001b[36m28.4065\u001b[0m       27.3163  0.0113\n",
      "     33       \u001b[36m28.4037\u001b[0m       27.3136  0.0120\n",
      "     34       28.4042       27.3062  0.0118\n",
      "     35       28.4042       27.3232  0.0123\n",
      "     36       28.4052       27.3045  0.0127\n",
      "     37       28.4174       27.3114  0.0162\n",
      "     38       28.4223       27.3817  0.0133\n",
      "     39       28.4457       27.2076  0.0134\n",
      "     40       28.4141       27.3503  0.0124\n",
      "     41       \u001b[36m28.3988\u001b[0m       27.2897  0.0123\n",
      "     42       \u001b[36m28.3982\u001b[0m       27.2703  0.0130\n",
      "     43       \u001b[36m28.3914\u001b[0m       27.2862  0.0125\n",
      "     44       28.3939       27.2891  0.0130\n",
      "     45       \u001b[36m28.3869\u001b[0m       27.2919  0.0120\n",
      "     46       28.3890       27.2645  0.0119\n",
      "     47       \u001b[36m28.3864\u001b[0m       27.2837  0.0119\n",
      "     48       \u001b[36m28.3851\u001b[0m       27.3075  0.0119\n",
      "     49       \u001b[36m28.3837\u001b[0m       27.2775  0.0119\n",
      "     50       28.3838       27.2786  0.0119\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m42.8388\u001b[0m       \u001b[32m43.2734\u001b[0m  0.0111\n",
      "      2       \u001b[36m40.8966\u001b[0m       \u001b[32m41.0478\u001b[0m  0.0117\n",
      "      3       \u001b[36m39.2128\u001b[0m       \u001b[32m39.0511\u001b[0m  0.0112\n",
      "      4       \u001b[36m37.7300\u001b[0m       \u001b[32m37.2567\u001b[0m  0.0113\n",
      "      5       \u001b[36m36.4366\u001b[0m       \u001b[32m35.6321\u001b[0m  0.0105\n",
      "      6       \u001b[36m35.3143\u001b[0m       \u001b[32m34.1593\u001b[0m  0.0108\n",
      "      7       \u001b[36m34.3758\u001b[0m       \u001b[32m32.9006\u001b[0m  0.0117\n",
      "      8       \u001b[36m33.6663\u001b[0m       \u001b[32m31.9206\u001b[0m  0.0119\n",
      "      9       \u001b[36m33.1946\u001b[0m       \u001b[32m31.2373\u001b[0m  0.0114\n",
      "     10       \u001b[36m32.9161\u001b[0m       \u001b[32m30.8011\u001b[0m  0.0119\n",
      "     11       \u001b[36m32.7630\u001b[0m       \u001b[32m30.5351\u001b[0m  0.0108\n",
      "     12       \u001b[36m32.6788\u001b[0m       \u001b[32m30.3748\u001b[0m  0.0109\n",
      "     13       \u001b[36m32.6276\u001b[0m       \u001b[32m30.2760\u001b[0m  0.0117\n",
      "     14       \u001b[36m32.5921\u001b[0m       \u001b[32m30.2123\u001b[0m  0.0114\n",
      "     15       \u001b[36m32.5644\u001b[0m       \u001b[32m30.1690\u001b[0m  0.0110\n",
      "     16       \u001b[36m32.5412\u001b[0m       \u001b[32m30.1377\u001b[0m  0.0112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m32.5212\u001b[0m       \u001b[32m30.1141\u001b[0m  0.0119\n",
      "     18       \u001b[36m32.5033\u001b[0m       \u001b[32m30.0949\u001b[0m  0.0109\n",
      "     19       \u001b[36m32.4873\u001b[0m       \u001b[32m30.0788\u001b[0m  0.0110\n",
      "     20       \u001b[36m32.4728\u001b[0m       \u001b[32m30.0649\u001b[0m  0.0106\n",
      "     21       \u001b[36m32.4597\u001b[0m       \u001b[32m30.0526\u001b[0m  0.0109\n",
      "     22       \u001b[36m32.4478\u001b[0m       \u001b[32m30.0416\u001b[0m  0.0117\n",
      "     23       \u001b[36m32.4368\u001b[0m       \u001b[32m30.0316\u001b[0m  0.0119\n",
      "     24       \u001b[36m32.4267\u001b[0m       \u001b[32m30.0226\u001b[0m  0.0127\n",
      "     25       \u001b[36m32.4173\u001b[0m       \u001b[32m30.0139\u001b[0m  0.0114\n",
      "     26       \u001b[36m32.4088\u001b[0m       \u001b[32m30.0057\u001b[0m  0.0111\n",
      "     27       \u001b[36m32.4008\u001b[0m       \u001b[32m29.9981\u001b[0m  0.0117\n",
      "     28       \u001b[36m32.3934\u001b[0m       \u001b[32m29.9908\u001b[0m  0.0115\n",
      "     29       \u001b[36m32.3865\u001b[0m       \u001b[32m29.9842\u001b[0m  0.0118\n",
      "     30       \u001b[36m32.3799\u001b[0m       \u001b[32m29.9778\u001b[0m  0.0114\n",
      "     31       \u001b[36m32.3738\u001b[0m       \u001b[32m29.9718\u001b[0m  0.0115\n",
      "     32       \u001b[36m32.3680\u001b[0m       \u001b[32m29.9660\u001b[0m  0.0114\n",
      "     33       \u001b[36m32.3625\u001b[0m       \u001b[32m29.9606\u001b[0m  0.0113\n",
      "     34       \u001b[36m32.3573\u001b[0m       \u001b[32m29.9554\u001b[0m  0.0112\n",
      "     35       \u001b[36m32.3523\u001b[0m       \u001b[32m29.9504\u001b[0m  0.0112\n",
      "     36       \u001b[36m32.3477\u001b[0m       \u001b[32m29.9453\u001b[0m  0.0111\n",
      "     37       \u001b[36m32.3433\u001b[0m       \u001b[32m29.9407\u001b[0m  0.0119\n",
      "     38       \u001b[36m32.3391\u001b[0m       \u001b[32m29.9362\u001b[0m  0.0110\n",
      "     39       \u001b[36m32.3352\u001b[0m       \u001b[32m29.9320\u001b[0m  0.0113\n",
      "     40       \u001b[36m32.3313\u001b[0m       \u001b[32m29.9281\u001b[0m  0.0106\n",
      "     41       \u001b[36m32.3277\u001b[0m       \u001b[32m29.9243\u001b[0m  0.0106\n",
      "     42       \u001b[36m32.3242\u001b[0m       \u001b[32m29.9207\u001b[0m  0.0111\n",
      "     43       \u001b[36m32.3209\u001b[0m       \u001b[32m29.9170\u001b[0m  0.0111\n",
      "     44       \u001b[36m32.3174\u001b[0m       \u001b[32m29.9138\u001b[0m  0.0110\n",
      "     45       \u001b[36m32.3142\u001b[0m       \u001b[32m29.9106\u001b[0m  0.0109\n",
      "     46       \u001b[36m32.3112\u001b[0m       \u001b[32m29.9076\u001b[0m  0.0106\n",
      "     47       \u001b[36m32.3081\u001b[0m       \u001b[32m29.9050\u001b[0m  0.0116\n",
      "     48       \u001b[36m32.3052\u001b[0m       \u001b[32m29.9024\u001b[0m  0.0117\n",
      "     49       \u001b[36m32.3023\u001b[0m       \u001b[32m29.8995\u001b[0m  0.0114\n",
      "     50       \u001b[36m32.2998\u001b[0m       \u001b[32m29.8968\u001b[0m  0.0115\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m32.8772\u001b[0m       \u001b[32m31.0365\u001b[0m  0.0110\n",
      "      2       \u001b[36m31.2302\u001b[0m       \u001b[32m29.8680\u001b[0m  0.0127\n",
      "      3       \u001b[36m29.6809\u001b[0m       \u001b[32m28.7915\u001b[0m  0.0115\n",
      "      4       \u001b[36m28.2201\u001b[0m       \u001b[32m27.8443\u001b[0m  0.0117\n",
      "      5       \u001b[36m26.8948\u001b[0m       \u001b[32m27.0895\u001b[0m  0.0113\n",
      "      6       \u001b[36m25.7751\u001b[0m       \u001b[32m26.5801\u001b[0m  0.0111\n",
      "      7       \u001b[36m24.9136\u001b[0m       \u001b[32m26.3186\u001b[0m  0.0113\n",
      "      8       \u001b[36m24.3160\u001b[0m       \u001b[32m26.2483\u001b[0m  0.0113\n",
      "      9       \u001b[36m23.9407\u001b[0m       26.2824  0.0111\n",
      "     10       \u001b[36m23.7217\u001b[0m       26.3477  0.0113\n",
      "     11       \u001b[36m23.5954\u001b[0m       26.4044  0.0113\n",
      "     12       \u001b[36m23.5191\u001b[0m       26.4415  0.0121\n",
      "     13       \u001b[36m23.4679\u001b[0m       26.4610  0.0107\n",
      "     14       \u001b[36m23.4299\u001b[0m       26.4689  0.0108\n",
      "     15       \u001b[36m23.4000\u001b[0m       26.4704  0.0112\n",
      "     16       \u001b[36m23.3751\u001b[0m       26.4679  0.0113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m23.3536\u001b[0m       26.4640  0.0129\n",
      "     18       \u001b[36m23.3347\u001b[0m       26.4593  0.0157\n",
      "     19       \u001b[36m23.3178\u001b[0m       26.4546  0.0131\n",
      "     20       \u001b[36m23.3025\u001b[0m       26.4499  0.0114\n",
      "     21       \u001b[36m23.2886\u001b[0m       26.4455  0.0117\n",
      "     22       \u001b[36m23.2760\u001b[0m       26.4416  0.0118\n",
      "     23       \u001b[36m23.2643\u001b[0m       26.4378  0.0136\n",
      "     24       \u001b[36m23.2535\u001b[0m       26.4343  0.0116\n",
      "     25       \u001b[36m23.2436\u001b[0m       26.4312  0.0115\n",
      "     26       \u001b[36m23.2346\u001b[0m       26.4287  0.0112\n",
      "     27       \u001b[36m23.2262\u001b[0m       26.4263  0.0114\n",
      "     28       \u001b[36m23.2184\u001b[0m       26.4241  0.0113\n",
      "     29       \u001b[36m23.2112\u001b[0m       26.4220  0.0113\n",
      "     30       \u001b[36m23.2045\u001b[0m       26.4202  0.0112\n",
      "     31       \u001b[36m23.1982\u001b[0m       26.4186  0.0113\n",
      "     32       \u001b[36m23.1924\u001b[0m       26.4171  0.0115\n",
      "     33       \u001b[36m23.1869\u001b[0m       26.4156  0.0114\n",
      "     34       \u001b[36m23.1817\u001b[0m       26.4142  0.0113\n",
      "     35       \u001b[36m23.1768\u001b[0m       26.4128  0.0114\n",
      "     36       \u001b[36m23.1722\u001b[0m       26.4116  0.0114\n",
      "     37       \u001b[36m23.1678\u001b[0m       26.4106  0.0112\n",
      "     38       \u001b[36m23.1636\u001b[0m       26.4096  0.0109\n",
      "     39       \u001b[36m23.1596\u001b[0m       26.4086  0.0126\n",
      "     40       \u001b[36m23.1558\u001b[0m       26.4078  0.0114\n",
      "     41       \u001b[36m23.1523\u001b[0m       26.4070  0.0107\n",
      "     42       \u001b[36m23.1489\u001b[0m       26.4062  0.0114\n",
      "     43       \u001b[36m23.1456\u001b[0m       26.4056  0.0111\n",
      "     44       \u001b[36m23.1425\u001b[0m       26.4048  0.0112\n",
      "     45       \u001b[36m23.1395\u001b[0m       26.4041  0.0112\n",
      "     46       \u001b[36m23.1366\u001b[0m       26.4034  0.0109\n",
      "     47       \u001b[36m23.1339\u001b[0m       26.4029  0.0114\n",
      "     48       \u001b[36m23.1312\u001b[0m       26.4024  0.0112\n",
      "     49       \u001b[36m23.1287\u001b[0m       26.4018  0.0112\n",
      "     50       \u001b[36m23.1263\u001b[0m       26.4013  0.0112\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m39.3133\u001b[0m       \u001b[32m30.5232\u001b[0m  0.0117\n",
      "      2       \u001b[36m37.1249\u001b[0m       \u001b[32m29.1746\u001b[0m  0.0115\n",
      "      3       \u001b[36m35.1297\u001b[0m       \u001b[32m28.0111\u001b[0m  0.0109\n",
      "      4       \u001b[36m33.3239\u001b[0m       \u001b[32m27.0948\u001b[0m  0.0111\n",
      "      5       \u001b[36m31.7694\u001b[0m       \u001b[32m26.4975\u001b[0m  0.0107\n",
      "      6       \u001b[36m30.5258\u001b[0m       \u001b[32m26.2671\u001b[0m  0.0110\n",
      "      7       \u001b[36m29.6550\u001b[0m       26.3600  0.0123\n",
      "      8       \u001b[36m29.1562\u001b[0m       26.6127  0.0136\n",
      "      9       \u001b[36m28.9220\u001b[0m       26.8620  0.0132\n",
      "     10       \u001b[36m28.8216\u001b[0m       27.0342  0.0107\n",
      "     11       \u001b[36m28.7726\u001b[0m       27.1341  0.0109\n",
      "     12       \u001b[36m28.7408\u001b[0m       27.1869  0.0111\n",
      "     13       \u001b[36m28.7155\u001b[0m       27.2125  0.0107\n",
      "     14       \u001b[36m28.6933\u001b[0m       27.2240  0.0107\n",
      "     15       \u001b[36m28.6731\u001b[0m       27.2300  0.0106\n",
      "     16       \u001b[36m28.6550\u001b[0m       27.2328  0.0108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m28.6388\u001b[0m       27.2351  0.0112\n",
      "     18       \u001b[36m28.6241\u001b[0m       27.2370  0.0111\n",
      "     19       \u001b[36m28.6108\u001b[0m       27.2381  0.0112\n",
      "     20       \u001b[36m28.5985\u001b[0m       27.2387  0.0106\n",
      "     21       \u001b[36m28.5871\u001b[0m       27.2393  0.0107\n",
      "     22       \u001b[36m28.5768\u001b[0m       27.2396  0.0117\n",
      "     23       \u001b[36m28.5672\u001b[0m       27.2401  0.0109\n",
      "     24       \u001b[36m28.5584\u001b[0m       27.2408  0.0109\n",
      "     25       \u001b[36m28.5502\u001b[0m       27.2418  0.0106\n",
      "     26       \u001b[36m28.5426\u001b[0m       27.2430  0.0106\n",
      "     27       \u001b[36m28.5356\u001b[0m       27.2448  0.0110\n",
      "     28       \u001b[36m28.5292\u001b[0m       27.2469  0.0113\n",
      "     29       \u001b[36m28.5232\u001b[0m       27.2488  0.0111\n",
      "     30       \u001b[36m28.5175\u001b[0m       27.2508  0.0106\n",
      "     31       \u001b[36m28.5124\u001b[0m       27.2532  0.0106\n",
      "     32       \u001b[36m28.5076\u001b[0m       27.2556  0.0110\n",
      "     33       \u001b[36m28.5033\u001b[0m       27.2575  0.0110\n",
      "     34       \u001b[36m28.4991\u001b[0m       27.2591  0.0107\n",
      "     35       \u001b[36m28.4951\u001b[0m       27.2608  0.0109\n",
      "     36       \u001b[36m28.4914\u001b[0m       27.2622  0.0112\n",
      "     37       \u001b[36m28.4879\u001b[0m       27.2634  0.0111\n",
      "     38       \u001b[36m28.4847\u001b[0m       27.2649  0.0113\n",
      "     39       \u001b[36m28.4816\u001b[0m       27.2661  0.0116\n",
      "     40       \u001b[36m28.4786\u001b[0m       27.2670  0.0113\n",
      "     41       \u001b[36m28.4758\u001b[0m       27.2680  0.0114\n",
      "     42       \u001b[36m28.4732\u001b[0m       27.2690  0.0120\n",
      "     43       \u001b[36m28.4707\u001b[0m       27.2698  0.0113\n",
      "     44       \u001b[36m28.4683\u001b[0m       27.2704  0.0112\n",
      "     45       \u001b[36m28.4662\u001b[0m       27.2710  0.0111\n",
      "     46       \u001b[36m28.4641\u001b[0m       27.2714  0.0110\n",
      "     47       \u001b[36m28.4621\u001b[0m       27.2718  0.0112\n",
      "     48       \u001b[36m28.4603\u001b[0m       27.2719  0.0111\n",
      "     49       \u001b[36m28.4585\u001b[0m       27.2719  0.0121\n",
      "     50       \u001b[36m28.4568\u001b[0m       27.2719  0.0144\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m40.2575\u001b[0m       \u001b[32m37.5505\u001b[0m  0.0159\n",
      "      2       \u001b[36m35.7277\u001b[0m       \u001b[32m31.8743\u001b[0m  0.0139\n",
      "      3       \u001b[36m34.4809\u001b[0m       \u001b[32m31.2154\u001b[0m  0.0153\n",
      "      4       \u001b[36m33.7493\u001b[0m       31.6538  0.0125\n",
      "      5       \u001b[36m33.3690\u001b[0m       31.7428  0.0153\n",
      "      6       \u001b[36m33.0717\u001b[0m       \u001b[32m30.7385\u001b[0m  0.0126\n",
      "      7       \u001b[36m32.8044\u001b[0m       \u001b[32m30.3066\u001b[0m  0.0122\n",
      "      8       \u001b[36m32.7268\u001b[0m       30.3941  0.0122\n",
      "      9       \u001b[36m32.6095\u001b[0m       30.5271  0.0118\n",
      "     10       \u001b[36m32.5348\u001b[0m       30.3757  0.0121\n",
      "     11       \u001b[36m32.4444\u001b[0m       \u001b[32m30.1683\u001b[0m  0.0121\n",
      "     12       \u001b[36m32.3951\u001b[0m       30.2147  0.0119\n",
      "     13       \u001b[36m32.3603\u001b[0m       30.2620  0.0120\n",
      "     14       \u001b[36m32.3298\u001b[0m       30.2334  0.0120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     15       \u001b[36m32.3031\u001b[0m       \u001b[32m30.1253\u001b[0m  0.0120\n",
      "     16       \u001b[36m32.2860\u001b[0m       30.2114  0.0120\n",
      "     17       \u001b[36m32.2741\u001b[0m       30.1278  0.0120\n",
      "     18       \u001b[36m32.2698\u001b[0m       30.2874  0.0117\n",
      "     19       32.2707       \u001b[32m30.0995\u001b[0m  0.0121\n",
      "     20       32.2986       30.3914  0.0116\n",
      "     21       32.2995       \u001b[32m30.0399\u001b[0m  0.0118\n",
      "     22       32.3079       30.0673  0.0128\n",
      "     23       \u001b[36m32.2503\u001b[0m       30.1180  0.0124\n",
      "     24       \u001b[36m32.2204\u001b[0m       \u001b[32m29.9320\u001b[0m  0.0121\n",
      "     25       \u001b[36m32.2179\u001b[0m       30.0645  0.0120\n",
      "     26       \u001b[36m32.2129\u001b[0m       30.0378  0.0118\n",
      "     27       \u001b[36m32.1910\u001b[0m       29.9633  0.0119\n",
      "     28       \u001b[36m32.1892\u001b[0m       30.0317  0.0116\n",
      "     29       \u001b[36m32.1862\u001b[0m       29.9984  0.0118\n",
      "     30       \u001b[36m32.1775\u001b[0m       30.0033  0.0121\n",
      "     31       \u001b[36m32.1746\u001b[0m       29.9989  0.0119\n",
      "     32       \u001b[36m32.1681\u001b[0m       29.9912  0.0117\n",
      "     33       \u001b[36m32.1652\u001b[0m       30.0084  0.0115\n",
      "     34       \u001b[36m32.1609\u001b[0m       29.9895  0.0114\n",
      "     35       \u001b[36m32.1565\u001b[0m       29.9957  0.0124\n",
      "     36       \u001b[36m32.1536\u001b[0m       29.9899  0.0118\n",
      "     37       \u001b[36m32.1494\u001b[0m       29.9913  0.0126\n",
      "     38       \u001b[36m32.1464\u001b[0m       29.9858  0.0119\n",
      "     39       \u001b[36m32.1427\u001b[0m       29.9868  0.0124\n",
      "     40       \u001b[36m32.1400\u001b[0m       29.9850  0.0135\n",
      "     41       \u001b[36m32.1368\u001b[0m       29.9850  0.0117\n",
      "     42       \u001b[36m32.1341\u001b[0m       29.9823  0.0116\n",
      "     43       \u001b[36m32.1313\u001b[0m       29.9824  0.0114\n",
      "     44       \u001b[36m32.1288\u001b[0m       29.9795  0.0119\n",
      "     45       \u001b[36m32.1261\u001b[0m       29.9807  0.0119\n",
      "     46       \u001b[36m32.1236\u001b[0m       29.9788  0.0116\n",
      "     47       \u001b[36m32.1212\u001b[0m       29.9813  0.0117\n",
      "     48       \u001b[36m32.1188\u001b[0m       29.9779  0.0117\n",
      "     49       \u001b[36m32.1165\u001b[0m       29.9803  0.0116\n",
      "     50       \u001b[36m32.1141\u001b[0m       29.9779  0.0126\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m32.7630\u001b[0m       \u001b[32m28.8677\u001b[0m  0.0118\n",
      "      2       \u001b[36m27.3438\u001b[0m       \u001b[32m28.5467\u001b[0m  0.0116\n",
      "      3       \u001b[36m25.4130\u001b[0m       \u001b[32m27.9893\u001b[0m  0.0118\n",
      "      4       \u001b[36m24.3668\u001b[0m       \u001b[32m26.4489\u001b[0m  0.0118\n",
      "      5       \u001b[36m24.2893\u001b[0m       26.4807  0.0122\n",
      "      6       \u001b[36m23.7875\u001b[0m       27.3872  0.0117\n",
      "      7       \u001b[36m23.6385\u001b[0m       27.7184  0.0123\n",
      "      8       \u001b[36m23.4592\u001b[0m       27.1429  0.0117\n",
      "      9       \u001b[36m23.4333\u001b[0m       26.9956  0.0115\n",
      "     10       \u001b[36m23.3389\u001b[0m       27.2049  0.0115\n",
      "     11       \u001b[36m23.2789\u001b[0m       27.1888  0.0115\n",
      "     12       \u001b[36m23.2430\u001b[0m       27.0854  0.0125\n",
      "     13       \u001b[36m23.2112\u001b[0m       27.0641  0.0117\n",
      "     14       \u001b[36m23.1871\u001b[0m       27.0521  0.0114\n",
      "     15       \u001b[36m23.1667\u001b[0m       27.0349  0.0115\n",
      "     16       \u001b[36m23.1451\u001b[0m       26.9640  0.0114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m23.1357\u001b[0m       26.9614  0.0119\n",
      "     18       \u001b[36m23.1211\u001b[0m       26.9656  0.0115\n",
      "     19       \u001b[36m23.1094\u001b[0m       26.9306  0.0114\n",
      "     20       \u001b[36m23.1000\u001b[0m       26.9106  0.0115\n",
      "     21       \u001b[36m23.0933\u001b[0m       26.9217  0.0115\n",
      "     22       \u001b[36m23.0838\u001b[0m       26.8990  0.0118\n",
      "     23       \u001b[36m23.0794\u001b[0m       26.8862  0.0118\n",
      "     24       \u001b[36m23.0730\u001b[0m       26.8669  0.0120\n",
      "     25       23.0740       26.8859  0.0120\n",
      "     26       \u001b[36m23.0708\u001b[0m       26.8417  0.0119\n",
      "     27       23.0769       26.8833  0.0156\n",
      "     28       23.0742       26.8214  0.0155\n",
      "     29       23.0869       26.9046  0.0124\n",
      "     30       23.0980       26.7940  0.0127\n",
      "     31       23.1165       26.9181  0.0124\n",
      "     32       23.1127       26.7159  0.0141\n",
      "     33       23.0930       26.8410  0.0125\n",
      "     34       \u001b[36m23.0486\u001b[0m       26.8135  0.0126\n",
      "     35       \u001b[36m23.0383\u001b[0m       26.7542  0.0121\n",
      "     36       \u001b[36m23.0346\u001b[0m       26.8519  0.0124\n",
      "     37       \u001b[36m23.0240\u001b[0m       26.7627  0.0122\n",
      "     38       23.0292       26.7896  0.0119\n",
      "     39       \u001b[36m23.0197\u001b[0m       26.7783  0.0121\n",
      "     40       \u001b[36m23.0186\u001b[0m       26.7633  0.0121\n",
      "     41       \u001b[36m23.0143\u001b[0m       26.7691  0.0118\n",
      "     42       \u001b[36m23.0119\u001b[0m       26.7560  0.0119\n",
      "     43       \u001b[36m23.0099\u001b[0m       26.7606  0.0115\n",
      "     44       \u001b[36m23.0070\u001b[0m       26.7487  0.0119\n",
      "     45       \u001b[36m23.0062\u001b[0m       26.7559  0.0116\n",
      "     46       \u001b[36m23.0031\u001b[0m       26.7454  0.0123\n",
      "     47       \u001b[36m23.0021\u001b[0m       26.7488  0.0114\n",
      "     48       \u001b[36m22.9990\u001b[0m       26.7422  0.0135\n",
      "     49       \u001b[36m22.9981\u001b[0m       26.7424  0.0121\n",
      "     50       \u001b[36m22.9956\u001b[0m       26.7400  0.0117\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m41.8997\u001b[0m       \u001b[32m32.4889\u001b[0m  0.0115\n",
      "      2       \u001b[36m39.5673\u001b[0m       \u001b[32m29.8992\u001b[0m  0.0115\n",
      "      3       \u001b[36m34.9533\u001b[0m       \u001b[32m27.0395\u001b[0m  0.0120\n",
      "      4       \u001b[36m31.1691\u001b[0m       31.2841  0.0119\n",
      "      5       31.8515       28.5110  0.0113\n",
      "      6       \u001b[36m30.1661\u001b[0m       \u001b[32m26.7195\u001b[0m  0.0124\n",
      "      7       \u001b[36m29.8733\u001b[0m       \u001b[32m26.7062\u001b[0m  0.0120\n",
      "      8       \u001b[36m29.4119\u001b[0m       27.7933  0.0118\n",
      "      9       \u001b[36m29.3546\u001b[0m       28.4470  0.0119\n",
      "     10       \u001b[36m29.1890\u001b[0m       27.6423  0.0116\n",
      "     11       \u001b[36m28.9350\u001b[0m       27.2752  0.0119\n",
      "     12       \u001b[36m28.8239\u001b[0m       27.4611  0.0120\n",
      "     13       \u001b[36m28.7750\u001b[0m       27.6579  0.0115\n",
      "     14       \u001b[36m28.7299\u001b[0m       27.4553  0.0117\n",
      "     15       \u001b[36m28.6501\u001b[0m       27.3210  0.0118\n",
      "     16       \u001b[36m28.6038\u001b[0m       27.4186  0.0119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m28.5849\u001b[0m       27.4373  0.0129\n",
      "     18       \u001b[36m28.5613\u001b[0m       27.3653  0.0117\n",
      "     19       \u001b[36m28.5374\u001b[0m       27.3179  0.0120\n",
      "     20       \u001b[36m28.5204\u001b[0m       27.3238  0.0117\n",
      "     21       \u001b[36m28.5088\u001b[0m       27.3303  0.0118\n",
      "     22       \u001b[36m28.4979\u001b[0m       27.3337  0.0119\n",
      "     23       \u001b[36m28.4892\u001b[0m       27.2842  0.0117\n",
      "     24       \u001b[36m28.4768\u001b[0m       27.3049  0.0115\n",
      "     25       \u001b[36m28.4721\u001b[0m       27.2959  0.0113\n",
      "     26       \u001b[36m28.4642\u001b[0m       27.2865  0.0114\n",
      "     27       \u001b[36m28.4570\u001b[0m       27.3001  0.0117\n",
      "     28       \u001b[36m28.4544\u001b[0m       27.2678  0.0116\n",
      "     29       \u001b[36m28.4487\u001b[0m       27.3060  0.0125\n",
      "     30       \u001b[36m28.4452\u001b[0m       27.2827  0.0112\n",
      "     31       28.4471       27.2448  0.0113\n",
      "     32       28.4533       27.3953  0.0117\n",
      "     33       28.4791       27.1916  0.0117\n",
      "     34       28.4569       27.3525  0.0116\n",
      "     35       28.4532       27.2992  0.0114\n",
      "     36       28.4568       27.1881  0.0115\n",
      "     37       28.4462       27.4141  0.0120\n",
      "     38       28.4710       27.2164  0.0114\n",
      "     39       \u001b[36m28.4166\u001b[0m       27.2864  0.0114\n",
      "     40       28.4314       27.3147  0.0114\n",
      "     41       28.4245       27.3162  0.0114\n",
      "     42       28.4327       27.2229  0.0117\n",
      "     43       \u001b[36m28.4147\u001b[0m       27.3498  0.0140\n",
      "     44       28.4339       27.2678  0.0143\n",
      "     45       \u001b[36m28.4108\u001b[0m       27.3005  0.0219\n",
      "     46       28.4253       27.2683  0.0163\n",
      "     47       \u001b[36m28.4108\u001b[0m       27.3290  0.0127\n",
      "     48       28.4248       27.2476  0.0122\n",
      "     49       \u001b[36m28.4064\u001b[0m       27.3298  0.0127\n",
      "     50       28.4235       27.2635  0.0126\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m43.1363\u001b[0m       \u001b[32m44.0119\u001b[0m  0.0115\n",
      "      2       \u001b[36m41.4613\u001b[0m       \u001b[32m41.8034\u001b[0m  0.0116\n",
      "      3       \u001b[36m39.7209\u001b[0m       \u001b[32m39.4740\u001b[0m  0.0146\n",
      "      4       \u001b[36m37.9411\u001b[0m       \u001b[32m37.1382\u001b[0m  0.0136\n",
      "      5       \u001b[36m36.2906\u001b[0m       \u001b[32m35.0607\u001b[0m  0.0119\n",
      "      6       \u001b[36m34.9653\u001b[0m       \u001b[32m33.3851\u001b[0m  0.0116\n",
      "      7       \u001b[36m34.0181\u001b[0m       \u001b[32m32.1482\u001b[0m  0.0126\n",
      "      8       \u001b[36m33.4197\u001b[0m       \u001b[32m31.3396\u001b[0m  0.0120\n",
      "      9       \u001b[36m33.0834\u001b[0m       \u001b[32m30.8574\u001b[0m  0.0123\n",
      "     10       \u001b[36m32.9023\u001b[0m       \u001b[32m30.5825\u001b[0m  0.0118\n",
      "     11       \u001b[36m32.7975\u001b[0m       \u001b[32m30.4229\u001b[0m  0.0124\n",
      "     12       \u001b[36m32.7276\u001b[0m       \u001b[32m30.3251\u001b[0m  0.0115\n",
      "     13       \u001b[36m32.6758\u001b[0m       \u001b[32m30.2609\u001b[0m  0.0112\n",
      "     14       \u001b[36m32.6348\u001b[0m       \u001b[32m30.2154\u001b[0m  0.0112\n",
      "     15       \u001b[36m32.6010\u001b[0m       \u001b[32m30.1815\u001b[0m  0.0116\n",
      "     16       \u001b[36m32.5725\u001b[0m       \u001b[32m30.1549\u001b[0m  0.0107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m32.5483\u001b[0m       \u001b[32m30.1332\u001b[0m  0.0118\n",
      "     18       \u001b[36m32.5278\u001b[0m       \u001b[32m30.1149\u001b[0m  0.0110\n",
      "     19       \u001b[36m32.5097\u001b[0m       \u001b[32m30.0990\u001b[0m  0.0113\n",
      "     20       \u001b[36m32.4939\u001b[0m       \u001b[32m30.0856\u001b[0m  0.0114\n",
      "     21       \u001b[36m32.4798\u001b[0m       \u001b[32m30.0730\u001b[0m  0.0116\n",
      "     22       \u001b[36m32.4671\u001b[0m       \u001b[32m30.0619\u001b[0m  0.0112\n",
      "     23       \u001b[36m32.4554\u001b[0m       \u001b[32m30.0518\u001b[0m  0.0110\n",
      "     24       \u001b[36m32.4450\u001b[0m       \u001b[32m30.0425\u001b[0m  0.0108\n",
      "     25       \u001b[36m32.4354\u001b[0m       \u001b[32m30.0339\u001b[0m  0.0105\n",
      "     26       \u001b[36m32.4266\u001b[0m       \u001b[32m30.0263\u001b[0m  0.0110\n",
      "     27       \u001b[36m32.4184\u001b[0m       \u001b[32m30.0188\u001b[0m  0.0117\n",
      "     28       \u001b[36m32.4107\u001b[0m       \u001b[32m30.0117\u001b[0m  0.0108\n",
      "     29       \u001b[36m32.4035\u001b[0m       \u001b[32m30.0050\u001b[0m  0.0112\n",
      "     30       \u001b[36m32.3967\u001b[0m       \u001b[32m29.9987\u001b[0m  0.0116\n",
      "     31       \u001b[36m32.3903\u001b[0m       \u001b[32m29.9930\u001b[0m  0.0113\n",
      "     32       \u001b[36m32.3843\u001b[0m       \u001b[32m29.9872\u001b[0m  0.0118\n",
      "     33       \u001b[36m32.3786\u001b[0m       \u001b[32m29.9818\u001b[0m  0.0122\n",
      "     34       \u001b[36m32.3732\u001b[0m       \u001b[32m29.9766\u001b[0m  0.0129\n",
      "     35       \u001b[36m32.3681\u001b[0m       \u001b[32m29.9717\u001b[0m  0.0133\n",
      "     36       \u001b[36m32.3630\u001b[0m       \u001b[32m29.9676\u001b[0m  0.0145\n",
      "     37       \u001b[36m32.3585\u001b[0m       \u001b[32m29.9631\u001b[0m  0.0121\n",
      "     38       \u001b[36m32.3541\u001b[0m       \u001b[32m29.9589\u001b[0m  0.0116\n",
      "     39       \u001b[36m32.3494\u001b[0m       \u001b[32m29.9551\u001b[0m  0.0117\n",
      "     40       \u001b[36m32.3459\u001b[0m       \u001b[32m29.9513\u001b[0m  0.0114\n",
      "     41       \u001b[36m32.3420\u001b[0m       \u001b[32m29.9473\u001b[0m  0.0116\n",
      "     42       \u001b[36m32.3378\u001b[0m       \u001b[32m29.9443\u001b[0m  0.0115\n",
      "     43       \u001b[36m32.3348\u001b[0m       \u001b[32m29.9407\u001b[0m  0.0114\n",
      "     44       \u001b[36m32.3314\u001b[0m       \u001b[32m29.9373\u001b[0m  0.0113\n",
      "     45       \u001b[36m32.3276\u001b[0m       \u001b[32m29.9345\u001b[0m  0.0112\n",
      "     46       \u001b[36m32.3251\u001b[0m       \u001b[32m29.9314\u001b[0m  0.0117\n",
      "     47       \u001b[36m32.3220\u001b[0m       \u001b[32m29.9288\u001b[0m  0.0114\n",
      "     48       \u001b[36m32.3189\u001b[0m       \u001b[32m29.9260\u001b[0m  0.0111\n",
      "     49       \u001b[36m32.3161\u001b[0m       \u001b[32m29.9234\u001b[0m  0.0112\n",
      "     50       \u001b[36m32.3133\u001b[0m       \u001b[32m29.9209\u001b[0m  0.0110\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m32.5475\u001b[0m       \u001b[32m31.0329\u001b[0m  0.0112\n",
      "      2       \u001b[36m31.4565\u001b[0m       \u001b[32m30.3009\u001b[0m  0.0106\n",
      "      3       \u001b[36m30.5000\u001b[0m       \u001b[32m29.6437\u001b[0m  0.0112\n",
      "      4       \u001b[36m29.6142\u001b[0m       \u001b[32m29.0397\u001b[0m  0.0110\n",
      "      5       \u001b[36m28.7756\u001b[0m       \u001b[32m28.4777\u001b[0m  0.0110\n",
      "      6       \u001b[36m27.9694\u001b[0m       \u001b[32m27.9535\u001b[0m  0.0107\n",
      "      7       \u001b[36m27.1965\u001b[0m       \u001b[32m27.4699\u001b[0m  0.0116\n",
      "      8       \u001b[36m26.4645\u001b[0m       \u001b[32m27.0336\u001b[0m  0.0114\n",
      "      9       \u001b[36m25.7792\u001b[0m       \u001b[32m26.6690\u001b[0m  0.0111\n",
      "     10       \u001b[36m25.1805\u001b[0m       \u001b[32m26.4023\u001b[0m  0.0110\n",
      "     11       \u001b[36m24.6896\u001b[0m       \u001b[32m26.2301\u001b[0m  0.0109\n",
      "     12       \u001b[36m24.3001\u001b[0m       \u001b[32m26.1394\u001b[0m  0.0115\n",
      "     13       \u001b[36m24.0000\u001b[0m       \u001b[32m26.1120\u001b[0m  0.0116\n",
      "     14       \u001b[36m23.7765\u001b[0m       26.1274  0.0110\n",
      "     15       \u001b[36m23.6154\u001b[0m       26.1665  0.0108\n",
      "     16       \u001b[36m23.5029\u001b[0m       26.2138  0.0109\n",
      "     17       \u001b[36m23.4255\u001b[0m       26.2595  0.0108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     18       \u001b[36m23.3723\u001b[0m       26.2986  0.0114\n",
      "     19       \u001b[36m23.3352\u001b[0m       26.3295  0.0111\n",
      "     20       \u001b[36m23.3084\u001b[0m       26.3527  0.0112\n",
      "     21       \u001b[36m23.2882\u001b[0m       26.3693  0.0107\n",
      "     22       \u001b[36m23.2723\u001b[0m       26.3810  0.0106\n",
      "     23       \u001b[36m23.2593\u001b[0m       26.3887  0.0116\n",
      "     24       \u001b[36m23.2482\u001b[0m       26.3936  0.0119\n",
      "     25       \u001b[36m23.2384\u001b[0m       26.3965  0.0110\n",
      "     26       \u001b[36m23.2297\u001b[0m       26.3981  0.0107\n",
      "     27       \u001b[36m23.2218\u001b[0m       26.3988  0.0109\n",
      "     28       \u001b[36m23.2145\u001b[0m       26.3988  0.0112\n",
      "     29       \u001b[36m23.2079\u001b[0m       26.3984  0.0112\n",
      "     30       \u001b[36m23.2017\u001b[0m       26.3977  0.0111\n",
      "     31       \u001b[36m23.1959\u001b[0m       26.3969  0.0109\n",
      "     32       \u001b[36m23.1904\u001b[0m       26.3959  0.0115\n",
      "     33       \u001b[36m23.1853\u001b[0m       26.3949  0.0111\n",
      "     34       \u001b[36m23.1804\u001b[0m       26.3938  0.0118\n",
      "     35       \u001b[36m23.1758\u001b[0m       26.3928  0.0137\n",
      "     36       \u001b[36m23.1715\u001b[0m       26.3919  0.0142\n",
      "     37       \u001b[36m23.1674\u001b[0m       26.3910  0.0118\n",
      "     38       \u001b[36m23.1635\u001b[0m       26.3902  0.0124\n",
      "     39       \u001b[36m23.1597\u001b[0m       26.3894  0.0117\n",
      "     40       \u001b[36m23.1562\u001b[0m       26.3887  0.0117\n",
      "     41       \u001b[36m23.1528\u001b[0m       26.3880  0.0133\n",
      "     42       \u001b[36m23.1496\u001b[0m       26.3875  0.0134\n",
      "     43       \u001b[36m23.1465\u001b[0m       26.3870  0.0116\n",
      "     44       \u001b[36m23.1435\u001b[0m       26.3865  0.0113\n",
      "     45       \u001b[36m23.1407\u001b[0m       26.3861  0.0112\n",
      "     46       \u001b[36m23.1380\u001b[0m       26.3857  0.0111\n",
      "     47       \u001b[36m23.1355\u001b[0m       26.3853  0.0115\n",
      "     48       \u001b[36m23.1330\u001b[0m       26.3850  0.0112\n",
      "     49       \u001b[36m23.1306\u001b[0m       26.3847  0.0117\n",
      "     50       \u001b[36m23.1284\u001b[0m       26.3845  0.0110\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m42.2725\u001b[0m       \u001b[32m32.6389\u001b[0m  0.0108\n",
      "      2       \u001b[36m39.9875\u001b[0m       \u001b[32m30.9842\u001b[0m  0.0117\n",
      "      3       \u001b[36m37.5256\u001b[0m       \u001b[32m29.2484\u001b[0m  0.0113\n",
      "      4       \u001b[36m34.9626\u001b[0m       \u001b[32m27.7400\u001b[0m  0.0112\n",
      "      5       \u001b[36m32.6816\u001b[0m       \u001b[32m26.7503\u001b[0m  0.0111\n",
      "      6       \u001b[36m30.9359\u001b[0m       \u001b[32m26.4102\u001b[0m  0.0112\n",
      "      7       \u001b[36m29.8547\u001b[0m       26.5692  0.0145\n",
      "      8       \u001b[36m29.3506\u001b[0m       26.8656  0.0114\n",
      "      9       \u001b[36m29.1527\u001b[0m       27.0826  0.0111\n",
      "     10       \u001b[36m29.0602\u001b[0m       27.2013  0.0116\n",
      "     11       \u001b[36m28.9979\u001b[0m       27.2583  0.0112\n",
      "     12       \u001b[36m28.9468\u001b[0m       27.2836  0.0115\n",
      "     13       \u001b[36m28.9024\u001b[0m       27.2930  0.0113\n",
      "     14       \u001b[36m28.8628\u001b[0m       27.2962  0.0112\n",
      "     15       \u001b[36m28.8279\u001b[0m       27.2965  0.0111\n",
      "     16       \u001b[36m28.7967\u001b[0m       27.2950  0.0116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m28.7689\u001b[0m       27.2945  0.0118\n",
      "     18       \u001b[36m28.7438\u001b[0m       27.2931  0.0115\n",
      "     19       \u001b[36m28.7211\u001b[0m       27.2923  0.0115\n",
      "     20       \u001b[36m28.7006\u001b[0m       27.2922  0.0109\n",
      "     21       \u001b[36m28.6823\u001b[0m       27.2921  0.0110\n",
      "     22       \u001b[36m28.6657\u001b[0m       27.2920  0.0117\n",
      "     23       \u001b[36m28.6505\u001b[0m       27.2927  0.0120\n",
      "     24       \u001b[36m28.6365\u001b[0m       27.2939  0.0115\n",
      "     25       \u001b[36m28.6241\u001b[0m       27.2944  0.0114\n",
      "     26       \u001b[36m28.6125\u001b[0m       27.2943  0.0115\n",
      "     27       \u001b[36m28.6021\u001b[0m       27.2941  0.0118\n",
      "     28       \u001b[36m28.5925\u001b[0m       27.2945  0.0119\n",
      "     29       \u001b[36m28.5837\u001b[0m       27.2936  0.0115\n",
      "     30       \u001b[36m28.5755\u001b[0m       27.2928  0.0114\n",
      "     31       \u001b[36m28.5680\u001b[0m       27.2918  0.0115\n",
      "     32       \u001b[36m28.5608\u001b[0m       27.2916  0.0117\n",
      "     33       \u001b[36m28.5543\u001b[0m       27.2911  0.0114\n",
      "     34       \u001b[36m28.5481\u001b[0m       27.2909  0.0114\n",
      "     35       \u001b[36m28.5425\u001b[0m       27.2901  0.0114\n",
      "     36       \u001b[36m28.5372\u001b[0m       27.2889  0.0113\n",
      "     37       \u001b[36m28.5321\u001b[0m       27.2884  0.0114\n",
      "     38       \u001b[36m28.5274\u001b[0m       27.2878  0.0115\n",
      "     39       \u001b[36m28.5230\u001b[0m       27.2874  0.0113\n",
      "     40       \u001b[36m28.5189\u001b[0m       27.2867  0.0115\n",
      "     41       \u001b[36m28.5149\u001b[0m       27.2860  0.0114\n",
      "     42       \u001b[36m28.5112\u001b[0m       27.2856  0.0119\n",
      "     43       \u001b[36m28.5077\u001b[0m       27.2853  0.0118\n",
      "     44       \u001b[36m28.5044\u001b[0m       27.2850  0.0116\n",
      "     45       \u001b[36m28.5012\u001b[0m       27.2845  0.0119\n",
      "     46       \u001b[36m28.4982\u001b[0m       27.2838  0.0111\n",
      "     47       \u001b[36m28.4953\u001b[0m       27.2832  0.0114\n",
      "     48       \u001b[36m28.4925\u001b[0m       27.2826  0.0114\n",
      "     49       \u001b[36m28.4899\u001b[0m       27.2819  0.0111\n",
      "     50       \u001b[36m28.4874\u001b[0m       27.2815  0.0112\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m42.0342\u001b[0m       \u001b[32m39.4670\u001b[0m  0.0118\n",
      "      2       \u001b[36m36.4374\u001b[0m       \u001b[32m31.4043\u001b[0m  0.0129\n",
      "      3       \u001b[36m34.6176\u001b[0m       \u001b[32m31.1048\u001b[0m  0.0122\n",
      "      4       \u001b[36m33.5245\u001b[0m       32.3978  0.0122\n",
      "      5       33.5573       31.4300  0.0121\n",
      "      6       \u001b[36m32.9858\u001b[0m       \u001b[32m30.4875\u001b[0m  0.0124\n",
      "      7       \u001b[36m32.8860\u001b[0m       \u001b[32m30.4130\u001b[0m  0.0136\n",
      "      8       \u001b[36m32.7218\u001b[0m       30.7603  0.0183\n",
      "      9       \u001b[36m32.6513\u001b[0m       30.4265  0.0219\n",
      "     10       \u001b[36m32.5267\u001b[0m       \u001b[32m30.2212\u001b[0m  0.0134\n",
      "     11       \u001b[36m32.4671\u001b[0m       30.3985  0.0152\n",
      "     12       \u001b[36m32.4253\u001b[0m       30.3466  0.0176\n",
      "     13       \u001b[36m32.3802\u001b[0m       30.2994  0.0151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     14       \u001b[36m32.3458\u001b[0m       30.2913  0.0143\n",
      "     15       \u001b[36m32.3221\u001b[0m       30.2561  0.0134\n",
      "     16       \u001b[36m32.3062\u001b[0m       30.3473  0.0129\n",
      "     17       \u001b[36m32.2897\u001b[0m       30.2378  0.0129\n",
      "     18       \u001b[36m32.2735\u001b[0m       30.3225  0.0137\n",
      "     19       \u001b[36m32.2673\u001b[0m       \u001b[32m30.2166\u001b[0m  0.0123\n",
      "     20       \u001b[36m32.2595\u001b[0m       30.3852  0.0144\n",
      "     21       \u001b[36m32.2591\u001b[0m       \u001b[32m30.1739\u001b[0m  0.0130\n",
      "     22       32.2643       30.4778  0.0128\n",
      "     23       32.2721       \u001b[32m30.1431\u001b[0m  0.0122\n",
      "     24       32.2872       30.4162  0.0119\n",
      "     25       32.2706       \u001b[32m30.1032\u001b[0m  0.0118\n",
      "     26       \u001b[36m32.2347\u001b[0m       30.1947  0.0118\n",
      "     27       \u001b[36m32.2241\u001b[0m       30.2252  0.0116\n",
      "     28       \u001b[36m32.1978\u001b[0m       30.1034  0.0117\n",
      "     29       32.1983       30.2319  0.0117\n",
      "     30       \u001b[36m32.1955\u001b[0m       30.1267  0.0120\n",
      "     31       \u001b[36m32.1836\u001b[0m       30.1696  0.0117\n",
      "     32       32.1843       30.1527  0.0122\n",
      "     33       \u001b[36m32.1742\u001b[0m       30.1516  0.0119\n",
      "     34       \u001b[36m32.1724\u001b[0m       30.1466  0.0119\n",
      "     35       \u001b[36m32.1664\u001b[0m       30.1406  0.0129\n",
      "     36       \u001b[36m32.1638\u001b[0m       30.1435  0.0127\n",
      "     37       \u001b[36m32.1591\u001b[0m       30.1369  0.0126\n",
      "     38       \u001b[36m32.1560\u001b[0m       30.1400  0.0123\n",
      "     39       \u001b[36m32.1524\u001b[0m       30.1315  0.0122\n",
      "     40       \u001b[36m32.1489\u001b[0m       30.1363  0.0123\n",
      "     41       \u001b[36m32.1458\u001b[0m       30.1288  0.0119\n",
      "     42       \u001b[36m32.1423\u001b[0m       30.1342  0.0118\n",
      "     43       \u001b[36m32.1393\u001b[0m       30.1243  0.0118\n",
      "     44       \u001b[36m32.1363\u001b[0m       30.1339  0.0117\n",
      "     45       \u001b[36m32.1330\u001b[0m       30.1255  0.0121\n",
      "     46       \u001b[36m32.1305\u001b[0m       30.1360  0.0118\n",
      "     47       \u001b[36m32.1274\u001b[0m       30.1261  0.0123\n",
      "     48       \u001b[36m32.1252\u001b[0m       30.1334  0.0121\n",
      "     49       \u001b[36m32.1218\u001b[0m       30.1321  0.0115\n",
      "     50       \u001b[36m32.1200\u001b[0m       30.1310  0.0127\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m32.5954\u001b[0m       \u001b[32m29.4598\u001b[0m  0.0122\n",
      "      2       \u001b[36m27.8451\u001b[0m       \u001b[32m27.4412\u001b[0m  0.0123\n",
      "      3       \u001b[36m25.1763\u001b[0m       28.4004  0.0125\n",
      "      4       \u001b[36m24.3352\u001b[0m       \u001b[32m26.3826\u001b[0m  0.0123\n",
      "      5       \u001b[36m24.2055\u001b[0m       26.4512  0.0119\n",
      "      6       \u001b[36m23.8372\u001b[0m       27.1682  0.0119\n",
      "      7       \u001b[36m23.6236\u001b[0m       27.5616  0.0118\n",
      "      8       \u001b[36m23.4868\u001b[0m       26.9401  0.0117\n",
      "      9       \u001b[36m23.4425\u001b[0m       26.8479  0.0116\n",
      "     10       \u001b[36m23.3481\u001b[0m       27.0823  0.0122\n",
      "     11       \u001b[36m23.2900\u001b[0m       27.0771  0.0119\n",
      "     12       \u001b[36m23.2385\u001b[0m       26.8712  0.0120\n",
      "     13       \u001b[36m23.2163\u001b[0m       26.8652  0.0122\n",
      "     14       \u001b[36m23.1859\u001b[0m       26.9616  0.0121\n",
      "     15       \u001b[36m23.1587\u001b[0m       26.8860  0.0125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     16       \u001b[36m23.1383\u001b[0m       26.7941  0.0131\n",
      "     17       \u001b[36m23.1274\u001b[0m       26.8135  0.0122\n",
      "     18       \u001b[36m23.1119\u001b[0m       26.8181  0.0124\n",
      "     19       \u001b[36m23.0986\u001b[0m       26.7678  0.0130\n",
      "     20       \u001b[36m23.0909\u001b[0m       26.7455  0.0123\n",
      "     21       \u001b[36m23.0820\u001b[0m       26.7669  0.0123\n",
      "     22       \u001b[36m23.0738\u001b[0m       26.7394  0.0122\n",
      "     23       \u001b[36m23.0671\u001b[0m       26.7444  0.0121\n",
      "     24       \u001b[36m23.0618\u001b[0m       26.7350  0.0121\n",
      "     25       \u001b[36m23.0558\u001b[0m       26.7445  0.0120\n",
      "     26       \u001b[36m23.0499\u001b[0m       26.7450  0.0123\n",
      "     27       \u001b[36m23.0481\u001b[0m       26.7199  0.0120\n",
      "     28       23.0481       26.7929  0.0117\n",
      "     29       23.0545       26.7193  0.0120\n",
      "     30       23.0660       26.7074  0.0123\n",
      "     31       23.0690       26.8710  0.0119\n",
      "     32       23.0947       26.7404  0.0123\n",
      "     33       23.0953       26.7885  0.0124\n",
      "     34       23.0882       26.7916  0.0126\n",
      "     35       23.1354       26.8045  0.0125\n",
      "     36       23.1485       26.7555  0.0129\n",
      "     37       23.1042       26.6889  0.0121\n",
      "     38       23.0689       26.7520  0.0119\n",
      "     39       \u001b[36m23.0268\u001b[0m       26.7445  0.0138\n",
      "     40       \u001b[36m23.0218\u001b[0m       26.6906  0.0180\n",
      "     41       23.0277       26.7651  0.0196\n",
      "     42       \u001b[36m23.0118\u001b[0m       26.7107  0.0207\n",
      "     43       \u001b[36m23.0106\u001b[0m       26.6921  0.0161\n",
      "     44       \u001b[36m23.0066\u001b[0m       26.7351  0.0131\n",
      "     45       \u001b[36m23.0010\u001b[0m       26.7037  0.0135\n",
      "     46       23.0011       26.7060  0.0119\n",
      "     47       \u001b[36m22.9973\u001b[0m       26.7122  0.0140\n",
      "     48       \u001b[36m22.9959\u001b[0m       26.6952  0.0120\n",
      "     49       \u001b[36m22.9930\u001b[0m       26.7027  0.0126\n",
      "     50       \u001b[36m22.9914\u001b[0m       26.7004  0.0120\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m38.8817\u001b[0m       \u001b[32m28.7879\u001b[0m  0.0123\n",
      "      2       \u001b[36m32.5953\u001b[0m       29.2467  0.0124\n",
      "      3       \u001b[36m31.2846\u001b[0m       \u001b[32m28.7520\u001b[0m  0.0119\n",
      "      4       \u001b[36m29.6970\u001b[0m       \u001b[32m26.5322\u001b[0m  0.0125\n",
      "      5       \u001b[36m29.4330\u001b[0m       26.8023  0.0124\n",
      "      6       \u001b[36m29.0513\u001b[0m       28.5428  0.0114\n",
      "      7       29.1683       28.1194  0.0114\n",
      "      8       \u001b[36m28.8160\u001b[0m       27.2088  0.0124\n",
      "      9       \u001b[36m28.6898\u001b[0m       27.3009  0.0137\n",
      "     10       \u001b[36m28.6505\u001b[0m       27.7672  0.0117\n",
      "     11       \u001b[36m28.6304\u001b[0m       27.4971  0.0115\n",
      "     12       \u001b[36m28.5384\u001b[0m       27.3809  0.0114\n",
      "     13       \u001b[36m28.5148\u001b[0m       27.5775  0.0122\n",
      "     14       \u001b[36m28.5126\u001b[0m       27.5336  0.0123\n",
      "     15       \u001b[36m28.4891\u001b[0m       27.4158  0.0119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     16       \u001b[36m28.4722\u001b[0m       27.5315  0.0121\n",
      "     17       28.4820       27.4781  0.0115\n",
      "     18       28.4737       27.4915  0.0120\n",
      "     19       \u001b[36m28.4628\u001b[0m       27.4504  0.0121\n",
      "     20       28.4770       27.4470  0.0117\n",
      "     21       28.4694       27.5520  0.0118\n",
      "     22       28.4832       27.3210  0.0118\n",
      "     23       \u001b[36m28.4224\u001b[0m       27.5068  0.0118\n",
      "     24       28.4354       27.4807  0.0117\n",
      "     25       \u001b[36m28.4210\u001b[0m       27.4207  0.0118\n",
      "     26       28.4228       27.4407  0.0114\n",
      "     27       \u001b[36m28.4141\u001b[0m       27.5148  0.0120\n",
      "     28       28.4159       27.4235  0.0115\n",
      "     29       \u001b[36m28.4047\u001b[0m       27.4667  0.0126\n",
      "     30       28.4091       27.4521  0.0118\n",
      "     31       \u001b[36m28.4036\u001b[0m       27.4492  0.0158\n",
      "     32       \u001b[36m28.4013\u001b[0m       27.4392  0.0126\n",
      "     33       \u001b[36m28.4007\u001b[0m       27.4470  0.0120\n",
      "     34       \u001b[36m28.3974\u001b[0m       27.4435  0.0114\n",
      "     35       \u001b[36m28.3967\u001b[0m       27.4353  0.0116\n",
      "     36       \u001b[36m28.3956\u001b[0m       27.4498  0.0123\n",
      "     37       \u001b[36m28.3929\u001b[0m       27.4385  0.0117\n",
      "     38       28.3937       27.4332  0.0120\n",
      "     39       28.3933       27.4553  0.0117\n",
      "     40       \u001b[36m28.3910\u001b[0m       27.4176  0.0117\n",
      "     41       28.3952       27.4421  0.0118\n",
      "     42       28.3952       27.4439  0.0117\n",
      "     43       28.3985       27.4139  0.0117\n",
      "     44       28.4071       27.4165  0.0115\n",
      "     45       28.3969       27.4107  0.0113\n",
      "     46       28.4064       27.4118  0.0122\n",
      "     47       28.4024       27.4378  0.0116\n",
      "     48       \u001b[36m28.3883\u001b[0m       27.3591  0.0118\n",
      "     49       \u001b[36m28.3872\u001b[0m       27.4185  0.0114\n",
      "     50       \u001b[36m28.3803\u001b[0m       27.4044  0.0114\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m42.0042\u001b[0m       \u001b[32m42.5367\u001b[0m  0.0116\n",
      "      2       \u001b[36m40.3131\u001b[0m       \u001b[32m40.3972\u001b[0m  0.0111\n",
      "      3       \u001b[36m38.6529\u001b[0m       \u001b[32m38.2502\u001b[0m  0.0111\n",
      "      4       \u001b[36m37.0639\u001b[0m       \u001b[32m36.2484\u001b[0m  0.0108\n",
      "      5       \u001b[36m35.6780\u001b[0m       \u001b[32m34.5266\u001b[0m  0.0108\n",
      "      6       \u001b[36m34.5849\u001b[0m       \u001b[32m33.1324\u001b[0m  0.0106\n",
      "      7       \u001b[36m33.8010\u001b[0m       \u001b[32m32.0829\u001b[0m  0.0112\n",
      "      8       \u001b[36m33.2971\u001b[0m       \u001b[32m31.3610\u001b[0m  0.0107\n",
      "      9       \u001b[36m33.0034\u001b[0m       \u001b[32m30.9051\u001b[0m  0.0111\n",
      "     10       \u001b[36m32.8415\u001b[0m       \u001b[32m30.6314\u001b[0m  0.0106\n",
      "     11       \u001b[36m32.7499\u001b[0m       \u001b[32m30.4680\u001b[0m  0.0106\n",
      "     12       \u001b[36m32.6936\u001b[0m       \u001b[32m30.3680\u001b[0m  0.0114\n",
      "     13       \u001b[36m32.6540\u001b[0m       \u001b[32m30.3036\u001b[0m  0.0113\n",
      "     14       \u001b[36m32.6229\u001b[0m       \u001b[32m30.2598\u001b[0m  0.0111\n",
      "     15       \u001b[36m32.5969\u001b[0m       \u001b[32m30.2278\u001b[0m  0.0108\n",
      "     16       \u001b[36m32.5744\u001b[0m       \u001b[32m30.2026\u001b[0m  0.0130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m32.5545\u001b[0m       \u001b[32m30.1823\u001b[0m  0.0167\n",
      "     18       \u001b[36m32.5366\u001b[0m       \u001b[32m30.1652\u001b[0m  0.0124\n",
      "     19       \u001b[36m32.5205\u001b[0m       \u001b[32m30.1498\u001b[0m  0.0121\n",
      "     20       \u001b[36m32.5057\u001b[0m       \u001b[32m30.1364\u001b[0m  0.0116\n",
      "     21       \u001b[36m32.4922\u001b[0m       \u001b[32m30.1237\u001b[0m  0.0114\n",
      "     22       \u001b[36m32.4797\u001b[0m       \u001b[32m30.1121\u001b[0m  0.0145\n",
      "     23       \u001b[36m32.4682\u001b[0m       \u001b[32m30.1012\u001b[0m  0.0133\n",
      "     24       \u001b[36m32.4576\u001b[0m       \u001b[32m30.0911\u001b[0m  0.0118\n",
      "     25       \u001b[36m32.4477\u001b[0m       \u001b[32m30.0816\u001b[0m  0.0114\n",
      "     26       \u001b[36m32.4385\u001b[0m       \u001b[32m30.0726\u001b[0m  0.0112\n",
      "     27       \u001b[36m32.4298\u001b[0m       \u001b[32m30.0644\u001b[0m  0.0116\n",
      "     28       \u001b[36m32.4217\u001b[0m       \u001b[32m30.0564\u001b[0m  0.0111\n",
      "     29       \u001b[36m32.4141\u001b[0m       \u001b[32m30.0491\u001b[0m  0.0112\n",
      "     30       \u001b[36m32.4071\u001b[0m       \u001b[32m30.0420\u001b[0m  0.0109\n",
      "     31       \u001b[36m32.4002\u001b[0m       \u001b[32m30.0352\u001b[0m  0.0108\n",
      "     32       \u001b[36m32.3938\u001b[0m       \u001b[32m30.0289\u001b[0m  0.0113\n",
      "     33       \u001b[36m32.3877\u001b[0m       \u001b[32m30.0228\u001b[0m  0.0111\n",
      "     34       \u001b[36m32.3819\u001b[0m       \u001b[32m30.0170\u001b[0m  0.0113\n",
      "     35       \u001b[36m32.3764\u001b[0m       \u001b[32m30.0114\u001b[0m  0.0109\n",
      "     36       \u001b[36m32.3711\u001b[0m       \u001b[32m30.0062\u001b[0m  0.0106\n",
      "     37       \u001b[36m32.3661\u001b[0m       \u001b[32m30.0010\u001b[0m  0.0110\n",
      "     38       \u001b[36m32.3613\u001b[0m       \u001b[32m29.9962\u001b[0m  0.0109\n",
      "     39       \u001b[36m32.3568\u001b[0m       \u001b[32m29.9919\u001b[0m  0.0108\n",
      "     40       \u001b[36m32.3525\u001b[0m       \u001b[32m29.9876\u001b[0m  0.0107\n",
      "     41       \u001b[36m32.3484\u001b[0m       \u001b[32m29.9837\u001b[0m  0.0112\n",
      "     42       \u001b[36m32.3444\u001b[0m       \u001b[32m29.9798\u001b[0m  0.0112\n",
      "     43       \u001b[36m32.3405\u001b[0m       \u001b[32m29.9765\u001b[0m  0.0115\n",
      "     44       \u001b[36m32.3369\u001b[0m       \u001b[32m29.9729\u001b[0m  0.0112\n",
      "     45       \u001b[36m32.3334\u001b[0m       \u001b[32m29.9697\u001b[0m  0.0109\n",
      "     46       \u001b[36m32.3300\u001b[0m       \u001b[32m29.9666\u001b[0m  0.0108\n",
      "     47       \u001b[36m32.3267\u001b[0m       \u001b[32m29.9635\u001b[0m  0.0113\n",
      "     48       \u001b[36m32.3236\u001b[0m       \u001b[32m29.9608\u001b[0m  0.0109\n",
      "     49       \u001b[36m32.3205\u001b[0m       \u001b[32m29.9580\u001b[0m  0.0112\n",
      "     50       \u001b[36m32.3176\u001b[0m       \u001b[32m29.9557\u001b[0m  0.0112\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m33.6105\u001b[0m       \u001b[32m31.4505\u001b[0m  0.0108\n",
      "      2       \u001b[36m31.7065\u001b[0m       \u001b[32m30.1074\u001b[0m  0.0119\n",
      "      3       \u001b[36m29.9179\u001b[0m       \u001b[32m28.8728\u001b[0m  0.0110\n",
      "      4       \u001b[36m28.2434\u001b[0m       \u001b[32m27.8391\u001b[0m  0.0111\n",
      "      5       \u001b[36m26.8029\u001b[0m       \u001b[32m27.1053\u001b[0m  0.0110\n",
      "      6       \u001b[36m25.6828\u001b[0m       \u001b[32m26.6693\u001b[0m  0.0109\n",
      "      7       \u001b[36m24.8685\u001b[0m       \u001b[32m26.4710\u001b[0m  0.0131\n",
      "      8       \u001b[36m24.3149\u001b[0m       \u001b[32m26.4286\u001b[0m  0.0119\n",
      "      9       \u001b[36m23.9666\u001b[0m       26.4591  0.0112\n",
      "     10       \u001b[36m23.7558\u001b[0m       26.5075  0.0117\n",
      "     11       \u001b[36m23.6276\u001b[0m       26.5470  0.0109\n",
      "     12       \u001b[36m23.5457\u001b[0m       26.5749  0.0115\n",
      "     13       \u001b[36m23.4895\u001b[0m       26.5903  0.0125\n",
      "     14       \u001b[36m23.4487\u001b[0m       26.5970  0.0111\n",
      "     15       \u001b[36m23.4170\u001b[0m       26.5987  0.0108\n",
      "     16       \u001b[36m23.3910\u001b[0m       26.5969  0.0109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m23.3688\u001b[0m       26.5935  0.0115\n",
      "     18       \u001b[36m23.3495\u001b[0m       26.5892  0.0111\n",
      "     19       \u001b[36m23.3323\u001b[0m       26.5842  0.0111\n",
      "     20       \u001b[36m23.3168\u001b[0m       26.5796  0.0111\n",
      "     21       \u001b[36m23.3026\u001b[0m       26.5747  0.0107\n",
      "     22       \u001b[36m23.2897\u001b[0m       26.5705  0.0118\n",
      "     23       \u001b[36m23.2779\u001b[0m       26.5670  0.0116\n",
      "     24       \u001b[36m23.2670\u001b[0m       26.5635  0.0111\n",
      "     25       \u001b[36m23.2570\u001b[0m       26.5606  0.0107\n",
      "     26       \u001b[36m23.2477\u001b[0m       26.5577  0.0110\n",
      "     27       \u001b[36m23.2392\u001b[0m       26.5551  0.0113\n",
      "     28       \u001b[36m23.2313\u001b[0m       26.5525  0.0111\n",
      "     29       \u001b[36m23.2239\u001b[0m       26.5508  0.0109\n",
      "     30       \u001b[36m23.2170\u001b[0m       26.5490  0.0109\n",
      "     31       \u001b[36m23.2106\u001b[0m       26.5471  0.0108\n",
      "     32       \u001b[36m23.2045\u001b[0m       26.5456  0.0111\n",
      "     33       \u001b[36m23.1989\u001b[0m       26.5442  0.0108\n",
      "     34       \u001b[36m23.1935\u001b[0m       26.5428  0.0109\n",
      "     35       \u001b[36m23.1885\u001b[0m       26.5417  0.0107\n",
      "     36       \u001b[36m23.1838\u001b[0m       26.5408  0.0107\n",
      "     37       \u001b[36m23.1792\u001b[0m       26.5396  0.0121\n",
      "     38       \u001b[36m23.1750\u001b[0m       26.5385  0.0108\n",
      "     39       \u001b[36m23.1709\u001b[0m       26.5378  0.0113\n",
      "     40       \u001b[36m23.1671\u001b[0m       26.5370  0.0111\n",
      "     41       \u001b[36m23.1634\u001b[0m       26.5363  0.0108\n",
      "     42       \u001b[36m23.1599\u001b[0m       26.5358  0.0111\n",
      "     43       \u001b[36m23.1565\u001b[0m       26.5352  0.0111\n",
      "     44       \u001b[36m23.1534\u001b[0m       26.5349  0.0115\n",
      "     45       \u001b[36m23.1503\u001b[0m       26.5343  0.0108\n",
      "     46       \u001b[36m23.1474\u001b[0m       26.5340  0.0122\n",
      "     47       \u001b[36m23.1446\u001b[0m       26.5335  0.0130\n",
      "     48       \u001b[36m23.1419\u001b[0m       26.5332  0.0115\n",
      "     49       \u001b[36m23.1393\u001b[0m       26.5328  0.0175\n",
      "     50       \u001b[36m23.1368\u001b[0m       26.5322  0.0121\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m40.0220\u001b[0m       \u001b[32m31.0263\u001b[0m  0.0124\n",
      "      2       \u001b[36m37.7725\u001b[0m       \u001b[32m29.5642\u001b[0m  0.0130\n",
      "      3       \u001b[36m35.5689\u001b[0m       \u001b[32m28.2221\u001b[0m  0.0125\n",
      "      4       \u001b[36m33.5458\u001b[0m       \u001b[32m27.1633\u001b[0m  0.0126\n",
      "      5       \u001b[36m31.8567\u001b[0m       \u001b[32m26.4987\u001b[0m  0.0125\n",
      "      6       \u001b[36m30.5510\u001b[0m       \u001b[32m26.2667\u001b[0m  0.0112\n",
      "      7       \u001b[36m29.6737\u001b[0m       26.3867  0.0116\n",
      "      8       \u001b[36m29.2004\u001b[0m       26.6523  0.0116\n",
      "      9       \u001b[36m28.9899\u001b[0m       26.8890  0.0112\n",
      "     10       \u001b[36m28.8974\u001b[0m       27.0439  0.0109\n",
      "     11       \u001b[36m28.8464\u001b[0m       27.1296  0.0109\n",
      "     12       \u001b[36m28.8088\u001b[0m       27.1729  0.0110\n",
      "     13       \u001b[36m28.7768\u001b[0m       27.1942  0.0112\n",
      "     14       \u001b[36m28.7485\u001b[0m       27.2058  0.0111\n",
      "     15       \u001b[36m28.7239\u001b[0m       27.2104  0.0109\n",
      "     16       \u001b[36m28.7019\u001b[0m       27.2124  0.0111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m28.6824\u001b[0m       27.2127  0.0117\n",
      "     18       \u001b[36m28.6649\u001b[0m       27.2124  0.0111\n",
      "     19       \u001b[36m28.6492\u001b[0m       27.2121  0.0113\n",
      "     20       \u001b[36m28.6349\u001b[0m       27.2116  0.0110\n",
      "     21       \u001b[36m28.6220\u001b[0m       27.2108  0.0115\n",
      "     22       \u001b[36m28.6103\u001b[0m       27.2107  0.0127\n",
      "     23       \u001b[36m28.5997\u001b[0m       27.2105  0.0113\n",
      "     24       \u001b[36m28.5899\u001b[0m       27.2112  0.0115\n",
      "     25       \u001b[36m28.5811\u001b[0m       27.2109  0.0109\n",
      "     26       \u001b[36m28.5729\u001b[0m       27.2099  0.0108\n",
      "     27       \u001b[36m28.5652\u001b[0m       27.2093  0.0116\n",
      "     28       \u001b[36m28.5581\u001b[0m       27.2093  0.0111\n",
      "     29       \u001b[36m28.5516\u001b[0m       27.2096  0.0120\n",
      "     30       \u001b[36m28.5456\u001b[0m       27.2095  0.0109\n",
      "     31       \u001b[36m28.5400\u001b[0m       27.2094  0.0108\n",
      "     32       \u001b[36m28.5347\u001b[0m       27.2089  0.0110\n",
      "     33       \u001b[36m28.5297\u001b[0m       27.2090  0.0110\n",
      "     34       \u001b[36m28.5250\u001b[0m       27.2088  0.0112\n",
      "     35       \u001b[36m28.5205\u001b[0m       27.2086  0.0107\n",
      "     36       \u001b[36m28.5163\u001b[0m       27.2084  0.0109\n",
      "     37       \u001b[36m28.5124\u001b[0m       27.2084  0.0112\n",
      "     38       \u001b[36m28.5086\u001b[0m       27.2084  0.0109\n",
      "     39       \u001b[36m28.5051\u001b[0m       27.2081  0.0114\n",
      "     40       \u001b[36m28.5016\u001b[0m       27.2083  0.0109\n",
      "     41       \u001b[36m28.4985\u001b[0m       27.2083  0.0108\n",
      "     42       \u001b[36m28.4954\u001b[0m       27.2085  0.0114\n",
      "     43       \u001b[36m28.4925\u001b[0m       27.2083  0.0112\n",
      "     44       \u001b[36m28.4897\u001b[0m       27.2081  0.0113\n",
      "     45       \u001b[36m28.4870\u001b[0m       27.2078  0.0105\n",
      "     46       \u001b[36m28.4844\u001b[0m       27.2076  0.0110\n",
      "     47       \u001b[36m28.4819\u001b[0m       27.2075  0.0112\n",
      "     48       \u001b[36m28.4796\u001b[0m       27.2071  0.0109\n",
      "     49       \u001b[36m28.4773\u001b[0m       27.2069  0.0110\n",
      "     50       \u001b[36m28.4751\u001b[0m       27.2068  0.0110\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m40.4464\u001b[0m       \u001b[32m38.4470\u001b[0m  0.0117\n",
      "      2       \u001b[36m35.6681\u001b[0m       \u001b[32m31.5080\u001b[0m  0.0119\n",
      "      3       \u001b[36m34.3340\u001b[0m       32.0440  0.0117\n",
      "      4       \u001b[36m33.6526\u001b[0m       31.9075  0.0115\n",
      "      5       \u001b[36m33.1380\u001b[0m       \u001b[32m30.8865\u001b[0m  0.0116\n",
      "      6       \u001b[36m32.9454\u001b[0m       \u001b[32m30.3252\u001b[0m  0.0111\n",
      "      7       \u001b[36m32.7306\u001b[0m       30.7041  0.0112\n",
      "      8       \u001b[36m32.6840\u001b[0m       30.5735  0.0121\n",
      "      9       \u001b[36m32.5347\u001b[0m       \u001b[32m30.1538\u001b[0m  0.0116\n",
      "     10       \u001b[36m32.4751\u001b[0m       30.2082  0.0116\n",
      "     11       \u001b[36m32.4302\u001b[0m       30.3812  0.0114\n",
      "     12       \u001b[36m32.3929\u001b[0m       30.2380  0.0117\n",
      "     13       \u001b[36m32.3529\u001b[0m       30.1757  0.0115\n",
      "     14       \u001b[36m32.3397\u001b[0m       30.2322  0.0111\n",
      "     15       \u001b[36m32.3197\u001b[0m       30.2327  0.0116\n",
      "     16       \u001b[36m32.3009\u001b[0m       30.2025  0.0116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m32.2868\u001b[0m       30.1894  0.0121\n",
      "     18       \u001b[36m32.2757\u001b[0m       30.2092  0.0114\n",
      "     19       \u001b[36m32.2660\u001b[0m       30.2119  0.0115\n",
      "     20       \u001b[36m32.2543\u001b[0m       30.1869  0.0123\n",
      "     21       \u001b[36m32.2452\u001b[0m       30.2031  0.0116\n",
      "     22       \u001b[36m32.2379\u001b[0m       30.1903  0.0114\n",
      "     23       \u001b[36m32.2284\u001b[0m       30.1855  0.0120\n",
      "     24       \u001b[36m32.2216\u001b[0m       30.1750  0.0112\n",
      "     25       \u001b[36m32.2147\u001b[0m       30.1894  0.0120\n",
      "     26       \u001b[36m32.2078\u001b[0m       30.1592  0.0115\n",
      "     27       \u001b[36m32.2021\u001b[0m       30.1969  0.0120\n",
      "     28       \u001b[36m32.1962\u001b[0m       \u001b[32m30.1414\u001b[0m  0.0116\n",
      "     29       \u001b[36m32.1938\u001b[0m       30.2129  0.0115\n",
      "     30       \u001b[36m32.1873\u001b[0m       \u001b[32m30.1167\u001b[0m  0.0186\n",
      "     31       32.1915       30.2690  0.0139\n",
      "     32       \u001b[36m32.1845\u001b[0m       \u001b[32m30.0980\u001b[0m  0.0126\n",
      "     33       32.1990       30.3800  0.0127\n",
      "     34       32.2000       \u001b[32m30.0704\u001b[0m  0.0127\n",
      "     35       32.2335       30.4313  0.0144\n",
      "     36       32.2165       \u001b[32m29.9785\u001b[0m  0.0126\n",
      "     37       32.2052       30.3092  0.0132\n",
      "     38       32.1989       30.0519  0.0122\n",
      "     39       \u001b[36m32.1509\u001b[0m       30.0956  0.0125\n",
      "     40       32.1693       30.1907  0.0118\n",
      "     41       \u001b[36m32.1450\u001b[0m       30.0461  0.0119\n",
      "     42       32.1474       30.1803  0.0119\n",
      "     43       32.1470       30.0912  0.0119\n",
      "     44       \u001b[36m32.1379\u001b[0m       30.1457  0.0116\n",
      "     45       32.1416       30.1167  0.0116\n",
      "     46       \u001b[36m32.1334\u001b[0m       30.1347  0.0116\n",
      "     47       32.1346       30.1266  0.0123\n",
      "     48       \u001b[36m32.1289\u001b[0m       30.1347  0.0119\n",
      "     49       32.1291       30.1385  0.0118\n",
      "     50       \u001b[36m32.1252\u001b[0m       30.1327  0.0119\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m31.5471\u001b[0m       \u001b[32m27.1984\u001b[0m  0.0116\n",
      "      2       \u001b[36m25.8277\u001b[0m       28.8987  0.0118\n",
      "      3       \u001b[36m24.3543\u001b[0m       \u001b[32m26.3095\u001b[0m  0.0118\n",
      "      4       24.3852       26.3891  0.0120\n",
      "      5       \u001b[36m23.7663\u001b[0m       27.8226  0.0118\n",
      "      6       \u001b[36m23.6389\u001b[0m       27.2666  0.0117\n",
      "      7       \u001b[36m23.4636\u001b[0m       26.7651  0.0114\n",
      "      8       \u001b[36m23.4230\u001b[0m       27.0786  0.0117\n",
      "      9       \u001b[36m23.3055\u001b[0m       27.1494  0.0114\n",
      "     10       \u001b[36m23.2377\u001b[0m       26.8098  0.0121\n",
      "     11       \u001b[36m23.2283\u001b[0m       26.9522  0.0119\n",
      "     12       \u001b[36m23.1874\u001b[0m       27.0527  0.0117\n",
      "     13       \u001b[36m23.1604\u001b[0m       26.8890  0.0115\n",
      "     14       \u001b[36m23.1441\u001b[0m       26.8669  0.0112\n",
      "     15       \u001b[36m23.1308\u001b[0m       26.9409  0.0116\n",
      "     16       \u001b[36m23.1138\u001b[0m       26.8976  0.0119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m23.1036\u001b[0m       26.8475  0.0121\n",
      "     18       \u001b[36m23.0968\u001b[0m       26.9007  0.0115\n",
      "     19       \u001b[36m23.0842\u001b[0m       26.8767  0.0113\n",
      "     20       \u001b[36m23.0785\u001b[0m       26.8527  0.0119\n",
      "     21       \u001b[36m23.0714\u001b[0m       26.8670  0.0118\n",
      "     22       \u001b[36m23.0656\u001b[0m       26.8520  0.0119\n",
      "     23       \u001b[36m23.0594\u001b[0m       26.8392  0.0114\n",
      "     24       \u001b[36m23.0555\u001b[0m       26.8485  0.0115\n",
      "     25       \u001b[36m23.0492\u001b[0m       26.8314  0.0119\n",
      "     26       \u001b[36m23.0448\u001b[0m       26.8274  0.0120\n",
      "     27       \u001b[36m23.0406\u001b[0m       26.8177  0.0120\n",
      "     28       \u001b[36m23.0347\u001b[0m       26.8218  0.0116\n",
      "     29       \u001b[36m23.0328\u001b[0m       26.8109  0.0113\n",
      "     30       \u001b[36m23.0296\u001b[0m       26.8188  0.0119\n",
      "     31       \u001b[36m23.0255\u001b[0m       26.8107  0.0116\n",
      "     32       23.0277       26.7958  0.0119\n",
      "     33       23.0287       26.8373  0.0116\n",
      "     34       23.0325       26.7877  0.0116\n",
      "     35       23.0376       26.7781  0.0121\n",
      "     36       23.0269       26.8371  0.0117\n",
      "     37       \u001b[36m23.0197\u001b[0m       26.7778  0.0117\n",
      "     38       \u001b[36m23.0190\u001b[0m       26.7911  0.0115\n",
      "     39       \u001b[36m23.0064\u001b[0m       26.8061  0.0114\n",
      "     40       \u001b[36m23.0017\u001b[0m       26.7802  0.0119\n",
      "     41       23.0019       26.7949  0.0117\n",
      "     42       \u001b[36m22.9985\u001b[0m       26.7826  0.0115\n",
      "     43       \u001b[36m22.9960\u001b[0m       26.7914  0.0122\n",
      "     44       \u001b[36m22.9949\u001b[0m       26.7853  0.0118\n",
      "     45       \u001b[36m22.9920\u001b[0m       26.7873  0.0116\n",
      "     46       \u001b[36m22.9897\u001b[0m       26.7829  0.0114\n",
      "     47       \u001b[36m22.9888\u001b[0m       26.7772  0.0123\n",
      "     48       \u001b[36m22.9859\u001b[0m       26.7875  0.0116\n",
      "     49       \u001b[36m22.9844\u001b[0m       26.7708  0.0120\n",
      "     50       \u001b[36m22.9841\u001b[0m       26.7704  0.0115\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m39.3799\u001b[0m       \u001b[32m27.9096\u001b[0m  0.0116\n",
      "      2       \u001b[36m31.6600\u001b[0m       32.5820  0.0125\n",
      "      3       \u001b[36m31.0092\u001b[0m       \u001b[32m26.7852\u001b[0m  0.0121\n",
      "      4       \u001b[36m29.7000\u001b[0m       \u001b[32m26.4612\u001b[0m  0.0117\n",
      "      5       \u001b[36m29.1816\u001b[0m       28.2341  0.0119\n",
      "      6       29.2584       28.3333  0.0114\n",
      "      7       \u001b[36m28.8659\u001b[0m       27.2379  0.0112\n",
      "      8       \u001b[36m28.7130\u001b[0m       27.4965  0.0149\n",
      "      9       \u001b[36m28.6902\u001b[0m       27.8084  0.0149\n",
      "     10       \u001b[36m28.6253\u001b[0m       27.2626  0.0121\n",
      "     11       \u001b[36m28.5432\u001b[0m       27.5521  0.0121\n",
      "     12       28.5490       27.7258  0.0121\n",
      "     13       \u001b[36m28.5222\u001b[0m       27.3787  0.0122\n",
      "     14       \u001b[36m28.4839\u001b[0m       27.4409  0.0140\n",
      "     15       28.4849       27.5501  0.0131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     16       \u001b[36m28.4749\u001b[0m       27.4077  0.0125\n",
      "     17       \u001b[36m28.4566\u001b[0m       27.4619  0.0119\n",
      "     18       28.4588       27.4591  0.0123\n",
      "     19       28.4614       27.4091  0.0120\n",
      "     20       28.4598       27.4750  0.0121\n",
      "     21       28.4954       27.4528  0.0117\n",
      "     22       28.5127       27.5328  0.0118\n",
      "     23       28.5109       27.3120  0.0120\n",
      "     24       \u001b[36m28.4432\u001b[0m       27.5155  0.0124\n",
      "     25       \u001b[36m28.4382\u001b[0m       27.4269  0.0123\n",
      "     26       \u001b[36m28.4213\u001b[0m       27.3862  0.0120\n",
      "     27       28.4234       27.4507  0.0112\n",
      "     28       \u001b[36m28.4158\u001b[0m       27.4334  0.0115\n",
      "     29       \u001b[36m28.4088\u001b[0m       27.4057  0.0118\n",
      "     30       \u001b[36m28.4073\u001b[0m       27.4146  0.0116\n",
      "     31       \u001b[36m28.4050\u001b[0m       27.3888  0.0138\n",
      "     32       \u001b[36m28.4006\u001b[0m       27.4112  0.0117\n",
      "     33       \u001b[36m28.3996\u001b[0m       27.3902  0.0111\n",
      "     34       \u001b[36m28.3963\u001b[0m       27.3860  0.0117\n",
      "     35       \u001b[36m28.3955\u001b[0m       27.3857  0.0119\n",
      "     36       \u001b[36m28.3928\u001b[0m       27.3868  0.0114\n",
      "     37       \u001b[36m28.3914\u001b[0m       27.3844  0.0117\n",
      "     38       \u001b[36m28.3903\u001b[0m       27.3810  0.0119\n",
      "     39       \u001b[36m28.3885\u001b[0m       27.3816  0.0119\n",
      "     40       \u001b[36m28.3874\u001b[0m       27.3796  0.0113\n",
      "     41       \u001b[36m28.3861\u001b[0m       27.3762  0.0111\n",
      "     42       \u001b[36m28.3849\u001b[0m       27.3755  0.0115\n",
      "     43       \u001b[36m28.3838\u001b[0m       27.3723  0.0113\n",
      "     44       \u001b[36m28.3826\u001b[0m       27.3721  0.0124\n",
      "     45       \u001b[36m28.3817\u001b[0m       27.3723  0.0115\n",
      "     46       \u001b[36m28.3806\u001b[0m       27.3681  0.0116\n",
      "     47       \u001b[36m28.3795\u001b[0m       27.3725  0.0114\n",
      "     48       \u001b[36m28.3788\u001b[0m       27.3671  0.0114\n",
      "     49       \u001b[36m28.3778\u001b[0m       27.3701  0.0121\n",
      "     50       \u001b[36m28.3767\u001b[0m       27.3670  0.0115\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m43.3957\u001b[0m       \u001b[32m43.9470\u001b[0m  0.0111\n",
      "      2       \u001b[36m41.3064\u001b[0m       \u001b[32m41.4295\u001b[0m  0.0107\n",
      "      3       \u001b[36m39.3815\u001b[0m       \u001b[32m39.0157\u001b[0m  0.0110\n",
      "      4       \u001b[36m37.5908\u001b[0m       \u001b[32m36.7850\u001b[0m  0.0122\n",
      "      5       \u001b[36m36.0327\u001b[0m       \u001b[32m34.8322\u001b[0m  0.0117\n",
      "      6       \u001b[36m34.7891\u001b[0m       \u001b[32m33.2375\u001b[0m  0.0111\n",
      "      7       \u001b[36m33.9034\u001b[0m       \u001b[32m32.0761\u001b[0m  0.0105\n",
      "      8       \u001b[36m33.3595\u001b[0m       \u001b[32m31.3361\u001b[0m  0.0109\n",
      "      9       \u001b[36m33.0635\u001b[0m       \u001b[32m30.9044\u001b[0m  0.0108\n",
      "     10       \u001b[36m32.9041\u001b[0m       \u001b[32m30.6581\u001b[0m  0.0109\n",
      "     11       \u001b[36m32.8091\u001b[0m       \u001b[32m30.5134\u001b[0m  0.0108\n",
      "     12       \u001b[36m32.7444\u001b[0m       \u001b[32m30.4225\u001b[0m  0.0111\n",
      "     13       \u001b[36m32.6961\u001b[0m       \u001b[32m30.3614\u001b[0m  0.0113\n",
      "     14       \u001b[36m32.6570\u001b[0m       \u001b[32m30.3170\u001b[0m  0.0112\n",
      "     15       \u001b[36m32.6238\u001b[0m       \u001b[32m30.2822\u001b[0m  0.0109\n",
      "     16       \u001b[36m32.5954\u001b[0m       \u001b[32m30.2540\u001b[0m  0.0111\n",
      "     17       \u001b[36m32.5708\u001b[0m       \u001b[32m30.2302\u001b[0m  0.0108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     18       \u001b[36m32.5492\u001b[0m       \u001b[32m30.2095\u001b[0m  0.0117\n",
      "     19       \u001b[36m32.5299\u001b[0m       \u001b[32m30.1911\u001b[0m  0.0108\n",
      "     20       \u001b[36m32.5126\u001b[0m       \u001b[32m30.1747\u001b[0m  0.0108\n",
      "     21       \u001b[36m32.4970\u001b[0m       \u001b[32m30.1600\u001b[0m  0.0111\n",
      "     22       \u001b[36m32.4829\u001b[0m       \u001b[32m30.1463\u001b[0m  0.0107\n",
      "     23       \u001b[36m32.4700\u001b[0m       \u001b[32m30.1339\u001b[0m  0.0113\n",
      "     24       \u001b[36m32.4581\u001b[0m       \u001b[32m30.1224\u001b[0m  0.0112\n",
      "     25       \u001b[36m32.4471\u001b[0m       \u001b[32m30.1118\u001b[0m  0.0110\n",
      "     26       \u001b[36m32.4369\u001b[0m       \u001b[32m30.1019\u001b[0m  0.0107\n",
      "     27       \u001b[36m32.4274\u001b[0m       \u001b[32m30.0926\u001b[0m  0.0106\n",
      "     28       \u001b[36m32.4186\u001b[0m       \u001b[32m30.0839\u001b[0m  0.0109\n",
      "     29       \u001b[36m32.4103\u001b[0m       \u001b[32m30.0757\u001b[0m  0.0110\n",
      "     30       \u001b[36m32.4026\u001b[0m       \u001b[32m30.0681\u001b[0m  0.0109\n",
      "     31       \u001b[36m32.3954\u001b[0m       \u001b[32m30.0608\u001b[0m  0.0102\n",
      "     32       \u001b[36m32.3886\u001b[0m       \u001b[32m30.0541\u001b[0m  0.0105\n",
      "     33       \u001b[36m32.3822\u001b[0m       \u001b[32m30.0477\u001b[0m  0.0111\n",
      "     34       \u001b[36m32.3762\u001b[0m       \u001b[32m30.0417\u001b[0m  0.0111\n",
      "     35       \u001b[36m32.3705\u001b[0m       \u001b[32m30.0360\u001b[0m  0.0114\n",
      "     36       \u001b[36m32.3652\u001b[0m       \u001b[32m30.0306\u001b[0m  0.0104\n",
      "     37       \u001b[36m32.3601\u001b[0m       \u001b[32m30.0255\u001b[0m  0.0105\n",
      "     38       \u001b[36m32.3553\u001b[0m       \u001b[32m30.0205\u001b[0m  0.0109\n",
      "     39       \u001b[36m32.3507\u001b[0m       \u001b[32m30.0159\u001b[0m  0.0111\n",
      "     40       \u001b[36m32.3464\u001b[0m       \u001b[32m30.0114\u001b[0m  0.0192\n",
      "     41       \u001b[36m32.3422\u001b[0m       \u001b[32m30.0071\u001b[0m  0.0152\n",
      "     42       \u001b[36m32.3383\u001b[0m       \u001b[32m30.0030\u001b[0m  0.0129\n",
      "     43       \u001b[36m32.3345\u001b[0m       \u001b[32m29.9992\u001b[0m  0.0172\n",
      "     44       \u001b[36m32.3308\u001b[0m       \u001b[32m29.9955\u001b[0m  0.0220\n",
      "     45       \u001b[36m32.3273\u001b[0m       \u001b[32m29.9920\u001b[0m  0.0132\n",
      "     46       \u001b[36m32.3240\u001b[0m       \u001b[32m29.9887\u001b[0m  0.0114\n",
      "     47       \u001b[36m32.3207\u001b[0m       \u001b[32m29.9855\u001b[0m  0.0115\n",
      "     48       \u001b[36m32.3177\u001b[0m       \u001b[32m29.9825\u001b[0m  0.0114\n",
      "     49       \u001b[36m32.3147\u001b[0m       \u001b[32m29.9793\u001b[0m  0.0113\n",
      "     50       \u001b[36m32.3118\u001b[0m       \u001b[32m29.9764\u001b[0m  0.0115\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m34.0215\u001b[0m       \u001b[32m31.8288\u001b[0m  0.0115\n",
      "      2       \u001b[36m32.1768\u001b[0m       \u001b[32m30.4946\u001b[0m  0.0115\n",
      "      3       \u001b[36m30.4246\u001b[0m       \u001b[32m29.2193\u001b[0m  0.0115\n",
      "      4       \u001b[36m28.7201\u001b[0m       \u001b[32m28.0702\u001b[0m  0.0111\n",
      "      5       \u001b[36m27.1715\u001b[0m       \u001b[32m27.1751\u001b[0m  0.0112\n",
      "      6       \u001b[36m25.9146\u001b[0m       \u001b[32m26.6085\u001b[0m  0.0111\n",
      "      7       \u001b[36m24.9963\u001b[0m       \u001b[32m26.3448\u001b[0m  0.0110\n",
      "      8       \u001b[36m24.3821\u001b[0m       \u001b[32m26.2896\u001b[0m  0.0110\n",
      "      9       \u001b[36m24.0057\u001b[0m       26.3298  0.0109\n",
      "     10       \u001b[36m23.7875\u001b[0m       26.3884  0.0107\n",
      "     11       \u001b[36m23.6596\u001b[0m       26.4329  0.0103\n",
      "     12       \u001b[36m23.5798\u001b[0m       26.4587  0.0110\n",
      "     13       \u001b[36m23.5248\u001b[0m       26.4699  0.0109\n",
      "     14       \u001b[36m23.4834\u001b[0m       26.4718  0.0110\n",
      "     15       \u001b[36m23.4502\u001b[0m       26.4687  0.0107\n",
      "     16       \u001b[36m23.4224\u001b[0m       26.4628  0.0105\n",
      "     17       \u001b[36m23.3984\u001b[0m       26.4558  0.0109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     18       \u001b[36m23.3772\u001b[0m       26.4485  0.0117\n",
      "     19       \u001b[36m23.3584\u001b[0m       26.4419  0.0104\n",
      "     20       \u001b[36m23.3415\u001b[0m       26.4354  0.0106\n",
      "     21       \u001b[36m23.3263\u001b[0m       26.4294  0.0106\n",
      "     22       \u001b[36m23.3124\u001b[0m       26.4240  0.0111\n",
      "     23       \u001b[36m23.2997\u001b[0m       26.4193  0.0112\n",
      "     24       \u001b[36m23.2881\u001b[0m       26.4151  0.0112\n",
      "     25       \u001b[36m23.2773\u001b[0m       26.4115  0.0107\n",
      "     26       \u001b[36m23.2673\u001b[0m       26.4081  0.0109\n",
      "     27       \u001b[36m23.2582\u001b[0m       26.4056  0.0107\n",
      "     28       \u001b[36m23.2496\u001b[0m       26.4033  0.0108\n",
      "     29       \u001b[36m23.2416\u001b[0m       26.4013  0.0104\n",
      "     30       \u001b[36m23.2342\u001b[0m       26.3996  0.0107\n",
      "     31       \u001b[36m23.2272\u001b[0m       26.3982  0.0107\n",
      "     32       \u001b[36m23.2205\u001b[0m       26.3967  0.0106\n",
      "     33       \u001b[36m23.2144\u001b[0m       26.3958  0.0112\n",
      "     34       \u001b[36m23.2086\u001b[0m       26.3949  0.0111\n",
      "     35       \u001b[36m23.2031\u001b[0m       26.3941  0.0110\n",
      "     36       \u001b[36m23.1978\u001b[0m       26.3934  0.0111\n",
      "     37       \u001b[36m23.1929\u001b[0m       26.3929  0.0111\n",
      "     38       \u001b[36m23.1883\u001b[0m       26.3925  0.0109\n",
      "     39       \u001b[36m23.1838\u001b[0m       26.3922  0.0111\n",
      "     40       \u001b[36m23.1796\u001b[0m       26.3920  0.0110\n",
      "     41       \u001b[36m23.1755\u001b[0m       26.3918  0.0107\n",
      "     42       \u001b[36m23.1717\u001b[0m       26.3917  0.0105\n",
      "     43       \u001b[36m23.1679\u001b[0m       26.3916  0.0108\n",
      "     44       \u001b[36m23.1644\u001b[0m       26.3916  0.0111\n",
      "     45       \u001b[36m23.1610\u001b[0m       26.3917  0.0112\n",
      "     46       \u001b[36m23.1577\u001b[0m       26.3919  0.0105\n",
      "     47       \u001b[36m23.1546\u001b[0m       26.3921  0.0105\n",
      "     48       \u001b[36m23.1516\u001b[0m       26.3923  0.0108\n",
      "     49       \u001b[36m23.1487\u001b[0m       26.3925  0.0109\n",
      "     50       \u001b[36m23.1460\u001b[0m       26.3927  0.0107\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m41.2743\u001b[0m       \u001b[32m31.7410\u001b[0m  0.0109\n",
      "      2       \u001b[36m38.7336\u001b[0m       \u001b[32m30.0444\u001b[0m  0.0103\n",
      "      3       \u001b[36m36.2596\u001b[0m       \u001b[32m28.4454\u001b[0m  0.0110\n",
      "      4       \u001b[36m33.8811\u001b[0m       \u001b[32m27.1507\u001b[0m  0.0107\n",
      "      5       \u001b[36m31.8251\u001b[0m       \u001b[32m26.4324\u001b[0m  0.0108\n",
      "      6       \u001b[36m30.3299\u001b[0m       \u001b[32m26.3661\u001b[0m  0.0106\n",
      "      7       \u001b[36m29.4943\u001b[0m       26.6599  0.0108\n",
      "      8       \u001b[36m29.1351\u001b[0m       26.9660  0.0107\n",
      "      9       \u001b[36m28.9954\u001b[0m       27.1568  0.0111\n",
      "     10       \u001b[36m28.9275\u001b[0m       27.2501  0.0110\n",
      "     11       \u001b[36m28.8807\u001b[0m       27.2905  0.0107\n",
      "     12       \u001b[36m28.8428\u001b[0m       27.3040  0.0105\n",
      "     13       \u001b[36m28.8095\u001b[0m       27.3061  0.0108\n",
      "     14       \u001b[36m28.7801\u001b[0m       27.3044  0.0124\n",
      "     15       \u001b[36m28.7545\u001b[0m       27.3009  0.0113\n",
      "     16       \u001b[36m28.7319\u001b[0m       27.2974  0.0108\n",
      "     17       \u001b[36m28.7118\u001b[0m       27.2932  0.0104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     18       \u001b[36m28.6938\u001b[0m       27.2896  0.0116\n",
      "     19       \u001b[36m28.6777\u001b[0m       27.2869  0.0111\n",
      "     20       \u001b[36m28.6632\u001b[0m       27.2844  0.0105\n",
      "     21       \u001b[36m28.6499\u001b[0m       27.2822  0.0111\n",
      "     22       \u001b[36m28.6378\u001b[0m       27.2792  0.0143\n",
      "     23       \u001b[36m28.6266\u001b[0m       27.2775  0.0174\n",
      "     24       \u001b[36m28.6163\u001b[0m       27.2759  0.0124\n",
      "     25       \u001b[36m28.6067\u001b[0m       27.2747  0.0123\n",
      "     26       \u001b[36m28.5978\u001b[0m       27.2732  0.0138\n",
      "     27       \u001b[36m28.5894\u001b[0m       27.2720  0.0129\n",
      "     28       \u001b[36m28.5816\u001b[0m       27.2707  0.0143\n",
      "     29       \u001b[36m28.5742\u001b[0m       27.2694  0.0134\n",
      "     30       \u001b[36m28.5673\u001b[0m       27.2684  0.0124\n",
      "     31       \u001b[36m28.5608\u001b[0m       27.2672  0.0140\n",
      "     32       \u001b[36m28.5547\u001b[0m       27.2659  0.0121\n",
      "     33       \u001b[36m28.5490\u001b[0m       27.2644  0.0114\n",
      "     34       \u001b[36m28.5436\u001b[0m       27.2636  0.0121\n",
      "     35       \u001b[36m28.5385\u001b[0m       27.2622  0.0114\n",
      "     36       \u001b[36m28.5336\u001b[0m       27.2607  0.0125\n",
      "     37       \u001b[36m28.5289\u001b[0m       27.2595  0.0111\n",
      "     38       \u001b[36m28.5245\u001b[0m       27.2580  0.0117\n",
      "     39       \u001b[36m28.5202\u001b[0m       27.2568  0.0110\n",
      "     40       \u001b[36m28.5162\u001b[0m       27.2556  0.0122\n",
      "     41       \u001b[36m28.5124\u001b[0m       27.2544  0.0107\n",
      "     42       \u001b[36m28.5087\u001b[0m       27.2531  0.0108\n",
      "     43       \u001b[36m28.5052\u001b[0m       27.2517  0.0110\n",
      "     44       \u001b[36m28.5018\u001b[0m       27.2506  0.0108\n",
      "     45       \u001b[36m28.4986\u001b[0m       27.2497  0.0122\n",
      "     46       \u001b[36m28.4956\u001b[0m       27.2487  0.0114\n",
      "     47       \u001b[36m28.4926\u001b[0m       27.2474  0.0116\n",
      "     48       \u001b[36m28.4898\u001b[0m       27.2464  0.0111\n",
      "     49       \u001b[36m28.4872\u001b[0m       27.2455  0.0107\n",
      "     50       \u001b[36m28.4846\u001b[0m       27.2442  0.0109\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m44.2214\u001b[0m       \u001b[32m44.7269\u001b[0m  0.0121\n",
      "      2       \u001b[36m41.5408\u001b[0m       \u001b[32m40.1592\u001b[0m  0.0115\n",
      "      3       \u001b[36m37.8049\u001b[0m       \u001b[32m35.2401\u001b[0m  0.0115\n",
      "      4       \u001b[36m35.0631\u001b[0m       \u001b[32m32.2657\u001b[0m  0.0116\n",
      "      5       \u001b[36m34.9636\u001b[0m       \u001b[32m31.7630\u001b[0m  0.0115\n",
      "      6       \u001b[36m34.2181\u001b[0m       31.9018  0.0116\n",
      "      7       \u001b[36m33.6790\u001b[0m       32.3026  0.0117\n",
      "      8       \u001b[36m33.4760\u001b[0m       \u001b[32m31.7078\u001b[0m  0.0114\n",
      "      9       \u001b[36m33.1092\u001b[0m       \u001b[32m30.9676\u001b[0m  0.0116\n",
      "     10       \u001b[36m32.9362\u001b[0m       \u001b[32m30.6617\u001b[0m  0.0119\n",
      "     11       \u001b[36m32.8223\u001b[0m       \u001b[32m30.6591\u001b[0m  0.0114\n",
      "     12       \u001b[36m32.7165\u001b[0m       30.7009  0.0114\n",
      "     13       \u001b[36m32.6450\u001b[0m       \u001b[32m30.6284\u001b[0m  0.0117\n",
      "     14       \u001b[36m32.5626\u001b[0m       \u001b[32m30.4654\u001b[0m  0.0117\n",
      "     15       \u001b[36m32.4928\u001b[0m       \u001b[32m30.3582\u001b[0m  0.0117\n",
      "     16       \u001b[36m32.4459\u001b[0m       30.3772  0.0114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m32.4037\u001b[0m       30.3811  0.0120\n",
      "     18       \u001b[36m32.3696\u001b[0m       30.3723  0.0121\n",
      "     19       \u001b[36m32.3392\u001b[0m       \u001b[32m30.2940\u001b[0m  0.0120\n",
      "     20       \u001b[36m32.3158\u001b[0m       \u001b[32m30.2903\u001b[0m  0.0134\n",
      "     21       \u001b[36m32.2985\u001b[0m       \u001b[32m30.2492\u001b[0m  0.0137\n",
      "     22       \u001b[36m32.2892\u001b[0m       30.3111  0.0115\n",
      "     23       \u001b[36m32.2798\u001b[0m       \u001b[32m30.2342\u001b[0m  0.0124\n",
      "     24       32.2849       30.3338  0.0209\n",
      "     25       32.2861       \u001b[32m30.2011\u001b[0m  0.0154\n",
      "     26       32.3032       \u001b[32m30.2008\u001b[0m  0.0117\n",
      "     27       \u001b[36m32.2545\u001b[0m       \u001b[32m30.1919\u001b[0m  0.0118\n",
      "     28       \u001b[36m32.2279\u001b[0m       \u001b[32m30.0963\u001b[0m  0.0118\n",
      "     29       \u001b[36m32.2242\u001b[0m       30.1514  0.0117\n",
      "     30       \u001b[36m32.2210\u001b[0m       30.1489  0.0118\n",
      "     31       \u001b[36m32.2052\u001b[0m       30.1237  0.0116\n",
      "     32       \u001b[36m32.1994\u001b[0m       30.1419  0.0119\n",
      "     33       \u001b[36m32.1946\u001b[0m       30.1173  0.0116\n",
      "     34       \u001b[36m32.1880\u001b[0m       30.1307  0.0120\n",
      "     35       \u001b[36m32.1848\u001b[0m       30.1194  0.0127\n",
      "     36       \u001b[36m32.1779\u001b[0m       30.1132  0.0117\n",
      "     37       \u001b[36m32.1744\u001b[0m       30.1103  0.0125\n",
      "     38       \u001b[36m32.1699\u001b[0m       30.1057  0.0113\n",
      "     39       \u001b[36m32.1666\u001b[0m       30.1041  0.0127\n",
      "     40       \u001b[36m32.1627\u001b[0m       30.0971  0.0147\n",
      "     41       \u001b[36m32.1594\u001b[0m       \u001b[32m30.0945\u001b[0m  0.0123\n",
      "     42       \u001b[36m32.1565\u001b[0m       \u001b[32m30.0882\u001b[0m  0.0124\n",
      "     43       \u001b[36m32.1532\u001b[0m       \u001b[32m30.0857\u001b[0m  0.0120\n",
      "     44       \u001b[36m32.1506\u001b[0m       \u001b[32m30.0775\u001b[0m  0.0130\n",
      "     45       \u001b[36m32.1476\u001b[0m       \u001b[32m30.0747\u001b[0m  0.0138\n",
      "     46       \u001b[36m32.1452\u001b[0m       \u001b[32m30.0692\u001b[0m  0.0139\n",
      "     47       \u001b[36m32.1426\u001b[0m       \u001b[32m30.0686\u001b[0m  0.0131\n",
      "     48       \u001b[36m32.1402\u001b[0m       \u001b[32m30.0590\u001b[0m  0.0113\n",
      "     49       \u001b[36m32.1376\u001b[0m       \u001b[32m30.0570\u001b[0m  0.0177\n",
      "     50       \u001b[36m32.1355\u001b[0m       \u001b[32m30.0489\u001b[0m  0.0138\n",
      "     51       \u001b[36m32.1332\u001b[0m       30.0499  0.0128\n",
      "     52       \u001b[36m32.1313\u001b[0m       \u001b[32m30.0397\u001b[0m  0.0127\n",
      "     53       \u001b[36m32.1289\u001b[0m       30.0443  0.0133\n",
      "     54       \u001b[36m32.1272\u001b[0m       \u001b[32m30.0337\u001b[0m  0.0142\n",
      "     55       \u001b[36m32.1250\u001b[0m       30.0431  0.0123\n",
      "     56       \u001b[36m32.1233\u001b[0m       \u001b[32m30.0296\u001b[0m  0.0131\n",
      "     57       \u001b[36m32.1212\u001b[0m       30.0414  0.0122\n",
      "     58       \u001b[36m32.1195\u001b[0m       \u001b[32m30.0234\u001b[0m  0.0119\n",
      "     59       \u001b[36m32.1178\u001b[0m       30.0405  0.0119\n",
      "     60       \u001b[36m32.1162\u001b[0m       \u001b[32m30.0187\u001b[0m  0.0118\n",
      "     61       \u001b[36m32.1146\u001b[0m       30.0395  0.0117\n",
      "     62       \u001b[36m32.1130\u001b[0m       \u001b[32m30.0110\u001b[0m  0.0117\n",
      "     63       \u001b[36m32.1117\u001b[0m       30.0418  0.0119\n",
      "     64       \u001b[36m32.1107\u001b[0m       \u001b[32m30.0019\u001b[0m  0.0116\n",
      "     65       \u001b[36m32.1096\u001b[0m       30.0492  0.0120\n",
      "     66       \u001b[36m32.1095\u001b[0m       \u001b[32m29.9921\u001b[0m  0.0116\n",
      "     67       \u001b[36m32.1090\u001b[0m       30.0733  0.0119\n",
      "     68       32.1106       \u001b[32m29.9855\u001b[0m  0.0118\n",
      "     69       32.1117       30.1066  0.0116\n",
      "     70       32.1164       \u001b[32m29.9822\u001b[0m  0.0115\n",
      "     71       32.1212       30.1112  0.0115\n",
      "     72       32.1229       \u001b[32m29.9641\u001b[0m  0.0117\n",
      "     73       32.1197       30.0277  0.0118\n",
      "     74       32.1158       \u001b[32m29.9547\u001b[0m  0.0117\n",
      "     75       \u001b[36m32.1041\u001b[0m       29.9744  0.0116\n",
      "     76       \u001b[36m32.1012\u001b[0m       29.9815  0.0117\n",
      "     77       \u001b[36m32.0930\u001b[0m       29.9646  0.0113\n",
      "     78       \u001b[36m32.0927\u001b[0m       30.0043  0.0113\n",
      "     79       \u001b[36m32.0910\u001b[0m       29.9747  0.0119\n",
      "     80       \u001b[36m32.0894\u001b[0m       30.0027  0.0116\n",
      "     81       \u001b[36m32.0892\u001b[0m       29.9816  0.0117\n",
      "     82       \u001b[36m32.0866\u001b[0m       29.9974  0.0140\n",
      "     83       32.0868       29.9833  0.0130\n",
      "     84       \u001b[36m32.0843\u001b[0m       29.9909  0.0136\n",
      "     85       \u001b[36m32.0842\u001b[0m       29.9827  0.0134\n",
      "     86       \u001b[36m32.0824\u001b[0m       29.9872  0.0118\n",
      "     87       \u001b[36m32.0818\u001b[0m       29.9800  0.0115\n",
      "     88       \u001b[36m32.0804\u001b[0m       29.9838  0.0113\n",
      "     89       \u001b[36m32.0799\u001b[0m       29.9817  0.0121\n",
      "     90       \u001b[36m32.0786\u001b[0m       29.9850  0.0115\n",
      "     91       \u001b[36m32.0779\u001b[0m       29.9808  0.0119\n",
      "     92       \u001b[36m32.0769\u001b[0m       29.9838  0.0116\n",
      "     93       \u001b[36m32.0761\u001b[0m       29.9798  0.0117\n",
      "     94       \u001b[36m32.0751\u001b[0m       29.9830  0.0121\n",
      "     95       \u001b[36m32.0744\u001b[0m       29.9797  0.0119\n",
      "     96       \u001b[36m32.0733\u001b[0m       29.9814  0.0119\n",
      "     97       \u001b[36m32.0726\u001b[0m       29.9761  0.0119\n",
      "     98       \u001b[36m32.0717\u001b[0m       29.9818  0.0117\n",
      "     99       \u001b[36m32.0709\u001b[0m       29.9771  0.0121\n",
      "    100       \u001b[36m32.0700\u001b[0m       29.9829  0.0116\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m32.1147\u001b[0m       \u001b[32m30.4223\u001b[0m  0.0117\n",
      "      2       \u001b[36m30.0273\u001b[0m       \u001b[32m28.7868\u001b[0m  0.0111\n",
      "      3       \u001b[36m27.7155\u001b[0m       \u001b[32m27.1001\u001b[0m  0.0111\n",
      "      4       \u001b[36m25.2436\u001b[0m       27.5969  0.0121\n",
      "      5       \u001b[36m24.9652\u001b[0m       28.1669  0.0116\n",
      "      6       \u001b[36m24.6214\u001b[0m       \u001b[32m26.8958\u001b[0m  0.0118\n",
      "      7       \u001b[36m24.2288\u001b[0m       \u001b[32m26.4789\u001b[0m  0.0113\n",
      "      8       \u001b[36m24.1213\u001b[0m       26.5300  0.0114\n",
      "      9       \u001b[36m23.9036\u001b[0m       26.8166  0.0121\n",
      "     10       \u001b[36m23.7058\u001b[0m       27.1176  0.0119\n",
      "     11       \u001b[36m23.6188\u001b[0m       27.0339  0.0118\n",
      "     12       \u001b[36m23.5209\u001b[0m       26.8862  0.0117\n",
      "     13       \u001b[36m23.4690\u001b[0m       26.8979  0.0116\n",
      "     14       \u001b[36m23.4084\u001b[0m       26.9981  0.0125\n",
      "     15       \u001b[36m23.3528\u001b[0m       27.0428  0.0114\n",
      "     16       \u001b[36m23.3099\u001b[0m       26.9709  0.0117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m23.2765\u001b[0m       26.8899  0.0118\n",
      "     18       \u001b[36m23.2465\u001b[0m       26.8749  0.0114\n",
      "     19       \u001b[36m23.2199\u001b[0m       26.8997  0.0117\n",
      "     20       \u001b[36m23.1945\u001b[0m       26.8987  0.0116\n",
      "     21       \u001b[36m23.1724\u001b[0m       26.8717  0.0117\n",
      "     22       \u001b[36m23.1551\u001b[0m       26.8497  0.0114\n",
      "     23       \u001b[36m23.1399\u001b[0m       26.8392  0.0111\n",
      "     24       \u001b[36m23.1265\u001b[0m       26.8167  0.0122\n",
      "     25       \u001b[36m23.1146\u001b[0m       26.7799  0.0120\n",
      "     26       \u001b[36m23.1039\u001b[0m       26.7540  0.0121\n",
      "     27       \u001b[36m23.0963\u001b[0m       26.7293  0.0138\n",
      "     28       \u001b[36m23.0883\u001b[0m       26.7097  0.0169\n",
      "     29       23.0921       26.7028  0.0125\n",
      "     30       23.0954       26.6799  0.0124\n",
      "     31       23.1130       26.6970  0.0126\n",
      "     32       23.1188       26.6142  0.0136\n",
      "     33       \u001b[36m23.0779\u001b[0m       26.6138  0.0131\n",
      "     34       \u001b[36m23.0650\u001b[0m       26.6501  0.0131\n",
      "     35       \u001b[36m23.0564\u001b[0m       26.6202  0.0122\n",
      "     36       23.0589       26.6140  0.0124\n",
      "     37       \u001b[36m23.0514\u001b[0m       26.6049  0.0121\n",
      "     38       \u001b[36m23.0470\u001b[0m       26.6001  0.0119\n",
      "     39       \u001b[36m23.0441\u001b[0m       26.6021  0.0121\n",
      "     40       \u001b[36m23.0401\u001b[0m       26.5848  0.0118\n",
      "     41       \u001b[36m23.0390\u001b[0m       26.5789  0.0121\n",
      "     42       \u001b[36m23.0355\u001b[0m       26.5760  0.0118\n",
      "     43       \u001b[36m23.0332\u001b[0m       26.5714  0.0114\n",
      "     44       \u001b[36m23.0308\u001b[0m       26.5660  0.0125\n",
      "     45       \u001b[36m23.0286\u001b[0m       26.5595  0.0120\n",
      "     46       \u001b[36m23.0271\u001b[0m       26.5578  0.0120\n",
      "     47       \u001b[36m23.0250\u001b[0m       26.5545  0.0141\n",
      "     48       \u001b[36m23.0236\u001b[0m       26.5541  0.0120\n",
      "     49       \u001b[36m23.0217\u001b[0m       26.5521  0.0127\n",
      "     50       \u001b[36m23.0204\u001b[0m       26.5485  0.0147\n",
      "     51       \u001b[36m23.0188\u001b[0m       26.5429  0.0129\n",
      "     52       \u001b[36m23.0176\u001b[0m       26.5428  0.0120\n",
      "     53       \u001b[36m23.0160\u001b[0m       26.5427  0.0122\n",
      "     54       \u001b[36m23.0149\u001b[0m       26.5388  0.0121\n",
      "     55       \u001b[36m23.0134\u001b[0m       26.5372  0.0121\n",
      "     56       \u001b[36m23.0124\u001b[0m       26.5389  0.0122\n",
      "     57       \u001b[36m23.0108\u001b[0m       26.5363  0.0125\n",
      "     58       \u001b[36m23.0100\u001b[0m       26.5352  0.0118\n",
      "     59       \u001b[36m23.0087\u001b[0m       26.5337  0.0122\n",
      "     60       \u001b[36m23.0079\u001b[0m       26.5348  0.0122\n",
      "     61       \u001b[36m23.0064\u001b[0m       26.5299  0.0117\n",
      "     62       \u001b[36m23.0059\u001b[0m       26.5289  0.0113\n",
      "     63       \u001b[36m23.0045\u001b[0m       26.5283  0.0118\n",
      "     64       \u001b[36m23.0042\u001b[0m       26.5320  0.0126\n",
      "     65       \u001b[36m23.0025\u001b[0m       26.5278  0.0123\n",
      "     66       23.0029       26.5293  0.0123\n",
      "     67       \u001b[36m23.0009\u001b[0m       26.5254  0.0126\n",
      "     68       23.0021       26.5327  0.0124\n",
      "     69       \u001b[36m22.9996\u001b[0m       26.5250  0.0125\n",
      "     70       23.0023       26.5336  0.0121\n",
      "     71       22.9996       26.5250  0.0122\n",
      "     72       23.0050       26.5392  0.0126\n",
      "     73       23.0027       26.5258  0.0117\n",
      "     74       23.0100       26.5458  0.0118\n",
      "     75       23.0076       26.5252  0.0124\n",
      "     76       23.0120       26.5440  0.0123\n",
      "     77       23.0072       26.5212  0.0122\n",
      "     78       23.0080       26.5336  0.0119\n",
      "     79       23.0000       26.5243  0.0118\n",
      "     80       \u001b[36m22.9990\u001b[0m       26.5287  0.0121\n",
      "     81       \u001b[36m22.9929\u001b[0m       26.5292  0.0119\n",
      "     82       \u001b[36m22.9922\u001b[0m       26.5271  0.0120\n",
      "     83       \u001b[36m22.9906\u001b[0m       26.5308  0.0117\n",
      "     84       \u001b[36m22.9896\u001b[0m       26.5272  0.0116\n",
      "     85       \u001b[36m22.9896\u001b[0m       26.5300  0.0128\n",
      "     86       \u001b[36m22.9884\u001b[0m       26.5280  0.0135\n",
      "     87       22.9885       26.5301  0.0125\n",
      "     88       \u001b[36m22.9874\u001b[0m       26.5282  0.0126\n",
      "     89       22.9875       26.5307  0.0127\n",
      "     90       \u001b[36m22.9864\u001b[0m       26.5307  0.0134\n",
      "     91       \u001b[36m22.9863\u001b[0m       26.5318  0.0132\n",
      "     92       \u001b[36m22.9854\u001b[0m       26.5284  0.0123\n",
      "     93       \u001b[36m22.9853\u001b[0m       26.5302  0.0119\n",
      "     94       \u001b[36m22.9845\u001b[0m       26.5315  0.0119\n",
      "     95       \u001b[36m22.9842\u001b[0m       26.5316  0.0122\n",
      "     96       \u001b[36m22.9835\u001b[0m       26.5299  0.0122\n",
      "     97       \u001b[36m22.9834\u001b[0m       26.5333  0.0122\n",
      "     98       \u001b[36m22.9827\u001b[0m       26.5327  0.0122\n",
      "     99       \u001b[36m22.9824\u001b[0m       26.5335  0.0121\n",
      "    100       \u001b[36m22.9818\u001b[0m       26.5334  0.0121\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m38.6784\u001b[0m       \u001b[32m29.9708\u001b[0m  0.0134\n",
      "      2       \u001b[36m36.0841\u001b[0m       \u001b[32m28.0997\u001b[0m  0.0123\n",
      "      3       \u001b[36m32.9065\u001b[0m       \u001b[32m26.8571\u001b[0m  0.0181\n",
      "      4       \u001b[36m30.5020\u001b[0m       29.6772  0.0146\n",
      "      5       30.9433       28.7119  0.0120\n",
      "      6       \u001b[36m29.9400\u001b[0m       26.9111  0.0123\n",
      "      7       \u001b[36m29.5563\u001b[0m       \u001b[32m26.6367\u001b[0m  0.0131\n",
      "      8       \u001b[36m29.3433\u001b[0m       26.9900  0.0140\n",
      "      9       \u001b[36m29.1257\u001b[0m       27.7846  0.0125\n",
      "     10       29.1264       27.9043  0.0129\n",
      "     11       \u001b[36m29.0103\u001b[0m       27.4562  0.0125\n",
      "     12       \u001b[36m28.8627\u001b[0m       27.2214  0.0125\n",
      "     13       \u001b[36m28.7888\u001b[0m       27.2636  0.0126\n",
      "     14       \u001b[36m28.7390\u001b[0m       27.3839  0.0123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     15       \u001b[36m28.7037\u001b[0m       27.4259  0.0140\n",
      "     16       \u001b[36m28.6608\u001b[0m       27.3333  0.0131\n",
      "     17       \u001b[36m28.6157\u001b[0m       27.2714  0.0125\n",
      "     18       \u001b[36m28.5847\u001b[0m       27.3139  0.0134\n",
      "     19       \u001b[36m28.5673\u001b[0m       27.3275  0.0127\n",
      "     20       \u001b[36m28.5489\u001b[0m       27.2969  0.0122\n",
      "     21       \u001b[36m28.5302\u001b[0m       27.2435  0.0123\n",
      "     22       \u001b[36m28.5141\u001b[0m       27.2246  0.0123\n",
      "     23       \u001b[36m28.5027\u001b[0m       27.2257  0.0124\n",
      "     24       \u001b[36m28.4923\u001b[0m       27.2207  0.0120\n",
      "     25       \u001b[36m28.4826\u001b[0m       27.2041  0.0121\n",
      "     26       \u001b[36m28.4729\u001b[0m       27.1956  0.0116\n",
      "     27       \u001b[36m28.4653\u001b[0m       27.1970  0.0118\n",
      "     28       \u001b[36m28.4588\u001b[0m       27.2014  0.0121\n",
      "     29       \u001b[36m28.4528\u001b[0m       27.1989  0.0125\n",
      "     30       \u001b[36m28.4470\u001b[0m       27.2015  0.0119\n",
      "     31       \u001b[36m28.4418\u001b[0m       27.2051  0.0114\n",
      "     32       \u001b[36m28.4374\u001b[0m       27.2084  0.0114\n",
      "     33       \u001b[36m28.4330\u001b[0m       27.2168  0.0116\n",
      "     34       \u001b[36m28.4294\u001b[0m       27.2141  0.0122\n",
      "     35       \u001b[36m28.4261\u001b[0m       27.2142  0.0123\n",
      "     36       \u001b[36m28.4226\u001b[0m       27.2249  0.0121\n",
      "     37       \u001b[36m28.4202\u001b[0m       27.2148  0.0118\n",
      "     38       \u001b[36m28.4186\u001b[0m       27.2173  0.0125\n",
      "     39       \u001b[36m28.4155\u001b[0m       27.2459  0.0120\n",
      "     40       28.4201       27.2079  0.0125\n",
      "     41       28.4262       27.2174  0.0121\n",
      "     42       28.4300       27.3127  0.0121\n",
      "     43       28.4781       27.1676  0.0124\n",
      "     44       28.4300       27.2769  0.0120\n",
      "     45       28.4238       27.2188  0.0123\n",
      "     46       \u001b[36m28.4080\u001b[0m       27.2426  0.0119\n",
      "     47       28.4106       27.2003  0.0120\n",
      "     48       \u001b[36m28.3999\u001b[0m       27.2463  0.0123\n",
      "     49       28.4054       27.2260  0.0120\n",
      "     50       \u001b[36m28.3974\u001b[0m       27.2328  0.0118\n",
      "     51       28.3996       27.2195  0.0126\n",
      "     52       \u001b[36m28.3942\u001b[0m       27.2345  0.0117\n",
      "     53       28.3969       27.2196  0.0118\n",
      "     54       \u001b[36m28.3920\u001b[0m       27.2317  0.0119\n",
      "     55       28.3937       27.2206  0.0120\n",
      "     56       \u001b[36m28.3902\u001b[0m       27.2283  0.0124\n",
      "     57       28.3912       27.2152  0.0124\n",
      "     58       \u001b[36m28.3883\u001b[0m       27.2255  0.0125\n",
      "     59       28.3887       27.2178  0.0124\n",
      "     60       \u001b[36m28.3870\u001b[0m       27.2197  0.0120\n",
      "     61       \u001b[36m28.3863\u001b[0m       27.2125  0.0118\n",
      "     62       \u001b[36m28.3856\u001b[0m       27.2162  0.0122\n",
      "     63       \u001b[36m28.3847\u001b[0m       27.2159  0.0124\n",
      "     64       \u001b[36m28.3843\u001b[0m       27.2117  0.0118\n",
      "     65       \u001b[36m28.3833\u001b[0m       27.2138  0.0124\n",
      "     66       \u001b[36m28.3829\u001b[0m       27.2147  0.0125\n",
      "     67       \u001b[36m28.3823\u001b[0m       27.2129  0.0121\n",
      "     68       \u001b[36m28.3815\u001b[0m       27.2137  0.0120\n",
      "     69       \u001b[36m28.3810\u001b[0m       27.2134  0.0120\n",
      "     70       \u001b[36m28.3806\u001b[0m       27.2154  0.0125\n",
      "     71       \u001b[36m28.3797\u001b[0m       27.2131  0.0117\n",
      "     72       \u001b[36m28.3794\u001b[0m       27.2148  0.0122\n",
      "     73       \u001b[36m28.3787\u001b[0m       27.2131  0.0120\n",
      "     74       \u001b[36m28.3780\u001b[0m       27.2153  0.0119\n",
      "     75       \u001b[36m28.3777\u001b[0m       27.2139  0.0124\n",
      "     76       \u001b[36m28.3771\u001b[0m       27.2142  0.0129\n",
      "     77       \u001b[36m28.3764\u001b[0m       27.2152  0.0119\n",
      "     78       \u001b[36m28.3763\u001b[0m       27.2177  0.0120\n",
      "     79       \u001b[36m28.3756\u001b[0m       27.2171  0.0135\n",
      "     80       \u001b[36m28.3752\u001b[0m       27.2201  0.0168\n",
      "     81       28.3754       27.2165  0.0187\n",
      "     82       28.3756       27.2198  0.0202\n",
      "     83       \u001b[36m28.3749\u001b[0m       27.2148  0.0234\n",
      "     84       \u001b[36m28.3739\u001b[0m       27.2136  0.0240\n",
      "     85       \u001b[36m28.3733\u001b[0m       27.2218  0.0142\n",
      "     86       28.3736       27.2133  0.0120\n",
      "     87       \u001b[36m28.3732\u001b[0m       27.2129  0.0118\n",
      "     88       \u001b[36m28.3721\u001b[0m       27.2325  0.0117\n",
      "     89       28.3732       27.2060  0.0119\n",
      "     90       28.3733       27.2174  0.0120\n",
      "     91       28.3735       27.2504  0.0118\n",
      "     92       28.3761       27.1985  0.0118\n",
      "     93       28.3799       27.2279  0.0119\n",
      "     94       28.3847       27.2687  0.0120\n",
      "     95       28.3895       27.1874  0.0119\n",
      "     96       28.3875       27.2577  0.0118\n",
      "     97       28.3910       27.2310  0.0114\n",
      "     98       28.3931       27.1785  0.0113\n",
      "     99       28.3864       27.3031  0.0121\n",
      "    100       28.3998       27.1778  0.0119\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m44.1095\u001b[0m       \u001b[32m45.2156\u001b[0m  0.0121\n",
      "      2       \u001b[36m42.4178\u001b[0m       \u001b[32m43.1558\u001b[0m  0.0107\n",
      "      3       \u001b[36m40.7658\u001b[0m       \u001b[32m41.0760\u001b[0m  0.0110\n",
      "      4       \u001b[36m39.1115\u001b[0m       \u001b[32m38.9425\u001b[0m  0.0114\n",
      "      5       \u001b[36m37.4683\u001b[0m       \u001b[32m36.8125\u001b[0m  0.0108\n",
      "      6       \u001b[36m35.9268\u001b[0m       \u001b[32m34.8242\u001b[0m  0.0107\n",
      "      7       \u001b[36m34.6268\u001b[0m       \u001b[32m33.1696\u001b[0m  0.0106\n",
      "      8       \u001b[36m33.6987\u001b[0m       \u001b[32m31.9865\u001b[0m  0.0110\n",
      "      9       \u001b[36m33.1579\u001b[0m       \u001b[32m31.2539\u001b[0m  0.0110\n",
      "     10       \u001b[36m32.8940\u001b[0m       \u001b[32m30.8381\u001b[0m  0.0109\n",
      "     11       \u001b[36m32.7724\u001b[0m       \u001b[32m30.6056\u001b[0m  0.0109\n",
      "     12       \u001b[36m32.7082\u001b[0m       \u001b[32m30.4699\u001b[0m  0.0106\n",
      "     13       \u001b[36m32.6656\u001b[0m       \u001b[32m30.3850\u001b[0m  0.0109\n",
      "     14       \u001b[36m32.6315\u001b[0m       \u001b[32m30.3274\u001b[0m  0.0111\n",
      "     15       \u001b[36m32.6017\u001b[0m       \u001b[32m30.2855\u001b[0m  0.0109\n",
      "     16       \u001b[36m32.5752\u001b[0m       \u001b[32m30.2524\u001b[0m  0.0106\n",
      "     17       \u001b[36m32.5512\u001b[0m       \u001b[32m30.2248\u001b[0m  0.0107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     18       \u001b[36m32.5299\u001b[0m       \u001b[32m30.2012\u001b[0m  0.0119\n",
      "     19       \u001b[36m32.5105\u001b[0m       \u001b[32m30.1801\u001b[0m  0.0115\n",
      "     20       \u001b[36m32.4936\u001b[0m       \u001b[32m30.1608\u001b[0m  0.0108\n",
      "     21       \u001b[36m32.4787\u001b[0m       \u001b[32m30.1439\u001b[0m  0.0106\n",
      "     22       \u001b[36m32.4650\u001b[0m       \u001b[32m30.1286\u001b[0m  0.0106\n",
      "     23       \u001b[36m32.4526\u001b[0m       \u001b[32m30.1147\u001b[0m  0.0110\n",
      "     24       \u001b[36m32.4414\u001b[0m       \u001b[32m30.1025\u001b[0m  0.0112\n",
      "     25       \u001b[36m32.4309\u001b[0m       \u001b[32m30.0912\u001b[0m  0.0111\n",
      "     26       \u001b[36m32.4213\u001b[0m       \u001b[32m30.0807\u001b[0m  0.0109\n",
      "     27       \u001b[36m32.4126\u001b[0m       \u001b[32m30.0715\u001b[0m  0.0107\n",
      "     28       \u001b[36m32.4042\u001b[0m       \u001b[32m30.0622\u001b[0m  0.0109\n",
      "     29       \u001b[36m32.3967\u001b[0m       \u001b[32m30.0542\u001b[0m  0.0110\n",
      "     30       \u001b[36m32.3894\u001b[0m       \u001b[32m30.0462\u001b[0m  0.0112\n",
      "     31       \u001b[36m32.3828\u001b[0m       \u001b[32m30.0392\u001b[0m  0.0114\n",
      "     32       \u001b[36m32.3764\u001b[0m       \u001b[32m30.0320\u001b[0m  0.0106\n",
      "     33       \u001b[36m32.3707\u001b[0m       \u001b[32m30.0257\u001b[0m  0.0110\n",
      "     34       \u001b[36m32.3650\u001b[0m       \u001b[32m30.0193\u001b[0m  0.0108\n",
      "     35       \u001b[36m32.3598\u001b[0m       \u001b[32m30.0133\u001b[0m  0.0108\n",
      "     36       \u001b[36m32.3550\u001b[0m       \u001b[32m30.0082\u001b[0m  0.0106\n",
      "     37       \u001b[36m32.3502\u001b[0m       \u001b[32m30.0026\u001b[0m  0.0108\n",
      "     38       \u001b[36m32.3457\u001b[0m       \u001b[32m29.9975\u001b[0m  0.0111\n",
      "     39       \u001b[36m32.3416\u001b[0m       \u001b[32m29.9928\u001b[0m  0.0110\n",
      "     40       \u001b[36m32.3376\u001b[0m       \u001b[32m29.9878\u001b[0m  0.0109\n",
      "     41       \u001b[36m32.3338\u001b[0m       \u001b[32m29.9831\u001b[0m  0.0108\n",
      "     42       \u001b[36m32.3303\u001b[0m       \u001b[32m29.9786\u001b[0m  0.0107\n",
      "     43       \u001b[36m32.3269\u001b[0m       \u001b[32m29.9740\u001b[0m  0.0111\n",
      "     44       \u001b[36m32.3238\u001b[0m       \u001b[32m29.9703\u001b[0m  0.0109\n",
      "     45       \u001b[36m32.3207\u001b[0m       \u001b[32m29.9663\u001b[0m  0.0110\n",
      "     46       \u001b[36m32.3178\u001b[0m       \u001b[32m29.9625\u001b[0m  0.0107\n",
      "     47       \u001b[36m32.3150\u001b[0m       \u001b[32m29.9589\u001b[0m  0.0112\n",
      "     48       \u001b[36m32.3124\u001b[0m       \u001b[32m29.9557\u001b[0m  0.0114\n",
      "     49       \u001b[36m32.3098\u001b[0m       \u001b[32m29.9526\u001b[0m  0.0112\n",
      "     50       \u001b[36m32.3074\u001b[0m       \u001b[32m29.9494\u001b[0m  0.0105\n",
      "     51       \u001b[36m32.3051\u001b[0m       \u001b[32m29.9465\u001b[0m  0.0105\n",
      "     52       \u001b[36m32.3028\u001b[0m       \u001b[32m29.9438\u001b[0m  0.0107\n",
      "     53       \u001b[36m32.3006\u001b[0m       \u001b[32m29.9411\u001b[0m  0.0109\n",
      "     54       \u001b[36m32.2985\u001b[0m       \u001b[32m29.9385\u001b[0m  0.0110\n",
      "     55       \u001b[36m32.2964\u001b[0m       \u001b[32m29.9359\u001b[0m  0.0108\n",
      "     56       \u001b[36m32.2944\u001b[0m       \u001b[32m29.9334\u001b[0m  0.0109\n",
      "     57       \u001b[36m32.2924\u001b[0m       \u001b[32m29.9308\u001b[0m  0.0107\n",
      "     58       \u001b[36m32.2905\u001b[0m       \u001b[32m29.9285\u001b[0m  0.0112\n",
      "     59       \u001b[36m32.2886\u001b[0m       \u001b[32m29.9260\u001b[0m  0.0111\n",
      "     60       \u001b[36m32.2868\u001b[0m       \u001b[32m29.9239\u001b[0m  0.0149\n",
      "     61       \u001b[36m32.2850\u001b[0m       \u001b[32m29.9217\u001b[0m  0.0169\n",
      "     62       \u001b[36m32.2834\u001b[0m       \u001b[32m29.9195\u001b[0m  0.0118\n",
      "     63       \u001b[36m32.2817\u001b[0m       \u001b[32m29.9174\u001b[0m  0.0125\n",
      "     64       \u001b[36m32.2801\u001b[0m       \u001b[32m29.9153\u001b[0m  0.0117\n",
      "     65       \u001b[36m32.2785\u001b[0m       \u001b[32m29.9135\u001b[0m  0.0198\n",
      "     66       \u001b[36m32.2770\u001b[0m       \u001b[32m29.9114\u001b[0m  0.0111\n",
      "     67       \u001b[36m32.2755\u001b[0m       \u001b[32m29.9096\u001b[0m  0.0152\n",
      "     68       \u001b[36m32.2740\u001b[0m       \u001b[32m29.9078\u001b[0m  0.0111\n",
      "     69       \u001b[36m32.2726\u001b[0m       \u001b[32m29.9059\u001b[0m  0.0112\n",
      "     70       \u001b[36m32.2712\u001b[0m       \u001b[32m29.9042\u001b[0m  0.0108\n",
      "     71       \u001b[36m32.2699\u001b[0m       \u001b[32m29.9026\u001b[0m  0.0118\n",
      "     72       \u001b[36m32.2686\u001b[0m       \u001b[32m29.9009\u001b[0m  0.0109\n",
      "     73       \u001b[36m32.2673\u001b[0m       \u001b[32m29.8995\u001b[0m  0.0110\n",
      "     74       \u001b[36m32.2660\u001b[0m       \u001b[32m29.8979\u001b[0m  0.0110\n",
      "     75       \u001b[36m32.2648\u001b[0m       \u001b[32m29.8965\u001b[0m  0.0108\n",
      "     76       \u001b[36m32.2635\u001b[0m       \u001b[32m29.8951\u001b[0m  0.0112\n",
      "     77       \u001b[36m32.2623\u001b[0m       \u001b[32m29.8936\u001b[0m  0.0111\n",
      "     78       \u001b[36m32.2612\u001b[0m       \u001b[32m29.8921\u001b[0m  0.0108\n",
      "     79       \u001b[36m32.2600\u001b[0m       \u001b[32m29.8910\u001b[0m  0.0109\n",
      "     80       \u001b[36m32.2589\u001b[0m       \u001b[32m29.8896\u001b[0m  0.0106\n",
      "     81       \u001b[36m32.2578\u001b[0m       \u001b[32m29.8884\u001b[0m  0.0109\n",
      "     82       \u001b[36m32.2567\u001b[0m       \u001b[32m29.8871\u001b[0m  0.0110\n",
      "     83       \u001b[36m32.2557\u001b[0m       \u001b[32m29.8860\u001b[0m  0.0110\n",
      "     84       \u001b[36m32.2547\u001b[0m       \u001b[32m29.8848\u001b[0m  0.0107\n",
      "     85       \u001b[36m32.2537\u001b[0m       \u001b[32m29.8835\u001b[0m  0.0108\n",
      "     86       \u001b[36m32.2528\u001b[0m       \u001b[32m29.8823\u001b[0m  0.0110\n",
      "     87       \u001b[36m32.2518\u001b[0m       \u001b[32m29.8812\u001b[0m  0.0109\n",
      "     88       \u001b[36m32.2509\u001b[0m       \u001b[32m29.8800\u001b[0m  0.0109\n",
      "     89       \u001b[36m32.2499\u001b[0m       \u001b[32m29.8789\u001b[0m  0.0108\n",
      "     90       \u001b[36m32.2490\u001b[0m       \u001b[32m29.8779\u001b[0m  0.0106\n",
      "     91       \u001b[36m32.2481\u001b[0m       \u001b[32m29.8769\u001b[0m  0.0109\n",
      "     92       \u001b[36m32.2473\u001b[0m       \u001b[32m29.8758\u001b[0m  0.0109\n",
      "     93       \u001b[36m32.2464\u001b[0m       \u001b[32m29.8749\u001b[0m  0.0110\n",
      "     94       \u001b[36m32.2456\u001b[0m       \u001b[32m29.8739\u001b[0m  0.0107\n",
      "     95       \u001b[36m32.2448\u001b[0m       \u001b[32m29.8729\u001b[0m  0.0105\n",
      "     96       \u001b[36m32.2440\u001b[0m       \u001b[32m29.8718\u001b[0m  0.0108\n",
      "     97       \u001b[36m32.2432\u001b[0m       \u001b[32m29.8709\u001b[0m  0.0114\n",
      "     98       \u001b[36m32.2424\u001b[0m       \u001b[32m29.8700\u001b[0m  0.0109\n",
      "     99       \u001b[36m32.2417\u001b[0m       \u001b[32m29.8692\u001b[0m  0.0108\n",
      "    100       \u001b[36m32.2409\u001b[0m       \u001b[32m29.8682\u001b[0m  0.0107\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m33.5411\u001b[0m       \u001b[32m31.7188\u001b[0m  0.0110\n",
      "      2       \u001b[36m32.2893\u001b[0m       \u001b[32m30.8119\u001b[0m  0.0112\n",
      "      3       \u001b[36m31.1141\u001b[0m       \u001b[32m29.9553\u001b[0m  0.0115\n",
      "      4       \u001b[36m29.9754\u001b[0m       \u001b[32m29.1393\u001b[0m  0.0114\n",
      "      5       \u001b[36m28.8591\u001b[0m       \u001b[32m28.3661\u001b[0m  0.0109\n",
      "      6       \u001b[36m27.7658\u001b[0m       \u001b[32m27.6460\u001b[0m  0.0106\n",
      "      7       \u001b[36m26.7130\u001b[0m       \u001b[32m27.0178\u001b[0m  0.0110\n",
      "      8       \u001b[36m25.7501\u001b[0m       \u001b[32m26.5309\u001b[0m  0.0110\n",
      "      9       \u001b[36m24.9392\u001b[0m       \u001b[32m26.2249\u001b[0m  0.0109\n",
      "     10       \u001b[36m24.3281\u001b[0m       \u001b[32m26.0969\u001b[0m  0.0107\n",
      "     11       \u001b[36m23.9212\u001b[0m       26.0975  0.0106\n",
      "     12       \u001b[36m23.6792\u001b[0m       26.1598  0.0109\n",
      "     13       \u001b[36m23.5445\u001b[0m       26.2331  0.0111\n",
      "     14       \u001b[36m23.4691\u001b[0m       26.2932  0.0107\n",
      "     15       \u001b[36m23.4233\u001b[0m       26.3345  0.0106\n",
      "     16       \u001b[36m23.3920\u001b[0m       26.3599  0.0112\n",
      "     17       \u001b[36m23.3680\u001b[0m       26.3742  0.0112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     18       \u001b[36m23.3480\u001b[0m       26.3812  0.0115\n",
      "     19       \u001b[36m23.3308\u001b[0m       26.3839  0.0111\n",
      "     20       \u001b[36m23.3154\u001b[0m       26.3837  0.0108\n",
      "     21       \u001b[36m23.3015\u001b[0m       26.3822  0.0108\n",
      "     22       \u001b[36m23.2888\u001b[0m       26.3800  0.0109\n",
      "     23       \u001b[36m23.2770\u001b[0m       26.3774  0.0111\n",
      "     24       \u001b[36m23.2661\u001b[0m       26.3747  0.0111\n",
      "     25       \u001b[36m23.2559\u001b[0m       26.3720  0.0107\n",
      "     26       \u001b[36m23.2464\u001b[0m       26.3694  0.0106\n",
      "     27       \u001b[36m23.2376\u001b[0m       26.3674  0.0108\n",
      "     28       \u001b[36m23.2294\u001b[0m       26.3653  0.0111\n",
      "     29       \u001b[36m23.2218\u001b[0m       26.3634  0.0108\n",
      "     30       \u001b[36m23.2146\u001b[0m       26.3616  0.0107\n",
      "     31       \u001b[36m23.2078\u001b[0m       26.3601  0.0109\n",
      "     32       \u001b[36m23.2015\u001b[0m       26.3587  0.0108\n",
      "     33       \u001b[36m23.1956\u001b[0m       26.3576  0.0111\n",
      "     34       \u001b[36m23.1900\u001b[0m       26.3565  0.0109\n",
      "     35       \u001b[36m23.1846\u001b[0m       26.3555  0.0110\n",
      "     36       \u001b[36m23.1795\u001b[0m       26.3545  0.0108\n",
      "     37       \u001b[36m23.1746\u001b[0m       26.3536  0.0105\n",
      "     38       \u001b[36m23.1700\u001b[0m       26.3529  0.0111\n",
      "     39       \u001b[36m23.1655\u001b[0m       26.3524  0.0111\n",
      "     40       \u001b[36m23.1612\u001b[0m       26.3519  0.0110\n",
      "     41       \u001b[36m23.1571\u001b[0m       26.3515  0.0109\n",
      "     42       \u001b[36m23.1532\u001b[0m       26.3511  0.0108\n",
      "     43       \u001b[36m23.1495\u001b[0m       26.3508  0.0114\n",
      "     44       \u001b[36m23.1459\u001b[0m       26.3506  0.0174\n",
      "     45       \u001b[36m23.1424\u001b[0m       26.3504  0.0136\n",
      "     46       \u001b[36m23.1391\u001b[0m       26.3504  0.0113\n",
      "     47       \u001b[36m23.1360\u001b[0m       26.3504  0.0142\n",
      "     48       \u001b[36m23.1329\u001b[0m       26.3505  0.0140\n",
      "     49       \u001b[36m23.1301\u001b[0m       26.3507  0.0172\n",
      "     50       \u001b[36m23.1273\u001b[0m       26.3509  0.0138\n",
      "     51       \u001b[36m23.1247\u001b[0m       26.3512  0.0117\n",
      "     52       \u001b[36m23.1221\u001b[0m       26.3514  0.0112\n",
      "     53       \u001b[36m23.1197\u001b[0m       26.3517  0.0111\n",
      "     54       \u001b[36m23.1174\u001b[0m       26.3521  0.0111\n",
      "     55       \u001b[36m23.1151\u001b[0m       26.3524  0.0109\n",
      "     56       \u001b[36m23.1130\u001b[0m       26.3528  0.0109\n",
      "     57       \u001b[36m23.1109\u001b[0m       26.3531  0.0112\n",
      "     58       \u001b[36m23.1089\u001b[0m       26.3535  0.0108\n",
      "     59       \u001b[36m23.1070\u001b[0m       26.3539  0.0108\n",
      "     60       \u001b[36m23.1052\u001b[0m       26.3544  0.0110\n",
      "     61       \u001b[36m23.1035\u001b[0m       26.3549  0.0113\n",
      "     62       \u001b[36m23.1018\u001b[0m       26.3555  0.0111\n",
      "     63       \u001b[36m23.1002\u001b[0m       26.3560  0.0112\n",
      "     64       \u001b[36m23.0987\u001b[0m       26.3566  0.0109\n",
      "     65       \u001b[36m23.0972\u001b[0m       26.3571  0.0108\n",
      "     66       \u001b[36m23.0958\u001b[0m       26.3577  0.0112\n",
      "     67       \u001b[36m23.0945\u001b[0m       26.3583  0.0112\n",
      "     68       \u001b[36m23.0932\u001b[0m       26.3589  0.0118\n",
      "     69       \u001b[36m23.0920\u001b[0m       26.3595  0.0112\n",
      "     70       \u001b[36m23.0908\u001b[0m       26.3601  0.0106\n",
      "     71       \u001b[36m23.0896\u001b[0m       26.3607  0.0109\n",
      "     72       \u001b[36m23.0885\u001b[0m       26.3612  0.0107\n",
      "     73       \u001b[36m23.0875\u001b[0m       26.3617  0.0107\n",
      "     74       \u001b[36m23.0864\u001b[0m       26.3623  0.0106\n",
      "     75       \u001b[36m23.0854\u001b[0m       26.3628  0.0109\n",
      "     76       \u001b[36m23.0844\u001b[0m       26.3632  0.0112\n",
      "     77       \u001b[36m23.0835\u001b[0m       26.3637  0.0109\n",
      "     78       \u001b[36m23.0825\u001b[0m       26.3642  0.0109\n",
      "     79       \u001b[36m23.0816\u001b[0m       26.3646  0.0110\n",
      "     80       \u001b[36m23.0808\u001b[0m       26.3651  0.0107\n",
      "     81       \u001b[36m23.0799\u001b[0m       26.3656  0.0111\n",
      "     82       \u001b[36m23.0791\u001b[0m       26.3660  0.0108\n",
      "     83       \u001b[36m23.0784\u001b[0m       26.3664  0.0108\n",
      "     84       \u001b[36m23.0776\u001b[0m       26.3668  0.0109\n",
      "     85       \u001b[36m23.0769\u001b[0m       26.3673  0.0108\n",
      "     86       \u001b[36m23.0761\u001b[0m       26.3677  0.0113\n",
      "     87       \u001b[36m23.0754\u001b[0m       26.3682  0.0111\n",
      "     88       \u001b[36m23.0748\u001b[0m       26.3686  0.0108\n",
      "     89       \u001b[36m23.0741\u001b[0m       26.3690  0.0105\n",
      "     90       \u001b[36m23.0735\u001b[0m       26.3694  0.0106\n",
      "     91       \u001b[36m23.0729\u001b[0m       26.3697  0.0107\n",
      "     92       \u001b[36m23.0723\u001b[0m       26.3701  0.0107\n",
      "     93       \u001b[36m23.0717\u001b[0m       26.3704  0.0108\n",
      "     94       \u001b[36m23.0711\u001b[0m       26.3707  0.0106\n",
      "     95       \u001b[36m23.0706\u001b[0m       26.3710  0.0105\n",
      "     96       \u001b[36m23.0700\u001b[0m       26.3713  0.0110\n",
      "     97       \u001b[36m23.0695\u001b[0m       26.3716  0.0112\n",
      "     98       \u001b[36m23.0690\u001b[0m       26.3719  0.0110\n",
      "     99       \u001b[36m23.0684\u001b[0m       26.3722  0.0106\n",
      "    100       \u001b[36m23.0679\u001b[0m       26.3725  0.0107\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m39.6366\u001b[0m       \u001b[32m31.0726\u001b[0m  0.0117\n",
      "      2       \u001b[36m38.2669\u001b[0m       \u001b[32m30.2313\u001b[0m  0.0108\n",
      "      3       \u001b[36m37.0733\u001b[0m       \u001b[32m29.5004\u001b[0m  0.0106\n",
      "      4       \u001b[36m35.9961\u001b[0m       \u001b[32m28.8560\u001b[0m  0.0108\n",
      "      5       \u001b[36m35.0078\u001b[0m       \u001b[32m28.2819\u001b[0m  0.0103\n",
      "      6       \u001b[36m34.0918\u001b[0m       \u001b[32m27.7698\u001b[0m  0.0105\n",
      "      7       \u001b[36m33.2363\u001b[0m       \u001b[32m27.3128\u001b[0m  0.0118\n",
      "      8       \u001b[36m32.4312\u001b[0m       \u001b[32m26.9072\u001b[0m  0.0108\n",
      "      9       \u001b[36m31.6669\u001b[0m       \u001b[32m26.5578\u001b[0m  0.0108\n",
      "     10       \u001b[36m30.9470\u001b[0m       \u001b[32m26.2818\u001b[0m  0.0109\n",
      "     11       \u001b[36m30.2929\u001b[0m       \u001b[32m26.1050\u001b[0m  0.0104\n",
      "     12       \u001b[36m29.7379\u001b[0m       \u001b[32m26.0450\u001b[0m  0.0108\n",
      "     13       \u001b[36m29.3031\u001b[0m       26.0959  0.0108\n",
      "     14       \u001b[36m28.9929\u001b[0m       26.2250  0.0111\n",
      "     15       \u001b[36m28.7947\u001b[0m       26.3941  0.0108\n",
      "     16       \u001b[36m28.6813\u001b[0m       26.5611  0.0104\n",
      "     17       \u001b[36m28.6216\u001b[0m       26.7047  0.0106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     18       \u001b[36m28.5916\u001b[0m       26.8160  0.0112\n",
      "     19       \u001b[36m28.5757\u001b[0m       26.8968  0.0112\n",
      "     20       \u001b[36m28.5657\u001b[0m       26.9544  0.0111\n",
      "     21       \u001b[36m28.5580\u001b[0m       26.9938  0.0107\n",
      "     22       \u001b[36m28.5508\u001b[0m       27.0219  0.0113\n",
      "     23       \u001b[36m28.5440\u001b[0m       27.0410  0.0111\n",
      "     24       \u001b[36m28.5372\u001b[0m       27.0558  0.0110\n",
      "     25       \u001b[36m28.5308\u001b[0m       27.0661  0.0112\n",
      "     26       \u001b[36m28.5245\u001b[0m       27.0754  0.0107\n",
      "     27       \u001b[36m28.5186\u001b[0m       27.0819  0.0128\n",
      "     28       \u001b[36m28.5128\u001b[0m       27.0870  0.0157\n",
      "     29       \u001b[36m28.5073\u001b[0m       27.0914  0.0124\n",
      "     30       \u001b[36m28.5022\u001b[0m       27.0953  0.0119\n",
      "     31       \u001b[36m28.4974\u001b[0m       27.0988  0.0118\n",
      "     32       \u001b[36m28.4929\u001b[0m       27.1020  0.0117\n",
      "     33       \u001b[36m28.4886\u001b[0m       27.1047  0.0146\n",
      "     34       \u001b[36m28.4845\u001b[0m       27.1074  0.0128\n",
      "     35       \u001b[36m28.4807\u001b[0m       27.1105  0.0116\n",
      "     36       \u001b[36m28.4773\u001b[0m       27.1128  0.0112\n",
      "     37       \u001b[36m28.4739\u001b[0m       27.1152  0.0112\n",
      "     38       \u001b[36m28.4708\u001b[0m       27.1165  0.0123\n",
      "     39       \u001b[36m28.4678\u001b[0m       27.1184  0.0113\n",
      "     40       \u001b[36m28.4650\u001b[0m       27.1202  0.0114\n",
      "     41       \u001b[36m28.4623\u001b[0m       27.1225  0.0109\n",
      "     42       \u001b[36m28.4599\u001b[0m       27.1243  0.0109\n",
      "     43       \u001b[36m28.4576\u001b[0m       27.1263  0.0119\n",
      "     44       \u001b[36m28.4555\u001b[0m       27.1279  0.0117\n",
      "     45       \u001b[36m28.4534\u001b[0m       27.1296  0.0121\n",
      "     46       \u001b[36m28.4515\u001b[0m       27.1312  0.0107\n",
      "     47       \u001b[36m28.4497\u001b[0m       27.1326  0.0106\n",
      "     48       \u001b[36m28.4480\u001b[0m       27.1336  0.0122\n",
      "     49       \u001b[36m28.4464\u001b[0m       27.1348  0.0114\n",
      "     50       \u001b[36m28.4449\u001b[0m       27.1359  0.0113\n",
      "     51       \u001b[36m28.4434\u001b[0m       27.1361  0.0109\n",
      "     52       \u001b[36m28.4420\u001b[0m       27.1370  0.0107\n",
      "     53       \u001b[36m28.4407\u001b[0m       27.1377  0.0123\n",
      "     54       \u001b[36m28.4394\u001b[0m       27.1386  0.0116\n",
      "     55       \u001b[36m28.4382\u001b[0m       27.1394  0.0114\n",
      "     56       \u001b[36m28.4371\u001b[0m       27.1402  0.0106\n",
      "     57       \u001b[36m28.4360\u001b[0m       27.1403  0.0107\n",
      "     58       \u001b[36m28.4350\u001b[0m       27.1411  0.0129\n",
      "     59       \u001b[36m28.4340\u001b[0m       27.1416  0.0120\n",
      "     60       \u001b[36m28.4330\u001b[0m       27.1422  0.0112\n",
      "     61       \u001b[36m28.4322\u001b[0m       27.1426  0.0109\n",
      "     62       \u001b[36m28.4313\u001b[0m       27.1426  0.0111\n",
      "     63       \u001b[36m28.4305\u001b[0m       27.1430  0.0121\n",
      "     64       \u001b[36m28.4297\u001b[0m       27.1429  0.0112\n",
      "     65       \u001b[36m28.4288\u001b[0m       27.1428  0.0114\n",
      "     66       \u001b[36m28.4280\u001b[0m       27.1425  0.0105\n",
      "     67       \u001b[36m28.4272\u001b[0m       27.1427  0.0107\n",
      "     68       \u001b[36m28.4265\u001b[0m       27.1432  0.0122\n",
      "     69       \u001b[36m28.4258\u001b[0m       27.1430  0.0114\n",
      "     70       \u001b[36m28.4251\u001b[0m       27.1437  0.0117\n",
      "     71       \u001b[36m28.4245\u001b[0m       27.1440  0.0108\n",
      "     72       \u001b[36m28.4239\u001b[0m       27.1442  0.0107\n",
      "     73       \u001b[36m28.4233\u001b[0m       27.1445  0.0125\n",
      "     74       \u001b[36m28.4227\u001b[0m       27.1450  0.0116\n",
      "     75       \u001b[36m28.4222\u001b[0m       27.1448  0.0113\n",
      "     76       \u001b[36m28.4216\u001b[0m       27.1453  0.0108\n",
      "     77       \u001b[36m28.4212\u001b[0m       27.1457  0.0107\n",
      "     78       \u001b[36m28.4207\u001b[0m       27.1458  0.0132\n",
      "     79       \u001b[36m28.4202\u001b[0m       27.1461  0.0119\n",
      "     80       \u001b[36m28.4198\u001b[0m       27.1462  0.0112\n",
      "     81       \u001b[36m28.4194\u001b[0m       27.1466  0.0106\n",
      "     82       \u001b[36m28.4189\u001b[0m       27.1469  0.0109\n",
      "     83       \u001b[36m28.4185\u001b[0m       27.1469  0.0124\n",
      "     84       \u001b[36m28.4181\u001b[0m       27.1474  0.0112\n",
      "     85       \u001b[36m28.4178\u001b[0m       27.1473  0.0114\n",
      "     86       \u001b[36m28.4174\u001b[0m       27.1477  0.0108\n",
      "     87       \u001b[36m28.4170\u001b[0m       27.1479  0.0110\n",
      "     88       \u001b[36m28.4167\u001b[0m       27.1480  0.0121\n",
      "     89       \u001b[36m28.4163\u001b[0m       27.1478  0.0113\n",
      "     90       \u001b[36m28.4160\u001b[0m       27.1479  0.0115\n",
      "     91       \u001b[36m28.4156\u001b[0m       27.1479  0.0106\n",
      "     92       \u001b[36m28.4153\u001b[0m       27.1478  0.0106\n",
      "     93       \u001b[36m28.4149\u001b[0m       27.1480  0.0123\n",
      "     94       \u001b[36m28.4146\u001b[0m       27.1483  0.0114\n",
      "     95       \u001b[36m28.4144\u001b[0m       27.1484  0.0118\n",
      "     96       \u001b[36m28.4141\u001b[0m       27.1484  0.0105\n",
      "     97       \u001b[36m28.4138\u001b[0m       27.1487  0.0107\n",
      "     98       \u001b[36m28.4135\u001b[0m       27.1484  0.0120\n",
      "     99       \u001b[36m28.4133\u001b[0m       27.1486  0.0114\n",
      "    100       \u001b[36m28.4130\u001b[0m       27.1483  0.0112\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m42.4010\u001b[0m       \u001b[32m41.2980\u001b[0m  0.0116\n",
      "      2       \u001b[36m38.8781\u001b[0m       \u001b[32m36.4170\u001b[0m  0.0112\n",
      "      3       \u001b[36m35.2929\u001b[0m       \u001b[32m31.8933\u001b[0m  0.0135\n",
      "      4       \u001b[36m34.4592\u001b[0m       \u001b[32m31.2932\u001b[0m  0.0124\n",
      "      5       \u001b[36m33.9138\u001b[0m       31.3744  0.0113\n",
      "      6       \u001b[36m33.3507\u001b[0m       31.6401  0.0112\n",
      "      7       \u001b[36m33.1459\u001b[0m       \u001b[32m31.0802\u001b[0m  0.0134\n",
      "      8       \u001b[36m32.8512\u001b[0m       \u001b[32m30.4513\u001b[0m  0.0139\n",
      "      9       \u001b[36m32.6995\u001b[0m       \u001b[32m30.2302\u001b[0m  0.0168\n",
      "     10       \u001b[36m32.5895\u001b[0m       30.3150  0.0123\n",
      "     11       \u001b[36m32.5107\u001b[0m       30.4017  0.0122\n",
      "     12       \u001b[36m32.4502\u001b[0m       30.2916  0.0127\n",
      "     13       \u001b[36m32.3804\u001b[0m       \u001b[32m30.1785\u001b[0m  0.0128\n",
      "     14       \u001b[36m32.3406\u001b[0m       \u001b[32m30.1782\u001b[0m  0.0142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     15       \u001b[36m32.3119\u001b[0m       30.2007  0.0129\n",
      "     16       \u001b[36m32.2888\u001b[0m       \u001b[32m30.1702\u001b[0m  0.0130\n",
      "     17       \u001b[36m32.2675\u001b[0m       \u001b[32m30.1043\u001b[0m  0.0119\n",
      "     18       \u001b[36m32.2524\u001b[0m       \u001b[32m30.0972\u001b[0m  0.0120\n",
      "     19       \u001b[36m32.2411\u001b[0m       \u001b[32m30.0869\u001b[0m  0.0116\n",
      "     20       \u001b[36m32.2302\u001b[0m       30.0915  0.0117\n",
      "     21       \u001b[36m32.2202\u001b[0m       \u001b[32m30.0473\u001b[0m  0.0120\n",
      "     22       \u001b[36m32.2125\u001b[0m       30.0557  0.0119\n",
      "     23       \u001b[36m32.2047\u001b[0m       \u001b[32m30.0373\u001b[0m  0.0117\n",
      "     24       \u001b[36m32.1983\u001b[0m       30.0520  0.0118\n",
      "     25       \u001b[36m32.1918\u001b[0m       \u001b[32m30.0261\u001b[0m  0.0120\n",
      "     26       \u001b[36m32.1866\u001b[0m       30.0385  0.0125\n",
      "     27       \u001b[36m32.1811\u001b[0m       \u001b[32m30.0195\u001b[0m  0.0115\n",
      "     28       \u001b[36m32.1773\u001b[0m       30.0365  0.0117\n",
      "     29       \u001b[36m32.1710\u001b[0m       \u001b[32m30.0172\u001b[0m  0.0117\n",
      "     30       \u001b[36m32.1675\u001b[0m       30.0225  0.0116\n",
      "     31       \u001b[36m32.1616\u001b[0m       30.0227  0.0117\n",
      "     32       \u001b[36m32.1583\u001b[0m       30.0177  0.0123\n",
      "     33       \u001b[36m32.1554\u001b[0m       30.0280  0.0138\n",
      "     34       \u001b[36m32.1517\u001b[0m       \u001b[32m30.0166\u001b[0m  0.0113\n",
      "     35       \u001b[36m32.1505\u001b[0m       \u001b[32m30.0147\u001b[0m  0.0118\n",
      "     36       32.1512       30.0652  0.0117\n",
      "     37       32.1546       \u001b[32m30.0065\u001b[0m  0.0116\n",
      "     38       \u001b[36m32.1499\u001b[0m       30.0286  0.0119\n",
      "     39       \u001b[36m32.1402\u001b[0m       \u001b[32m29.9938\u001b[0m  0.0118\n",
      "     40       \u001b[36m32.1356\u001b[0m       30.0171  0.0115\n",
      "     41       \u001b[36m32.1344\u001b[0m       30.0006  0.0115\n",
      "     42       \u001b[36m32.1332\u001b[0m       30.0484  0.0117\n",
      "     43       \u001b[36m32.1298\u001b[0m       30.0095  0.0114\n",
      "     44       \u001b[36m32.1291\u001b[0m       30.0509  0.0112\n",
      "     45       \u001b[36m32.1272\u001b[0m       30.0111  0.0115\n",
      "     46       32.1275       30.0878  0.0116\n",
      "     47       32.1277       30.0210  0.0126\n",
      "     48       32.1305       30.1328  0.0118\n",
      "     49       32.1343       30.0365  0.0118\n",
      "     50       32.1414       30.1943  0.0120\n",
      "     51       32.1495       30.0431  0.0116\n",
      "     52       32.1672       30.1181  0.0121\n",
      "     53       32.1549       \u001b[32m29.9731\u001b[0m  0.0116\n",
      "     54       32.1442       29.9982  0.0117\n",
      "     55       \u001b[36m32.1239\u001b[0m       30.0094  0.0117\n",
      "     56       \u001b[36m32.1095\u001b[0m       29.9763  0.0116\n",
      "     57       \u001b[36m32.1080\u001b[0m       30.0442  0.0117\n",
      "     58       32.1084       29.9941  0.0115\n",
      "     59       \u001b[36m32.1047\u001b[0m       30.0310  0.0112\n",
      "     60       \u001b[36m32.1034\u001b[0m       30.0224  0.0119\n",
      "     61       \u001b[36m32.0986\u001b[0m       30.0287  0.0117\n",
      "     62       \u001b[36m32.0978\u001b[0m       30.0436  0.0121\n",
      "     63       \u001b[36m32.0956\u001b[0m       30.0353  0.0117\n",
      "     64       \u001b[36m32.0943\u001b[0m       30.0546  0.0118\n",
      "     65       \u001b[36m32.0930\u001b[0m       30.0473  0.0126\n",
      "     66       \u001b[36m32.0912\u001b[0m       30.0625  0.0122\n",
      "     67       \u001b[36m32.0903\u001b[0m       30.0615  0.0119\n",
      "     68       \u001b[36m32.0887\u001b[0m       30.0709  0.0116\n",
      "     69       \u001b[36m32.0877\u001b[0m       30.0731  0.0117\n",
      "     70       \u001b[36m32.0861\u001b[0m       30.0807  0.0118\n",
      "     71       \u001b[36m32.0851\u001b[0m       30.0839  0.0118\n",
      "     72       \u001b[36m32.0839\u001b[0m       30.0942  0.0118\n",
      "     73       \u001b[36m32.0828\u001b[0m       30.0953  0.0117\n",
      "     74       \u001b[36m32.0817\u001b[0m       30.1063  0.0118\n",
      "     75       \u001b[36m32.0805\u001b[0m       30.1075  0.0125\n",
      "     76       \u001b[36m32.0794\u001b[0m       30.1174  0.0116\n",
      "     77       \u001b[36m32.0784\u001b[0m       30.1217  0.0116\n",
      "     78       \u001b[36m32.0774\u001b[0m       30.1320  0.0116\n",
      "     79       \u001b[36m32.0762\u001b[0m       30.1340  0.0116\n",
      "     80       \u001b[36m32.0754\u001b[0m       30.1429  0.0125\n",
      "     81       \u001b[36m32.0744\u001b[0m       30.1485  0.0119\n",
      "     82       \u001b[36m32.0732\u001b[0m       30.1509  0.0122\n",
      "     83       \u001b[36m32.0723\u001b[0m       30.1612  0.0116\n",
      "     84       \u001b[36m32.0713\u001b[0m       30.1695  0.0114\n",
      "     85       \u001b[36m32.0705\u001b[0m       30.1721  0.0122\n",
      "     86       \u001b[36m32.0694\u001b[0m       30.1836  0.0122\n",
      "     87       \u001b[36m32.0685\u001b[0m       30.1849  0.0143\n",
      "     88       \u001b[36m32.0677\u001b[0m       30.1984  0.0184\n",
      "     89       \u001b[36m32.0665\u001b[0m       30.1979  0.0131\n",
      "     90       \u001b[36m32.0657\u001b[0m       30.2077  0.0135\n",
      "     91       \u001b[36m32.0647\u001b[0m       30.2129  0.0141\n",
      "     92       \u001b[36m32.0637\u001b[0m       30.2196  0.0154\n",
      "     93       \u001b[36m32.0628\u001b[0m       30.2274  0.0147\n",
      "     94       \u001b[36m32.0620\u001b[0m       30.2290  0.0128\n",
      "     95       \u001b[36m32.0610\u001b[0m       30.2408  0.0119\n",
      "     96       \u001b[36m32.0600\u001b[0m       30.2454  0.0116\n",
      "     97       \u001b[36m32.0591\u001b[0m       30.2528  0.0123\n",
      "     98       \u001b[36m32.0583\u001b[0m       30.2557  0.0124\n",
      "     99       \u001b[36m32.0570\u001b[0m       30.2612  0.0122\n",
      "    100       \u001b[36m32.0569\u001b[0m       30.2703  0.0121\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m32.6968\u001b[0m       \u001b[32m30.5645\u001b[0m  0.0123\n",
      "      2       \u001b[36m30.1201\u001b[0m       \u001b[32m28.3916\u001b[0m  0.0119\n",
      "      3       \u001b[36m26.7397\u001b[0m       \u001b[32m26.9499\u001b[0m  0.0124\n",
      "      4       \u001b[36m24.8057\u001b[0m       29.0954  0.0119\n",
      "      5       \u001b[36m24.6540\u001b[0m       27.0395  0.0118\n",
      "      6       \u001b[36m23.9489\u001b[0m       \u001b[32m26.4063\u001b[0m  0.0122\n",
      "      7       \u001b[36m23.9007\u001b[0m       \u001b[32m26.3974\u001b[0m  0.0122\n",
      "      8       \u001b[36m23.6459\u001b[0m       26.8541  0.0116\n",
      "      9       \u001b[36m23.5242\u001b[0m       27.2327  0.0119\n",
      "     10       \u001b[36m23.4421\u001b[0m       27.0166  0.0117\n",
      "     11       \u001b[36m23.3871\u001b[0m       26.7952  0.0119\n",
      "     12       \u001b[36m23.3515\u001b[0m       26.7808  0.0119\n",
      "     13       \u001b[36m23.3015\u001b[0m       26.8221  0.0117\n",
      "     14       \u001b[36m23.2638\u001b[0m       26.7813  0.0126\n",
      "     15       \u001b[36m23.2340\u001b[0m       26.7276  0.0127\n",
      "     16       \u001b[36m23.2146\u001b[0m       26.7135  0.0115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m23.1969\u001b[0m       26.7106  0.0125\n",
      "     18       \u001b[36m23.1781\u001b[0m       26.6976  0.0129\n",
      "     19       \u001b[36m23.1612\u001b[0m       26.6593  0.0116\n",
      "     20       \u001b[36m23.1496\u001b[0m       26.6262  0.0120\n",
      "     21       \u001b[36m23.1399\u001b[0m       26.6250  0.0117\n",
      "     22       \u001b[36m23.1288\u001b[0m       26.6217  0.0124\n",
      "     23       \u001b[36m23.1190\u001b[0m       26.6001  0.0121\n",
      "     24       \u001b[36m23.1103\u001b[0m       26.5834  0.0120\n",
      "     25       \u001b[36m23.1022\u001b[0m       26.5713  0.0121\n",
      "     26       \u001b[36m23.0945\u001b[0m       26.5645  0.0121\n",
      "     27       \u001b[36m23.0870\u001b[0m       26.5574  0.0121\n",
      "     28       \u001b[36m23.0802\u001b[0m       26.5493  0.0125\n",
      "     29       \u001b[36m23.0743\u001b[0m       26.5492  0.0124\n",
      "     30       \u001b[36m23.0683\u001b[0m       26.5456  0.0120\n",
      "     31       \u001b[36m23.0634\u001b[0m       26.5422  0.0124\n",
      "     32       \u001b[36m23.0584\u001b[0m       26.5344  0.0122\n",
      "     33       \u001b[36m23.0547\u001b[0m       26.5370  0.0121\n",
      "     34       \u001b[36m23.0501\u001b[0m       26.5260  0.0126\n",
      "     35       \u001b[36m23.0476\u001b[0m       26.5318  0.0123\n",
      "     36       \u001b[36m23.0433\u001b[0m       26.5134  0.0123\n",
      "     37       \u001b[36m23.0419\u001b[0m       26.5228  0.0118\n",
      "     38       \u001b[36m23.0379\u001b[0m       26.5019  0.0122\n",
      "     39       23.0380       26.5231  0.0118\n",
      "     40       \u001b[36m23.0340\u001b[0m       26.4933  0.0144\n",
      "     41       23.0365       26.5275  0.0117\n",
      "     42       \u001b[36m23.0326\u001b[0m       26.4810  0.0118\n",
      "     43       23.0379       26.5370  0.0115\n",
      "     44       23.0336       26.4744  0.0116\n",
      "     45       23.0442       26.5509  0.0116\n",
      "     46       23.0406       26.4542  0.0116\n",
      "     47       23.0406       26.5265  0.0114\n",
      "     48       \u001b[36m23.0275\u001b[0m       26.4834  0.0118\n",
      "     49       \u001b[36m23.0246\u001b[0m       26.4826  0.0117\n",
      "     50       \u001b[36m23.0178\u001b[0m       26.4963  0.0116\n",
      "     51       \u001b[36m23.0153\u001b[0m       26.4736  0.0113\n",
      "     52       23.0158       26.4967  0.0115\n",
      "     53       \u001b[36m23.0123\u001b[0m       26.4770  0.0122\n",
      "     54       23.0131       26.4917  0.0120\n",
      "     55       \u001b[36m23.0096\u001b[0m       26.4782  0.0120\n",
      "     56       23.0100       26.4897  0.0122\n",
      "     57       \u001b[36m23.0073\u001b[0m       26.4832  0.0125\n",
      "     58       \u001b[36m23.0070\u001b[0m       26.4898  0.0128\n",
      "     59       \u001b[36m23.0048\u001b[0m       26.4852  0.0124\n",
      "     60       \u001b[36m23.0043\u001b[0m       26.4885  0.0117\n",
      "     61       \u001b[36m23.0027\u001b[0m       26.4877  0.0117\n",
      "     62       \u001b[36m23.0021\u001b[0m       26.4918  0.0119\n",
      "     63       \u001b[36m23.0007\u001b[0m       26.4877  0.0117\n",
      "     64       \u001b[36m23.0002\u001b[0m       26.4911  0.0159\n",
      "     65       \u001b[36m22.9990\u001b[0m       26.4901  0.0127\n",
      "     66       \u001b[36m22.9982\u001b[0m       26.4920  0.0121\n",
      "     67       \u001b[36m22.9969\u001b[0m       26.4919  0.0126\n",
      "     68       \u001b[36m22.9961\u001b[0m       26.4960  0.0122\n",
      "     69       \u001b[36m22.9949\u001b[0m       26.4919  0.0136\n",
      "     70       \u001b[36m22.9942\u001b[0m       26.5002  0.0122\n",
      "     71       \u001b[36m22.9930\u001b[0m       26.4959  0.0132\n",
      "     72       \u001b[36m22.9921\u001b[0m       26.4987  0.0118\n",
      "     73       \u001b[36m22.9914\u001b[0m       26.5014  0.0118\n",
      "     74       \u001b[36m22.9900\u001b[0m       26.4993  0.0116\n",
      "     75       \u001b[36m22.9899\u001b[0m       26.5060  0.0114\n",
      "     76       \u001b[36m22.9884\u001b[0m       26.5031  0.0117\n",
      "     77       \u001b[36m22.9880\u001b[0m       26.5103  0.0118\n",
      "     78       22.9885       26.5112  0.0117\n",
      "     79       \u001b[36m22.9859\u001b[0m       26.5073  0.0114\n",
      "     80       22.9887       26.5215  0.0114\n",
      "     81       22.9886       26.5122  0.0117\n",
      "     82       22.9887       26.5255  0.0117\n",
      "     83       22.9910       26.5190  0.0119\n",
      "     84       \u001b[36m22.9851\u001b[0m       26.5208  0.0114\n",
      "     85       22.9855       26.5256  0.0120\n",
      "     86       22.9858       26.5148  0.0121\n",
      "     87       \u001b[36m22.9812\u001b[0m       26.5344  0.0121\n",
      "     88       \u001b[36m22.9792\u001b[0m       26.5164  0.0122\n",
      "     89       \u001b[36m22.9791\u001b[0m       26.5349  0.0126\n",
      "     90       \u001b[36m22.9779\u001b[0m       26.5270  0.0123\n",
      "     91       \u001b[36m22.9767\u001b[0m       26.5264  0.0116\n",
      "     92       \u001b[36m22.9767\u001b[0m       26.5284  0.0128\n",
      "     93       \u001b[36m22.9747\u001b[0m       26.5334  0.0116\n",
      "     94       \u001b[36m22.9744\u001b[0m       26.5339  0.0122\n",
      "     95       \u001b[36m22.9732\u001b[0m       26.5343  0.0115\n",
      "     96       \u001b[36m22.9725\u001b[0m       26.5374  0.0115\n",
      "     97       \u001b[36m22.9721\u001b[0m       26.5399  0.0124\n",
      "     98       \u001b[36m22.9704\u001b[0m       26.5433  0.0118\n",
      "     99       22.9707       26.5442  0.0123\n",
      "    100       \u001b[36m22.9697\u001b[0m       26.5474  0.0118\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m42.0840\u001b[0m       \u001b[32m32.5291\u001b[0m  0.0117\n",
      "      2       \u001b[36m39.5483\u001b[0m       \u001b[32m30.3554\u001b[0m  0.0122\n",
      "      3       \u001b[36m36.2690\u001b[0m       \u001b[32m27.5825\u001b[0m  0.0117\n",
      "      4       \u001b[36m31.8232\u001b[0m       29.1737  0.0122\n",
      "      5       32.0047       29.7486  0.0123\n",
      "      6       \u001b[36m30.9212\u001b[0m       \u001b[32m27.1180\u001b[0m  0.0123\n",
      "      7       \u001b[36m30.2360\u001b[0m       \u001b[32m26.7109\u001b[0m  0.0116\n",
      "      8       \u001b[36m29.9513\u001b[0m       27.0697  0.0118\n",
      "      9       \u001b[36m29.5147\u001b[0m       28.1302  0.0121\n",
      "     10       \u001b[36m29.4703\u001b[0m       27.9206  0.0129\n",
      "     11       \u001b[36m29.1755\u001b[0m       27.3154  0.0118\n",
      "     12       \u001b[36m29.0084\u001b[0m       27.3236  0.0116\n",
      "     13       \u001b[36m28.8947\u001b[0m       27.6843  0.0118\n",
      "     14       \u001b[36m28.8531\u001b[0m       27.6689  0.0121\n",
      "     15       \u001b[36m28.7677\u001b[0m       27.4470  0.0120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     16       \u001b[36m28.6872\u001b[0m       27.4597  0.0126\n",
      "     17       \u001b[36m28.6475\u001b[0m       27.5280  0.0122\n",
      "     18       \u001b[36m28.6180\u001b[0m       27.5097  0.0123\n",
      "     19       \u001b[36m28.5872\u001b[0m       27.4683  0.0115\n",
      "     20       \u001b[36m28.5621\u001b[0m       27.4088  0.0121\n",
      "     21       \u001b[36m28.5414\u001b[0m       27.4034  0.0125\n",
      "     22       \u001b[36m28.5289\u001b[0m       27.4189  0.0122\n",
      "     23       \u001b[36m28.5176\u001b[0m       27.3925  0.0117\n",
      "     24       \u001b[36m28.5057\u001b[0m       27.3839  0.0119\n",
      "     25       \u001b[36m28.4977\u001b[0m       27.3808  0.0122\n",
      "     26       \u001b[36m28.4901\u001b[0m       27.3694  0.0115\n",
      "     27       \u001b[36m28.4834\u001b[0m       27.3614  0.0118\n",
      "     28       \u001b[36m28.4773\u001b[0m       27.3512  0.0121\n",
      "     29       \u001b[36m28.4715\u001b[0m       27.3455  0.0120\n",
      "     30       \u001b[36m28.4669\u001b[0m       27.3380  0.0125\n",
      "     31       \u001b[36m28.4626\u001b[0m       27.3309  0.0120\n",
      "     32       \u001b[36m28.4582\u001b[0m       27.3273  0.0117\n",
      "     33       \u001b[36m28.4543\u001b[0m       27.3272  0.0116\n",
      "     34       \u001b[36m28.4509\u001b[0m       27.3319  0.0122\n",
      "     35       \u001b[36m28.4481\u001b[0m       27.3248  0.0122\n",
      "     36       \u001b[36m28.4445\u001b[0m       27.3218  0.0118\n",
      "     37       \u001b[36m28.4417\u001b[0m       27.3257  0.0120\n",
      "     38       \u001b[36m28.4387\u001b[0m       27.3255  0.0136\n",
      "     39       \u001b[36m28.4362\u001b[0m       27.3226  0.0129\n",
      "     40       \u001b[36m28.4336\u001b[0m       27.3214  0.0122\n",
      "     41       \u001b[36m28.4313\u001b[0m       27.3185  0.0117\n",
      "     42       \u001b[36m28.4286\u001b[0m       27.3206  0.0154\n",
      "     43       \u001b[36m28.4269\u001b[0m       27.3243  0.0149\n",
      "     44       \u001b[36m28.4248\u001b[0m       27.3273  0.0125\n",
      "     45       \u001b[36m28.4227\u001b[0m       27.3270  0.0128\n",
      "     46       \u001b[36m28.4209\u001b[0m       27.3270  0.0141\n",
      "     47       \u001b[36m28.4193\u001b[0m       27.3272  0.0148\n",
      "     48       \u001b[36m28.4173\u001b[0m       27.3202  0.0145\n",
      "     49       \u001b[36m28.4157\u001b[0m       27.3238  0.0131\n",
      "     50       \u001b[36m28.4143\u001b[0m       27.3246  0.0123\n",
      "     51       \u001b[36m28.4127\u001b[0m       27.3148  0.0125\n",
      "     52       \u001b[36m28.4113\u001b[0m       27.3189  0.0129\n",
      "     53       \u001b[36m28.4101\u001b[0m       27.3193  0.0125\n",
      "     54       \u001b[36m28.4085\u001b[0m       27.3150  0.0129\n",
      "     55       \u001b[36m28.4072\u001b[0m       27.3192  0.0129\n",
      "     56       \u001b[36m28.4064\u001b[0m       27.3202  0.0119\n",
      "     57       \u001b[36m28.4047\u001b[0m       27.3208  0.0129\n",
      "     58       \u001b[36m28.4036\u001b[0m       27.3154  0.0137\n",
      "     59       \u001b[36m28.4024\u001b[0m       27.3344  0.0126\n",
      "     60       28.4028       27.3086  0.0127\n",
      "     61       \u001b[36m28.3994\u001b[0m       27.3260  0.0183\n",
      "     62       28.4011       27.3024  0.0150\n",
      "     63       \u001b[36m28.3971\u001b[0m       27.3344  0.0121\n",
      "     64       28.4009       27.3006  0.0121\n",
      "     65       \u001b[36m28.3954\u001b[0m       27.3377  0.0122\n",
      "     66       28.3994       27.2886  0.0120\n",
      "     67       \u001b[36m28.3935\u001b[0m       27.3344  0.0125\n",
      "     68       28.3972       27.2894  0.0124\n",
      "     69       \u001b[36m28.3926\u001b[0m       27.3070  0.0120\n",
      "     70       28.3935       27.3278  0.0121\n",
      "     71       28.4012       27.2634  0.0121\n",
      "     72       28.3952       27.3673  0.0120\n",
      "     73       28.4110       27.2275  0.0119\n",
      "     74       \u001b[36m28.3911\u001b[0m       27.3435  0.0121\n",
      "     75       28.4043       27.2655  0.0125\n",
      "     76       \u001b[36m28.3853\u001b[0m       27.2706  0.0128\n",
      "     77       28.3920       27.2947  0.0145\n",
      "     78       28.3862       27.2773  0.0156\n",
      "     79       28.3889       27.2652  0.0159\n",
      "     80       \u001b[36m28.3844\u001b[0m       27.2855  0.0121\n",
      "     81       28.3878       27.2559  0.0124\n",
      "     82       \u001b[36m28.3828\u001b[0m       27.2610  0.0125\n",
      "     83       28.3857       27.2669  0.0125\n",
      "     84       \u001b[36m28.3826\u001b[0m       27.2456  0.0121\n",
      "     85       28.3830       27.2510  0.0120\n",
      "     86       \u001b[36m28.3821\u001b[0m       27.2404  0.0120\n",
      "     87       \u001b[36m28.3813\u001b[0m       27.2387  0.0121\n",
      "     88       28.3813       27.2444  0.0120\n",
      "     89       \u001b[36m28.3806\u001b[0m       27.2199  0.0119\n",
      "     90       \u001b[36m28.3794\u001b[0m       27.2370  0.0116\n",
      "     91       28.3804       27.2292  0.0117\n",
      "     92       \u001b[36m28.3789\u001b[0m       27.2142  0.0115\n",
      "     93       \u001b[36m28.3779\u001b[0m       27.2272  0.0118\n",
      "     94       28.3784       27.2141  0.0133\n",
      "     95       \u001b[36m28.3770\u001b[0m       27.2216  0.0115\n",
      "     96       28.3773       27.2123  0.0224\n",
      "     97       \u001b[36m28.3758\u001b[0m       27.2105  0.0121\n",
      "     98       28.3772       27.2174  0.0120\n",
      "     99       \u001b[36m28.3748\u001b[0m       27.2052  0.0122\n",
      "    100       28.3758       27.2152  0.0122\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m42.6432\u001b[0m       \u001b[32m43.2485\u001b[0m  0.0111\n",
      "      2       \u001b[36m40.8331\u001b[0m       \u001b[32m41.0322\u001b[0m  0.0113\n",
      "      3       \u001b[36m39.0984\u001b[0m       \u001b[32m38.8609\u001b[0m  0.0116\n",
      "      4       \u001b[36m37.4329\u001b[0m       \u001b[32m36.7685\u001b[0m  0.0118\n",
      "      5       \u001b[36m35.8941\u001b[0m       \u001b[32m34.8486\u001b[0m  0.0110\n",
      "      6       \u001b[36m34.5883\u001b[0m       \u001b[32m33.2304\u001b[0m  0.0109\n",
      "      7       \u001b[36m33.6234\u001b[0m       \u001b[32m32.0233\u001b[0m  0.0110\n",
      "      8       \u001b[36m33.0309\u001b[0m       \u001b[32m31.2340\u001b[0m  0.0111\n",
      "      9       \u001b[36m32.7318\u001b[0m       \u001b[32m30.7692\u001b[0m  0.0113\n",
      "     10       \u001b[36m32.6013\u001b[0m       \u001b[32m30.5083\u001b[0m  0.0109\n",
      "     11       \u001b[36m32.5446\u001b[0m       \u001b[32m30.3615\u001b[0m  0.0108\n",
      "     12       \u001b[36m32.5151\u001b[0m       \u001b[32m30.2753\u001b[0m  0.0112\n",
      "     13       \u001b[36m32.4950\u001b[0m       \u001b[32m30.2210\u001b[0m  0.0113\n",
      "     14       \u001b[36m32.4783\u001b[0m       \u001b[32m30.1839\u001b[0m  0.0130\n",
      "     15       \u001b[36m32.4636\u001b[0m       \u001b[32m30.1565\u001b[0m  0.0118\n",
      "     16       \u001b[36m32.4504\u001b[0m       \u001b[32m30.1351\u001b[0m  0.0132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m32.4382\u001b[0m       \u001b[32m30.1172\u001b[0m  0.0160\n",
      "     18       \u001b[36m32.4271\u001b[0m       \u001b[32m30.1019\u001b[0m  0.0133\n",
      "     19       \u001b[36m32.4169\u001b[0m       \u001b[32m30.0884\u001b[0m  0.0144\n",
      "     20       \u001b[36m32.4075\u001b[0m       \u001b[32m30.0754\u001b[0m  0.0134\n",
      "     21       \u001b[36m32.3989\u001b[0m       \u001b[32m30.0643\u001b[0m  0.0134\n",
      "     22       \u001b[36m32.3908\u001b[0m       \u001b[32m30.0533\u001b[0m  0.0120\n",
      "     23       \u001b[36m32.3833\u001b[0m       \u001b[32m30.0436\u001b[0m  0.0119\n",
      "     24       \u001b[36m32.3763\u001b[0m       \u001b[32m30.0350\u001b[0m  0.0113\n",
      "     25       \u001b[36m32.3698\u001b[0m       \u001b[32m30.0261\u001b[0m  0.0112\n",
      "     26       \u001b[36m32.3637\u001b[0m       \u001b[32m30.0181\u001b[0m  0.0111\n",
      "     27       \u001b[36m32.3579\u001b[0m       \u001b[32m30.0106\u001b[0m  0.0108\n",
      "     28       \u001b[36m32.3526\u001b[0m       \u001b[32m30.0043\u001b[0m  0.0109\n",
      "     29       \u001b[36m32.3476\u001b[0m       \u001b[32m29.9975\u001b[0m  0.0111\n",
      "     30       \u001b[36m32.3428\u001b[0m       \u001b[32m29.9911\u001b[0m  0.0112\n",
      "     31       \u001b[36m32.3383\u001b[0m       \u001b[32m29.9857\u001b[0m  0.0111\n",
      "     32       \u001b[36m32.3341\u001b[0m       \u001b[32m29.9802\u001b[0m  0.0111\n",
      "     33       \u001b[36m32.3300\u001b[0m       \u001b[32m29.9751\u001b[0m  0.0107\n",
      "     34       \u001b[36m32.3263\u001b[0m       \u001b[32m29.9706\u001b[0m  0.0110\n",
      "     35       \u001b[36m32.3226\u001b[0m       \u001b[32m29.9662\u001b[0m  0.0110\n",
      "     36       \u001b[36m32.3192\u001b[0m       \u001b[32m29.9613\u001b[0m  0.0111\n",
      "     37       \u001b[36m32.3158\u001b[0m       \u001b[32m29.9571\u001b[0m  0.0116\n",
      "     38       \u001b[36m32.3126\u001b[0m       \u001b[32m29.9524\u001b[0m  0.0115\n",
      "     39       \u001b[36m32.3096\u001b[0m       \u001b[32m29.9493\u001b[0m  0.0116\n",
      "     40       \u001b[36m32.3066\u001b[0m       \u001b[32m29.9452\u001b[0m  0.0117\n",
      "     41       \u001b[36m32.3039\u001b[0m       \u001b[32m29.9412\u001b[0m  0.0114\n",
      "     42       \u001b[36m32.3011\u001b[0m       \u001b[32m29.9381\u001b[0m  0.0123\n",
      "     43       \u001b[36m32.2986\u001b[0m       \u001b[32m29.9352\u001b[0m  0.0122\n",
      "     44       \u001b[36m32.2961\u001b[0m       \u001b[32m29.9318\u001b[0m  0.0114\n",
      "     45       \u001b[36m32.2937\u001b[0m       \u001b[32m29.9290\u001b[0m  0.0113\n",
      "     46       \u001b[36m32.2914\u001b[0m       \u001b[32m29.9262\u001b[0m  0.0114\n",
      "     47       \u001b[36m32.2891\u001b[0m       \u001b[32m29.9237\u001b[0m  0.0111\n",
      "     48       \u001b[36m32.2869\u001b[0m       \u001b[32m29.9213\u001b[0m  0.0111\n",
      "     49       \u001b[36m32.2848\u001b[0m       \u001b[32m29.9188\u001b[0m  0.0114\n",
      "     50       \u001b[36m32.2828\u001b[0m       \u001b[32m29.9164\u001b[0m  0.0117\n",
      "     51       \u001b[36m32.2808\u001b[0m       \u001b[32m29.9141\u001b[0m  0.0113\n",
      "     52       \u001b[36m32.2789\u001b[0m       \u001b[32m29.9124\u001b[0m  0.0109\n",
      "     53       \u001b[36m32.2771\u001b[0m       \u001b[32m29.9104\u001b[0m  0.0112\n",
      "     54       \u001b[36m32.2753\u001b[0m       \u001b[32m29.9079\u001b[0m  0.0120\n",
      "     55       \u001b[36m32.2736\u001b[0m       \u001b[32m29.9063\u001b[0m  0.0116\n",
      "     56       \u001b[36m32.2719\u001b[0m       \u001b[32m29.9042\u001b[0m  0.0115\n",
      "     57       \u001b[36m32.2703\u001b[0m       \u001b[32m29.9027\u001b[0m  0.0126\n",
      "     58       \u001b[36m32.2687\u001b[0m       \u001b[32m29.9010\u001b[0m  0.0115\n",
      "     59       \u001b[36m32.2672\u001b[0m       \u001b[32m29.8997\u001b[0m  0.0122\n",
      "     60       \u001b[36m32.2657\u001b[0m       \u001b[32m29.8978\u001b[0m  0.0115\n",
      "     61       \u001b[36m32.2643\u001b[0m       \u001b[32m29.8967\u001b[0m  0.0113\n",
      "     62       \u001b[36m32.2629\u001b[0m       \u001b[32m29.8950\u001b[0m  0.0113\n",
      "     63       \u001b[36m32.2615\u001b[0m       \u001b[32m29.8940\u001b[0m  0.0113\n",
      "     64       \u001b[36m32.2602\u001b[0m       \u001b[32m29.8928\u001b[0m  0.0128\n",
      "     65       \u001b[36m32.2589\u001b[0m       \u001b[32m29.8914\u001b[0m  0.0113\n",
      "     66       \u001b[36m32.2577\u001b[0m       \u001b[32m29.8900\u001b[0m  0.0112\n",
      "     67       \u001b[36m32.2565\u001b[0m       \u001b[32m29.8891\u001b[0m  0.0112\n",
      "     68       \u001b[36m32.2554\u001b[0m       \u001b[32m29.8876\u001b[0m  0.0110\n",
      "     69       \u001b[36m32.2543\u001b[0m       \u001b[32m29.8871\u001b[0m  0.0121\n",
      "     70       \u001b[36m32.2532\u001b[0m       \u001b[32m29.8858\u001b[0m  0.0111\n",
      "     71       \u001b[36m32.2521\u001b[0m       \u001b[32m29.8851\u001b[0m  0.0116\n",
      "     72       \u001b[36m32.2511\u001b[0m       \u001b[32m29.8842\u001b[0m  0.0115\n",
      "     73       \u001b[36m32.2501\u001b[0m       \u001b[32m29.8834\u001b[0m  0.0114\n",
      "     74       \u001b[36m32.2491\u001b[0m       \u001b[32m29.8826\u001b[0m  0.0122\n",
      "     75       \u001b[36m32.2481\u001b[0m       \u001b[32m29.8821\u001b[0m  0.0119\n",
      "     76       \u001b[36m32.2471\u001b[0m       \u001b[32m29.8811\u001b[0m  0.0117\n",
      "     77       \u001b[36m32.2463\u001b[0m       \u001b[32m29.8803\u001b[0m  0.0211\n",
      "     78       \u001b[36m32.2454\u001b[0m       \u001b[32m29.8795\u001b[0m  0.0118\n",
      "     79       \u001b[36m32.2445\u001b[0m       \u001b[32m29.8788\u001b[0m  0.0115\n",
      "     80       \u001b[36m32.2436\u001b[0m       \u001b[32m29.8778\u001b[0m  0.0113\n",
      "     81       \u001b[36m32.2428\u001b[0m       \u001b[32m29.8772\u001b[0m  0.0113\n",
      "     82       \u001b[36m32.2420\u001b[0m       \u001b[32m29.8763\u001b[0m  0.0112\n",
      "     83       \u001b[36m32.2412\u001b[0m       \u001b[32m29.8758\u001b[0m  0.0115\n",
      "     84       \u001b[36m32.2404\u001b[0m       \u001b[32m29.8751\u001b[0m  0.0113\n",
      "     85       \u001b[36m32.2397\u001b[0m       \u001b[32m29.8744\u001b[0m  0.0113\n",
      "     86       \u001b[36m32.2389\u001b[0m       \u001b[32m29.8737\u001b[0m  0.0113\n",
      "     87       \u001b[36m32.2382\u001b[0m       \u001b[32m29.8729\u001b[0m  0.0114\n",
      "     88       \u001b[36m32.2375\u001b[0m       \u001b[32m29.8724\u001b[0m  0.0116\n",
      "     89       \u001b[36m32.2368\u001b[0m       \u001b[32m29.8714\u001b[0m  0.0118\n",
      "     90       \u001b[36m32.2362\u001b[0m       \u001b[32m29.8710\u001b[0m  0.0113\n",
      "     91       \u001b[36m32.2355\u001b[0m       \u001b[32m29.8703\u001b[0m  0.0113\n",
      "     92       \u001b[36m32.2348\u001b[0m       \u001b[32m29.8698\u001b[0m  0.0111\n",
      "     93       \u001b[36m32.2342\u001b[0m       \u001b[32m29.8693\u001b[0m  0.0117\n",
      "     94       \u001b[36m32.2336\u001b[0m       \u001b[32m29.8687\u001b[0m  0.0122\n",
      "     95       \u001b[36m32.2330\u001b[0m       \u001b[32m29.8682\u001b[0m  0.0114\n",
      "     96       \u001b[36m32.2324\u001b[0m       \u001b[32m29.8680\u001b[0m  0.0120\n",
      "     97       \u001b[36m32.2318\u001b[0m       \u001b[32m29.8673\u001b[0m  0.0151\n",
      "     98       \u001b[36m32.2312\u001b[0m       \u001b[32m29.8668\u001b[0m  0.0185\n",
      "     99       \u001b[36m32.2307\u001b[0m       \u001b[32m29.8664\u001b[0m  0.0130\n",
      "    100       \u001b[36m32.2301\u001b[0m       \u001b[32m29.8660\u001b[0m  0.0117\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m33.0731\u001b[0m       \u001b[32m31.2480\u001b[0m  0.0127\n",
      "      2       \u001b[36m31.5511\u001b[0m       \u001b[32m30.1966\u001b[0m  0.0122\n",
      "      3       \u001b[36m30.1870\u001b[0m       \u001b[32m29.2571\u001b[0m  0.0124\n",
      "      4       \u001b[36m28.9240\u001b[0m       \u001b[32m28.4087\u001b[0m  0.0114\n",
      "      5       \u001b[36m27.7446\u001b[0m       \u001b[32m27.6603\u001b[0m  0.0111\n",
      "      6       \u001b[36m26.6651\u001b[0m       \u001b[32m27.0380\u001b[0m  0.0112\n",
      "      7       \u001b[36m25.7204\u001b[0m       \u001b[32m26.5708\u001b[0m  0.0112\n",
      "      8       \u001b[36m24.9452\u001b[0m       \u001b[32m26.2745\u001b[0m  0.0110\n",
      "      9       \u001b[36m24.3602\u001b[0m       \u001b[32m26.1363\u001b[0m  0.0113\n",
      "     10       \u001b[36m23.9576\u001b[0m       \u001b[32m26.1162\u001b[0m  0.0108\n",
      "     11       \u001b[36m23.7025\u001b[0m       26.1614  0.0109\n",
      "     12       \u001b[36m23.5491\u001b[0m       26.2282  0.0111\n",
      "     13       \u001b[36m23.4577\u001b[0m       26.2913  0.0112\n",
      "     14       \u001b[36m23.4010\u001b[0m       26.3416  0.0115\n",
      "     15       \u001b[36m23.3638\u001b[0m       26.3776  0.0117\n",
      "     16       \u001b[36m23.3366\u001b[0m       26.4024  0.0108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m23.3152\u001b[0m       26.4188  0.0118\n",
      "     18       \u001b[36m23.2975\u001b[0m       26.4294  0.0112\n",
      "     19       \u001b[36m23.2823\u001b[0m       26.4366  0.0113\n",
      "     20       \u001b[36m23.2689\u001b[0m       26.4411  0.0108\n",
      "     21       \u001b[36m23.2569\u001b[0m       26.4444  0.0109\n",
      "     22       \u001b[36m23.2459\u001b[0m       26.4466  0.0113\n",
      "     23       \u001b[36m23.2361\u001b[0m       26.4480  0.0107\n",
      "     24       \u001b[36m23.2270\u001b[0m       26.4490  0.0109\n",
      "     25       \u001b[36m23.2186\u001b[0m       26.4497  0.0107\n",
      "     26       \u001b[36m23.2107\u001b[0m       26.4505  0.0105\n",
      "     27       \u001b[36m23.2034\u001b[0m       26.4512  0.0109\n",
      "     28       \u001b[36m23.1965\u001b[0m       26.4518  0.0109\n",
      "     29       \u001b[36m23.1902\u001b[0m       26.4522  0.0111\n",
      "     30       \u001b[36m23.1842\u001b[0m       26.4526  0.0107\n",
      "     31       \u001b[36m23.1786\u001b[0m       26.4529  0.0107\n",
      "     32       \u001b[36m23.1733\u001b[0m       26.4531  0.0114\n",
      "     33       \u001b[36m23.1683\u001b[0m       26.4532  0.0110\n",
      "     34       \u001b[36m23.1637\u001b[0m       26.4534  0.0110\n",
      "     35       \u001b[36m23.1593\u001b[0m       26.4535  0.0107\n",
      "     36       \u001b[36m23.1552\u001b[0m       26.4535  0.0108\n",
      "     37       \u001b[36m23.1513\u001b[0m       26.4536  0.0112\n",
      "     38       \u001b[36m23.1476\u001b[0m       26.4537  0.0111\n",
      "     39       \u001b[36m23.1442\u001b[0m       26.4538  0.0112\n",
      "     40       \u001b[36m23.1409\u001b[0m       26.4537  0.0108\n",
      "     41       \u001b[36m23.1378\u001b[0m       26.4537  0.0106\n",
      "     42       \u001b[36m23.1349\u001b[0m       26.4535  0.0115\n",
      "     43       \u001b[36m23.1321\u001b[0m       26.4535  0.0109\n",
      "     44       \u001b[36m23.1294\u001b[0m       26.4534  0.0109\n",
      "     45       \u001b[36m23.1269\u001b[0m       26.4532  0.0109\n",
      "     46       \u001b[36m23.1244\u001b[0m       26.4531  0.0108\n",
      "     47       \u001b[36m23.1221\u001b[0m       26.4527  0.0111\n",
      "     48       \u001b[36m23.1198\u001b[0m       26.4525  0.0112\n",
      "     49       \u001b[36m23.1177\u001b[0m       26.4523  0.0111\n",
      "     50       \u001b[36m23.1156\u001b[0m       26.4521  0.0105\n",
      "     51       \u001b[36m23.1136\u001b[0m       26.4521  0.0107\n",
      "     52       \u001b[36m23.1117\u001b[0m       26.4518  0.0114\n",
      "     53       \u001b[36m23.1099\u001b[0m       26.4517  0.0114\n",
      "     54       \u001b[36m23.1082\u001b[0m       26.4516  0.0110\n",
      "     55       \u001b[36m23.1065\u001b[0m       26.4513  0.0107\n",
      "     56       \u001b[36m23.1049\u001b[0m       26.4511  0.0109\n",
      "     57       \u001b[36m23.1034\u001b[0m       26.4509  0.0109\n",
      "     58       \u001b[36m23.1019\u001b[0m       26.4508  0.0110\n",
      "     59       \u001b[36m23.1004\u001b[0m       26.4506  0.0111\n",
      "     60       \u001b[36m23.0991\u001b[0m       26.4506  0.0112\n",
      "     61       \u001b[36m23.0978\u001b[0m       26.4505  0.0109\n",
      "     62       \u001b[36m23.0965\u001b[0m       26.4505  0.0127\n",
      "     63       \u001b[36m23.0953\u001b[0m       26.4503  0.0125\n",
      "     64       \u001b[36m23.0941\u001b[0m       26.4503  0.0114\n",
      "     65       \u001b[36m23.0930\u001b[0m       26.4502  0.0112\n",
      "     66       \u001b[36m23.0919\u001b[0m       26.4501  0.0110\n",
      "     67       \u001b[36m23.0908\u001b[0m       26.4500  0.0123\n",
      "     68       \u001b[36m23.0898\u001b[0m       26.4499  0.0115\n",
      "     69       \u001b[36m23.0888\u001b[0m       26.4496  0.0118\n",
      "     70       \u001b[36m23.0878\u001b[0m       26.4494  0.0108\n",
      "     71       \u001b[36m23.0869\u001b[0m       26.4492  0.0107\n",
      "     72       \u001b[36m23.0860\u001b[0m       26.4491  0.0119\n",
      "     73       \u001b[36m23.0851\u001b[0m       26.4489  0.0120\n",
      "     74       \u001b[36m23.0842\u001b[0m       26.4486  0.0120\n",
      "     75       \u001b[36m23.0834\u001b[0m       26.4484  0.0116\n",
      "     76       \u001b[36m23.0826\u001b[0m       26.4481  0.0109\n",
      "     77       \u001b[36m23.0818\u001b[0m       26.4479  0.0126\n",
      "     78       \u001b[36m23.0810\u001b[0m       26.4477  0.0117\n",
      "     79       \u001b[36m23.0802\u001b[0m       26.4473  0.0172\n",
      "     80       \u001b[36m23.0795\u001b[0m       26.4471  0.0147\n",
      "     81       \u001b[36m23.0787\u001b[0m       26.4468  0.0137\n",
      "     82       \u001b[36m23.0780\u001b[0m       26.4466  0.0126\n",
      "     83       \u001b[36m23.0773\u001b[0m       26.4465  0.0136\n",
      "     84       \u001b[36m23.0766\u001b[0m       26.4463  0.0156\n",
      "     85       \u001b[36m23.0760\u001b[0m       26.4462  0.0129\n",
      "     86       \u001b[36m23.0753\u001b[0m       26.4459  0.0113\n",
      "     87       \u001b[36m23.0747\u001b[0m       26.4458  0.0113\n",
      "     88       \u001b[36m23.0741\u001b[0m       26.4456  0.0111\n",
      "     89       \u001b[36m23.0735\u001b[0m       26.4455  0.0110\n",
      "     90       \u001b[36m23.0729\u001b[0m       26.4452  0.0114\n",
      "     91       \u001b[36m23.0723\u001b[0m       26.4451  0.0110\n",
      "     92       \u001b[36m23.0718\u001b[0m       26.4449  0.0111\n",
      "     93       \u001b[36m23.0712\u001b[0m       26.4447  0.0119\n",
      "     94       \u001b[36m23.0707\u001b[0m       26.4444  0.0110\n",
      "     95       \u001b[36m23.0702\u001b[0m       26.4441  0.0113\n",
      "     96       \u001b[36m23.0696\u001b[0m       26.4439  0.0114\n",
      "     97       \u001b[36m23.0691\u001b[0m       26.4437  0.0111\n",
      "     98       \u001b[36m23.0686\u001b[0m       26.4435  0.0107\n",
      "     99       \u001b[36m23.0682\u001b[0m       26.4432  0.0107\n",
      "    100       \u001b[36m23.0677\u001b[0m       26.4430  0.0111\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m41.5100\u001b[0m       \u001b[32m32.0899\u001b[0m  0.0112\n",
      "      2       \u001b[36m39.3832\u001b[0m       \u001b[32m30.6848\u001b[0m  0.0110\n",
      "      3       \u001b[36m37.4246\u001b[0m       \u001b[32m29.4100\u001b[0m  0.0108\n",
      "      4       \u001b[36m35.5863\u001b[0m       \u001b[32m28.2656\u001b[0m  0.0120\n",
      "      5       \u001b[36m33.8707\u001b[0m       \u001b[32m27.2975\u001b[0m  0.0141\n",
      "      6       \u001b[36m32.3118\u001b[0m       \u001b[32m26.5990\u001b[0m  0.0157\n",
      "      7       \u001b[36m30.9814\u001b[0m       \u001b[32m26.2766\u001b[0m  0.0141\n",
      "      8       \u001b[36m29.9818\u001b[0m       26.3282  0.0112\n",
      "      9       \u001b[36m29.3635\u001b[0m       26.5927  0.0111\n",
      "     10       \u001b[36m29.0496\u001b[0m       26.8712  0.0107\n",
      "     11       \u001b[36m28.9047\u001b[0m       27.0628  0.0109\n",
      "     12       \u001b[36m28.8299\u001b[0m       27.1685  0.0112\n",
      "     13       \u001b[36m28.7803\u001b[0m       27.2146  0.0110\n",
      "     14       \u001b[36m28.7402\u001b[0m       27.2292  0.0107\n",
      "     15       \u001b[36m28.7054\u001b[0m       27.2285  0.0108\n",
      "     16       \u001b[36m28.6751\u001b[0m       27.2239  0.0116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m28.6490\u001b[0m       27.2197  0.0116\n",
      "     18       \u001b[36m28.6265\u001b[0m       27.2149  0.0112\n",
      "     19       \u001b[36m28.6069\u001b[0m       27.2110  0.0112\n",
      "     20       \u001b[36m28.5896\u001b[0m       27.2069  0.0112\n",
      "     21       \u001b[36m28.5741\u001b[0m       27.2042  0.0110\n",
      "     22       \u001b[36m28.5607\u001b[0m       27.2032  0.0116\n",
      "     23       \u001b[36m28.5494\u001b[0m       27.2038  0.0114\n",
      "     24       \u001b[36m28.5397\u001b[0m       27.2049  0.0110\n",
      "     25       \u001b[36m28.5313\u001b[0m       27.2057  0.0107\n",
      "     26       \u001b[36m28.5238\u001b[0m       27.2062  0.0106\n",
      "     27       \u001b[36m28.5171\u001b[0m       27.2077  0.0110\n",
      "     28       \u001b[36m28.5111\u001b[0m       27.2089  0.0109\n",
      "     29       \u001b[36m28.5057\u001b[0m       27.2098  0.0108\n",
      "     30       \u001b[36m28.5007\u001b[0m       27.2115  0.0105\n",
      "     31       \u001b[36m28.4961\u001b[0m       27.2141  0.0108\n",
      "     32       \u001b[36m28.4923\u001b[0m       27.2156  0.0112\n",
      "     33       \u001b[36m28.4887\u001b[0m       27.2165  0.0109\n",
      "     34       \u001b[36m28.4853\u001b[0m       27.2173  0.0114\n",
      "     35       \u001b[36m28.4821\u001b[0m       27.2179  0.0108\n",
      "     36       \u001b[36m28.4791\u001b[0m       27.2181  0.0109\n",
      "     37       \u001b[36m28.4762\u001b[0m       27.2182  0.0117\n",
      "     38       \u001b[36m28.4735\u001b[0m       27.2181  0.0114\n",
      "     39       \u001b[36m28.4710\u001b[0m       27.2179  0.0110\n",
      "     40       \u001b[36m28.4686\u001b[0m       27.2177  0.0109\n",
      "     41       \u001b[36m28.4664\u001b[0m       27.2161  0.0108\n",
      "     42       \u001b[36m28.4640\u001b[0m       27.2169  0.0112\n",
      "     43       \u001b[36m28.4620\u001b[0m       27.2176  0.0120\n",
      "     44       \u001b[36m28.4600\u001b[0m       27.2174  0.0119\n",
      "     45       \u001b[36m28.4581\u001b[0m       27.2178  0.0110\n",
      "     46       \u001b[36m28.4564\u001b[0m       27.2182  0.0108\n",
      "     47       \u001b[36m28.4549\u001b[0m       27.2171  0.0113\n",
      "     48       \u001b[36m28.4531\u001b[0m       27.2184  0.0110\n",
      "     49       \u001b[36m28.4517\u001b[0m       27.2188  0.0113\n",
      "     50       \u001b[36m28.4502\u001b[0m       27.2169  0.0109\n",
      "     51       \u001b[36m28.4487\u001b[0m       27.2147  0.0108\n",
      "     52       \u001b[36m28.4470\u001b[0m       27.2149  0.0112\n",
      "     53       \u001b[36m28.4455\u001b[0m       27.2149  0.0110\n",
      "     54       \u001b[36m28.4443\u001b[0m       27.2133  0.0111\n",
      "     55       \u001b[36m28.4427\u001b[0m       27.2143  0.0108\n",
      "     56       \u001b[36m28.4415\u001b[0m       27.2144  0.0104\n",
      "     57       \u001b[36m28.4404\u001b[0m       27.2134  0.0112\n",
      "     58       \u001b[36m28.4391\u001b[0m       27.2144  0.0111\n",
      "     59       \u001b[36m28.4379\u001b[0m       27.2152  0.0110\n",
      "     60       \u001b[36m28.4370\u001b[0m       27.2144  0.0138\n",
      "     61       \u001b[36m28.4358\u001b[0m       27.2142  0.0169\n",
      "     62       \u001b[36m28.4349\u001b[0m       27.2146  0.0176\n",
      "     63       \u001b[36m28.4340\u001b[0m       27.2140  0.0171\n",
      "     64       \u001b[36m28.4329\u001b[0m       27.2152  0.0210\n",
      "     65       \u001b[36m28.4321\u001b[0m       27.2154  0.0196\n",
      "     66       \u001b[36m28.4314\u001b[0m       27.2148  0.0116\n",
      "     67       \u001b[36m28.4304\u001b[0m       27.2158  0.0112\n",
      "     68       \u001b[36m28.4297\u001b[0m       27.2154  0.0113\n",
      "     69       \u001b[36m28.4289\u001b[0m       27.2159  0.0110\n",
      "     70       \u001b[36m28.4283\u001b[0m       27.2150  0.0110\n",
      "     71       \u001b[36m28.4274\u001b[0m       27.2161  0.0110\n",
      "     72       \u001b[36m28.4268\u001b[0m       27.2164  0.0109\n",
      "     73       \u001b[36m28.4261\u001b[0m       27.2165  0.0110\n",
      "     74       \u001b[36m28.4255\u001b[0m       27.2157  0.0109\n",
      "     75       \u001b[36m28.4248\u001b[0m       27.2156  0.0109\n",
      "     76       \u001b[36m28.4241\u001b[0m       27.2160  0.0109\n",
      "     77       \u001b[36m28.4236\u001b[0m       27.2160  0.0107\n",
      "     78       \u001b[36m28.4231\u001b[0m       27.2155  0.0107\n",
      "     79       \u001b[36m28.4224\u001b[0m       27.2157  0.0109\n",
      "     80       \u001b[36m28.4219\u001b[0m       27.2164  0.0111\n",
      "     81       \u001b[36m28.4215\u001b[0m       27.2153  0.0114\n",
      "     82       \u001b[36m28.4208\u001b[0m       27.2160  0.0112\n",
      "     83       \u001b[36m28.4204\u001b[0m       27.2156  0.0109\n",
      "     84       \u001b[36m28.4199\u001b[0m       27.2160  0.0106\n",
      "     85       \u001b[36m28.4195\u001b[0m       27.2151  0.0110\n",
      "     86       \u001b[36m28.4189\u001b[0m       27.2156  0.0111\n",
      "     87       \u001b[36m28.4186\u001b[0m       27.2153  0.0110\n",
      "     88       \u001b[36m28.4181\u001b[0m       27.2154  0.0108\n",
      "     89       \u001b[36m28.4177\u001b[0m       27.2165  0.0110\n",
      "     90       \u001b[36m28.4175\u001b[0m       27.2158  0.0109\n",
      "     91       \u001b[36m28.4169\u001b[0m       27.2163  0.0109\n",
      "     92       \u001b[36m28.4167\u001b[0m       27.2152  0.0109\n",
      "     93       \u001b[36m28.4162\u001b[0m       27.2160  0.0110\n",
      "     94       \u001b[36m28.4158\u001b[0m       27.2164  0.0110\n",
      "     95       \u001b[36m28.4156\u001b[0m       27.2154  0.0108\n",
      "     96       \u001b[36m28.4151\u001b[0m       27.2159  0.0110\n",
      "     97       \u001b[36m28.4148\u001b[0m       27.2153  0.0108\n",
      "     98       \u001b[36m28.4144\u001b[0m       27.2157  0.0109\n",
      "     99       \u001b[36m28.4142\u001b[0m       27.2145  0.0109\n",
      "    100       \u001b[36m28.4137\u001b[0m       27.2148  0.0109\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m41.6556\u001b[0m       \u001b[32m39.7197\u001b[0m  0.0117\n",
      "      2       \u001b[36m37.1863\u001b[0m       \u001b[32m32.9535\u001b[0m  0.0115\n",
      "      3       \u001b[36m34.2817\u001b[0m       \u001b[32m31.3035\u001b[0m  0.0115\n",
      "      4       \u001b[36m34.0576\u001b[0m       \u001b[32m31.2425\u001b[0m  0.0115\n",
      "      5       \u001b[36m33.2916\u001b[0m       31.7850  0.0114\n",
      "      6       \u001b[36m33.1065\u001b[0m       \u001b[32m30.7512\u001b[0m  0.0111\n",
      "      7       \u001b[36m32.7420\u001b[0m       \u001b[32m30.1202\u001b[0m  0.0122\n",
      "      8       \u001b[36m32.6829\u001b[0m       30.2806  0.0114\n",
      "      9       \u001b[36m32.5808\u001b[0m       30.5104  0.0116\n",
      "     10       \u001b[36m32.5038\u001b[0m       30.3098  0.0117\n",
      "     11       \u001b[36m32.4273\u001b[0m       30.1314  0.0114\n",
      "     12       \u001b[36m32.3908\u001b[0m       30.1832  0.0113\n",
      "     13       \u001b[36m32.3582\u001b[0m       30.2115  0.0118\n",
      "     14       \u001b[36m32.3324\u001b[0m       30.2221  0.0118\n",
      "     15       \u001b[36m32.3080\u001b[0m       30.1820  0.0115\n",
      "     16       \u001b[36m32.2904\u001b[0m       \u001b[32m30.1159\u001b[0m  0.0110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m32.2767\u001b[0m       30.1608  0.0113\n",
      "     18       \u001b[36m32.2673\u001b[0m       30.1558  0.0118\n",
      "     19       \u001b[36m32.2537\u001b[0m       30.1173  0.0116\n",
      "     20       \u001b[36m32.2448\u001b[0m       \u001b[32m30.1067\u001b[0m  0.0116\n",
      "     21       \u001b[36m32.2371\u001b[0m       \u001b[32m30.1024\u001b[0m  0.0114\n",
      "     22       \u001b[36m32.2294\u001b[0m       \u001b[32m30.0995\u001b[0m  0.0114\n",
      "     23       \u001b[36m32.2225\u001b[0m       \u001b[32m30.0709\u001b[0m  0.0118\n",
      "     24       \u001b[36m32.2156\u001b[0m       30.0769  0.0116\n",
      "     25       \u001b[36m32.2112\u001b[0m       \u001b[32m30.0423\u001b[0m  0.0117\n",
      "     26       \u001b[36m32.2065\u001b[0m       30.1149  0.0114\n",
      "     27       32.2130       \u001b[32m29.9986\u001b[0m  0.0114\n",
      "     28       32.2284       30.2109  0.0115\n",
      "     29       32.2537       \u001b[32m29.9487\u001b[0m  0.0114\n",
      "     30       32.2447       30.0452  0.0112\n",
      "     31       32.2090       30.1133  0.0118\n",
      "     32       \u001b[36m32.1889\u001b[0m       \u001b[32m29.9294\u001b[0m  0.0118\n",
      "     33       32.1890       30.0396  0.0114\n",
      "     34       \u001b[36m32.1857\u001b[0m       30.0579  0.0115\n",
      "     35       \u001b[36m32.1719\u001b[0m       29.9579  0.0112\n",
      "     36       \u001b[36m32.1698\u001b[0m       30.0190  0.0114\n",
      "     37       32.1701       30.0188  0.0112\n",
      "     38       \u001b[36m32.1626\u001b[0m       29.9898  0.0114\n",
      "     39       \u001b[36m32.1602\u001b[0m       30.0034  0.0122\n",
      "     40       \u001b[36m32.1582\u001b[0m       29.9894  0.0141\n",
      "     41       \u001b[36m32.1549\u001b[0m       29.9917  0.0160\n",
      "     42       \u001b[36m32.1529\u001b[0m       29.9794  0.0126\n",
      "     43       \u001b[36m32.1499\u001b[0m       29.9848  0.0122\n",
      "     44       \u001b[36m32.1482\u001b[0m       29.9760  0.0132\n",
      "     45       \u001b[36m32.1455\u001b[0m       29.9785  0.0126\n",
      "     46       \u001b[36m32.1438\u001b[0m       29.9672  0.0132\n",
      "     47       \u001b[36m32.1414\u001b[0m       29.9725  0.0132\n",
      "     48       \u001b[36m32.1401\u001b[0m       29.9591  0.0125\n",
      "     49       \u001b[36m32.1375\u001b[0m       29.9761  0.0123\n",
      "     50       \u001b[36m32.1369\u001b[0m       29.9494  0.0122\n",
      "     51       \u001b[36m32.1344\u001b[0m       29.9908  0.0122\n",
      "     52       32.1372       29.9327  0.0133\n",
      "     53       32.1368       30.0371  0.0122\n",
      "     54       32.1503       29.9316  0.0122\n",
      "     55       32.1600       30.0618  0.0120\n",
      "     56       32.1766       \u001b[32m29.9135\u001b[0m  0.0119\n",
      "     57       32.1670       29.9884  0.0124\n",
      "     58       32.1492       29.9571  0.0121\n",
      "     59       \u001b[36m32.1259\u001b[0m       29.9187  0.0118\n",
      "     60       32.1267       30.0141  0.0115\n",
      "     61       32.1261       29.9368  0.0115\n",
      "     62       \u001b[36m32.1213\u001b[0m       29.9607  0.0119\n",
      "     63       \u001b[36m32.1208\u001b[0m       29.9752  0.0116\n",
      "     64       \u001b[36m32.1165\u001b[0m       29.9586  0.0120\n",
      "     65       \u001b[36m32.1153\u001b[0m       29.9795  0.0115\n",
      "     66       \u001b[36m32.1142\u001b[0m       29.9597  0.0139\n",
      "     67       \u001b[36m32.1123\u001b[0m       29.9718  0.0127\n",
      "     68       \u001b[36m32.1114\u001b[0m       29.9653  0.0118\n",
      "     69       \u001b[36m32.1096\u001b[0m       29.9697  0.0115\n",
      "     70       \u001b[36m32.1086\u001b[0m       29.9647  0.0113\n",
      "     71       \u001b[36m32.1071\u001b[0m       29.9707  0.0113\n",
      "     72       \u001b[36m32.1060\u001b[0m       29.9628  0.0120\n",
      "     73       \u001b[36m32.1046\u001b[0m       29.9750  0.0115\n",
      "     74       \u001b[36m32.1036\u001b[0m       29.9602  0.0116\n",
      "     75       \u001b[36m32.1023\u001b[0m       29.9775  0.0115\n",
      "     76       \u001b[36m32.1017\u001b[0m       29.9564  0.0114\n",
      "     77       \u001b[36m32.1005\u001b[0m       29.9871  0.0120\n",
      "     78       \u001b[36m32.1005\u001b[0m       29.9507  0.0117\n",
      "     79       \u001b[36m32.0998\u001b[0m       30.0096  0.0116\n",
      "     80       32.1021       29.9445  0.0113\n",
      "     81       32.1036       30.0618  0.0114\n",
      "     82       32.1129       29.9383  0.0118\n",
      "     83       32.1234       30.1273  0.0118\n",
      "     84       32.1406       29.9510  0.0115\n",
      "     85       32.1459       30.0683  0.0119\n",
      "     86       32.1332       29.9721  0.0114\n",
      "     87       32.1105       29.9914  0.0121\n",
      "     88       32.1002       30.0609  0.0127\n",
      "     89       \u001b[36m32.0939\u001b[0m       29.9822  0.0124\n",
      "     90       \u001b[36m32.0929\u001b[0m       30.0426  0.0116\n",
      "     91       32.0929       30.0092  0.0114\n",
      "     92       \u001b[36m32.0896\u001b[0m       30.0087  0.0118\n",
      "     93       \u001b[36m32.0883\u001b[0m       30.0225  0.0117\n",
      "     94       \u001b[36m32.0866\u001b[0m       30.0120  0.0114\n",
      "     95       \u001b[36m32.0851\u001b[0m       30.0180  0.0116\n",
      "     96       \u001b[36m32.0844\u001b[0m       30.0139  0.0114\n",
      "     97       \u001b[36m32.0833\u001b[0m       30.0226  0.0117\n",
      "     98       \u001b[36m32.0823\u001b[0m       30.0203  0.0115\n",
      "     99       \u001b[36m32.0815\u001b[0m       30.0262  0.0118\n",
      "    100       \u001b[36m32.0806\u001b[0m       30.0227  0.0116\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m33.7586\u001b[0m       \u001b[32m30.7295\u001b[0m  0.0116\n",
      "      2       \u001b[36m30.2855\u001b[0m       \u001b[32m27.8473\u001b[0m  0.0121\n",
      "      3       \u001b[36m25.9237\u001b[0m       28.6095  0.0119\n",
      "      4       \u001b[36m25.2359\u001b[0m       27.9509  0.0114\n",
      "      5       \u001b[36m24.2207\u001b[0m       \u001b[32m26.4493\u001b[0m  0.0117\n",
      "      6       \u001b[36m24.0901\u001b[0m       \u001b[32m26.3752\u001b[0m  0.0113\n",
      "      7       \u001b[36m23.8345\u001b[0m       26.7461  0.0112\n",
      "      8       \u001b[36m23.6384\u001b[0m       27.1719  0.0119\n",
      "      9       \u001b[36m23.5196\u001b[0m       26.9679  0.0117\n",
      "     10       \u001b[36m23.4624\u001b[0m       26.8225  0.0116\n",
      "     11       \u001b[36m23.3910\u001b[0m       26.8523  0.0115\n",
      "     12       \u001b[36m23.3201\u001b[0m       26.8427  0.0113\n",
      "     13       \u001b[36m23.2701\u001b[0m       26.6986  0.0123\n",
      "     14       \u001b[36m23.2344\u001b[0m       26.6723  0.0126\n",
      "     15       \u001b[36m23.2084\u001b[0m       26.7464  0.0115\n",
      "     16       \u001b[36m23.1804\u001b[0m       26.7330  0.0115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m23.1577\u001b[0m       26.6243  0.0117\n",
      "     18       \u001b[36m23.1448\u001b[0m       26.6026  0.0116\n",
      "     19       \u001b[36m23.1316\u001b[0m       26.6040  0.0160\n",
      "     20       \u001b[36m23.1197\u001b[0m       26.6075  0.0154\n",
      "     21       \u001b[36m23.1056\u001b[0m       26.5555  0.0122\n",
      "     22       \u001b[36m23.0979\u001b[0m       26.5566  0.0124\n",
      "     23       \u001b[36m23.0844\u001b[0m       26.5581  0.0132\n",
      "     24       \u001b[36m23.0768\u001b[0m       26.5559  0.0166\n",
      "     25       \u001b[36m23.0680\u001b[0m       26.5541  0.0119\n",
      "     26       \u001b[36m23.0628\u001b[0m       26.5533  0.0123\n",
      "     27       \u001b[36m23.0567\u001b[0m       26.5519  0.0124\n",
      "     28       \u001b[36m23.0516\u001b[0m       26.5527  0.0120\n",
      "     29       \u001b[36m23.0469\u001b[0m       26.5550  0.0117\n",
      "     30       \u001b[36m23.0426\u001b[0m       26.5557  0.0123\n",
      "     31       \u001b[36m23.0389\u001b[0m       26.5572  0.0122\n",
      "     32       \u001b[36m23.0354\u001b[0m       26.5580  0.0118\n",
      "     33       \u001b[36m23.0320\u001b[0m       26.5532  0.0118\n",
      "     34       \u001b[36m23.0291\u001b[0m       26.5541  0.0117\n",
      "     35       \u001b[36m23.0264\u001b[0m       26.5551  0.0117\n",
      "     36       \u001b[36m23.0236\u001b[0m       26.5595  0.0125\n",
      "     37       \u001b[36m23.0212\u001b[0m       26.5632  0.0119\n",
      "     38       \u001b[36m23.0189\u001b[0m       26.5642  0.0119\n",
      "     39       \u001b[36m23.0167\u001b[0m       26.5662  0.0114\n",
      "     40       \u001b[36m23.0145\u001b[0m       26.5714  0.0113\n",
      "     41       \u001b[36m23.0126\u001b[0m       26.5727  0.0121\n",
      "     42       \u001b[36m23.0108\u001b[0m       26.5800  0.0117\n",
      "     43       \u001b[36m23.0090\u001b[0m       26.5838  0.0114\n",
      "     44       \u001b[36m23.0071\u001b[0m       26.5809  0.0113\n",
      "     45       \u001b[36m23.0056\u001b[0m       26.5823  0.0112\n",
      "     46       \u001b[36m23.0039\u001b[0m       26.5846  0.0117\n",
      "     47       \u001b[36m23.0022\u001b[0m       26.5867  0.0115\n",
      "     48       \u001b[36m23.0012\u001b[0m       26.5909  0.0119\n",
      "     49       \u001b[36m22.9995\u001b[0m       26.5943  0.0114\n",
      "     50       \u001b[36m22.9980\u001b[0m       26.5922  0.0113\n",
      "     51       \u001b[36m22.9976\u001b[0m       26.5939  0.0117\n",
      "     52       \u001b[36m22.9957\u001b[0m       26.5970  0.0115\n",
      "     53       \u001b[36m22.9953\u001b[0m       26.5951  0.0120\n",
      "     54       22.9963       26.5998  0.0112\n",
      "     55       \u001b[36m22.9942\u001b[0m       26.6003  0.0114\n",
      "     56       22.9977       26.6019  0.0120\n",
      "     57       23.0014       26.6061  0.0116\n",
      "     58       22.9962       26.6030  0.0115\n",
      "     59       22.9982       26.6062  0.0114\n",
      "     60       22.9988       26.6017  0.0113\n",
      "     61       \u001b[36m22.9914\u001b[0m       26.6095  0.0120\n",
      "     62       \u001b[36m22.9879\u001b[0m       26.6056  0.0116\n",
      "     63       \u001b[36m22.9846\u001b[0m       26.6042  0.0119\n",
      "     64       \u001b[36m22.9838\u001b[0m       26.6080  0.0118\n",
      "     65       \u001b[36m22.9822\u001b[0m       26.6030  0.0115\n",
      "     66       \u001b[36m22.9817\u001b[0m       26.6063  0.0119\n",
      "     67       \u001b[36m22.9807\u001b[0m       26.6138  0.0119\n",
      "     68       \u001b[36m22.9795\u001b[0m       26.6034  0.0119\n",
      "     69       \u001b[36m22.9791\u001b[0m       26.6081  0.0120\n",
      "     70       \u001b[36m22.9778\u001b[0m       26.6135  0.0113\n",
      "     71       \u001b[36m22.9770\u001b[0m       26.6082  0.0122\n",
      "     72       \u001b[36m22.9764\u001b[0m       26.6137  0.0116\n",
      "     73       \u001b[36m22.9752\u001b[0m       26.6141  0.0119\n",
      "     74       \u001b[36m22.9748\u001b[0m       26.6079  0.0113\n",
      "     75       \u001b[36m22.9739\u001b[0m       26.6106  0.0114\n",
      "     76       \u001b[36m22.9730\u001b[0m       26.6103  0.0122\n",
      "     77       22.9734       26.6101  0.0116\n",
      "     78       \u001b[36m22.9721\u001b[0m       26.6155  0.0116\n",
      "     79       \u001b[36m22.9720\u001b[0m       26.6070  0.0118\n",
      "     80       22.9746       26.6143  0.0116\n",
      "     81       22.9743       26.6273  0.0113\n",
      "     82       22.9792       26.6037  0.0113\n",
      "     83       22.9895       26.6327  0.0122\n",
      "     84       22.9938       26.6488  0.0117\n",
      "     85       23.0098       26.6104  0.0115\n",
      "     86       23.0185       26.6075  0.0116\n",
      "     87       22.9744       26.6475  0.0129\n",
      "     88       22.9731       26.6121  0.0118\n",
      "     89       22.9761       26.6136  0.0126\n",
      "     90       22.9733       26.6242  0.0120\n",
      "     91       \u001b[36m22.9717\u001b[0m       26.6168  0.0120\n",
      "     92       \u001b[36m22.9692\u001b[0m       26.6108  0.0113\n",
      "     93       22.9734       26.6303  0.0126\n",
      "     94       \u001b[36m22.9685\u001b[0m       26.6314  0.0116\n",
      "     95       22.9706       26.6157  0.0128\n",
      "     96       \u001b[36m22.9678\u001b[0m       26.6198  0.0149\n",
      "     97       22.9721       26.6272  0.0121\n",
      "     98       \u001b[36m22.9677\u001b[0m       26.6357  0.0230\n",
      "     99       22.9712       26.6315  0.0137\n",
      "    100       \u001b[36m22.9677\u001b[0m       26.6387  0.0141\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m39.3237\u001b[0m       \u001b[32m29.8040\u001b[0m  0.0128\n",
      "      2       \u001b[36m35.1319\u001b[0m       \u001b[32m26.8153\u001b[0m  0.0139\n",
      "      3       \u001b[36m30.5098\u001b[0m       30.4558  0.0144\n",
      "      4       31.1558       28.0788  0.0126\n",
      "      5       \u001b[36m29.5940\u001b[0m       \u001b[32m26.7066\u001b[0m  0.0122\n",
      "      6       \u001b[36m29.5225\u001b[0m       26.8154  0.0121\n",
      "      7       \u001b[36m29.0858\u001b[0m       27.8396  0.0121\n",
      "      8       29.1230       28.1039  0.0123\n",
      "      9       \u001b[36m28.9545\u001b[0m       27.2341  0.0122\n",
      "     10       \u001b[36m28.7809\u001b[0m       26.9962  0.0121\n",
      "     11       \u001b[36m28.6870\u001b[0m       27.3415  0.0122\n",
      "     12       \u001b[36m28.6699\u001b[0m       27.4873  0.0121\n",
      "     13       \u001b[36m28.6265\u001b[0m       27.2278  0.0122\n",
      "     14       \u001b[36m28.5538\u001b[0m       27.1286  0.0122\n",
      "     15       \u001b[36m28.5253\u001b[0m       27.2466  0.0122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     16       \u001b[36m28.5167\u001b[0m       27.3096  0.0126\n",
      "     17       \u001b[36m28.5031\u001b[0m       27.2145  0.0121\n",
      "     18       \u001b[36m28.4794\u001b[0m       27.1839  0.0138\n",
      "     19       \u001b[36m28.4692\u001b[0m       27.2487  0.0124\n",
      "     20       \u001b[36m28.4662\u001b[0m       27.2477  0.0122\n",
      "     21       \u001b[36m28.4554\u001b[0m       27.2073  0.0132\n",
      "     22       \u001b[36m28.4458\u001b[0m       27.2149  0.0139\n",
      "     23       \u001b[36m28.4409\u001b[0m       27.2374  0.0135\n",
      "     24       \u001b[36m28.4370\u001b[0m       27.2280  0.0123\n",
      "     25       \u001b[36m28.4310\u001b[0m       27.2179  0.0133\n",
      "     26       \u001b[36m28.4266\u001b[0m       27.2309  0.0137\n",
      "     27       \u001b[36m28.4239\u001b[0m       27.2354  0.0139\n",
      "     28       \u001b[36m28.4203\u001b[0m       27.2286  0.0129\n",
      "     29       \u001b[36m28.4168\u001b[0m       27.2302  0.0126\n",
      "     30       \u001b[36m28.4142\u001b[0m       27.2336  0.0129\n",
      "     31       \u001b[36m28.4119\u001b[0m       27.2287  0.0127\n",
      "     32       \u001b[36m28.4091\u001b[0m       27.2312  0.0126\n",
      "     33       \u001b[36m28.4073\u001b[0m       27.2361  0.0120\n",
      "     34       \u001b[36m28.4054\u001b[0m       27.2367  0.0126\n",
      "     35       \u001b[36m28.4035\u001b[0m       27.2411  0.0127\n",
      "     36       \u001b[36m28.4019\u001b[0m       27.2411  0.0123\n",
      "     37       \u001b[36m28.4003\u001b[0m       27.2459  0.0124\n",
      "     38       \u001b[36m28.3989\u001b[0m       27.2482  0.0127\n",
      "     39       \u001b[36m28.3975\u001b[0m       27.2531  0.0121\n",
      "     40       \u001b[36m28.3962\u001b[0m       27.2484  0.0123\n",
      "     41       \u001b[36m28.3950\u001b[0m       27.2611  0.0126\n",
      "     42       \u001b[36m28.3942\u001b[0m       27.2470  0.0125\n",
      "     43       \u001b[36m28.3933\u001b[0m       27.2424  0.0118\n",
      "     44       \u001b[36m28.3917\u001b[0m       27.2615  0.0115\n",
      "     45       \u001b[36m28.3908\u001b[0m       27.2412  0.0118\n",
      "     46       \u001b[36m28.3893\u001b[0m       27.2639  0.0115\n",
      "     47       \u001b[36m28.3888\u001b[0m       27.2556  0.0119\n",
      "     48       \u001b[36m28.3879\u001b[0m       27.2550  0.0113\n",
      "     49       \u001b[36m28.3869\u001b[0m       27.2691  0.0113\n",
      "     50       \u001b[36m28.3864\u001b[0m       27.2675  0.0118\n",
      "     51       \u001b[36m28.3855\u001b[0m       27.2720  0.0115\n",
      "     52       \u001b[36m28.3849\u001b[0m       27.2758  0.0117\n",
      "     53       \u001b[36m28.3843\u001b[0m       27.2749  0.0115\n",
      "     54       \u001b[36m28.3835\u001b[0m       27.2839  0.0112\n",
      "     55       \u001b[36m28.3829\u001b[0m       27.2901  0.0120\n",
      "     56       28.3831       27.2845  0.0118\n",
      "     57       \u001b[36m28.3812\u001b[0m       27.2943  0.0117\n",
      "     58       28.3820       27.2948  0.0117\n",
      "     59       28.3821       27.2891  0.0129\n",
      "     60       \u001b[36m28.3807\u001b[0m       27.3074  0.0119\n",
      "     61       28.3832       27.3002  0.0114\n",
      "     62       28.3809       27.2936  0.0113\n",
      "     63       28.3839       27.3162  0.0117\n",
      "     64       28.3858       27.2956  0.0116\n",
      "     65       \u001b[36m28.3778\u001b[0m       27.3047  0.0119\n",
      "     66       28.3819       27.3295  0.0115\n",
      "     67       28.3796       27.3111  0.0114\n",
      "     68       \u001b[36m28.3763\u001b[0m       27.3062  0.0118\n",
      "     69       28.3783       27.3314  0.0118\n",
      "     70       \u001b[36m28.3752\u001b[0m       27.2962  0.0116\n",
      "     71       28.3761       27.3336  0.0116\n",
      "     72       28.3755       27.3014  0.0115\n",
      "     73       28.3755       27.2832  0.0116\n",
      "     74       \u001b[36m28.3746\u001b[0m       27.3477  0.0177\n",
      "     75       \u001b[36m28.3727\u001b[0m       27.2910  0.0201\n",
      "     76       28.3734       27.3016  0.0199\n",
      "     77       28.3729       27.2950  0.0257\n",
      "     78       \u001b[36m28.3713\u001b[0m       27.2963  0.0195\n",
      "     79       28.3727       27.3052  0.0131\n",
      "     80       28.3716       27.2927  0.0118\n",
      "     81       \u001b[36m28.3708\u001b[0m       27.3166  0.0121\n",
      "     82       28.3724       27.3028  0.0123\n",
      "     83       \u001b[36m28.3701\u001b[0m       27.2955  0.0120\n",
      "     84       28.3705       27.3199  0.0119\n",
      "     85       28.3724       27.3032  0.0121\n",
      "     86       \u001b[36m28.3692\u001b[0m       27.2992  0.0122\n",
      "     87       28.3706       27.3217  0.0117\n",
      "     88       28.3733       27.3059  0.0120\n",
      "     89       \u001b[36m28.3681\u001b[0m       27.3064  0.0124\n",
      "     90       28.3718       27.3279  0.0117\n",
      "     91       28.3737       27.2972  0.0116\n",
      "     92       28.3694       27.3266  0.0115\n",
      "     93       28.3753       27.3395  0.0115\n",
      "     94       28.3736       27.2920  0.0117\n",
      "     95       28.3746       27.3554  0.0116\n",
      "     96       28.3765       27.3216  0.0117\n",
      "     97       \u001b[36m28.3666\u001b[0m       27.3195  0.0118\n",
      "     98       28.3719       27.3493  0.0118\n",
      "     99       28.3689       27.3018  0.0114\n",
      "    100       \u001b[36m28.3636\u001b[0m       27.3394  0.0118\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m41.9232\u001b[0m       \u001b[32m42.2786\u001b[0m  0.0110\n",
      "      2       \u001b[36m40.0500\u001b[0m       \u001b[32m39.9994\u001b[0m  0.0109\n",
      "      3       \u001b[36m38.3491\u001b[0m       \u001b[32m37.8342\u001b[0m  0.0111\n",
      "      4       \u001b[36m36.7899\u001b[0m       \u001b[32m35.8126\u001b[0m  0.0109\n",
      "      5       \u001b[36m35.4279\u001b[0m       \u001b[32m34.0454\u001b[0m  0.0107\n",
      "      6       \u001b[36m34.3519\u001b[0m       \u001b[32m32.6494\u001b[0m  0.0108\n",
      "      7       \u001b[36m33.6124\u001b[0m       \u001b[32m31.6724\u001b[0m  0.0109\n",
      "      8       \u001b[36m33.1726\u001b[0m       \u001b[32m31.0553\u001b[0m  0.0115\n",
      "      9       \u001b[36m32.9334\u001b[0m       \u001b[32m30.6873\u001b[0m  0.0110\n",
      "     10       \u001b[36m32.8007\u001b[0m       \u001b[32m30.4689\u001b[0m  0.0109\n",
      "     11       \u001b[36m32.7187\u001b[0m       \u001b[32m30.3346\u001b[0m  0.0106\n",
      "     12       \u001b[36m32.6619\u001b[0m       \u001b[32m30.2481\u001b[0m  0.0114\n",
      "     13       \u001b[36m32.6188\u001b[0m       \u001b[32m30.1899\u001b[0m  0.0109\n",
      "     14       \u001b[36m32.5841\u001b[0m       \u001b[32m30.1482\u001b[0m  0.0111\n",
      "     15       \u001b[36m32.5555\u001b[0m       \u001b[32m30.1167\u001b[0m  0.0109\n",
      "     16       \u001b[36m32.5319\u001b[0m       \u001b[32m30.0916\u001b[0m  0.0107\n",
      "     17       \u001b[36m32.5112\u001b[0m       \u001b[32m30.0713\u001b[0m  0.0110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     18       \u001b[36m32.4931\u001b[0m       \u001b[32m30.0542\u001b[0m  0.0117\n",
      "     19       \u001b[36m32.4772\u001b[0m       \u001b[32m30.0400\u001b[0m  0.0132\n",
      "     20       \u001b[36m32.4628\u001b[0m       \u001b[32m30.0278\u001b[0m  0.0109\n",
      "     21       \u001b[36m32.4497\u001b[0m       \u001b[32m30.0173\u001b[0m  0.0107\n",
      "     22       \u001b[36m32.4379\u001b[0m       \u001b[32m30.0082\u001b[0m  0.0108\n",
      "     23       \u001b[36m32.4272\u001b[0m       \u001b[32m30.0000\u001b[0m  0.0113\n",
      "     24       \u001b[36m32.4172\u001b[0m       \u001b[32m29.9927\u001b[0m  0.0112\n",
      "     25       \u001b[36m32.4081\u001b[0m       \u001b[32m29.9857\u001b[0m  0.0109\n",
      "     26       \u001b[36m32.3996\u001b[0m       \u001b[32m29.9792\u001b[0m  0.0109\n",
      "     27       \u001b[36m32.3918\u001b[0m       \u001b[32m29.9734\u001b[0m  0.0109\n",
      "     28       \u001b[36m32.3844\u001b[0m       \u001b[32m29.9680\u001b[0m  0.0112\n",
      "     29       \u001b[36m32.3777\u001b[0m       \u001b[32m29.9627\u001b[0m  0.0114\n",
      "     30       \u001b[36m32.3714\u001b[0m       \u001b[32m29.9579\u001b[0m  0.0112\n",
      "     31       \u001b[36m32.3656\u001b[0m       \u001b[32m29.9531\u001b[0m  0.0108\n",
      "     32       \u001b[36m32.3601\u001b[0m       \u001b[32m29.9485\u001b[0m  0.0108\n",
      "     33       \u001b[36m32.3549\u001b[0m       \u001b[32m29.9444\u001b[0m  0.0110\n",
      "     34       \u001b[36m32.3499\u001b[0m       \u001b[32m29.9397\u001b[0m  0.0111\n",
      "     35       \u001b[36m32.3455\u001b[0m       \u001b[32m29.9363\u001b[0m  0.0111\n",
      "     36       \u001b[36m32.3413\u001b[0m       \u001b[32m29.9322\u001b[0m  0.0108\n",
      "     37       \u001b[36m32.3373\u001b[0m       \u001b[32m29.9287\u001b[0m  0.0111\n",
      "     38       \u001b[36m32.3335\u001b[0m       \u001b[32m29.9253\u001b[0m  0.0111\n",
      "     39       \u001b[36m32.3299\u001b[0m       \u001b[32m29.9223\u001b[0m  0.0113\n",
      "     40       \u001b[36m32.3264\u001b[0m       \u001b[32m29.9194\u001b[0m  0.0109\n",
      "     41       \u001b[36m32.3232\u001b[0m       \u001b[32m29.9169\u001b[0m  0.0108\n",
      "     42       \u001b[36m32.3200\u001b[0m       \u001b[32m29.9142\u001b[0m  0.0109\n",
      "     43       \u001b[36m32.3171\u001b[0m       \u001b[32m29.9115\u001b[0m  0.0109\n",
      "     44       \u001b[36m32.3141\u001b[0m       \u001b[32m29.9091\u001b[0m  0.0202\n",
      "     45       \u001b[36m32.3114\u001b[0m       \u001b[32m29.9070\u001b[0m  0.0125\n",
      "     46       \u001b[36m32.3088\u001b[0m       \u001b[32m29.9049\u001b[0m  0.0107\n",
      "     47       \u001b[36m32.3063\u001b[0m       \u001b[32m29.9030\u001b[0m  0.0108\n",
      "     48       \u001b[36m32.3038\u001b[0m       \u001b[32m29.9010\u001b[0m  0.0112\n",
      "     49       \u001b[36m32.3014\u001b[0m       \u001b[32m29.8990\u001b[0m  0.0111\n",
      "     50       \u001b[36m32.2992\u001b[0m       \u001b[32m29.8971\u001b[0m  0.0110\n",
      "     51       \u001b[36m32.2970\u001b[0m       \u001b[32m29.8949\u001b[0m  0.0112\n",
      "     52       \u001b[36m32.2950\u001b[0m       \u001b[32m29.8934\u001b[0m  0.0168\n",
      "     53       \u001b[36m32.2929\u001b[0m       \u001b[32m29.8912\u001b[0m  0.0134\n",
      "     54       \u001b[36m32.2910\u001b[0m       \u001b[32m29.8899\u001b[0m  0.0118\n",
      "     55       \u001b[36m32.2891\u001b[0m       \u001b[32m29.8879\u001b[0m  0.0112\n",
      "     56       \u001b[36m32.2874\u001b[0m       \u001b[32m29.8868\u001b[0m  0.0118\n",
      "     57       \u001b[36m32.2856\u001b[0m       \u001b[32m29.8848\u001b[0m  0.0133\n",
      "     58       \u001b[36m32.2840\u001b[0m       \u001b[32m29.8836\u001b[0m  0.0117\n",
      "     59       \u001b[36m32.2823\u001b[0m       \u001b[32m29.8820\u001b[0m  0.0122\n",
      "     60       \u001b[36m32.2808\u001b[0m       \u001b[32m29.8809\u001b[0m  0.0113\n",
      "     61       \u001b[36m32.2792\u001b[0m       \u001b[32m29.8798\u001b[0m  0.0111\n",
      "     62       \u001b[36m32.2777\u001b[0m       \u001b[32m29.8783\u001b[0m  0.0111\n",
      "     63       \u001b[36m32.2764\u001b[0m       \u001b[32m29.8774\u001b[0m  0.0111\n",
      "     64       \u001b[36m32.2749\u001b[0m       \u001b[32m29.8761\u001b[0m  0.0111\n",
      "     65       \u001b[36m32.2737\u001b[0m       \u001b[32m29.8752\u001b[0m  0.0110\n",
      "     66       \u001b[36m32.2722\u001b[0m       \u001b[32m29.8739\u001b[0m  0.0107\n",
      "     67       \u001b[36m32.2709\u001b[0m       \u001b[32m29.8729\u001b[0m  0.0112\n",
      "     68       \u001b[36m32.2698\u001b[0m       \u001b[32m29.8720\u001b[0m  0.0110\n",
      "     69       \u001b[36m32.2685\u001b[0m       \u001b[32m29.8710\u001b[0m  0.0112\n",
      "     70       \u001b[36m32.2673\u001b[0m       \u001b[32m29.8700\u001b[0m  0.0109\n",
      "     71       \u001b[36m32.2662\u001b[0m       \u001b[32m29.8688\u001b[0m  0.0112\n",
      "     72       \u001b[36m32.2652\u001b[0m       \u001b[32m29.8683\u001b[0m  0.0106\n",
      "     73       \u001b[36m32.2640\u001b[0m       \u001b[32m29.8668\u001b[0m  0.0110\n",
      "     74       \u001b[36m32.2629\u001b[0m       \u001b[32m29.8657\u001b[0m  0.0109\n",
      "     75       \u001b[36m32.2620\u001b[0m       \u001b[32m29.8648\u001b[0m  0.0112\n",
      "     76       \u001b[36m32.2610\u001b[0m       \u001b[32m29.8639\u001b[0m  0.0106\n",
      "     77       \u001b[36m32.2599\u001b[0m       \u001b[32m29.8628\u001b[0m  0.0108\n",
      "     78       \u001b[36m32.2592\u001b[0m       \u001b[32m29.8622\u001b[0m  0.0112\n",
      "     79       \u001b[36m32.2582\u001b[0m       \u001b[32m29.8614\u001b[0m  0.0112\n",
      "     80       \u001b[36m32.2572\u001b[0m       \u001b[32m29.8601\u001b[0m  0.0126\n",
      "     81       \u001b[36m32.2565\u001b[0m       \u001b[32m29.8597\u001b[0m  0.0111\n",
      "     82       \u001b[36m32.2555\u001b[0m       \u001b[32m29.8581\u001b[0m  0.0107\n",
      "     83       \u001b[36m32.2547\u001b[0m       \u001b[32m29.8576\u001b[0m  0.0110\n",
      "     84       \u001b[36m32.2539\u001b[0m       \u001b[32m29.8561\u001b[0m  0.0111\n",
      "     85       \u001b[36m32.2532\u001b[0m       \u001b[32m29.8557\u001b[0m  0.0108\n",
      "     86       \u001b[36m32.2523\u001b[0m       \u001b[32m29.8545\u001b[0m  0.0107\n",
      "     87       \u001b[36m32.2515\u001b[0m       \u001b[32m29.8541\u001b[0m  0.0107\n",
      "     88       \u001b[36m32.2507\u001b[0m       \u001b[32m29.8527\u001b[0m  0.0114\n",
      "     89       \u001b[36m32.2502\u001b[0m       \u001b[32m29.8524\u001b[0m  0.0110\n",
      "     90       \u001b[36m32.2493\u001b[0m       \u001b[32m29.8513\u001b[0m  0.0109\n",
      "     91       \u001b[36m32.2485\u001b[0m       \u001b[32m29.8512\u001b[0m  0.0108\n",
      "     92       \u001b[36m32.2479\u001b[0m       \u001b[32m29.8500\u001b[0m  0.0108\n",
      "     93       \u001b[36m32.2472\u001b[0m       \u001b[32m29.8491\u001b[0m  0.0109\n",
      "     94       \u001b[36m32.2466\u001b[0m       \u001b[32m29.8488\u001b[0m  0.0111\n",
      "     95       \u001b[36m32.2458\u001b[0m       \u001b[32m29.8485\u001b[0m  0.0111\n",
      "     96       \u001b[36m32.2452\u001b[0m       \u001b[32m29.8476\u001b[0m  0.0108\n",
      "     97       \u001b[36m32.2446\u001b[0m       \u001b[32m29.8470\u001b[0m  0.0109\n",
      "     98       \u001b[36m32.2439\u001b[0m       \u001b[32m29.8463\u001b[0m  0.0109\n",
      "     99       \u001b[36m32.2432\u001b[0m       \u001b[32m29.8455\u001b[0m  0.0112\n",
      "    100       \u001b[36m32.2426\u001b[0m       \u001b[32m29.8453\u001b[0m  0.0117\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m34.2909\u001b[0m       \u001b[32m31.8681\u001b[0m  0.0107\n",
      "      2       \u001b[36m32.2979\u001b[0m       \u001b[32m30.5434\u001b[0m  0.0107\n",
      "      3       \u001b[36m30.5733\u001b[0m       \u001b[32m29.3737\u001b[0m  0.0113\n",
      "      4       \u001b[36m29.0003\u001b[0m       \u001b[32m28.3391\u001b[0m  0.0108\n",
      "      5       \u001b[36m27.5683\u001b[0m       \u001b[32m27.4689\u001b[0m  0.0110\n",
      "      6       \u001b[36m26.3181\u001b[0m       \u001b[32m26.8050\u001b[0m  0.0107\n",
      "      7       \u001b[36m25.2992\u001b[0m       \u001b[32m26.3730\u001b[0m  0.0108\n",
      "      8       \u001b[36m24.5372\u001b[0m       \u001b[32m26.1616\u001b[0m  0.0107\n",
      "      9       \u001b[36m24.0220\u001b[0m       \u001b[32m26.1174\u001b[0m  0.0111\n",
      "     10       \u001b[36m23.7075\u001b[0m       26.1648  0.0108\n",
      "     11       \u001b[36m23.5308\u001b[0m       26.2381  0.0107\n",
      "     12       \u001b[36m23.4332\u001b[0m       26.3016  0.0109\n",
      "     13       \u001b[36m23.3760\u001b[0m       26.3463  0.0118\n",
      "     14       \u001b[36m23.3392\u001b[0m       26.3738  0.0110\n",
      "     15       \u001b[36m23.3129\u001b[0m       26.3894  0.0109\n",
      "     16       \u001b[36m23.2924\u001b[0m       26.3973  0.0108\n",
      "     17       \u001b[36m23.2751\u001b[0m       26.4003  0.0108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     18       \u001b[36m23.2600\u001b[0m       26.4004  0.0111\n",
      "     19       \u001b[36m23.2466\u001b[0m       26.3993  0.0114\n",
      "     20       \u001b[36m23.2346\u001b[0m       26.3973  0.0107\n",
      "     21       \u001b[36m23.2239\u001b[0m       26.3957  0.0107\n",
      "     22       \u001b[36m23.2144\u001b[0m       26.3936  0.0108\n",
      "     23       \u001b[36m23.2057\u001b[0m       26.3913  0.0110\n",
      "     24       \u001b[36m23.1978\u001b[0m       26.3891  0.0119\n",
      "     25       \u001b[36m23.1905\u001b[0m       26.3871  0.0111\n",
      "     26       \u001b[36m23.1837\u001b[0m       26.3852  0.0106\n",
      "     27       \u001b[36m23.1775\u001b[0m       26.3834  0.0105\n",
      "     28       \u001b[36m23.1717\u001b[0m       26.3816  0.0109\n",
      "     29       \u001b[36m23.1663\u001b[0m       26.3799  0.0110\n",
      "     30       \u001b[36m23.1613\u001b[0m       26.3786  0.0111\n",
      "     31       \u001b[36m23.1566\u001b[0m       26.3773  0.0107\n",
      "     32       \u001b[36m23.1522\u001b[0m       26.3761  0.0114\n",
      "     33       \u001b[36m23.1480\u001b[0m       26.3752  0.0112\n",
      "     34       \u001b[36m23.1441\u001b[0m       26.3744  0.0140\n",
      "     35       \u001b[36m23.1404\u001b[0m       26.3738  0.0126\n",
      "     36       \u001b[36m23.1370\u001b[0m       26.3734  0.0168\n",
      "     37       \u001b[36m23.1337\u001b[0m       26.3729  0.0117\n",
      "     38       \u001b[36m23.1306\u001b[0m       26.3726  0.0118\n",
      "     39       \u001b[36m23.1277\u001b[0m       26.3723  0.0120\n",
      "     40       \u001b[36m23.1249\u001b[0m       26.3721  0.0114\n",
      "     41       \u001b[36m23.1223\u001b[0m       26.3719  0.0130\n",
      "     42       \u001b[36m23.1198\u001b[0m       26.3718  0.0120\n",
      "     43       \u001b[36m23.1174\u001b[0m       26.3718  0.0136\n",
      "     44       \u001b[36m23.1152\u001b[0m       26.3717  0.0117\n",
      "     45       \u001b[36m23.1130\u001b[0m       26.3718  0.0113\n",
      "     46       \u001b[36m23.1110\u001b[0m       26.3718  0.0114\n",
      "     47       \u001b[36m23.1090\u001b[0m       26.3718  0.0117\n",
      "     48       \u001b[36m23.1072\u001b[0m       26.3719  0.0110\n",
      "     49       \u001b[36m23.1054\u001b[0m       26.3719  0.0113\n",
      "     50       \u001b[36m23.1037\u001b[0m       26.3720  0.0114\n",
      "     51       \u001b[36m23.1021\u001b[0m       26.3721  0.0116\n",
      "     52       \u001b[36m23.1005\u001b[0m       26.3723  0.0116\n",
      "     53       \u001b[36m23.0990\u001b[0m       26.3723  0.0111\n",
      "     54       \u001b[36m23.0976\u001b[0m       26.3723  0.0110\n",
      "     55       \u001b[36m23.0962\u001b[0m       26.3723  0.0107\n",
      "     56       \u001b[36m23.0949\u001b[0m       26.3723  0.0107\n",
      "     57       \u001b[36m23.0937\u001b[0m       26.3724  0.0108\n",
      "     58       \u001b[36m23.0924\u001b[0m       26.3724  0.0114\n",
      "     59       \u001b[36m23.0913\u001b[0m       26.3725  0.0108\n",
      "     60       \u001b[36m23.0901\u001b[0m       26.3725  0.0108\n",
      "     61       \u001b[36m23.0890\u001b[0m       26.3726  0.0107\n",
      "     62       \u001b[36m23.0879\u001b[0m       26.3726  0.0106\n",
      "     63       \u001b[36m23.0869\u001b[0m       26.3727  0.0111\n",
      "     64       \u001b[36m23.0859\u001b[0m       26.3727  0.0110\n",
      "     65       \u001b[36m23.0849\u001b[0m       26.3727  0.0107\n",
      "     66       \u001b[36m23.0840\u001b[0m       26.3728  0.0107\n",
      "     67       \u001b[36m23.0831\u001b[0m       26.3727  0.0109\n",
      "     68       \u001b[36m23.0822\u001b[0m       26.3728  0.0108\n",
      "     69       \u001b[36m23.0813\u001b[0m       26.3729  0.0110\n",
      "     70       \u001b[36m23.0805\u001b[0m       26.3729  0.0108\n",
      "     71       \u001b[36m23.0797\u001b[0m       26.3731  0.0107\n",
      "     72       \u001b[36m23.0789\u001b[0m       26.3731  0.0110\n",
      "     73       \u001b[36m23.0782\u001b[0m       26.3731  0.0111\n",
      "     74       \u001b[36m23.0775\u001b[0m       26.3732  0.0109\n",
      "     75       \u001b[36m23.0768\u001b[0m       26.3731  0.0108\n",
      "     76       \u001b[36m23.0761\u001b[0m       26.3732  0.0108\n",
      "     77       \u001b[36m23.0754\u001b[0m       26.3731  0.0107\n",
      "     78       \u001b[36m23.0747\u001b[0m       26.3732  0.0108\n",
      "     79       \u001b[36m23.0741\u001b[0m       26.3732  0.0118\n",
      "     80       \u001b[36m23.0735\u001b[0m       26.3732  0.0121\n",
      "     81       \u001b[36m23.0729\u001b[0m       26.3732  0.0118\n",
      "     82       \u001b[36m23.0723\u001b[0m       26.3732  0.0119\n",
      "     83       \u001b[36m23.0717\u001b[0m       26.3732  0.0125\n",
      "     84       \u001b[36m23.0711\u001b[0m       26.3731  0.0111\n",
      "     85       \u001b[36m23.0706\u001b[0m       26.3732  0.0112\n",
      "     86       \u001b[36m23.0700\u001b[0m       26.3731  0.0108\n",
      "     87       \u001b[36m23.0695\u001b[0m       26.3731  0.0110\n",
      "     88       \u001b[36m23.0689\u001b[0m       26.3732  0.0111\n",
      "     89       \u001b[36m23.0684\u001b[0m       26.3732  0.0133\n",
      "     90       \u001b[36m23.0679\u001b[0m       26.3732  0.0111\n",
      "     91       \u001b[36m23.0674\u001b[0m       26.3733  0.0110\n",
      "     92       \u001b[36m23.0670\u001b[0m       26.3734  0.0114\n",
      "     93       \u001b[36m23.0665\u001b[0m       26.3735  0.0112\n",
      "     94       \u001b[36m23.0660\u001b[0m       26.3735  0.0119\n",
      "     95       \u001b[36m23.0656\u001b[0m       26.3736  0.0111\n",
      "     96       \u001b[36m23.0652\u001b[0m       26.3736  0.0110\n",
      "     97       \u001b[36m23.0647\u001b[0m       26.3736  0.0111\n",
      "     98       \u001b[36m23.0643\u001b[0m       26.3737  0.0110\n",
      "     99       \u001b[36m23.0639\u001b[0m       26.3737  0.0108\n",
      "    100       \u001b[36m23.0635\u001b[0m       26.3737  0.0103\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m39.8262\u001b[0m       \u001b[32m30.8691\u001b[0m  0.0105\n",
      "      2       \u001b[36m37.5867\u001b[0m       \u001b[32m29.4598\u001b[0m  0.0113\n",
      "      3       \u001b[36m35.5539\u001b[0m       \u001b[32m28.2377\u001b[0m  0.0107\n",
      "      4       \u001b[36m33.7028\u001b[0m       \u001b[32m27.2285\u001b[0m  0.0109\n",
      "      5       \u001b[36m32.0488\u001b[0m       \u001b[32m26.4983\u001b[0m  0.0109\n",
      "      6       \u001b[36m30.6611\u001b[0m       \u001b[32m26.1461\u001b[0m  0.0110\n",
      "      7       \u001b[36m29.6633\u001b[0m       26.1798  0.0120\n",
      "      8       \u001b[36m29.0973\u001b[0m       26.4341  0.0111\n",
      "      9       \u001b[36m28.8495\u001b[0m       26.7067  0.0111\n",
      "     10       \u001b[36m28.7577\u001b[0m       26.9019  0.0110\n",
      "     11       \u001b[36m28.7211\u001b[0m       27.0185  0.0107\n",
      "     12       \u001b[36m28.6989\u001b[0m       27.0821  0.0117\n",
      "     13       \u001b[36m28.6799\u001b[0m       27.1158  0.0112\n",
      "     14       \u001b[36m28.6621\u001b[0m       27.1339  0.0110\n",
      "     15       \u001b[36m28.6454\u001b[0m       27.1442  0.0110\n",
      "     16       \u001b[36m28.6304\u001b[0m       27.1514  0.0113\n",
      "     17       \u001b[36m28.6168\u001b[0m       27.1559  0.0110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     18       \u001b[36m28.6042\u001b[0m       27.1598  0.0119\n",
      "     19       \u001b[36m28.5936\u001b[0m       27.1645  0.0150\n",
      "     20       \u001b[36m28.5838\u001b[0m       27.1678  0.0126\n",
      "     21       \u001b[36m28.5745\u001b[0m       27.1700  0.0111\n",
      "     22       \u001b[36m28.5661\u001b[0m       27.1725  0.0114\n",
      "     23       \u001b[36m28.5581\u001b[0m       27.1741  0.0116\n",
      "     24       \u001b[36m28.5508\u001b[0m       27.1761  0.0131\n",
      "     25       \u001b[36m28.5440\u001b[0m       27.1776  0.0113\n",
      "     26       \u001b[36m28.5379\u001b[0m       27.1789  0.0119\n",
      "     27       \u001b[36m28.5320\u001b[0m       27.1799  0.0111\n",
      "     28       \u001b[36m28.5266\u001b[0m       27.1811  0.0115\n",
      "     29       \u001b[36m28.5216\u001b[0m       27.1822  0.0134\n",
      "     30       \u001b[36m28.5169\u001b[0m       27.1829  0.0113\n",
      "     31       \u001b[36m28.5126\u001b[0m       27.1834  0.0113\n",
      "     32       \u001b[36m28.5086\u001b[0m       27.1840  0.0128\n",
      "     33       \u001b[36m28.5047\u001b[0m       27.1845  0.0122\n",
      "     34       \u001b[36m28.5011\u001b[0m       27.1847  0.0113\n",
      "     35       \u001b[36m28.4976\u001b[0m       27.1856  0.0109\n",
      "     36       \u001b[36m28.4944\u001b[0m       27.1860  0.0111\n",
      "     37       \u001b[36m28.4913\u001b[0m       27.1865  0.0110\n",
      "     38       \u001b[36m28.4884\u001b[0m       27.1874  0.0109\n",
      "     39       \u001b[36m28.4857\u001b[0m       27.1878  0.0109\n",
      "     40       \u001b[36m28.4831\u001b[0m       27.1890  0.0119\n",
      "     41       \u001b[36m28.4808\u001b[0m       27.1896  0.0107\n",
      "     42       \u001b[36m28.4785\u001b[0m       27.1900  0.0107\n",
      "     43       \u001b[36m28.4763\u001b[0m       27.1905  0.0115\n",
      "     44       \u001b[36m28.4743\u001b[0m       27.1908  0.0113\n",
      "     45       \u001b[36m28.4723\u001b[0m       27.1910  0.0116\n",
      "     46       \u001b[36m28.4704\u001b[0m       27.1911  0.0109\n",
      "     47       \u001b[36m28.4686\u001b[0m       27.1912  0.0129\n",
      "     48       \u001b[36m28.4668\u001b[0m       27.1913  0.0118\n",
      "     49       \u001b[36m28.4651\u001b[0m       27.1912  0.0115\n",
      "     50       \u001b[36m28.4635\u001b[0m       27.1912  0.0113\n",
      "     51       \u001b[36m28.4620\u001b[0m       27.1910  0.0114\n",
      "     52       \u001b[36m28.4604\u001b[0m       27.1908  0.0113\n",
      "     53       \u001b[36m28.4590\u001b[0m       27.1906  0.0118\n",
      "     54       \u001b[36m28.4575\u001b[0m       27.1903  0.0120\n",
      "     55       \u001b[36m28.4561\u001b[0m       27.1904  0.0114\n",
      "     56       \u001b[36m28.4548\u001b[0m       27.1902  0.0111\n",
      "     57       \u001b[36m28.4535\u001b[0m       27.1899  0.0114\n",
      "     58       \u001b[36m28.4523\u001b[0m       27.1898  0.0117\n",
      "     59       \u001b[36m28.4511\u001b[0m       27.1895  0.0113\n",
      "     60       \u001b[36m28.4499\u001b[0m       27.1894  0.0113\n",
      "     61       \u001b[36m28.4488\u001b[0m       27.1893  0.0112\n",
      "     62       \u001b[36m28.4477\u001b[0m       27.1890  0.0111\n",
      "     63       \u001b[36m28.4466\u001b[0m       27.1888  0.0110\n",
      "     64       \u001b[36m28.4456\u001b[0m       27.1885  0.0115\n",
      "     65       \u001b[36m28.4446\u001b[0m       27.1885  0.0134\n",
      "     66       \u001b[36m28.4437\u001b[0m       27.1882  0.0111\n",
      "     67       \u001b[36m28.4427\u001b[0m       27.1880  0.0122\n",
      "     68       \u001b[36m28.4419\u001b[0m       27.1879  0.0119\n",
      "     69       \u001b[36m28.4410\u001b[0m       27.1877  0.0118\n",
      "     70       \u001b[36m28.4401\u001b[0m       27.1875  0.0119\n",
      "     71       \u001b[36m28.4393\u001b[0m       27.1873  0.0112\n",
      "     72       \u001b[36m28.4385\u001b[0m       27.1875  0.0112\n",
      "     73       \u001b[36m28.4377\u001b[0m       27.1873  0.0113\n",
      "     74       \u001b[36m28.4370\u001b[0m       27.1871  0.0111\n",
      "     75       \u001b[36m28.4363\u001b[0m       27.1865  0.0109\n",
      "     76       \u001b[36m28.4356\u001b[0m       27.1861  0.0109\n",
      "     77       \u001b[36m28.4349\u001b[0m       27.1856  0.0108\n",
      "     78       \u001b[36m28.4342\u001b[0m       27.1851  0.0117\n",
      "     79       \u001b[36m28.4335\u001b[0m       27.1844  0.0112\n",
      "     80       \u001b[36m28.4328\u001b[0m       27.1842  0.0111\n",
      "     81       \u001b[36m28.4322\u001b[0m       27.1839  0.0114\n",
      "     82       \u001b[36m28.4315\u001b[0m       27.1836  0.0115\n",
      "     83       \u001b[36m28.4309\u001b[0m       27.1833  0.0118\n",
      "     84       \u001b[36m28.4303\u001b[0m       27.1830  0.0119\n",
      "     85       \u001b[36m28.4298\u001b[0m       27.1829  0.0113\n",
      "     86       \u001b[36m28.4293\u001b[0m       27.1827  0.0110\n",
      "     87       \u001b[36m28.4287\u001b[0m       27.1825  0.0191\n",
      "     88       \u001b[36m28.4281\u001b[0m       27.1822  0.0145\n",
      "     89       \u001b[36m28.4276\u001b[0m       27.1819  0.0113\n",
      "     90       \u001b[36m28.4271\u001b[0m       27.1815  0.0111\n",
      "     91       \u001b[36m28.4266\u001b[0m       27.1813  0.0110\n",
      "     92       \u001b[36m28.4261\u001b[0m       27.1809  0.0115\n",
      "     93       \u001b[36m28.4256\u001b[0m       27.1808  0.0121\n",
      "     94       \u001b[36m28.4251\u001b[0m       27.1805  0.0110\n",
      "     95       \u001b[36m28.4246\u001b[0m       27.1804  0.0110\n",
      "     96       \u001b[36m28.4242\u001b[0m       27.1801  0.0113\n",
      "     97       \u001b[36m28.4237\u001b[0m       27.1799  0.0127\n",
      "     98       \u001b[36m28.4233\u001b[0m       27.1795  0.0129\n",
      "     99       \u001b[36m28.4229\u001b[0m       27.1795  0.0140\n",
      "    100       \u001b[36m28.4224\u001b[0m       27.1794  0.0150\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m41.8416\u001b[0m       \u001b[32m41.7469\u001b[0m  0.0135\n",
      "      2       \u001b[36m39.6081\u001b[0m       \u001b[32m38.2343\u001b[0m  0.0133\n",
      "      3       \u001b[36m36.3306\u001b[0m       \u001b[32m32.6131\u001b[0m  0.0131\n",
      "      4       \u001b[36m33.9955\u001b[0m       \u001b[32m31.1878\u001b[0m  0.0143\n",
      "      5       34.0876       \u001b[32m31.0093\u001b[0m  0.0125\n",
      "      6       \u001b[36m33.2470\u001b[0m       31.5854  0.0132\n",
      "      7       \u001b[36m33.1416\u001b[0m       31.2417  0.0128\n",
      "      8       \u001b[36m32.8804\u001b[0m       \u001b[32m30.5396\u001b[0m  0.0124\n",
      "      9       \u001b[36m32.7205\u001b[0m       \u001b[32m30.2646\u001b[0m  0.0122\n",
      "     10       \u001b[36m32.6504\u001b[0m       30.3179  0.0121\n",
      "     11       \u001b[36m32.5564\u001b[0m       30.4554  0.0127\n",
      "     12       \u001b[36m32.4947\u001b[0m       30.3684  0.0127\n",
      "     13       \u001b[36m32.4226\u001b[0m       \u001b[32m30.1920\u001b[0m  0.0141\n",
      "     14       \u001b[36m32.3810\u001b[0m       \u001b[32m30.1768\u001b[0m  0.0122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     15       \u001b[36m32.3494\u001b[0m       30.2176  0.0127\n",
      "     16       \u001b[36m32.3234\u001b[0m       30.2376  0.0125\n",
      "     17       \u001b[36m32.2988\u001b[0m       \u001b[32m30.1562\u001b[0m  0.0118\n",
      "     18       \u001b[36m32.2796\u001b[0m       \u001b[32m30.1515\u001b[0m  0.0123\n",
      "     19       \u001b[36m32.2664\u001b[0m       \u001b[32m30.1294\u001b[0m  0.0119\n",
      "     20       \u001b[36m32.2578\u001b[0m       30.2084  0.0117\n",
      "     21       \u001b[36m32.2493\u001b[0m       \u001b[32m30.1290\u001b[0m  0.0116\n",
      "     22       \u001b[36m32.2483\u001b[0m       30.2557  0.0119\n",
      "     23       32.2511       30.1680  0.0118\n",
      "     24       32.2843       30.4423  0.0119\n",
      "     25       32.2979       30.1798  0.0120\n",
      "     26       32.3545       \u001b[32m30.1198\u001b[0m  0.0122\n",
      "     27       32.2484       30.1596  0.0125\n",
      "     28       \u001b[36m32.2076\u001b[0m       \u001b[32m29.9916\u001b[0m  0.0121\n",
      "     29       32.2136       30.0825  0.0121\n",
      "     30       32.2080       30.1043  0.0119\n",
      "     31       \u001b[36m32.1878\u001b[0m       30.0368  0.0125\n",
      "     32       \u001b[36m32.1856\u001b[0m       30.0801  0.0123\n",
      "     33       \u001b[36m32.1830\u001b[0m       30.0800  0.0122\n",
      "     34       \u001b[36m32.1746\u001b[0m       30.0734  0.0122\n",
      "     35       \u001b[36m32.1719\u001b[0m       30.0918  0.0118\n",
      "     36       \u001b[36m32.1681\u001b[0m       30.0716  0.0119\n",
      "     37       \u001b[36m32.1634\u001b[0m       30.0788  0.0117\n",
      "     38       \u001b[36m32.1611\u001b[0m       30.0850  0.0118\n",
      "     39       \u001b[36m32.1569\u001b[0m       30.0773  0.0121\n",
      "     40       \u001b[36m32.1538\u001b[0m       30.0847  0.0121\n",
      "     41       \u001b[36m32.1509\u001b[0m       30.0822  0.0126\n",
      "     42       \u001b[36m32.1477\u001b[0m       30.0870  0.0124\n",
      "     43       \u001b[36m32.1454\u001b[0m       30.0858  0.0129\n",
      "     44       \u001b[36m32.1424\u001b[0m       30.0872  0.0124\n",
      "     45       \u001b[36m32.1399\u001b[0m       30.0880  0.0123\n",
      "     46       \u001b[36m32.1375\u001b[0m       30.0898  0.0130\n",
      "     47       \u001b[36m32.1351\u001b[0m       30.0908  0.0118\n",
      "     48       \u001b[36m32.1328\u001b[0m       30.0886  0.0147\n",
      "     49       \u001b[36m32.1305\u001b[0m       30.0884  0.0123\n",
      "     50       \u001b[36m32.1283\u001b[0m       30.0870  0.0117\n",
      "     51       \u001b[36m32.1262\u001b[0m       30.0873  0.0117\n",
      "     52       \u001b[36m32.1241\u001b[0m       30.0864  0.0117\n",
      "     53       \u001b[36m32.1221\u001b[0m       30.0869  0.0120\n",
      "     54       \u001b[36m32.1200\u001b[0m       30.0871  0.0118\n",
      "     55       \u001b[36m32.1183\u001b[0m       30.0855  0.0118\n",
      "     56       \u001b[36m32.1163\u001b[0m       30.0850  0.0115\n",
      "     57       \u001b[36m32.1146\u001b[0m       30.0845  0.0115\n",
      "     58       \u001b[36m32.1128\u001b[0m       30.0873  0.0120\n",
      "     59       \u001b[36m32.1111\u001b[0m       30.0852  0.0117\n",
      "     60       \u001b[36m32.1093\u001b[0m       30.0869  0.0118\n",
      "     61       \u001b[36m32.1077\u001b[0m       30.0842  0.0115\n",
      "     62       \u001b[36m32.1059\u001b[0m       30.0891  0.0115\n",
      "     63       \u001b[36m32.1044\u001b[0m       30.0841  0.0122\n",
      "     64       \u001b[36m32.1029\u001b[0m       30.0911  0.0115\n",
      "     65       \u001b[36m32.1012\u001b[0m       30.0850  0.0177\n",
      "     66       \u001b[36m32.0998\u001b[0m       30.0918  0.0121\n",
      "     67       \u001b[36m32.0981\u001b[0m       30.0863  0.0114\n",
      "     68       \u001b[36m32.0968\u001b[0m       30.0924  0.0121\n",
      "     69       \u001b[36m32.0952\u001b[0m       30.0878  0.0118\n",
      "     70       \u001b[36m32.0939\u001b[0m       30.0951  0.0114\n",
      "     71       \u001b[36m32.0924\u001b[0m       30.0906  0.0122\n",
      "     72       \u001b[36m32.0911\u001b[0m       30.0993  0.0131\n",
      "     73       \u001b[36m32.0897\u001b[0m       30.0938  0.0122\n",
      "     74       \u001b[36m32.0885\u001b[0m       30.1015  0.0119\n",
      "     75       \u001b[36m32.0870\u001b[0m       30.0957  0.0130\n",
      "     76       \u001b[36m32.0860\u001b[0m       30.1011  0.0192\n",
      "     77       \u001b[36m32.0846\u001b[0m       30.0996  0.0138\n",
      "     78       \u001b[36m32.0836\u001b[0m       30.1017  0.0136\n",
      "     79       \u001b[36m32.0822\u001b[0m       30.1002  0.0141\n",
      "     80       \u001b[36m32.0812\u001b[0m       30.1038  0.0154\n",
      "     81       \u001b[36m32.0799\u001b[0m       30.1035  0.0132\n",
      "     82       \u001b[36m32.0788\u001b[0m       30.1024  0.0132\n",
      "     83       \u001b[36m32.0777\u001b[0m       30.1040  0.0122\n",
      "     84       \u001b[36m32.0766\u001b[0m       30.1047  0.0115\n",
      "     85       \u001b[36m32.0755\u001b[0m       30.1054  0.0120\n",
      "     86       \u001b[36m32.0744\u001b[0m       30.1099  0.0117\n",
      "     87       \u001b[36m32.0734\u001b[0m       30.1073  0.0118\n",
      "     88       \u001b[36m32.0724\u001b[0m       30.1126  0.0116\n",
      "     89       \u001b[36m32.0711\u001b[0m       30.1099  0.0116\n",
      "     90       \u001b[36m32.0704\u001b[0m       30.1115  0.0119\n",
      "     91       \u001b[36m32.0692\u001b[0m       30.1169  0.0118\n",
      "     92       \u001b[36m32.0681\u001b[0m       30.1146  0.0123\n",
      "     93       \u001b[36m32.0671\u001b[0m       30.1188  0.0118\n",
      "     94       \u001b[36m32.0658\u001b[0m       30.1202  0.0116\n",
      "     95       \u001b[36m32.0652\u001b[0m       30.1214  0.0114\n",
      "     96       \u001b[36m32.0639\u001b[0m       30.1265  0.0119\n",
      "     97       \u001b[36m32.0627\u001b[0m       30.1244  0.0122\n",
      "     98       \u001b[36m32.0622\u001b[0m       30.1268  0.0123\n",
      "     99       \u001b[36m32.0608\u001b[0m       30.1336  0.0126\n",
      "    100       \u001b[36m32.0597\u001b[0m       30.1278  0.0125\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m35.0102\u001b[0m       \u001b[32m32.1936\u001b[0m  0.0120\n",
      "      2       \u001b[36m32.3685\u001b[0m       \u001b[32m29.5550\u001b[0m  0.0133\n",
      "      3       \u001b[36m28.6748\u001b[0m       \u001b[32m27.3317\u001b[0m  0.0121\n",
      "      4       \u001b[36m25.7270\u001b[0m       29.3867  0.0116\n",
      "      5       \u001b[36m25.5526\u001b[0m       28.2452  0.0114\n",
      "      6       \u001b[36m24.5800\u001b[0m       \u001b[32m26.6691\u001b[0m  0.0112\n",
      "      7       \u001b[36m24.3648\u001b[0m       \u001b[32m26.4770\u001b[0m  0.0118\n",
      "      8       \u001b[36m24.1573\u001b[0m       26.6674  0.0117\n",
      "      9       \u001b[36m23.8188\u001b[0m       27.3094  0.0116\n",
      "     10       \u001b[36m23.7024\u001b[0m       27.4237  0.0119\n",
      "     11       \u001b[36m23.5681\u001b[0m       27.1415  0.0114\n",
      "     12       \u001b[36m23.5027\u001b[0m       27.0102  0.0115\n",
      "     13       \u001b[36m23.4335\u001b[0m       27.0544  0.0118\n",
      "     14       \u001b[36m23.3674\u001b[0m       27.0982  0.0122\n",
      "     15       \u001b[36m23.3213\u001b[0m       27.1003  0.0118\n",
      "     16       \u001b[36m23.2808\u001b[0m       27.0835  0.0114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m23.2550\u001b[0m       27.0319  0.0116\n",
      "     18       \u001b[36m23.2347\u001b[0m       27.0169  0.0136\n",
      "     19       \u001b[36m23.2129\u001b[0m       27.0293  0.0123\n",
      "     20       \u001b[36m23.1934\u001b[0m       27.0101  0.0118\n",
      "     21       \u001b[36m23.1798\u001b[0m       26.9998  0.0124\n",
      "     22       \u001b[36m23.1679\u001b[0m       27.0106  0.0121\n",
      "     23       \u001b[36m23.1569\u001b[0m       27.0167  0.0128\n",
      "     24       \u001b[36m23.1469\u001b[0m       27.0221  0.0124\n",
      "     25       \u001b[36m23.1371\u001b[0m       27.0177  0.0121\n",
      "     26       \u001b[36m23.1298\u001b[0m       27.0194  0.0118\n",
      "     27       \u001b[36m23.1227\u001b[0m       27.0252  0.0116\n",
      "     28       \u001b[36m23.1160\u001b[0m       27.0274  0.0120\n",
      "     29       \u001b[36m23.1097\u001b[0m       27.0308  0.0120\n",
      "     30       \u001b[36m23.1039\u001b[0m       27.0293  0.0117\n",
      "     31       \u001b[36m23.0988\u001b[0m       27.0303  0.0118\n",
      "     32       \u001b[36m23.0936\u001b[0m       27.0286  0.0115\n",
      "     33       \u001b[36m23.0888\u001b[0m       27.0282  0.0120\n",
      "     34       \u001b[36m23.0842\u001b[0m       27.0255  0.0118\n",
      "     35       \u001b[36m23.0798\u001b[0m       27.0211  0.0123\n",
      "     36       \u001b[36m23.0753\u001b[0m       27.0182  0.0118\n",
      "     37       \u001b[36m23.0716\u001b[0m       27.0148  0.0119\n",
      "     38       \u001b[36m23.0671\u001b[0m       27.0037  0.0129\n",
      "     39       \u001b[36m23.0639\u001b[0m       27.0002  0.0124\n",
      "     40       \u001b[36m23.0597\u001b[0m       26.9919  0.0125\n",
      "     41       \u001b[36m23.0566\u001b[0m       26.9892  0.0114\n",
      "     42       \u001b[36m23.0526\u001b[0m       26.9720  0.0116\n",
      "     43       \u001b[36m23.0502\u001b[0m       26.9606  0.0122\n",
      "     44       \u001b[36m23.0461\u001b[0m       26.9492  0.0129\n",
      "     45       \u001b[36m23.0445\u001b[0m       26.9440  0.0116\n",
      "     46       \u001b[36m23.0403\u001b[0m       26.9320  0.0115\n",
      "     47       23.0408       26.9241  0.0114\n",
      "     48       \u001b[36m23.0368\u001b[0m       26.9079  0.0118\n",
      "     49       23.0398       26.9089  0.0117\n",
      "     50       \u001b[36m23.0356\u001b[0m       26.8783  0.0120\n",
      "     51       23.0397       26.9030  0.0119\n",
      "     52       \u001b[36m23.0337\u001b[0m       26.8701  0.0137\n",
      "     53       23.0455       26.8948  0.0154\n",
      "     54       23.0463       26.8893  0.0127\n",
      "     55       23.0901       26.9678  0.0122\n",
      "     56       23.1187       26.8182  0.0125\n",
      "     57       23.1657       26.9837  0.0124\n",
      "     58       23.1483       26.7410  0.0135\n",
      "     59       23.0824       26.7666  0.0127\n",
      "     60       23.0364       26.9211  0.0123\n",
      "     61       \u001b[36m23.0258\u001b[0m       26.7964  0.0118\n",
      "     62       23.0383       26.8170  0.0119\n",
      "     63       \u001b[36m23.0209\u001b[0m       26.8353  0.0120\n",
      "     64       \u001b[36m23.0152\u001b[0m       26.8149  0.0120\n",
      "     65       23.0157       26.8424  0.0119\n",
      "     66       \u001b[36m23.0105\u001b[0m       26.8158  0.0118\n",
      "     67       23.0118       26.8132  0.0116\n",
      "     68       \u001b[36m23.0080\u001b[0m       26.8164  0.0121\n",
      "     69       \u001b[36m23.0064\u001b[0m       26.8036  0.0120\n",
      "     70       \u001b[36m23.0061\u001b[0m       26.8073  0.0121\n",
      "     71       \u001b[36m23.0038\u001b[0m       26.8001  0.0114\n",
      "     72       \u001b[36m23.0035\u001b[0m       26.8031  0.0115\n",
      "     73       \u001b[36m23.0017\u001b[0m       26.7988  0.0129\n",
      "     74       \u001b[36m23.0010\u001b[0m       26.7956  0.0119\n",
      "     75       \u001b[36m22.9998\u001b[0m       26.7932  0.0125\n",
      "     76       \u001b[36m22.9988\u001b[0m       26.7912  0.0122\n",
      "     77       \u001b[36m22.9977\u001b[0m       26.7868  0.0116\n",
      "     78       \u001b[36m22.9967\u001b[0m       26.7807  0.0126\n",
      "     79       \u001b[36m22.9958\u001b[0m       26.7789  0.0125\n",
      "     80       \u001b[36m22.9947\u001b[0m       26.7751  0.0119\n",
      "     81       \u001b[36m22.9937\u001b[0m       26.7703  0.0117\n",
      "     82       \u001b[36m22.9928\u001b[0m       26.7666  0.0116\n",
      "     83       \u001b[36m22.9919\u001b[0m       26.7701  0.0122\n",
      "     84       \u001b[36m22.9910\u001b[0m       26.7644  0.0119\n",
      "     85       \u001b[36m22.9901\u001b[0m       26.7625  0.0120\n",
      "     86       \u001b[36m22.9893\u001b[0m       26.7652  0.0118\n",
      "     87       \u001b[36m22.9885\u001b[0m       26.7636  0.0116\n",
      "     88       \u001b[36m22.9876\u001b[0m       26.7615  0.0122\n",
      "     89       \u001b[36m22.9868\u001b[0m       26.7634  0.0119\n",
      "     90       \u001b[36m22.9859\u001b[0m       26.7613  0.0125\n",
      "     91       \u001b[36m22.9852\u001b[0m       26.7646  0.0117\n",
      "     92       \u001b[36m22.9844\u001b[0m       26.7650  0.0114\n",
      "     93       \u001b[36m22.9836\u001b[0m       26.7628  0.0121\n",
      "     94       \u001b[36m22.9829\u001b[0m       26.7658  0.0123\n",
      "     95       \u001b[36m22.9819\u001b[0m       26.7668  0.0124\n",
      "     96       \u001b[36m22.9814\u001b[0m       26.7660  0.0119\n",
      "     97       \u001b[36m22.9806\u001b[0m       26.7648  0.0118\n",
      "     98       \u001b[36m22.9801\u001b[0m       26.7705  0.0125\n",
      "     99       \u001b[36m22.9791\u001b[0m       26.7641  0.0121\n",
      "    100       \u001b[36m22.9786\u001b[0m       26.7705  0.0117\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m40.8958\u001b[0m       \u001b[32m31.1352\u001b[0m  0.0120\n",
      "      2       \u001b[36m37.4123\u001b[0m       \u001b[32m28.4838\u001b[0m  0.0115\n",
      "      3       \u001b[36m33.4430\u001b[0m       \u001b[32m27.1502\u001b[0m  0.0120\n",
      "      4       \u001b[36m31.1051\u001b[0m       30.2945  0.0117\n",
      "      5       31.4683       28.1866  0.0115\n",
      "      6       \u001b[36m30.2385\u001b[0m       \u001b[32m26.7725\u001b[0m  0.0115\n",
      "      7       \u001b[36m30.0036\u001b[0m       \u001b[32m26.6776\u001b[0m  0.0121\n",
      "      8       \u001b[36m29.5837\u001b[0m       27.3203  0.0117\n",
      "      9       \u001b[36m29.3799\u001b[0m       28.1006  0.0117\n",
      "     10       \u001b[36m29.2979\u001b[0m       27.8348  0.0117\n",
      "     11       \u001b[36m29.0387\u001b[0m       27.3702  0.0118\n",
      "     12       \u001b[36m28.9030\u001b[0m       27.2825  0.0123\n",
      "     13       \u001b[36m28.8153\u001b[0m       27.4917  0.0126\n",
      "     14       \u001b[36m28.7612\u001b[0m       27.6019  0.0126\n",
      "     15       \u001b[36m28.7099\u001b[0m       27.4815  0.0117\n",
      "     16       \u001b[36m28.6431\u001b[0m       27.3868  0.0118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m28.6008\u001b[0m       27.4056  0.0129\n",
      "     18       \u001b[36m28.5773\u001b[0m       27.4513  0.0129\n",
      "     19       \u001b[36m28.5571\u001b[0m       27.4636  0.0120\n",
      "     20       \u001b[36m28.5425\u001b[0m       27.3902  0.0122\n",
      "     21       \u001b[36m28.5184\u001b[0m       27.4327  0.0118\n",
      "     22       28.5254       27.3765  0.0128\n",
      "     23       28.5197       27.4845  0.0147\n",
      "     24       28.5376       27.3125  0.0119\n",
      "     25       \u001b[36m28.4981\u001b[0m       27.4204  0.0119\n",
      "     26       28.5165       27.3475  0.0115\n",
      "     27       \u001b[36m28.4855\u001b[0m       27.3841  0.0113\n",
      "     28       28.4966       27.3288  0.0121\n",
      "     29       \u001b[36m28.4637\u001b[0m       27.3627  0.0119\n",
      "     30       28.4715       27.3779  0.0146\n",
      "     31       \u001b[36m28.4449\u001b[0m       27.3251  0.0137\n",
      "     32       28.4454       27.3764  0.0123\n",
      "     33       \u001b[36m28.4369\u001b[0m       27.3151  0.0123\n",
      "     34       \u001b[36m28.4326\u001b[0m       27.3628  0.0126\n",
      "     35       \u001b[36m28.4314\u001b[0m       27.3270  0.0131\n",
      "     36       \u001b[36m28.4251\u001b[0m       27.3437  0.0130\n",
      "     37       \u001b[36m28.4248\u001b[0m       27.3333  0.0125\n",
      "     38       \u001b[36m28.4193\u001b[0m       27.3334  0.0128\n",
      "     39       \u001b[36m28.4185\u001b[0m       27.3299  0.0119\n",
      "     40       \u001b[36m28.4147\u001b[0m       27.3245  0.0118\n",
      "     41       \u001b[36m28.4136\u001b[0m       27.3256  0.0113\n",
      "     42       \u001b[36m28.4109\u001b[0m       27.3194  0.0119\n",
      "     43       \u001b[36m28.4093\u001b[0m       27.3203  0.0117\n",
      "     44       \u001b[36m28.4074\u001b[0m       27.3149  0.0116\n",
      "     45       \u001b[36m28.4057\u001b[0m       27.3116  0.0112\n",
      "     46       \u001b[36m28.4039\u001b[0m       27.3044  0.0114\n",
      "     47       \u001b[36m28.4024\u001b[0m       27.3035  0.0122\n",
      "     48       \u001b[36m28.4012\u001b[0m       27.2981  0.0119\n",
      "     49       \u001b[36m28.3997\u001b[0m       27.2901  0.0118\n",
      "     50       \u001b[36m28.3984\u001b[0m       27.2933  0.0113\n",
      "     51       \u001b[36m28.3970\u001b[0m       27.2883  0.0114\n",
      "     52       \u001b[36m28.3961\u001b[0m       27.2913  0.0117\n",
      "     53       \u001b[36m28.3946\u001b[0m       27.2820  0.0118\n",
      "     54       \u001b[36m28.3936\u001b[0m       27.2859  0.0115\n",
      "     55       \u001b[36m28.3924\u001b[0m       27.2785  0.0117\n",
      "     56       \u001b[36m28.3914\u001b[0m       27.2848  0.0116\n",
      "     57       \u001b[36m28.3904\u001b[0m       27.2816  0.0120\n",
      "     58       \u001b[36m28.3896\u001b[0m       27.2812  0.0125\n",
      "     59       \u001b[36m28.3885\u001b[0m       27.2807  0.0135\n",
      "     60       \u001b[36m28.3883\u001b[0m       27.2785  0.0119\n",
      "     61       \u001b[36m28.3873\u001b[0m       27.2819  0.0122\n",
      "     62       \u001b[36m28.3866\u001b[0m       27.2768  0.0117\n",
      "     63       \u001b[36m28.3860\u001b[0m       27.2764  0.0114\n",
      "     64       \u001b[36m28.3856\u001b[0m       27.2764  0.0117\n",
      "     65       \u001b[36m28.3844\u001b[0m       27.2666  0.0121\n",
      "     66       \u001b[36m28.3842\u001b[0m       27.2769  0.0116\n",
      "     67       \u001b[36m28.3827\u001b[0m       27.2635  0.0120\n",
      "     68       \u001b[36m28.3824\u001b[0m       27.2763  0.0119\n",
      "     69       \u001b[36m28.3816\u001b[0m       27.2647  0.0119\n",
      "     70       \u001b[36m28.3809\u001b[0m       27.2724  0.0116\n",
      "     71       \u001b[36m28.3805\u001b[0m       27.2658  0.0117\n",
      "     72       \u001b[36m28.3799\u001b[0m       27.2711  0.0122\n",
      "     73       \u001b[36m28.3793\u001b[0m       27.2687  0.0115\n",
      "     74       \u001b[36m28.3791\u001b[0m       27.2666  0.0119\n",
      "     75       \u001b[36m28.3785\u001b[0m       27.2689  0.0118\n",
      "     76       \u001b[36m28.3779\u001b[0m       27.2715  0.0120\n",
      "     77       28.3785       27.2617  0.0127\n",
      "     78       \u001b[36m28.3777\u001b[0m       27.2810  0.0128\n",
      "     79       28.3783       27.2602  0.0120\n",
      "     80       28.3802       27.2728  0.0126\n",
      "     81       28.3820       27.2777  0.0122\n",
      "     82       28.3864       27.2498  0.0117\n",
      "     83       28.3918       27.2767  0.0118\n",
      "     84       28.3883       27.2852  0.0119\n",
      "     85       28.4089       27.2538  0.0119\n",
      "     86       28.4045       27.3061  0.0118\n",
      "     87       28.3895       27.2500  0.0118\n",
      "     88       28.3830       27.2589  0.0117\n",
      "     89       \u001b[36m28.3737\u001b[0m       27.2854  0.0117\n",
      "     90       28.3783       27.2819  0.0119\n",
      "     91       \u001b[36m28.3726\u001b[0m       27.2695  0.0122\n",
      "     92       \u001b[36m28.3725\u001b[0m       27.2621  0.0122\n",
      "     93       28.3730       27.2813  0.0118\n",
      "     94       \u001b[36m28.3717\u001b[0m       27.2550  0.0114\n",
      "     95       \u001b[36m28.3705\u001b[0m       27.2712  0.0115\n",
      "     96       28.3712       27.2698  0.0128\n",
      "     97       28.3706       27.2640  0.0119\n",
      "     98       \u001b[36m28.3695\u001b[0m       27.2693  0.0121\n",
      "     99       28.3702       27.2719  0.0116\n",
      "    100       \u001b[36m28.3690\u001b[0m       27.2675  0.0116\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m42.3929\u001b[0m       \u001b[32m43.4693\u001b[0m  0.0115\n",
      "      2       \u001b[36m41.1826\u001b[0m       \u001b[32m42.1323\u001b[0m  0.0135\n",
      "      3       \u001b[36m40.1434\u001b[0m       \u001b[32m40.9095\u001b[0m  0.0128\n",
      "      4       \u001b[36m39.1811\u001b[0m       \u001b[32m39.7036\u001b[0m  0.0110\n",
      "      5       \u001b[36m38.2287\u001b[0m       \u001b[32m38.4700\u001b[0m  0.0112\n",
      "      6       \u001b[36m37.2879\u001b[0m       \u001b[32m37.2421\u001b[0m  0.0111\n",
      "      7       \u001b[36m36.3809\u001b[0m       \u001b[32m36.0284\u001b[0m  0.0109\n",
      "      8       \u001b[36m35.5285\u001b[0m       \u001b[32m34.8729\u001b[0m  0.0116\n",
      "      9       \u001b[36m34.7623\u001b[0m       \u001b[32m33.8144\u001b[0m  0.0153\n",
      "     10       \u001b[36m34.1100\u001b[0m       \u001b[32m32.8899\u001b[0m  0.0118\n",
      "     11       \u001b[36m33.5876\u001b[0m       \u001b[32m32.1207\u001b[0m  0.0110\n",
      "     12       \u001b[36m33.1948\u001b[0m       \u001b[32m31.5123\u001b[0m  0.0116\n",
      "     13       \u001b[36m32.9173\u001b[0m       \u001b[32m31.0543\u001b[0m  0.0119\n",
      "     14       \u001b[36m32.7318\u001b[0m       \u001b[32m30.7239\u001b[0m  0.0118\n",
      "     15       \u001b[36m32.6126\u001b[0m       \u001b[32m30.4935\u001b[0m  0.0120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     16       \u001b[36m32.5372\u001b[0m       \u001b[32m30.3362\u001b[0m  0.0121\n",
      "     17       \u001b[36m32.4888\u001b[0m       \u001b[32m30.2295\u001b[0m  0.0112\n",
      "     18       \u001b[36m32.4560\u001b[0m       \u001b[32m30.1568\u001b[0m  0.0110\n",
      "     19       \u001b[36m32.4332\u001b[0m       \u001b[32m30.1067\u001b[0m  0.0112\n",
      "     20       \u001b[36m32.4159\u001b[0m       \u001b[32m30.0717\u001b[0m  0.0108\n",
      "     21       \u001b[36m32.4022\u001b[0m       \u001b[32m30.0465\u001b[0m  0.0107\n",
      "     22       \u001b[36m32.3909\u001b[0m       \u001b[32m30.0277\u001b[0m  0.0108\n",
      "     23       \u001b[36m32.3811\u001b[0m       \u001b[32m30.0132\u001b[0m  0.0110\n",
      "     24       \u001b[36m32.3724\u001b[0m       \u001b[32m30.0016\u001b[0m  0.0111\n",
      "     25       \u001b[36m32.3648\u001b[0m       \u001b[32m29.9922\u001b[0m  0.0111\n",
      "     26       \u001b[36m32.3579\u001b[0m       \u001b[32m29.9842\u001b[0m  0.0109\n",
      "     27       \u001b[36m32.3517\u001b[0m       \u001b[32m29.9773\u001b[0m  0.0114\n",
      "     28       \u001b[36m32.3461\u001b[0m       \u001b[32m29.9713\u001b[0m  0.0113\n",
      "     29       \u001b[36m32.3408\u001b[0m       \u001b[32m29.9659\u001b[0m  0.0111\n",
      "     30       \u001b[36m32.3359\u001b[0m       \u001b[32m29.9610\u001b[0m  0.0107\n",
      "     31       \u001b[36m32.3313\u001b[0m       \u001b[32m29.9565\u001b[0m  0.0109\n",
      "     32       \u001b[36m32.3271\u001b[0m       \u001b[32m29.9523\u001b[0m  0.0108\n",
      "     33       \u001b[36m32.3230\u001b[0m       \u001b[32m29.9483\u001b[0m  0.0110\n",
      "     34       \u001b[36m32.3192\u001b[0m       \u001b[32m29.9445\u001b[0m  0.0108\n",
      "     35       \u001b[36m32.3157\u001b[0m       \u001b[32m29.9409\u001b[0m  0.0108\n",
      "     36       \u001b[36m32.3123\u001b[0m       \u001b[32m29.9374\u001b[0m  0.0113\n",
      "     37       \u001b[36m32.3091\u001b[0m       \u001b[32m29.9340\u001b[0m  0.0113\n",
      "     38       \u001b[36m32.3061\u001b[0m       \u001b[32m29.9307\u001b[0m  0.0111\n",
      "     39       \u001b[36m32.3032\u001b[0m       \u001b[32m29.9276\u001b[0m  0.0116\n",
      "     40       \u001b[36m32.3004\u001b[0m       \u001b[32m29.9242\u001b[0m  0.0108\n",
      "     41       \u001b[36m32.2979\u001b[0m       \u001b[32m29.9214\u001b[0m  0.0110\n",
      "     42       \u001b[36m32.2954\u001b[0m       \u001b[32m29.9185\u001b[0m  0.0110\n",
      "     43       \u001b[36m32.2929\u001b[0m       \u001b[32m29.9157\u001b[0m  0.0108\n",
      "     44       \u001b[36m32.2906\u001b[0m       \u001b[32m29.9131\u001b[0m  0.0110\n",
      "     45       \u001b[36m32.2883\u001b[0m       \u001b[32m29.9107\u001b[0m  0.0107\n",
      "     46       \u001b[36m32.2861\u001b[0m       \u001b[32m29.9081\u001b[0m  0.0111\n",
      "     47       \u001b[36m32.2840\u001b[0m       \u001b[32m29.9057\u001b[0m  0.0109\n",
      "     48       \u001b[36m32.2819\u001b[0m       \u001b[32m29.9033\u001b[0m  0.0109\n",
      "     49       \u001b[36m32.2799\u001b[0m       \u001b[32m29.9012\u001b[0m  0.0119\n",
      "     50       \u001b[36m32.2780\u001b[0m       \u001b[32m29.8991\u001b[0m  0.0110\n",
      "     51       \u001b[36m32.2760\u001b[0m       \u001b[32m29.8968\u001b[0m  0.0111\n",
      "     52       \u001b[36m32.2743\u001b[0m       \u001b[32m29.8948\u001b[0m  0.0119\n",
      "     53       \u001b[36m32.2725\u001b[0m       \u001b[32m29.8926\u001b[0m  0.0119\n",
      "     54       \u001b[36m32.2707\u001b[0m       \u001b[32m29.8904\u001b[0m  0.0113\n",
      "     55       \u001b[36m32.2692\u001b[0m       \u001b[32m29.8886\u001b[0m  0.0115\n",
      "     56       \u001b[36m32.2675\u001b[0m       \u001b[32m29.8866\u001b[0m  0.0114\n",
      "     57       \u001b[36m32.2660\u001b[0m       \u001b[32m29.8847\u001b[0m  0.0126\n",
      "     58       \u001b[36m32.2645\u001b[0m       \u001b[32m29.8828\u001b[0m  0.0110\n",
      "     59       \u001b[36m32.2630\u001b[0m       \u001b[32m29.8810\u001b[0m  0.0113\n",
      "     60       \u001b[36m32.2616\u001b[0m       \u001b[32m29.8793\u001b[0m  0.0112\n",
      "     61       \u001b[36m32.2602\u001b[0m       \u001b[32m29.8776\u001b[0m  0.0110\n",
      "     62       \u001b[36m32.2589\u001b[0m       \u001b[32m29.8761\u001b[0m  0.0115\n",
      "     63       \u001b[36m32.2576\u001b[0m       \u001b[32m29.8743\u001b[0m  0.0113\n",
      "     64       \u001b[36m32.2564\u001b[0m       \u001b[32m29.8728\u001b[0m  0.0112\n",
      "     65       \u001b[36m32.2551\u001b[0m       \u001b[32m29.8714\u001b[0m  0.0112\n",
      "     66       \u001b[36m32.2540\u001b[0m       \u001b[32m29.8701\u001b[0m  0.0110\n",
      "     67       \u001b[36m32.2528\u001b[0m       \u001b[32m29.8687\u001b[0m  0.0113\n",
      "     68       \u001b[36m32.2516\u001b[0m       \u001b[32m29.8671\u001b[0m  0.0112\n",
      "     69       \u001b[36m32.2505\u001b[0m       \u001b[32m29.8658\u001b[0m  0.0112\n",
      "     70       \u001b[36m32.2495\u001b[0m       \u001b[32m29.8644\u001b[0m  0.0110\n",
      "     71       \u001b[36m32.2485\u001b[0m       \u001b[32m29.8633\u001b[0m  0.0109\n",
      "     72       \u001b[36m32.2474\u001b[0m       \u001b[32m29.8619\u001b[0m  0.0110\n",
      "     73       \u001b[36m32.2464\u001b[0m       \u001b[32m29.8607\u001b[0m  0.0115\n",
      "     74       \u001b[36m32.2455\u001b[0m       \u001b[32m29.8595\u001b[0m  0.0113\n",
      "     75       \u001b[36m32.2445\u001b[0m       \u001b[32m29.8583\u001b[0m  0.0113\n",
      "     76       \u001b[36m32.2436\u001b[0m       \u001b[32m29.8572\u001b[0m  0.0106\n",
      "     77       \u001b[36m32.2427\u001b[0m       \u001b[32m29.8562\u001b[0m  0.0112\n",
      "     78       \u001b[36m32.2418\u001b[0m       \u001b[32m29.8550\u001b[0m  0.0109\n",
      "     79       \u001b[36m32.2410\u001b[0m       \u001b[32m29.8542\u001b[0m  0.0110\n",
      "     80       \u001b[36m32.2400\u001b[0m       \u001b[32m29.8530\u001b[0m  0.0110\n",
      "     81       \u001b[36m32.2393\u001b[0m       \u001b[32m29.8522\u001b[0m  0.0109\n",
      "     82       \u001b[36m32.2385\u001b[0m       \u001b[32m29.8513\u001b[0m  0.0110\n",
      "     83       \u001b[36m32.2376\u001b[0m       \u001b[32m29.8502\u001b[0m  0.0110\n",
      "     84       \u001b[36m32.2369\u001b[0m       \u001b[32m29.8494\u001b[0m  0.0110\n",
      "     85       \u001b[36m32.2361\u001b[0m       \u001b[32m29.8485\u001b[0m  0.0110\n",
      "     86       \u001b[36m32.2354\u001b[0m       \u001b[32m29.8477\u001b[0m  0.0107\n",
      "     87       \u001b[36m32.2346\u001b[0m       \u001b[32m29.8468\u001b[0m  0.0120\n",
      "     88       \u001b[36m32.2338\u001b[0m       \u001b[32m29.8458\u001b[0m  0.0138\n",
      "     89       \u001b[36m32.2332\u001b[0m       \u001b[32m29.8451\u001b[0m  0.0114\n",
      "     90       \u001b[36m32.2324\u001b[0m       \u001b[32m29.8442\u001b[0m  0.0115\n",
      "     91       \u001b[36m32.2318\u001b[0m       \u001b[32m29.8435\u001b[0m  0.0115\n",
      "     92       \u001b[36m32.2311\u001b[0m       \u001b[32m29.8427\u001b[0m  0.0132\n",
      "     93       \u001b[36m32.2304\u001b[0m       \u001b[32m29.8418\u001b[0m  0.0167\n",
      "     94       \u001b[36m32.2298\u001b[0m       \u001b[32m29.8411\u001b[0m  0.0120\n",
      "     95       \u001b[36m32.2291\u001b[0m       \u001b[32m29.8402\u001b[0m  0.0116\n",
      "     96       \u001b[36m32.2286\u001b[0m       \u001b[32m29.8395\u001b[0m  0.0118\n",
      "     97       \u001b[36m32.2279\u001b[0m       \u001b[32m29.8387\u001b[0m  0.0120\n",
      "     98       \u001b[36m32.2273\u001b[0m       \u001b[32m29.8378\u001b[0m  0.0136\n",
      "     99       \u001b[36m32.2268\u001b[0m       \u001b[32m29.8373\u001b[0m  0.0126\n",
      "    100       \u001b[36m32.2261\u001b[0m       \u001b[32m29.8365\u001b[0m  0.0117\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m32.0642\u001b[0m       \u001b[32m30.6221\u001b[0m  0.0117\n",
      "      2       \u001b[36m30.8502\u001b[0m       \u001b[32m29.7499\u001b[0m  0.0113\n",
      "      3       \u001b[36m29.6699\u001b[0m       \u001b[32m28.9048\u001b[0m  0.0111\n",
      "      4       \u001b[36m28.4982\u001b[0m       \u001b[32m28.1017\u001b[0m  0.0111\n",
      "      5       \u001b[36m27.3564\u001b[0m       \u001b[32m27.3810\u001b[0m  0.0110\n",
      "      6       \u001b[36m26.2925\u001b[0m       \u001b[32m26.7996\u001b[0m  0.0114\n",
      "      7       \u001b[36m25.3701\u001b[0m       \u001b[32m26.4031\u001b[0m  0.0113\n",
      "      8       \u001b[36m24.6410\u001b[0m       \u001b[32m26.1982\u001b[0m  0.0111\n",
      "      9       \u001b[36m24.1219\u001b[0m       \u001b[32m26.1490\u001b[0m  0.0110\n",
      "     10       \u001b[36m23.7886\u001b[0m       26.1927  0.0111\n",
      "     11       \u001b[36m23.5911\u001b[0m       26.2692  0.0107\n",
      "     12       \u001b[36m23.4787\u001b[0m       26.3406  0.0110\n",
      "     13       \u001b[36m23.4128\u001b[0m       26.3938  0.0114\n",
      "     14       \u001b[36m23.3708\u001b[0m       26.4285  0.0117\n",
      "     15       \u001b[36m23.3413\u001b[0m       26.4493  0.0115\n",
      "     16       \u001b[36m23.3183\u001b[0m       26.4605  0.0116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m23.2994\u001b[0m       26.4657  0.0118\n",
      "     18       \u001b[36m23.2834\u001b[0m       26.4674  0.0122\n",
      "     19       \u001b[36m23.2695\u001b[0m       26.4672  0.0116\n",
      "     20       \u001b[36m23.2571\u001b[0m       26.4658  0.0112\n",
      "     21       \u001b[36m23.2459\u001b[0m       26.4637  0.0111\n",
      "     22       \u001b[36m23.2358\u001b[0m       26.4617  0.0130\n",
      "     23       \u001b[36m23.2265\u001b[0m       26.4596  0.0112\n",
      "     24       \u001b[36m23.2179\u001b[0m       26.4576  0.0111\n",
      "     25       \u001b[36m23.2100\u001b[0m       26.4556  0.0110\n",
      "     26       \u001b[36m23.2027\u001b[0m       26.4540  0.0109\n",
      "     27       \u001b[36m23.1960\u001b[0m       26.4523  0.0112\n",
      "     28       \u001b[36m23.1898\u001b[0m       26.4507  0.0110\n",
      "     29       \u001b[36m23.1840\u001b[0m       26.4491  0.0108\n",
      "     30       \u001b[36m23.1786\u001b[0m       26.4477  0.0109\n",
      "     31       \u001b[36m23.1736\u001b[0m       26.4463  0.0108\n",
      "     32       \u001b[36m23.1688\u001b[0m       26.4450  0.0110\n",
      "     33       \u001b[36m23.1644\u001b[0m       26.4437  0.0109\n",
      "     34       \u001b[36m23.1602\u001b[0m       26.4428  0.0108\n",
      "     35       \u001b[36m23.1564\u001b[0m       26.4421  0.0109\n",
      "     36       \u001b[36m23.1527\u001b[0m       26.4412  0.0107\n",
      "     37       \u001b[36m23.1493\u001b[0m       26.4403  0.0118\n",
      "     38       \u001b[36m23.1460\u001b[0m       26.4395  0.0113\n",
      "     39       \u001b[36m23.1429\u001b[0m       26.4388  0.0116\n",
      "     40       \u001b[36m23.1400\u001b[0m       26.4380  0.0109\n",
      "     41       \u001b[36m23.1372\u001b[0m       26.4374  0.0112\n",
      "     42       \u001b[36m23.1345\u001b[0m       26.4367  0.0118\n",
      "     43       \u001b[36m23.1320\u001b[0m       26.4361  0.0110\n",
      "     44       \u001b[36m23.1296\u001b[0m       26.4355  0.0110\n",
      "     45       \u001b[36m23.1273\u001b[0m       26.4350  0.0111\n",
      "     46       \u001b[36m23.1252\u001b[0m       26.4346  0.0110\n",
      "     47       \u001b[36m23.1231\u001b[0m       26.4342  0.0110\n",
      "     48       \u001b[36m23.1211\u001b[0m       26.4339  0.0113\n",
      "     49       \u001b[36m23.1192\u001b[0m       26.4335  0.0111\n",
      "     50       \u001b[36m23.1173\u001b[0m       26.4333  0.0108\n",
      "     51       \u001b[36m23.1156\u001b[0m       26.4332  0.0111\n",
      "     52       \u001b[36m23.1139\u001b[0m       26.4332  0.0111\n",
      "     53       \u001b[36m23.1123\u001b[0m       26.4332  0.0110\n",
      "     54       \u001b[36m23.1107\u001b[0m       26.4332  0.0107\n",
      "     55       \u001b[36m23.1092\u001b[0m       26.4332  0.0113\n",
      "     56       \u001b[36m23.1077\u001b[0m       26.4333  0.0114\n",
      "     57       \u001b[36m23.1063\u001b[0m       26.4333  0.0116\n",
      "     58       \u001b[36m23.1050\u001b[0m       26.4333  0.0120\n",
      "     59       \u001b[36m23.1037\u001b[0m       26.4334  0.0113\n",
      "     60       \u001b[36m23.1024\u001b[0m       26.4334  0.0107\n",
      "     61       \u001b[36m23.1012\u001b[0m       26.4335  0.0107\n",
      "     62       \u001b[36m23.1000\u001b[0m       26.4336  0.0110\n",
      "     63       \u001b[36m23.0988\u001b[0m       26.4337  0.0112\n",
      "     64       \u001b[36m23.0977\u001b[0m       26.4338  0.0110\n",
      "     65       \u001b[36m23.0966\u001b[0m       26.4340  0.0110\n",
      "     66       \u001b[36m23.0956\u001b[0m       26.4342  0.0110\n",
      "     67       \u001b[36m23.0945\u001b[0m       26.4344  0.0108\n",
      "     68       \u001b[36m23.0935\u001b[0m       26.4345  0.0110\n",
      "     69       \u001b[36m23.0926\u001b[0m       26.4347  0.0112\n",
      "     70       \u001b[36m23.0916\u001b[0m       26.4349  0.0107\n",
      "     71       \u001b[36m23.0907\u001b[0m       26.4350  0.0127\n",
      "     72       \u001b[36m23.0898\u001b[0m       26.4352  0.0143\n",
      "     73       \u001b[36m23.0889\u001b[0m       26.4353  0.0116\n",
      "     74       \u001b[36m23.0881\u001b[0m       26.4355  0.0130\n",
      "     75       \u001b[36m23.0872\u001b[0m       26.4356  0.0179\n",
      "     76       \u001b[36m23.0864\u001b[0m       26.4356  0.0147\n",
      "     77       \u001b[36m23.0856\u001b[0m       26.4357  0.0120\n",
      "     78       \u001b[36m23.0848\u001b[0m       26.4358  0.0124\n",
      "     79       \u001b[36m23.0841\u001b[0m       26.4358  0.0127\n",
      "     80       \u001b[36m23.0833\u001b[0m       26.4360  0.0131\n",
      "     81       \u001b[36m23.0826\u001b[0m       26.4361  0.0166\n",
      "     82       \u001b[36m23.0819\u001b[0m       26.4362  0.0119\n",
      "     83       \u001b[36m23.0812\u001b[0m       26.4363  0.0117\n",
      "     84       \u001b[36m23.0805\u001b[0m       26.4364  0.0118\n",
      "     85       \u001b[36m23.0799\u001b[0m       26.4363  0.0115\n",
      "     86       \u001b[36m23.0792\u001b[0m       26.4364  0.0114\n",
      "     87       \u001b[36m23.0786\u001b[0m       26.4364  0.0116\n",
      "     88       \u001b[36m23.0779\u001b[0m       26.4363  0.0110\n",
      "     89       \u001b[36m23.0773\u001b[0m       26.4364  0.0114\n",
      "     90       \u001b[36m23.0768\u001b[0m       26.4363  0.0114\n",
      "     91       \u001b[36m23.0762\u001b[0m       26.4363  0.0139\n",
      "     92       \u001b[36m23.0756\u001b[0m       26.4363  0.0121\n",
      "     93       \u001b[36m23.0751\u001b[0m       26.4361  0.0119\n",
      "     94       \u001b[36m23.0745\u001b[0m       26.4361  0.0113\n",
      "     95       \u001b[36m23.0740\u001b[0m       26.4361  0.0111\n",
      "     96       \u001b[36m23.0734\u001b[0m       26.4361  0.0129\n",
      "     97       \u001b[36m23.0729\u001b[0m       26.4358  0.0120\n",
      "     98       \u001b[36m23.0724\u001b[0m       26.4358  0.0120\n",
      "     99       \u001b[36m23.0719\u001b[0m       26.4356  0.0112\n",
      "    100       \u001b[36m23.0714\u001b[0m       26.4355  0.0111\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m39.5010\u001b[0m       \u001b[32m30.8293\u001b[0m  0.0133\n",
      "      2       \u001b[36m37.7902\u001b[0m       \u001b[32m29.7651\u001b[0m  0.0119\n",
      "      3       \u001b[36m36.2303\u001b[0m       \u001b[32m28.7851\u001b[0m  0.0119\n",
      "      4       \u001b[36m34.7239\u001b[0m       \u001b[32m27.8842\u001b[0m  0.0115\n",
      "      5       \u001b[36m33.2892\u001b[0m       \u001b[32m27.1144\u001b[0m  0.0108\n",
      "      6       \u001b[36m31.9939\u001b[0m       \u001b[32m26.5345\u001b[0m  0.0107\n",
      "      7       \u001b[36m30.8899\u001b[0m       \u001b[32m26.1866\u001b[0m  0.0129\n",
      "      8       \u001b[36m30.0084\u001b[0m       \u001b[32m26.0856\u001b[0m  0.0122\n",
      "      9       \u001b[36m29.3784\u001b[0m       26.1945  0.0141\n",
      "     10       \u001b[36m28.9957\u001b[0m       26.4160  0.0114\n",
      "     11       \u001b[36m28.7964\u001b[0m       26.6498  0.0116\n",
      "     12       \u001b[36m28.7047\u001b[0m       26.8379  0.0145\n",
      "     13       \u001b[36m28.6634\u001b[0m       26.9681  0.0157\n",
      "     14       \u001b[36m28.6421\u001b[0m       27.0507  0.0118\n",
      "     15       \u001b[36m28.6276\u001b[0m       27.0998  0.0109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     16       \u001b[36m28.6152\u001b[0m       27.1279  0.0119\n",
      "     17       \u001b[36m28.6038\u001b[0m       27.1435  0.0115\n",
      "     18       \u001b[36m28.5932\u001b[0m       27.1523  0.0114\n",
      "     19       \u001b[36m28.5835\u001b[0m       27.1567  0.0111\n",
      "     20       \u001b[36m28.5744\u001b[0m       27.1588  0.0113\n",
      "     21       \u001b[36m28.5661\u001b[0m       27.1593  0.0121\n",
      "     22       \u001b[36m28.5584\u001b[0m       27.1590  0.0117\n",
      "     23       \u001b[36m28.5512\u001b[0m       27.1583  0.0115\n",
      "     24       \u001b[36m28.5445\u001b[0m       27.1575  0.0115\n",
      "     25       \u001b[36m28.5383\u001b[0m       27.1572  0.0127\n",
      "     26       \u001b[36m28.5327\u001b[0m       27.1566  0.0124\n",
      "     27       \u001b[36m28.5274\u001b[0m       27.1561  0.0112\n",
      "     28       \u001b[36m28.5225\u001b[0m       27.1557  0.0113\n",
      "     29       \u001b[36m28.5179\u001b[0m       27.1552  0.0110\n",
      "     30       \u001b[36m28.5135\u001b[0m       27.1547  0.0108\n",
      "     31       \u001b[36m28.5094\u001b[0m       27.1547  0.0110\n",
      "     32       \u001b[36m28.5056\u001b[0m       27.1544  0.0113\n",
      "     33       \u001b[36m28.5020\u001b[0m       27.1544  0.0110\n",
      "     34       \u001b[36m28.4987\u001b[0m       27.1543  0.0110\n",
      "     35       \u001b[36m28.4954\u001b[0m       27.1541  0.0110\n",
      "     36       \u001b[36m28.4924\u001b[0m       27.1541  0.0113\n",
      "     37       \u001b[36m28.4895\u001b[0m       27.1540  0.0111\n",
      "     38       \u001b[36m28.4867\u001b[0m       27.1542  0.0116\n",
      "     39       \u001b[36m28.4841\u001b[0m       27.1542  0.0110\n",
      "     40       \u001b[36m28.4817\u001b[0m       27.1543  0.0111\n",
      "     41       \u001b[36m28.4793\u001b[0m       27.1543  0.0112\n",
      "     42       \u001b[36m28.4771\u001b[0m       27.1545  0.0111\n",
      "     43       \u001b[36m28.4750\u001b[0m       27.1545  0.0111\n",
      "     44       \u001b[36m28.4730\u001b[0m       27.1546  0.0109\n",
      "     45       \u001b[36m28.4710\u001b[0m       27.1546  0.0109\n",
      "     46       \u001b[36m28.4691\u001b[0m       27.1546  0.0112\n",
      "     47       \u001b[36m28.4673\u001b[0m       27.1544  0.0111\n",
      "     48       \u001b[36m28.4656\u001b[0m       27.1544  0.0124\n",
      "     49       \u001b[36m28.4640\u001b[0m       27.1541  0.0114\n",
      "     50       \u001b[36m28.4624\u001b[0m       27.1541  0.0108\n",
      "     51       \u001b[36m28.4608\u001b[0m       27.1539  0.0109\n",
      "     52       \u001b[36m28.4594\u001b[0m       27.1541  0.0112\n",
      "     53       \u001b[36m28.4580\u001b[0m       27.1540  0.0123\n",
      "     54       \u001b[36m28.4567\u001b[0m       27.1542  0.0165\n",
      "     55       \u001b[36m28.4554\u001b[0m       27.1541  0.0123\n",
      "     56       \u001b[36m28.4542\u001b[0m       27.1541  0.0118\n",
      "     57       \u001b[36m28.4530\u001b[0m       27.1540  0.0124\n",
      "     58       \u001b[36m28.4518\u001b[0m       27.1540  0.0120\n",
      "     59       \u001b[36m28.4507\u001b[0m       27.1539  0.0129\n",
      "     60       \u001b[36m28.4496\u001b[0m       27.1541  0.0121\n",
      "     61       \u001b[36m28.4485\u001b[0m       27.1539  0.0126\n",
      "     62       \u001b[36m28.4475\u001b[0m       27.1536  0.0116\n",
      "     63       \u001b[36m28.4464\u001b[0m       27.1537  0.0110\n",
      "     64       \u001b[36m28.4455\u001b[0m       27.1535  0.0111\n",
      "     65       \u001b[36m28.4445\u001b[0m       27.1536  0.0113\n",
      "     66       \u001b[36m28.4436\u001b[0m       27.1531  0.0113\n",
      "     67       \u001b[36m28.4427\u001b[0m       27.1532  0.0113\n",
      "     68       \u001b[36m28.4418\u001b[0m       27.1527  0.0112\n",
      "     69       \u001b[36m28.4410\u001b[0m       27.1522  0.0111\n",
      "     70       \u001b[36m28.4401\u001b[0m       27.1519  0.0114\n",
      "     71       \u001b[36m28.4393\u001b[0m       27.1519  0.0111\n",
      "     72       \u001b[36m28.4385\u001b[0m       27.1516  0.0113\n",
      "     73       \u001b[36m28.4378\u001b[0m       27.1513  0.0110\n",
      "     74       \u001b[36m28.4370\u001b[0m       27.1511  0.0107\n",
      "     75       \u001b[36m28.4363\u001b[0m       27.1509  0.0110\n",
      "     76       \u001b[36m28.4356\u001b[0m       27.1506  0.0109\n",
      "     77       \u001b[36m28.4349\u001b[0m       27.1506  0.0110\n",
      "     78       \u001b[36m28.4342\u001b[0m       27.1503  0.0107\n",
      "     79       \u001b[36m28.4336\u001b[0m       27.1502  0.0107\n",
      "     80       \u001b[36m28.4329\u001b[0m       27.1502  0.0109\n",
      "     81       \u001b[36m28.4323\u001b[0m       27.1499  0.0112\n",
      "     82       \u001b[36m28.4317\u001b[0m       27.1497  0.0110\n",
      "     83       \u001b[36m28.4311\u001b[0m       27.1493  0.0106\n",
      "     84       \u001b[36m28.4305\u001b[0m       27.1492  0.0133\n",
      "     85       \u001b[36m28.4299\u001b[0m       27.1492  0.0113\n",
      "     86       \u001b[36m28.4294\u001b[0m       27.1491  0.0112\n",
      "     87       \u001b[36m28.4289\u001b[0m       27.1488  0.0110\n",
      "     88       \u001b[36m28.4283\u001b[0m       27.1486  0.0110\n",
      "     89       \u001b[36m28.4278\u001b[0m       27.1484  0.0107\n",
      "     90       \u001b[36m28.4273\u001b[0m       27.1483  0.0112\n",
      "     91       \u001b[36m28.4267\u001b[0m       27.1481  0.0111\n",
      "     92       \u001b[36m28.4263\u001b[0m       27.1479  0.0108\n",
      "     93       \u001b[36m28.4258\u001b[0m       27.1477  0.0108\n",
      "     94       \u001b[36m28.4253\u001b[0m       27.1476  0.0106\n",
      "     95       \u001b[36m28.4248\u001b[0m       27.1475  0.0109\n",
      "     96       \u001b[36m28.4243\u001b[0m       27.1474  0.0109\n",
      "     97       \u001b[36m28.4239\u001b[0m       27.1475  0.0108\n",
      "     98       \u001b[36m28.4235\u001b[0m       27.1474  0.0109\n",
      "     99       \u001b[36m28.4230\u001b[0m       27.1473  0.0106\n",
      "    100       \u001b[36m28.4226\u001b[0m       27.1472  0.0111\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m42.6665\u001b[0m       \u001b[32m41.5842\u001b[0m  0.0121\n",
      "      2       \u001b[36m38.2635\u001b[0m       \u001b[32m33.4250\u001b[0m  0.0118\n",
      "      3       \u001b[36m34.4357\u001b[0m       \u001b[32m31.6749\u001b[0m  0.0119\n",
      "      4       \u001b[36m34.3618\u001b[0m       \u001b[32m31.5202\u001b[0m  0.0117\n",
      "      5       \u001b[36m33.4550\u001b[0m       32.2650  0.0115\n",
      "      6       \u001b[36m33.3323\u001b[0m       \u001b[32m31.2872\u001b[0m  0.0117\n",
      "      7       \u001b[36m32.9079\u001b[0m       \u001b[32m30.4825\u001b[0m  0.0117\n",
      "      8       \u001b[36m32.8155\u001b[0m       30.4941  0.0118\n",
      "      9       \u001b[36m32.6974\u001b[0m       30.6963  0.0117\n",
      "     10       \u001b[36m32.5979\u001b[0m       30.6140  0.0116\n",
      "     11       \u001b[36m32.5087\u001b[0m       \u001b[32m30.3885\u001b[0m  0.0116\n",
      "     12       \u001b[36m32.4389\u001b[0m       \u001b[32m30.3580\u001b[0m  0.0116\n",
      "     13       \u001b[36m32.3939\u001b[0m       30.3897  0.0118\n",
      "     14       \u001b[36m32.3617\u001b[0m       30.4643  0.0116\n",
      "     15       \u001b[36m32.3318\u001b[0m       \u001b[32m30.3580\u001b[0m  0.0115\n",
      "     16       \u001b[36m32.3032\u001b[0m       \u001b[32m30.3037\u001b[0m  0.0113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m32.2902\u001b[0m       30.3335  0.0127\n",
      "     18       \u001b[36m32.2756\u001b[0m       30.3349  0.0116\n",
      "     19       \u001b[36m32.2610\u001b[0m       \u001b[32m30.2898\u001b[0m  0.0115\n",
      "     20       \u001b[36m32.2487\u001b[0m       \u001b[32m30.2480\u001b[0m  0.0116\n",
      "     21       \u001b[36m32.2400\u001b[0m       30.2526  0.0113\n",
      "     22       \u001b[36m32.2323\u001b[0m       \u001b[32m30.2282\u001b[0m  0.0123\n",
      "     23       \u001b[36m32.2239\u001b[0m       \u001b[32m30.2144\u001b[0m  0.0117\n",
      "     24       \u001b[36m32.2169\u001b[0m       \u001b[32m30.1872\u001b[0m  0.0118\n",
      "     25       \u001b[36m32.2107\u001b[0m       30.2062  0.0113\n",
      "     26       \u001b[36m32.2052\u001b[0m       \u001b[32m30.1681\u001b[0m  0.0110\n",
      "     27       \u001b[36m32.1995\u001b[0m       30.1969  0.0120\n",
      "     28       \u001b[36m32.1951\u001b[0m       \u001b[32m30.1406\u001b[0m  0.0117\n",
      "     29       \u001b[36m32.1913\u001b[0m       30.2028  0.0119\n",
      "     30       \u001b[36m32.1866\u001b[0m       \u001b[32m30.1178\u001b[0m  0.0116\n",
      "     31       \u001b[36m32.1853\u001b[0m       30.2112  0.0116\n",
      "     32       \u001b[36m32.1806\u001b[0m       \u001b[32m30.0959\u001b[0m  0.0147\n",
      "     33       32.1818       30.2339  0.0139\n",
      "     34       \u001b[36m32.1772\u001b[0m       \u001b[32m30.0806\u001b[0m  0.0140\n",
      "     35       32.1800       30.2846  0.0168\n",
      "     36       32.1802       \u001b[32m30.0764\u001b[0m  0.0128\n",
      "     37       32.1898       30.4249  0.0129\n",
      "     38       32.2048       30.0963  0.0128\n",
      "     39       32.2369       30.5022  0.0127\n",
      "     40       32.2445       \u001b[32m30.0642\u001b[0m  0.0137\n",
      "     41       32.2698       30.2075  0.0121\n",
      "     42       32.2134       \u001b[32m30.0465\u001b[0m  0.0125\n",
      "     43       \u001b[36m32.1565\u001b[0m       \u001b[32m29.9861\u001b[0m  0.0125\n",
      "     44       \u001b[36m32.1560\u001b[0m       30.1861  0.0118\n",
      "     45       \u001b[36m32.1541\u001b[0m       30.0115  0.0116\n",
      "     46       \u001b[36m32.1423\u001b[0m       30.0736  0.0119\n",
      "     47       32.1436       30.0931  0.0116\n",
      "     48       \u001b[36m32.1321\u001b[0m       30.0481  0.0116\n",
      "     49       \u001b[36m32.1320\u001b[0m       30.0993  0.0119\n",
      "     50       \u001b[36m32.1301\u001b[0m       30.0571  0.0118\n",
      "     51       \u001b[36m32.1258\u001b[0m       30.0804  0.0120\n",
      "     52       \u001b[36m32.1254\u001b[0m       30.0694  0.0116\n",
      "     53       \u001b[36m32.1209\u001b[0m       30.0678  0.0115\n",
      "     54       \u001b[36m32.1202\u001b[0m       30.0750  0.0118\n",
      "     55       \u001b[36m32.1173\u001b[0m       30.0653  0.0117\n",
      "     56       \u001b[36m32.1153\u001b[0m       30.0723  0.0116\n",
      "     57       \u001b[36m32.1132\u001b[0m       30.0649  0.0116\n",
      "     58       \u001b[36m32.1109\u001b[0m       30.0699  0.0113\n",
      "     59       \u001b[36m32.1091\u001b[0m       30.0648  0.0124\n",
      "     60       \u001b[36m32.1066\u001b[0m       30.0684  0.0120\n",
      "     61       \u001b[36m32.1049\u001b[0m       30.0661  0.0121\n",
      "     62       \u001b[36m32.1027\u001b[0m       30.0657  0.0117\n",
      "     63       \u001b[36m32.1008\u001b[0m       30.0661  0.0116\n",
      "     64       \u001b[36m32.0986\u001b[0m       30.0668  0.0129\n",
      "     65       \u001b[36m32.0967\u001b[0m       30.0668  0.0119\n",
      "     66       \u001b[36m32.0949\u001b[0m       30.0653  0.0127\n",
      "     67       \u001b[36m32.0927\u001b[0m       30.0680  0.0117\n",
      "     68       \u001b[36m32.0911\u001b[0m       30.0666  0.0118\n",
      "     69       \u001b[36m32.0891\u001b[0m       30.0707  0.0127\n",
      "     70       \u001b[36m32.0872\u001b[0m       30.0680  0.0120\n",
      "     71       \u001b[36m32.0853\u001b[0m       30.0733  0.0122\n",
      "     72       \u001b[36m32.0837\u001b[0m       30.0722  0.0115\n",
      "     73       \u001b[36m32.0818\u001b[0m       30.0771  0.0115\n",
      "     74       \u001b[36m32.0797\u001b[0m       30.0737  0.0120\n",
      "     75       \u001b[36m32.0781\u001b[0m       30.0810  0.0117\n",
      "     76       \u001b[36m32.0761\u001b[0m       30.0745  0.0116\n",
      "     77       \u001b[36m32.0744\u001b[0m       30.0804  0.0115\n",
      "     78       \u001b[36m32.0723\u001b[0m       30.0817  0.0117\n",
      "     79       \u001b[36m32.0710\u001b[0m       30.0814  0.0136\n",
      "     80       \u001b[36m32.0688\u001b[0m       30.0827  0.0162\n",
      "     81       \u001b[36m32.0673\u001b[0m       30.0837  0.0161\n",
      "     82       \u001b[36m32.0657\u001b[0m       30.0846  0.0165\n",
      "     83       \u001b[36m32.0640\u001b[0m       30.0822  0.0130\n",
      "     84       \u001b[36m32.0623\u001b[0m       30.0933  0.0122\n",
      "     85       \u001b[36m32.0606\u001b[0m       30.0833  0.0119\n",
      "     86       \u001b[36m32.0594\u001b[0m       30.1024  0.0117\n",
      "     87       \u001b[36m32.0575\u001b[0m       30.0959  0.0118\n",
      "     88       \u001b[36m32.0564\u001b[0m       30.0957  0.0124\n",
      "     89       \u001b[36m32.0544\u001b[0m       30.1058  0.0120\n",
      "     90       \u001b[36m32.0533\u001b[0m       30.1023  0.0121\n",
      "     91       \u001b[36m32.0520\u001b[0m       30.1058  0.0115\n",
      "     92       \u001b[36m32.0502\u001b[0m       30.1082  0.0113\n",
      "     93       \u001b[36m32.0495\u001b[0m       30.1071  0.0122\n",
      "     94       \u001b[36m32.0477\u001b[0m       30.1105  0.0119\n",
      "     95       \u001b[36m32.0466\u001b[0m       30.1127  0.0121\n",
      "     96       \u001b[36m32.0455\u001b[0m       30.1074  0.0119\n",
      "     97       \u001b[36m32.0440\u001b[0m       30.1171  0.0120\n",
      "     98       \u001b[36m32.0435\u001b[0m       30.1073  0.0129\n",
      "     99       \u001b[36m32.0418\u001b[0m       30.1227  0.0121\n",
      "    100       \u001b[36m32.0409\u001b[0m       30.1126  0.0117\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m31.8721\u001b[0m       \u001b[32m29.5045\u001b[0m  0.0118\n",
      "      2       \u001b[36m28.4657\u001b[0m       \u001b[32m26.9215\u001b[0m  0.0114\n",
      "      3       \u001b[36m25.2577\u001b[0m       29.3060  0.0121\n",
      "      4       \u001b[36m25.0366\u001b[0m       26.9249  0.0116\n",
      "      5       \u001b[36m24.3018\u001b[0m       \u001b[32m26.3846\u001b[0m  0.0116\n",
      "      6       \u001b[36m24.2220\u001b[0m       26.4644  0.0114\n",
      "      7       \u001b[36m23.8136\u001b[0m       27.1766  0.0114\n",
      "      8       \u001b[36m23.6652\u001b[0m       27.4024  0.0123\n",
      "      9       \u001b[36m23.5231\u001b[0m       26.9581  0.0120\n",
      "     10       \u001b[36m23.4657\u001b[0m       26.8830  0.0165\n",
      "     11       \u001b[36m23.3884\u001b[0m       27.0992  0.0155\n",
      "     12       \u001b[36m23.3162\u001b[0m       27.1985  0.0158\n",
      "     13       \u001b[36m23.2677\u001b[0m       27.0369  0.0129\n",
      "     14       \u001b[36m23.2375\u001b[0m       26.9571  0.0123\n",
      "     15       \u001b[36m23.2092\u001b[0m       26.9931  0.0120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     16       \u001b[36m23.1808\u001b[0m       26.9924  0.0146\n",
      "     17       \u001b[36m23.1585\u001b[0m       26.9391  0.0123\n",
      "     18       \u001b[36m23.1414\u001b[0m       26.8922  0.0124\n",
      "     19       \u001b[36m23.1299\u001b[0m       26.8836  0.0116\n",
      "     20       \u001b[36m23.1171\u001b[0m       26.8782  0.0118\n",
      "     21       \u001b[36m23.1050\u001b[0m       26.8302  0.0121\n",
      "     22       \u001b[36m23.0966\u001b[0m       26.7994  0.0116\n",
      "     23       \u001b[36m23.0893\u001b[0m       26.7967  0.0118\n",
      "     24       \u001b[36m23.0819\u001b[0m       26.7861  0.0113\n",
      "     25       \u001b[36m23.0753\u001b[0m       26.7578  0.0114\n",
      "     26       \u001b[36m23.0698\u001b[0m       26.7350  0.0120\n",
      "     27       \u001b[36m23.0648\u001b[0m       26.7318  0.0118\n",
      "     28       \u001b[36m23.0597\u001b[0m       26.7284  0.0119\n",
      "     29       \u001b[36m23.0553\u001b[0m       26.7172  0.0115\n",
      "     30       \u001b[36m23.0512\u001b[0m       26.7037  0.0115\n",
      "     31       \u001b[36m23.0475\u001b[0m       26.6981  0.0122\n",
      "     32       \u001b[36m23.0439\u001b[0m       26.6886  0.0118\n",
      "     33       \u001b[36m23.0407\u001b[0m       26.6806  0.0117\n",
      "     34       \u001b[36m23.0374\u001b[0m       26.6716  0.0117\n",
      "     35       \u001b[36m23.0346\u001b[0m       26.6625  0.0119\n",
      "     36       \u001b[36m23.0319\u001b[0m       26.6537  0.0117\n",
      "     37       \u001b[36m23.0293\u001b[0m       26.6490  0.0117\n",
      "     38       \u001b[36m23.0267\u001b[0m       26.6413  0.0115\n",
      "     39       \u001b[36m23.0244\u001b[0m       26.6362  0.0114\n",
      "     40       \u001b[36m23.0221\u001b[0m       26.6335  0.0119\n",
      "     41       \u001b[36m23.0200\u001b[0m       26.6293  0.0131\n",
      "     42       \u001b[36m23.0179\u001b[0m       26.6218  0.0118\n",
      "     43       \u001b[36m23.0160\u001b[0m       26.6205  0.0115\n",
      "     44       \u001b[36m23.0139\u001b[0m       26.6151  0.0116\n",
      "     45       \u001b[36m23.0122\u001b[0m       26.6125  0.0114\n",
      "     46       \u001b[36m23.0104\u001b[0m       26.6079  0.0137\n",
      "     47       \u001b[36m23.0087\u001b[0m       26.6046  0.0114\n",
      "     48       \u001b[36m23.0071\u001b[0m       26.6013  0.0113\n",
      "     49       \u001b[36m23.0055\u001b[0m       26.6008  0.0113\n",
      "     50       \u001b[36m23.0040\u001b[0m       26.5986  0.0114\n",
      "     51       \u001b[36m23.0025\u001b[0m       26.5995  0.0118\n",
      "     52       \u001b[36m23.0013\u001b[0m       26.5946  0.0117\n",
      "     53       \u001b[36m22.9998\u001b[0m       26.5957  0.0115\n",
      "     54       \u001b[36m22.9985\u001b[0m       26.5946  0.0112\n",
      "     55       \u001b[36m22.9973\u001b[0m       26.5909  0.0113\n",
      "     56       \u001b[36m22.9959\u001b[0m       26.5906  0.0119\n",
      "     57       \u001b[36m22.9948\u001b[0m       26.5892  0.0116\n",
      "     58       \u001b[36m22.9935\u001b[0m       26.5888  0.0115\n",
      "     59       \u001b[36m22.9923\u001b[0m       26.5877  0.0113\n",
      "     60       \u001b[36m22.9912\u001b[0m       26.5871  0.0113\n",
      "     61       \u001b[36m22.9900\u001b[0m       26.5875  0.0119\n",
      "     62       \u001b[36m22.9890\u001b[0m       26.5875  0.0117\n",
      "     63       \u001b[36m22.9878\u001b[0m       26.5917  0.0116\n",
      "     64       \u001b[36m22.9867\u001b[0m       26.5938  0.0112\n",
      "     65       \u001b[36m22.9857\u001b[0m       26.5935  0.0114\n",
      "     66       \u001b[36m22.9845\u001b[0m       26.5923  0.0120\n",
      "     67       \u001b[36m22.9837\u001b[0m       26.5931  0.0116\n",
      "     68       \u001b[36m22.9825\u001b[0m       26.5970  0.0116\n",
      "     69       \u001b[36m22.9813\u001b[0m       26.5949  0.0113\n",
      "     70       \u001b[36m22.9808\u001b[0m       26.5946  0.0114\n",
      "     71       \u001b[36m22.9792\u001b[0m       26.5963  0.0118\n",
      "     72       \u001b[36m22.9788\u001b[0m       26.5932  0.0115\n",
      "     73       \u001b[36m22.9782\u001b[0m       26.5949  0.0119\n",
      "     74       \u001b[36m22.9764\u001b[0m       26.5960  0.0113\n",
      "     75       22.9767       26.5909  0.0113\n",
      "     76       \u001b[36m22.9759\u001b[0m       26.6010  0.0123\n",
      "     77       \u001b[36m22.9744\u001b[0m       26.5952  0.0116\n",
      "     78       22.9754       26.5946  0.0113\n",
      "     79       \u001b[36m22.9726\u001b[0m       26.6022  0.0112\n",
      "     80       \u001b[36m22.9724\u001b[0m       26.5906  0.0112\n",
      "     81       22.9729       26.5926  0.0119\n",
      "     82       \u001b[36m22.9691\u001b[0m       26.6007  0.0116\n",
      "     83       22.9695       26.5906  0.0116\n",
      "     84       \u001b[36m22.9684\u001b[0m       26.6012  0.0115\n",
      "     85       \u001b[36m22.9663\u001b[0m       26.5982  0.0114\n",
      "     86       22.9664       26.5844  0.0117\n",
      "     87       \u001b[36m22.9645\u001b[0m       26.5958  0.0114\n",
      "     88       \u001b[36m22.9638\u001b[0m       26.5882  0.0111\n",
      "     89       22.9640       26.5940  0.0139\n",
      "     90       \u001b[36m22.9612\u001b[0m       26.5926  0.0119\n",
      "     91       22.9616       26.5847  0.0198\n",
      "     92       22.9618       26.6000  0.0151\n",
      "     93       \u001b[36m22.9587\u001b[0m       26.5860  0.0130\n",
      "     94       22.9600       26.5911  0.0123\n",
      "     95       \u001b[36m22.9587\u001b[0m       26.6027  0.0123\n",
      "     96       \u001b[36m22.9582\u001b[0m       26.5767  0.0144\n",
      "     97       22.9613       26.6072  0.0120\n",
      "     98       \u001b[36m22.9564\u001b[0m       26.5989  0.0126\n",
      "     99       22.9591       26.5686  0.0126\n",
      "    100       22.9612       26.6096  0.0120\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m39.4723\u001b[0m       \u001b[32m29.9670\u001b[0m  0.0122\n",
      "      2       \u001b[36m35.4658\u001b[0m       \u001b[32m26.8111\u001b[0m  0.0118\n",
      "      3       \u001b[36m30.6945\u001b[0m       31.0511  0.0119\n",
      "      4       31.1651       27.8296  0.0122\n",
      "      5       \u001b[36m29.6271\u001b[0m       \u001b[32m26.5518\u001b[0m  0.0116\n",
      "      6       \u001b[36m29.4482\u001b[0m       26.7594  0.0121\n",
      "      7       \u001b[36m29.0487\u001b[0m       28.1371  0.0118\n",
      "      8       29.1430       28.0204  0.0115\n",
      "      9       \u001b[36m28.8805\u001b[0m       27.1890  0.0116\n",
      "     10       \u001b[36m28.7364\u001b[0m       27.2025  0.0116\n",
      "     11       \u001b[36m28.6702\u001b[0m       27.5626  0.0122\n",
      "     12       \u001b[36m28.6674\u001b[0m       27.5333  0.0120\n",
      "     13       \u001b[36m28.5919\u001b[0m       27.2938  0.0117\n",
      "     14       \u001b[36m28.5410\u001b[0m       27.3311  0.0117\n",
      "     15       \u001b[36m28.5273\u001b[0m       27.4787  0.0115\n",
      "     16       \u001b[36m28.5212\u001b[0m       27.3630  0.0119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m28.4914\u001b[0m       27.2991  0.0120\n",
      "     18       \u001b[36m28.4794\u001b[0m       27.3513  0.0120\n",
      "     19       \u001b[36m28.4758\u001b[0m       27.3356  0.0120\n",
      "     20       \u001b[36m28.4660\u001b[0m       27.2598  0.0114\n",
      "     21       \u001b[36m28.4536\u001b[0m       27.2776  0.0115\n",
      "     22       \u001b[36m28.4502\u001b[0m       27.2814  0.0122\n",
      "     23       \u001b[36m28.4441\u001b[0m       27.2499  0.0120\n",
      "     24       \u001b[36m28.4359\u001b[0m       27.2488  0.0118\n",
      "     25       \u001b[36m28.4323\u001b[0m       27.2439  0.0117\n",
      "     26       \u001b[36m28.4288\u001b[0m       27.2334  0.0117\n",
      "     27       \u001b[36m28.4229\u001b[0m       27.2328  0.0121\n",
      "     28       \u001b[36m28.4218\u001b[0m       27.2086  0.0117\n",
      "     29       28.4253       27.2159  0.0117\n",
      "     30       28.4256       27.2478  0.0118\n",
      "     31       28.4380       27.2017  0.0115\n",
      "     32       28.4696       27.2354  0.0122\n",
      "     33       28.4851       27.2391  0.0119\n",
      "     34       28.4958       27.1653  0.0115\n",
      "     35       28.4820       27.2413  0.0114\n",
      "     36       28.4825       27.1063  0.0114\n",
      "     37       \u001b[36m28.4110\u001b[0m       27.1524  0.0119\n",
      "     38       28.4282       27.1922  0.0116\n",
      "     39       \u001b[36m28.4053\u001b[0m       27.1286  0.0117\n",
      "     40       \u001b[36m28.4008\u001b[0m       27.1680  0.0117\n",
      "     41       \u001b[36m28.3995\u001b[0m       27.1325  0.0115\n",
      "     42       \u001b[36m28.3954\u001b[0m       27.1567  0.0115\n",
      "     43       \u001b[36m28.3945\u001b[0m       27.1485  0.0114\n",
      "     44       \u001b[36m28.3903\u001b[0m       27.1566  0.0120\n",
      "     45       28.3915       27.1371  0.0118\n",
      "     46       \u001b[36m28.3883\u001b[0m       27.1443  0.0118\n",
      "     47       28.3883       27.1486  0.0118\n",
      "     48       \u001b[36m28.3859\u001b[0m       27.1453  0.0116\n",
      "     49       28.3861       27.1376  0.0118\n",
      "     50       \u001b[36m28.3841\u001b[0m       27.1400  0.0117\n",
      "     51       \u001b[36m28.3839\u001b[0m       27.1404  0.0114\n",
      "     52       \u001b[36m28.3823\u001b[0m       27.1402  0.0120\n",
      "     53       \u001b[36m28.3820\u001b[0m       27.1322  0.0112\n",
      "     54       \u001b[36m28.3807\u001b[0m       27.1401  0.0115\n",
      "     55       \u001b[36m28.3806\u001b[0m       27.1356  0.0121\n",
      "     56       \u001b[36m28.3791\u001b[0m       27.1322  0.0117\n",
      "     57       \u001b[36m28.3789\u001b[0m       27.1334  0.0117\n",
      "     58       \u001b[36m28.3781\u001b[0m       27.1331  0.0117\n",
      "     59       \u001b[36m28.3774\u001b[0m       27.1307  0.0115\n",
      "     60       \u001b[36m28.3767\u001b[0m       27.1283  0.0121\n",
      "     61       \u001b[36m28.3761\u001b[0m       27.1308  0.0119\n",
      "     62       \u001b[36m28.3755\u001b[0m       27.1254  0.0117\n",
      "     63       \u001b[36m28.3748\u001b[0m       27.1275  0.0112\n",
      "     64       \u001b[36m28.3743\u001b[0m       27.1266  0.0116\n",
      "     65       \u001b[36m28.3737\u001b[0m       27.1270  0.0175\n",
      "     66       \u001b[36m28.3732\u001b[0m       27.1268  0.0120\n",
      "     67       \u001b[36m28.3725\u001b[0m       27.1249  0.0120\n",
      "     68       \u001b[36m28.3720\u001b[0m       27.1248  0.0118\n",
      "     69       \u001b[36m28.3716\u001b[0m       27.1238  0.0184\n",
      "     70       \u001b[36m28.3707\u001b[0m       27.1211  0.0182\n",
      "     71       \u001b[36m28.3707\u001b[0m       27.1230  0.0128\n",
      "     72       \u001b[36m28.3698\u001b[0m       27.1209  0.0149\n",
      "     73       \u001b[36m28.3693\u001b[0m       27.1242  0.0155\n",
      "     74       \u001b[36m28.3689\u001b[0m       27.1214  0.0138\n",
      "     75       \u001b[36m28.3682\u001b[0m       27.1240  0.0195\n",
      "     76       \u001b[36m28.3679\u001b[0m       27.1220  0.0117\n",
      "     77       \u001b[36m28.3675\u001b[0m       27.1214  0.0130\n",
      "     78       \u001b[36m28.3667\u001b[0m       27.1233  0.0124\n",
      "     79       \u001b[36m28.3665\u001b[0m       27.1210  0.0125\n",
      "     80       \u001b[36m28.3664\u001b[0m       27.1192  0.0116\n",
      "     81       \u001b[36m28.3654\u001b[0m       27.1237  0.0115\n",
      "     82       \u001b[36m28.3650\u001b[0m       27.1223  0.0140\n",
      "     83       28.3655       27.1210  0.0133\n",
      "     84       \u001b[36m28.3644\u001b[0m       27.1206  0.0130\n",
      "     85       \u001b[36m28.3639\u001b[0m       27.1198  0.0123\n",
      "     86       28.3666       27.1156  0.0120\n",
      "     87       28.3648       27.1246  0.0113\n",
      "     88       28.3669       27.1173  0.0115\n",
      "     89       28.3772       27.1109  0.0142\n",
      "     90       28.3788       27.1517  0.0125\n",
      "     91       28.3917       27.0847  0.0132\n",
      "     92       28.4171       27.1243  0.0141\n",
      "     93       28.4131       27.2152  0.0118\n",
      "     94       28.4473       26.9997  0.0140\n",
      "     95       28.3988       27.1531  0.0128\n",
      "     96       28.3758       27.0946  0.0135\n",
      "     97       28.3762       27.0370  0.0118\n",
      "     98       \u001b[36m28.3627\u001b[0m       27.1298  0.0138\n",
      "     99       28.3703       27.0567  0.0128\n",
      "    100       \u001b[36m28.3597\u001b[0m       27.1009  0.0125\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m42.5357\u001b[0m       \u001b[32m43.2121\u001b[0m  0.0111\n",
      "      2       \u001b[36m40.8852\u001b[0m       \u001b[32m41.1531\u001b[0m  0.0118\n",
      "      3       \u001b[36m39.2824\u001b[0m       \u001b[32m39.0940\u001b[0m  0.0117\n",
      "      4       \u001b[36m37.7244\u001b[0m       \u001b[32m37.0730\u001b[0m  0.0117\n",
      "      5       \u001b[36m36.2751\u001b[0m       \u001b[32m35.2024\u001b[0m  0.0109\n",
      "      6       \u001b[36m35.0331\u001b[0m       \u001b[32m33.6066\u001b[0m  0.0106\n",
      "      7       \u001b[36m34.0812\u001b[0m       \u001b[32m32.3656\u001b[0m  0.0122\n",
      "      8       \u001b[36m33.4391\u001b[0m       \u001b[32m31.4963\u001b[0m  0.0119\n",
      "      9       \u001b[36m33.0573\u001b[0m       \u001b[32m30.9405\u001b[0m  0.0115\n",
      "     10       \u001b[36m32.8482\u001b[0m       \u001b[32m30.6061\u001b[0m  0.0112\n",
      "     11       \u001b[36m32.7321\u001b[0m       \u001b[32m30.4059\u001b[0m  0.0109\n",
      "     12       \u001b[36m32.6612\u001b[0m       \u001b[32m30.2842\u001b[0m  0.0131\n",
      "     13       \u001b[36m32.6122\u001b[0m       \u001b[32m30.2064\u001b[0m  0.0117\n",
      "     14       \u001b[36m32.5747\u001b[0m       \u001b[32m30.1544\u001b[0m  0.0116\n",
      "     15       \u001b[36m32.5449\u001b[0m       \u001b[32m30.1177\u001b[0m  0.0113\n",
      "     16       \u001b[36m32.5206\u001b[0m       \u001b[32m30.0899\u001b[0m  0.0113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m32.4999\u001b[0m       \u001b[32m30.0681\u001b[0m  0.0137\n",
      "     18       \u001b[36m32.4826\u001b[0m       \u001b[32m30.0505\u001b[0m  0.0112\n",
      "     19       \u001b[36m32.4673\u001b[0m       \u001b[32m30.0350\u001b[0m  0.0116\n",
      "     20       \u001b[36m32.4540\u001b[0m       \u001b[32m30.0217\u001b[0m  0.0111\n",
      "     21       \u001b[36m32.4423\u001b[0m       \u001b[32m30.0095\u001b[0m  0.0110\n",
      "     22       \u001b[36m32.4318\u001b[0m       \u001b[32m29.9989\u001b[0m  0.0129\n",
      "     23       \u001b[36m32.4224\u001b[0m       \u001b[32m29.9896\u001b[0m  0.0117\n",
      "     24       \u001b[36m32.4138\u001b[0m       \u001b[32m29.9810\u001b[0m  0.0114\n",
      "     25       \u001b[36m32.4058\u001b[0m       \u001b[32m29.9722\u001b[0m  0.0103\n",
      "     26       \u001b[36m32.3985\u001b[0m       \u001b[32m29.9642\u001b[0m  0.0108\n",
      "     27       \u001b[36m32.3917\u001b[0m       \u001b[32m29.9570\u001b[0m  0.0127\n",
      "     28       \u001b[36m32.3854\u001b[0m       \u001b[32m29.9503\u001b[0m  0.0115\n",
      "     29       \u001b[36m32.3794\u001b[0m       \u001b[32m29.9442\u001b[0m  0.0115\n",
      "     30       \u001b[36m32.3738\u001b[0m       \u001b[32m29.9387\u001b[0m  0.0109\n",
      "     31       \u001b[36m32.3684\u001b[0m       \u001b[32m29.9331\u001b[0m  0.0113\n",
      "     32       \u001b[36m32.3633\u001b[0m       \u001b[32m29.9277\u001b[0m  0.0133\n",
      "     33       \u001b[36m32.3584\u001b[0m       \u001b[32m29.9228\u001b[0m  0.0116\n",
      "     34       \u001b[36m32.3538\u001b[0m       \u001b[32m29.9181\u001b[0m  0.0118\n",
      "     35       \u001b[36m32.3494\u001b[0m       \u001b[32m29.9139\u001b[0m  0.0111\n",
      "     36       \u001b[36m32.3453\u001b[0m       \u001b[32m29.9092\u001b[0m  0.0107\n",
      "     37       \u001b[36m32.3413\u001b[0m       \u001b[32m29.9048\u001b[0m  0.0128\n",
      "     38       \u001b[36m32.3376\u001b[0m       \u001b[32m29.9012\u001b[0m  0.0117\n",
      "     39       \u001b[36m32.3340\u001b[0m       \u001b[32m29.8972\u001b[0m  0.0226\n",
      "     40       \u001b[36m32.3306\u001b[0m       \u001b[32m29.8937\u001b[0m  0.0157\n",
      "     41       \u001b[36m32.3273\u001b[0m       \u001b[32m29.8906\u001b[0m  0.0106\n",
      "     42       \u001b[36m32.3241\u001b[0m       \u001b[32m29.8874\u001b[0m  0.0115\n",
      "     43       \u001b[36m32.3210\u001b[0m       \u001b[32m29.8844\u001b[0m  0.0112\n",
      "     44       \u001b[36m32.3180\u001b[0m       \u001b[32m29.8816\u001b[0m  0.0142\n",
      "     45       \u001b[36m32.3152\u001b[0m       \u001b[32m29.8789\u001b[0m  0.0165\n",
      "     46       \u001b[36m32.3124\u001b[0m       \u001b[32m29.8763\u001b[0m  0.0170\n",
      "     47       \u001b[36m32.3098\u001b[0m       \u001b[32m29.8739\u001b[0m  0.0110\n",
      "     48       \u001b[36m32.3072\u001b[0m       \u001b[32m29.8715\u001b[0m  0.0117\n",
      "     49       \u001b[36m32.3047\u001b[0m       \u001b[32m29.8690\u001b[0m  0.0114\n",
      "     50       \u001b[36m32.3023\u001b[0m       \u001b[32m29.8671\u001b[0m  0.0127\n",
      "     51       \u001b[36m32.3000\u001b[0m       \u001b[32m29.8648\u001b[0m  0.0120\n",
      "     52       \u001b[36m32.2978\u001b[0m       \u001b[32m29.8628\u001b[0m  0.0112\n",
      "     53       \u001b[36m32.2956\u001b[0m       \u001b[32m29.8611\u001b[0m  0.0111\n",
      "     54       \u001b[36m32.2936\u001b[0m       \u001b[32m29.8593\u001b[0m  0.0109\n",
      "     55       \u001b[36m32.2916\u001b[0m       \u001b[32m29.8578\u001b[0m  0.0107\n",
      "     56       \u001b[36m32.2896\u001b[0m       \u001b[32m29.8560\u001b[0m  0.0110\n",
      "     57       \u001b[36m32.2877\u001b[0m       \u001b[32m29.8545\u001b[0m  0.0112\n",
      "     58       \u001b[36m32.2859\u001b[0m       \u001b[32m29.8528\u001b[0m  0.0111\n",
      "     59       \u001b[36m32.2841\u001b[0m       \u001b[32m29.8514\u001b[0m  0.0107\n",
      "     60       \u001b[36m32.2824\u001b[0m       \u001b[32m29.8501\u001b[0m  0.0111\n",
      "     61       \u001b[36m32.2807\u001b[0m       \u001b[32m29.8486\u001b[0m  0.0131\n",
      "     62       \u001b[36m32.2791\u001b[0m       \u001b[32m29.8473\u001b[0m  0.0111\n",
      "     63       \u001b[36m32.2775\u001b[0m       \u001b[32m29.8460\u001b[0m  0.0108\n",
      "     64       \u001b[36m32.2760\u001b[0m       \u001b[32m29.8449\u001b[0m  0.0107\n",
      "     65       \u001b[36m32.2745\u001b[0m       \u001b[32m29.8438\u001b[0m  0.0110\n",
      "     66       \u001b[36m32.2730\u001b[0m       \u001b[32m29.8426\u001b[0m  0.0110\n",
      "     67       \u001b[36m32.2716\u001b[0m       \u001b[32m29.8417\u001b[0m  0.0113\n",
      "     68       \u001b[36m32.2702\u001b[0m       \u001b[32m29.8406\u001b[0m  0.0110\n",
      "     69       \u001b[36m32.2688\u001b[0m       \u001b[32m29.8398\u001b[0m  0.0110\n",
      "     70       \u001b[36m32.2675\u001b[0m       \u001b[32m29.8387\u001b[0m  0.0107\n",
      "     71       \u001b[36m32.2662\u001b[0m       \u001b[32m29.8378\u001b[0m  0.0115\n",
      "     72       \u001b[36m32.2649\u001b[0m       \u001b[32m29.8370\u001b[0m  0.0111\n",
      "     73       \u001b[36m32.2637\u001b[0m       \u001b[32m29.8362\u001b[0m  0.0108\n",
      "     74       \u001b[36m32.2624\u001b[0m       \u001b[32m29.8352\u001b[0m  0.0107\n",
      "     75       \u001b[36m32.2613\u001b[0m       \u001b[32m29.8344\u001b[0m  0.0108\n",
      "     76       \u001b[36m32.2601\u001b[0m       \u001b[32m29.8337\u001b[0m  0.0109\n",
      "     77       \u001b[36m32.2589\u001b[0m       \u001b[32m29.8329\u001b[0m  0.0116\n",
      "     78       \u001b[36m32.2579\u001b[0m       \u001b[32m29.8323\u001b[0m  0.0109\n",
      "     79       \u001b[36m32.2567\u001b[0m       \u001b[32m29.8315\u001b[0m  0.0106\n",
      "     80       \u001b[36m32.2557\u001b[0m       \u001b[32m29.8306\u001b[0m  0.0108\n",
      "     81       \u001b[36m32.2546\u001b[0m       \u001b[32m29.8298\u001b[0m  0.0111\n",
      "     82       \u001b[36m32.2536\u001b[0m       \u001b[32m29.8291\u001b[0m  0.0110\n",
      "     83       \u001b[36m32.2526\u001b[0m       \u001b[32m29.8285\u001b[0m  0.0110\n",
      "     84       \u001b[36m32.2516\u001b[0m       \u001b[32m29.8279\u001b[0m  0.0108\n",
      "     85       \u001b[36m32.2507\u001b[0m       \u001b[32m29.8271\u001b[0m  0.0107\n",
      "     86       \u001b[36m32.2498\u001b[0m       \u001b[32m29.8266\u001b[0m  0.0110\n",
      "     87       \u001b[36m32.2489\u001b[0m       \u001b[32m29.8260\u001b[0m  0.0112\n",
      "     88       \u001b[36m32.2480\u001b[0m       \u001b[32m29.8253\u001b[0m  0.0111\n",
      "     89       \u001b[36m32.2471\u001b[0m       \u001b[32m29.8247\u001b[0m  0.0109\n",
      "     90       \u001b[36m32.2463\u001b[0m       \u001b[32m29.8242\u001b[0m  0.0108\n",
      "     91       \u001b[36m32.2454\u001b[0m       \u001b[32m29.8235\u001b[0m  0.0116\n",
      "     92       \u001b[36m32.2446\u001b[0m       \u001b[32m29.8229\u001b[0m  0.0111\n",
      "     93       \u001b[36m32.2438\u001b[0m       \u001b[32m29.8226\u001b[0m  0.0109\n",
      "     94       \u001b[36m32.2430\u001b[0m       \u001b[32m29.8220\u001b[0m  0.0109\n",
      "     95       \u001b[36m32.2423\u001b[0m       \u001b[32m29.8216\u001b[0m  0.0107\n",
      "     96       \u001b[36m32.2415\u001b[0m       \u001b[32m29.8211\u001b[0m  0.0115\n",
      "     97       \u001b[36m32.2407\u001b[0m       \u001b[32m29.8206\u001b[0m  0.0112\n",
      "     98       \u001b[36m32.2401\u001b[0m       \u001b[32m29.8203\u001b[0m  0.0111\n",
      "     99       \u001b[36m32.2393\u001b[0m       \u001b[32m29.8196\u001b[0m  0.0107\n",
      "    100       \u001b[36m32.2386\u001b[0m       \u001b[32m29.8193\u001b[0m  0.0107\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m33.1699\u001b[0m       \u001b[32m31.2436\u001b[0m  0.0113\n",
      "      2       \u001b[36m31.4070\u001b[0m       \u001b[32m30.0202\u001b[0m  0.0111\n",
      "      3       \u001b[36m29.7582\u001b[0m       \u001b[32m28.8918\u001b[0m  0.0108\n",
      "      4       \u001b[36m28.2095\u001b[0m       \u001b[32m27.8990\u001b[0m  0.0111\n",
      "      5       \u001b[36m26.8167\u001b[0m       \u001b[32m27.1053\u001b[0m  0.0111\n",
      "      6       \u001b[36m25.6534\u001b[0m       \u001b[32m26.5634\u001b[0m  0.0107\n",
      "      7       \u001b[36m24.7696\u001b[0m       \u001b[32m26.2810\u001b[0m  0.0111\n",
      "      8       \u001b[36m24.1698\u001b[0m       \u001b[32m26.2032\u001b[0m  0.0111\n",
      "      9       \u001b[36m23.8078\u001b[0m       26.2380  0.0107\n",
      "     10       \u001b[36m23.6091\u001b[0m       26.3075  0.0107\n",
      "     11       \u001b[36m23.5025\u001b[0m       26.3705  0.0107\n",
      "     12       \u001b[36m23.4422\u001b[0m       26.4147  0.0113\n",
      "     13       \u001b[36m23.4036\u001b[0m       26.4427  0.0110\n",
      "     14       \u001b[36m23.3754\u001b[0m       26.4594  0.0111\n",
      "     15       \u001b[36m23.3532\u001b[0m       26.4696  0.0108\n",
      "     16       \u001b[36m23.3342\u001b[0m       26.4753  0.0108\n",
      "     17       \u001b[36m23.3177\u001b[0m       26.4785  0.0109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     18       \u001b[36m23.3031\u001b[0m       26.4803  0.0119\n",
      "     19       \u001b[36m23.2898\u001b[0m       26.4810  0.0109\n",
      "     20       \u001b[36m23.2777\u001b[0m       26.4815  0.0115\n",
      "     21       \u001b[36m23.2666\u001b[0m       26.4816  0.0104\n",
      "     22       \u001b[36m23.2564\u001b[0m       26.4812  0.0106\n",
      "     23       \u001b[36m23.2469\u001b[0m       26.4807  0.0112\n",
      "     24       \u001b[36m23.2381\u001b[0m       26.4801  0.0109\n",
      "     25       \u001b[36m23.2299\u001b[0m       26.4793  0.0109\n",
      "     26       \u001b[36m23.2222\u001b[0m       26.4787  0.0107\n",
      "     27       \u001b[36m23.2150\u001b[0m       26.4781  0.0129\n",
      "     28       \u001b[36m23.2083\u001b[0m       26.4778  0.0153\n",
      "     29       \u001b[36m23.2020\u001b[0m       26.4773  0.0123\n",
      "     30       \u001b[36m23.1962\u001b[0m       26.4770  0.0115\n",
      "     31       \u001b[36m23.1907\u001b[0m       26.4765  0.0118\n",
      "     32       \u001b[36m23.1856\u001b[0m       26.4760  0.0115\n",
      "     33       \u001b[36m23.1807\u001b[0m       26.4753  0.0141\n",
      "     34       \u001b[36m23.1761\u001b[0m       26.4749  0.0113\n",
      "     35       \u001b[36m23.1718\u001b[0m       26.4745  0.0120\n",
      "     36       \u001b[36m23.1676\u001b[0m       26.4740  0.0111\n",
      "     37       \u001b[36m23.1637\u001b[0m       26.4735  0.0110\n",
      "     38       \u001b[36m23.1600\u001b[0m       26.4729  0.0115\n",
      "     39       \u001b[36m23.1565\u001b[0m       26.4725  0.0111\n",
      "     40       \u001b[36m23.1531\u001b[0m       26.4720  0.0109\n",
      "     41       \u001b[36m23.1498\u001b[0m       26.4714  0.0109\n",
      "     42       \u001b[36m23.1468\u001b[0m       26.4710  0.0105\n",
      "     43       \u001b[36m23.1438\u001b[0m       26.4705  0.0111\n",
      "     44       \u001b[36m23.1410\u001b[0m       26.4700  0.0113\n",
      "     45       \u001b[36m23.1383\u001b[0m       26.4695  0.0113\n",
      "     46       \u001b[36m23.1358\u001b[0m       26.4690  0.0109\n",
      "     47       \u001b[36m23.1333\u001b[0m       26.4685  0.0107\n",
      "     48       \u001b[36m23.1310\u001b[0m       26.4680  0.0114\n",
      "     49       \u001b[36m23.1287\u001b[0m       26.4675  0.0110\n",
      "     50       \u001b[36m23.1266\u001b[0m       26.4670  0.0109\n",
      "     51       \u001b[36m23.1245\u001b[0m       26.4666  0.0107\n",
      "     52       \u001b[36m23.1225\u001b[0m       26.4661  0.0108\n",
      "     53       \u001b[36m23.1206\u001b[0m       26.4656  0.0115\n",
      "     54       \u001b[36m23.1187\u001b[0m       26.4652  0.0110\n",
      "     55       \u001b[36m23.1170\u001b[0m       26.4647  0.0113\n",
      "     56       \u001b[36m23.1152\u001b[0m       26.4643  0.0106\n",
      "     57       \u001b[36m23.1136\u001b[0m       26.4638  0.0106\n",
      "     58       \u001b[36m23.1120\u001b[0m       26.4633  0.0111\n",
      "     59       \u001b[36m23.1104\u001b[0m       26.4628  0.0131\n",
      "     60       \u001b[36m23.1089\u001b[0m       26.4623  0.0117\n",
      "     61       \u001b[36m23.1074\u001b[0m       26.4619  0.0112\n",
      "     62       \u001b[36m23.1060\u001b[0m       26.4616  0.0107\n",
      "     63       \u001b[36m23.1047\u001b[0m       26.4613  0.0115\n",
      "     64       \u001b[36m23.1034\u001b[0m       26.4611  0.0111\n",
      "     65       \u001b[36m23.1021\u001b[0m       26.4607  0.0109\n",
      "     66       \u001b[36m23.1009\u001b[0m       26.4603  0.0106\n",
      "     67       \u001b[36m23.0997\u001b[0m       26.4600  0.0110\n",
      "     68       \u001b[36m23.0985\u001b[0m       26.4597  0.0115\n",
      "     69       \u001b[36m23.0974\u001b[0m       26.4595  0.0111\n",
      "     70       \u001b[36m23.0963\u001b[0m       26.4592  0.0109\n",
      "     71       \u001b[36m23.0952\u001b[0m       26.4589  0.0109\n",
      "     72       \u001b[36m23.0942\u001b[0m       26.4586  0.0107\n",
      "     73       \u001b[36m23.0932\u001b[0m       26.4583  0.0113\n",
      "     74       \u001b[36m23.0922\u001b[0m       26.4580  0.0110\n",
      "     75       \u001b[36m23.0912\u001b[0m       26.4577  0.0118\n",
      "     76       \u001b[36m23.0903\u001b[0m       26.4574  0.0107\n",
      "     77       \u001b[36m23.0894\u001b[0m       26.4571  0.0106\n",
      "     78       \u001b[36m23.0885\u001b[0m       26.4568  0.0111\n",
      "     79       \u001b[36m23.0876\u001b[0m       26.4564  0.0112\n",
      "     80       \u001b[36m23.0867\u001b[0m       26.4561  0.0108\n",
      "     81       \u001b[36m23.0859\u001b[0m       26.4557  0.0105\n",
      "     82       \u001b[36m23.0850\u001b[0m       26.4553  0.0106\n",
      "     83       \u001b[36m23.0842\u001b[0m       26.4549  0.0118\n",
      "     84       \u001b[36m23.0834\u001b[0m       26.4546  0.0110\n",
      "     85       \u001b[36m23.0826\u001b[0m       26.4542  0.0110\n",
      "     86       \u001b[36m23.0818\u001b[0m       26.4538  0.0107\n",
      "     87       \u001b[36m23.0811\u001b[0m       26.4534  0.0107\n",
      "     88       \u001b[36m23.0804\u001b[0m       26.4530  0.0113\n",
      "     89       \u001b[36m23.0796\u001b[0m       26.4527  0.0110\n",
      "     90       \u001b[36m23.0789\u001b[0m       26.4523  0.0109\n",
      "     91       \u001b[36m23.0783\u001b[0m       26.4519  0.0106\n",
      "     92       \u001b[36m23.0776\u001b[0m       26.4515  0.0108\n",
      "     93       \u001b[36m23.0769\u001b[0m       26.4512  0.0110\n",
      "     94       \u001b[36m23.0763\u001b[0m       26.4508  0.0111\n",
      "     95       \u001b[36m23.0756\u001b[0m       26.4503  0.0110\n",
      "     96       \u001b[36m23.0750\u001b[0m       26.4499  0.0106\n",
      "     97       \u001b[36m23.0744\u001b[0m       26.4495  0.0107\n",
      "     98       \u001b[36m23.0738\u001b[0m       26.4492  0.0114\n",
      "     99       \u001b[36m23.0732\u001b[0m       26.4487  0.0111\n",
      "    100       \u001b[36m23.0726\u001b[0m       26.4484  0.0110\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m39.6669\u001b[0m       \u001b[32m30.9316\u001b[0m  0.0106\n",
      "      2       \u001b[36m37.7587\u001b[0m       \u001b[32m29.6888\u001b[0m  0.0107\n",
      "      3       \u001b[36m35.9179\u001b[0m       \u001b[32m28.5186\u001b[0m  0.0112\n",
      "      4       \u001b[36m34.1302\u001b[0m       \u001b[32m27.4703\u001b[0m  0.0108\n",
      "      5       \u001b[36m32.4776\u001b[0m       \u001b[32m26.6546\u001b[0m  0.0111\n",
      "      6       \u001b[36m31.0633\u001b[0m       \u001b[32m26.1738\u001b[0m  0.0107\n",
      "      7       \u001b[36m29.9804\u001b[0m       \u001b[32m26.0660\u001b[0m  0.0104\n",
      "      8       \u001b[36m29.2829\u001b[0m       26.2392  0.0111\n",
      "      9       \u001b[36m28.9250\u001b[0m       26.5134  0.0112\n",
      "     10       \u001b[36m28.7761\u001b[0m       26.7526  0.0110\n",
      "     11       \u001b[36m28.7187\u001b[0m       26.9143  0.0134\n",
      "     12       \u001b[36m28.6918\u001b[0m       27.0118  0.0136\n",
      "     13       \u001b[36m28.6736\u001b[0m       27.0687  0.0129\n",
      "     14       \u001b[36m28.6581\u001b[0m       27.1020  0.0115\n",
      "     15       \u001b[36m28.6438\u001b[0m       27.1210  0.0115\n",
      "     16       \u001b[36m28.6304\u001b[0m       27.1327  0.0113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m28.6179\u001b[0m       27.1410  0.0149\n",
      "     18       \u001b[36m28.6066\u001b[0m       27.1471  0.0120\n",
      "     19       \u001b[36m28.5963\u001b[0m       27.1517  0.0154\n",
      "     20       \u001b[36m28.5869\u001b[0m       27.1552  0.0111\n",
      "     21       \u001b[36m28.5781\u001b[0m       27.1583  0.0112\n",
      "     22       \u001b[36m28.5700\u001b[0m       27.1600  0.0123\n",
      "     23       \u001b[36m28.5624\u001b[0m       27.1622  0.0110\n",
      "     24       \u001b[36m28.5554\u001b[0m       27.1645  0.0109\n",
      "     25       \u001b[36m28.5489\u001b[0m       27.1663  0.0109\n",
      "     26       \u001b[36m28.5428\u001b[0m       27.1684  0.0109\n",
      "     27       \u001b[36m28.5371\u001b[0m       27.1706  0.0112\n",
      "     28       \u001b[36m28.5318\u001b[0m       27.1724  0.0111\n",
      "     29       \u001b[36m28.5268\u001b[0m       27.1738  0.0110\n",
      "     30       \u001b[36m28.5221\u001b[0m       27.1753  0.0110\n",
      "     31       \u001b[36m28.5177\u001b[0m       27.1768  0.0108\n",
      "     32       \u001b[36m28.5135\u001b[0m       27.1780  0.0108\n",
      "     33       \u001b[36m28.5096\u001b[0m       27.1793  0.0113\n",
      "     34       \u001b[36m28.5059\u001b[0m       27.1804  0.0110\n",
      "     35       \u001b[36m28.5024\u001b[0m       27.1817  0.0108\n",
      "     36       \u001b[36m28.4991\u001b[0m       27.1828  0.0109\n",
      "     37       \u001b[36m28.4961\u001b[0m       27.1839  0.0110\n",
      "     38       \u001b[36m28.4931\u001b[0m       27.1845  0.0111\n",
      "     39       \u001b[36m28.4903\u001b[0m       27.1850  0.0110\n",
      "     40       \u001b[36m28.4875\u001b[0m       27.1852  0.0109\n",
      "     41       \u001b[36m28.4849\u001b[0m       27.1858  0.0107\n",
      "     42       \u001b[36m28.4824\u001b[0m       27.1863  0.0106\n",
      "     43       \u001b[36m28.4800\u001b[0m       27.1868  0.0112\n",
      "     44       \u001b[36m28.4777\u001b[0m       27.1873  0.0109\n",
      "     45       \u001b[36m28.4755\u001b[0m       27.1876  0.0109\n",
      "     46       \u001b[36m28.4734\u001b[0m       27.1877  0.0108\n",
      "     47       \u001b[36m28.4713\u001b[0m       27.1881  0.0110\n",
      "     48       \u001b[36m28.4694\u001b[0m       27.1883  0.0108\n",
      "     49       \u001b[36m28.4675\u001b[0m       27.1886  0.0111\n",
      "     50       \u001b[36m28.4657\u001b[0m       27.1888  0.0107\n",
      "     51       \u001b[36m28.4641\u001b[0m       27.1890  0.0109\n",
      "     52       \u001b[36m28.4624\u001b[0m       27.1889  0.0113\n",
      "     53       \u001b[36m28.4608\u001b[0m       27.1888  0.0112\n",
      "     54       \u001b[36m28.4593\u001b[0m       27.1885  0.0108\n",
      "     55       \u001b[36m28.4578\u001b[0m       27.1885  0.0107\n",
      "     56       \u001b[36m28.4563\u001b[0m       27.1885  0.0107\n",
      "     57       \u001b[36m28.4549\u001b[0m       27.1887  0.0114\n",
      "     58       \u001b[36m28.4537\u001b[0m       27.1884  0.0110\n",
      "     59       \u001b[36m28.4523\u001b[0m       27.1886  0.0111\n",
      "     60       \u001b[36m28.4511\u001b[0m       27.1889  0.0108\n",
      "     61       \u001b[36m28.4500\u001b[0m       27.1892  0.0106\n",
      "     62       \u001b[36m28.4489\u001b[0m       27.1896  0.0114\n",
      "     63       \u001b[36m28.4478\u001b[0m       27.1898  0.0110\n",
      "     64       \u001b[36m28.4468\u001b[0m       27.1896  0.0110\n",
      "     65       \u001b[36m28.4458\u001b[0m       27.1895  0.0106\n",
      "     66       \u001b[36m28.4448\u001b[0m       27.1894  0.0105\n",
      "     67       \u001b[36m28.4438\u001b[0m       27.1891  0.0109\n",
      "     68       \u001b[36m28.4429\u001b[0m       27.1890  0.0109\n",
      "     69       \u001b[36m28.4420\u001b[0m       27.1889  0.0112\n",
      "     70       \u001b[36m28.4411\u001b[0m       27.1888  0.0108\n",
      "     71       \u001b[36m28.4402\u001b[0m       27.1886  0.0108\n",
      "     72       \u001b[36m28.4394\u001b[0m       27.1882  0.0111\n",
      "     73       \u001b[36m28.4385\u001b[0m       27.1882  0.0109\n",
      "     74       \u001b[36m28.4377\u001b[0m       27.1881  0.0108\n",
      "     75       \u001b[36m28.4369\u001b[0m       27.1878  0.0107\n",
      "     76       \u001b[36m28.4362\u001b[0m       27.1877  0.0105\n",
      "     77       \u001b[36m28.4354\u001b[0m       27.1877  0.0109\n",
      "     78       \u001b[36m28.4347\u001b[0m       27.1875  0.0108\n",
      "     79       \u001b[36m28.4340\u001b[0m       27.1872  0.0110\n",
      "     80       \u001b[36m28.4333\u001b[0m       27.1869  0.0105\n",
      "     81       \u001b[36m28.4326\u001b[0m       27.1868  0.0106\n",
      "     82       \u001b[36m28.4319\u001b[0m       27.1865  0.0114\n",
      "     83       \u001b[36m28.4313\u001b[0m       27.1863  0.0116\n",
      "     84       \u001b[36m28.4306\u001b[0m       27.1859  0.0110\n",
      "     85       \u001b[36m28.4300\u001b[0m       27.1855  0.0110\n",
      "     86       \u001b[36m28.4294\u001b[0m       27.1848  0.0105\n",
      "     87       \u001b[36m28.4288\u001b[0m       27.1848  0.0107\n",
      "     88       \u001b[36m28.4282\u001b[0m       27.1844  0.0112\n",
      "     89       \u001b[36m28.4276\u001b[0m       27.1842  0.0109\n",
      "     90       \u001b[36m28.4271\u001b[0m       27.1835  0.0107\n",
      "     91       \u001b[36m28.4265\u001b[0m       27.1836  0.0107\n",
      "     92       \u001b[36m28.4260\u001b[0m       27.1835  0.0112\n",
      "     93       \u001b[36m28.4255\u001b[0m       27.1827  0.0110\n",
      "     94       \u001b[36m28.4250\u001b[0m       27.1825  0.0109\n",
      "     95       \u001b[36m28.4245\u001b[0m       27.1823  0.0109\n",
      "     96       \u001b[36m28.4240\u001b[0m       27.1819  0.0148\n",
      "     97       \u001b[36m28.4235\u001b[0m       27.1816  0.0159\n",
      "     98       \u001b[36m28.4230\u001b[0m       27.1815  0.0138\n",
      "     99       \u001b[36m28.4226\u001b[0m       27.1810  0.0139\n",
      "    100       \u001b[36m28.4221\u001b[0m       27.1803  0.0140\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m41.5669\u001b[0m       \u001b[32m40.0923\u001b[0m  0.0139\n",
      "      2       \u001b[36m36.9251\u001b[0m       \u001b[32m31.6755\u001b[0m  0.0139\n",
      "      3       \u001b[36m34.5662\u001b[0m       \u001b[32m31.1475\u001b[0m  0.0119\n",
      "      4       \u001b[36m33.6729\u001b[0m       32.0523  0.0119\n",
      "      5       \u001b[36m33.4804\u001b[0m       31.8171  0.0117\n",
      "      6       \u001b[36m33.0287\u001b[0m       \u001b[32m30.5319\u001b[0m  0.0117\n",
      "      7       \u001b[36m32.8650\u001b[0m       \u001b[32m30.3529\u001b[0m  0.0121\n",
      "      8       \u001b[36m32.7108\u001b[0m       30.7110  0.0118\n",
      "      9       \u001b[36m32.6822\u001b[0m       30.5065  0.0118\n",
      "     10       \u001b[36m32.5415\u001b[0m       \u001b[32m30.1786\u001b[0m  0.0118\n",
      "     11       \u001b[36m32.4888\u001b[0m       30.2719  0.0118\n",
      "     12       \u001b[36m32.4476\u001b[0m       30.3823  0.0121\n",
      "     13       \u001b[36m32.4031\u001b[0m       30.2260  0.0118\n",
      "     14       \u001b[36m32.3673\u001b[0m       30.1898  0.0118\n",
      "     15       \u001b[36m32.3479\u001b[0m       30.2229  0.0119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     16       \u001b[36m32.3244\u001b[0m       30.1815  0.0116\n",
      "     17       \u001b[36m32.3063\u001b[0m       \u001b[32m30.1735\u001b[0m  0.0112\n",
      "     18       \u001b[36m32.2924\u001b[0m       \u001b[32m30.1562\u001b[0m  0.0123\n",
      "     19       \u001b[36m32.2793\u001b[0m       \u001b[32m30.1410\u001b[0m  0.0114\n",
      "     20       \u001b[36m32.2694\u001b[0m       30.1514  0.0117\n",
      "     21       \u001b[36m32.2590\u001b[0m       \u001b[32m30.1273\u001b[0m  0.0114\n",
      "     22       \u001b[36m32.2488\u001b[0m       \u001b[32m30.1172\u001b[0m  0.0116\n",
      "     23       \u001b[36m32.2418\u001b[0m       \u001b[32m30.1168\u001b[0m  0.0119\n",
      "     24       \u001b[36m32.2339\u001b[0m       \u001b[32m30.1046\u001b[0m  0.0115\n",
      "     25       \u001b[36m32.2266\u001b[0m       \u001b[32m30.1018\u001b[0m  0.0131\n",
      "     26       \u001b[36m32.2205\u001b[0m       \u001b[32m30.0952\u001b[0m  0.0120\n",
      "     27       \u001b[36m32.2146\u001b[0m       30.0965  0.0110\n",
      "     28       \u001b[36m32.2094\u001b[0m       \u001b[32m30.0869\u001b[0m  0.0119\n",
      "     29       \u001b[36m32.2040\u001b[0m       30.0883  0.0121\n",
      "     30       \u001b[36m32.1997\u001b[0m       \u001b[32m30.0754\u001b[0m  0.0118\n",
      "     31       \u001b[36m32.1952\u001b[0m       \u001b[32m30.0746\u001b[0m  0.0118\n",
      "     32       \u001b[36m32.1912\u001b[0m       \u001b[32m30.0551\u001b[0m  0.0116\n",
      "     33       \u001b[36m32.1873\u001b[0m       30.0646  0.0116\n",
      "     34       \u001b[36m32.1840\u001b[0m       \u001b[32m30.0403\u001b[0m  0.0115\n",
      "     35       \u001b[36m32.1805\u001b[0m       30.0624  0.0117\n",
      "     36       \u001b[36m32.1775\u001b[0m       \u001b[32m30.0249\u001b[0m  0.0117\n",
      "     37       \u001b[36m32.1746\u001b[0m       30.0670  0.0116\n",
      "     38       \u001b[36m32.1724\u001b[0m       \u001b[32m30.0074\u001b[0m  0.0122\n",
      "     39       \u001b[36m32.1707\u001b[0m       30.0819  0.0116\n",
      "     40       \u001b[36m32.1699\u001b[0m       \u001b[32m29.9921\u001b[0m  0.0118\n",
      "     41       32.1699       30.1163  0.0117\n",
      "     42       32.1716       \u001b[32m29.9827\u001b[0m  0.0119\n",
      "     43       32.1739       30.1407  0.0122\n",
      "     44       32.1766       \u001b[32m29.9663\u001b[0m  0.0117\n",
      "     45       32.1741       30.1000  0.0116\n",
      "     46       32.1725       \u001b[32m29.9551\u001b[0m  0.0117\n",
      "     47       \u001b[36m32.1569\u001b[0m       30.0213  0.0117\n",
      "     48       32.1585       29.9906  0.0118\n",
      "     49       \u001b[36m32.1441\u001b[0m       29.9787  0.0116\n",
      "     50       32.1466       30.0134  0.0116\n",
      "     51       \u001b[36m32.1414\u001b[0m       29.9806  0.0119\n",
      "     52       \u001b[36m32.1401\u001b[0m       30.0108  0.0116\n",
      "     53       \u001b[36m32.1389\u001b[0m       29.9870  0.0117\n",
      "     54       \u001b[36m32.1356\u001b[0m       30.0071  0.0118\n",
      "     55       \u001b[36m32.1353\u001b[0m       29.9902  0.0113\n",
      "     56       \u001b[36m32.1317\u001b[0m       30.0050  0.0120\n",
      "     57       \u001b[36m32.1314\u001b[0m       29.9926  0.0117\n",
      "     58       \u001b[36m32.1285\u001b[0m       30.0059  0.0120\n",
      "     59       \u001b[36m32.1276\u001b[0m       29.9948  0.0114\n",
      "     60       \u001b[36m32.1252\u001b[0m       30.0085  0.0117\n",
      "     61       \u001b[36m32.1242\u001b[0m       29.9950  0.0120\n",
      "     62       \u001b[36m32.1221\u001b[0m       30.0089  0.0119\n",
      "     63       \u001b[36m32.1207\u001b[0m       29.9971  0.0113\n",
      "     64       \u001b[36m32.1189\u001b[0m       30.0120  0.0113\n",
      "     65       \u001b[36m32.1174\u001b[0m       29.9963  0.0113\n",
      "     66       \u001b[36m32.1161\u001b[0m       30.0147  0.0120\n",
      "     67       \u001b[36m32.1141\u001b[0m       29.9986  0.0117\n",
      "     68       \u001b[36m32.1130\u001b[0m       30.0143  0.0117\n",
      "     69       \u001b[36m32.1110\u001b[0m       29.9971  0.0115\n",
      "     70       \u001b[36m32.1103\u001b[0m       30.0127  0.0113\n",
      "     71       \u001b[36m32.1077\u001b[0m       29.9967  0.0121\n",
      "     72       \u001b[36m32.1075\u001b[0m       30.0108  0.0118\n",
      "     73       \u001b[36m32.1046\u001b[0m       30.0017  0.0117\n",
      "     74       32.1046       30.0117  0.0184\n",
      "     75       \u001b[36m32.1018\u001b[0m       30.0081  0.0191\n",
      "     76       \u001b[36m32.1015\u001b[0m       30.0106  0.0130\n",
      "     77       \u001b[36m32.0994\u001b[0m       30.0130  0.0139\n",
      "     78       \u001b[36m32.0982\u001b[0m       30.0123  0.0133\n",
      "     79       \u001b[36m32.0975\u001b[0m       30.0154  0.0167\n",
      "     80       \u001b[36m32.0948\u001b[0m       30.0171  0.0135\n",
      "     81       \u001b[36m32.0948\u001b[0m       30.0178  0.0122\n",
      "     82       \u001b[36m32.0931\u001b[0m       30.0218  0.0120\n",
      "     83       \u001b[36m32.0910\u001b[0m       30.0216  0.0118\n",
      "     84       32.0920       30.0214  0.0122\n",
      "     85       \u001b[36m32.0881\u001b[0m       30.0324  0.0122\n",
      "     86       32.0888       30.0313  0.0122\n",
      "     87       32.0902       30.0350  0.0126\n",
      "     88       \u001b[36m32.0831\u001b[0m       30.0413  0.0121\n",
      "     89       32.0895       30.0388  0.0119\n",
      "     90       32.0916       30.0764  0.0123\n",
      "     91       32.0885       30.0478  0.0124\n",
      "     92       32.0983       30.0465  0.0120\n",
      "     93       32.0854       30.0790  0.0115\n",
      "     94       32.0926       30.0529  0.0117\n",
      "     95       32.0954       30.0923  0.0114\n",
      "     96       \u001b[36m32.0763\u001b[0m       30.0121  0.0120\n",
      "     97       32.0835       30.0959  0.0122\n",
      "     98       \u001b[36m32.0743\u001b[0m       30.0256  0.0120\n",
      "     99       32.0771       30.0782  0.0118\n",
      "    100       \u001b[36m32.0719\u001b[0m       30.0474  0.0120\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m32.9260\u001b[0m       \u001b[32m28.7049\u001b[0m  0.0117\n",
      "      2       \u001b[36m27.3183\u001b[0m       29.0932  0.0117\n",
      "      3       \u001b[36m25.5370\u001b[0m       \u001b[32m27.4191\u001b[0m  0.0119\n",
      "      4       \u001b[36m24.4749\u001b[0m       \u001b[32m26.4362\u001b[0m  0.0118\n",
      "      5       \u001b[36m24.3536\u001b[0m       26.5438  0.0119\n",
      "      6       \u001b[36m23.8446\u001b[0m       27.5249  0.0114\n",
      "      7       \u001b[36m23.6555\u001b[0m       27.1809  0.0114\n",
      "      8       \u001b[36m23.5028\u001b[0m       26.7764  0.0121\n",
      "      9       \u001b[36m23.4607\u001b[0m       26.9671  0.0122\n",
      "     10       \u001b[36m23.3465\u001b[0m       27.2254  0.0118\n",
      "     11       \u001b[36m23.2842\u001b[0m       26.9929  0.0115\n",
      "     12       \u001b[36m23.2533\u001b[0m       26.9199  0.0115\n",
      "     13       \u001b[36m23.2270\u001b[0m       27.0707  0.0122\n",
      "     14       \u001b[36m23.1931\u001b[0m       27.0429  0.0117\n",
      "     15       \u001b[36m23.1739\u001b[0m       26.9407  0.0118\n",
      "     16       \u001b[36m23.1630\u001b[0m       26.9653  0.0117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m23.1477\u001b[0m       26.9894  0.0120\n",
      "     18       \u001b[36m23.1363\u001b[0m       26.9746  0.0124\n",
      "     19       \u001b[36m23.1274\u001b[0m       26.9729  0.0120\n",
      "     20       \u001b[36m23.1190\u001b[0m       26.9684  0.0118\n",
      "     21       \u001b[36m23.1124\u001b[0m       26.9784  0.0116\n",
      "     22       \u001b[36m23.1053\u001b[0m       26.9814  0.0115\n",
      "     23       \u001b[36m23.0997\u001b[0m       26.9774  0.0118\n",
      "     24       \u001b[36m23.0951\u001b[0m       26.9852  0.0118\n",
      "     25       \u001b[36m23.0898\u001b[0m       26.9885  0.0120\n",
      "     26       \u001b[36m23.0857\u001b[0m       26.9905  0.0116\n",
      "     27       \u001b[36m23.0816\u001b[0m       26.9891  0.0115\n",
      "     28       \u001b[36m23.0780\u001b[0m       26.9821  0.0119\n",
      "     29       \u001b[36m23.0742\u001b[0m       26.9735  0.0118\n",
      "     30       \u001b[36m23.0710\u001b[0m       26.9718  0.0115\n",
      "     31       \u001b[36m23.0678\u001b[0m       26.9681  0.0116\n",
      "     32       \u001b[36m23.0651\u001b[0m       26.9640  0.0114\n",
      "     33       \u001b[36m23.0623\u001b[0m       26.9612  0.0120\n",
      "     34       \u001b[36m23.0596\u001b[0m       26.9639  0.0117\n",
      "     35       \u001b[36m23.0573\u001b[0m       26.9605  0.0119\n",
      "     36       \u001b[36m23.0549\u001b[0m       26.9547  0.0116\n",
      "     37       \u001b[36m23.0527\u001b[0m       26.9484  0.0117\n",
      "     38       \u001b[36m23.0505\u001b[0m       26.9486  0.0118\n",
      "     39       \u001b[36m23.0486\u001b[0m       26.9483  0.0118\n",
      "     40       \u001b[36m23.0466\u001b[0m       26.9482  0.0116\n",
      "     41       \u001b[36m23.0447\u001b[0m       26.9513  0.0114\n",
      "     42       \u001b[36m23.0429\u001b[0m       26.9507  0.0114\n",
      "     43       \u001b[36m23.0410\u001b[0m       26.9537  0.0122\n",
      "     44       \u001b[36m23.0391\u001b[0m       26.9537  0.0115\n",
      "     45       \u001b[36m23.0377\u001b[0m       26.9505  0.0118\n",
      "     46       \u001b[36m23.0360\u001b[0m       26.9496  0.0113\n",
      "     47       \u001b[36m23.0343\u001b[0m       26.9511  0.0114\n",
      "     48       \u001b[36m23.0326\u001b[0m       26.9456  0.0121\n",
      "     49       \u001b[36m23.0310\u001b[0m       26.9447  0.0123\n",
      "     50       \u001b[36m23.0295\u001b[0m       26.9490  0.0120\n",
      "     51       \u001b[36m23.0279\u001b[0m       26.9505  0.0119\n",
      "     52       \u001b[36m23.0264\u001b[0m       26.9543  0.0167\n",
      "     53       \u001b[36m23.0249\u001b[0m       26.9554  0.0137\n",
      "     54       \u001b[36m23.0235\u001b[0m       26.9601  0.0121\n",
      "     55       \u001b[36m23.0220\u001b[0m       26.9595  0.0119\n",
      "     56       \u001b[36m23.0207\u001b[0m       26.9594  0.0121\n",
      "     57       \u001b[36m23.0191\u001b[0m       26.9662  0.0143\n",
      "     58       \u001b[36m23.0177\u001b[0m       26.9609  0.0125\n",
      "     59       \u001b[36m23.0164\u001b[0m       26.9662  0.0120\n",
      "     60       \u001b[36m23.0149\u001b[0m       26.9618  0.0116\n",
      "     61       \u001b[36m23.0138\u001b[0m       26.9623  0.0116\n",
      "     62       \u001b[36m23.0121\u001b[0m       26.9663  0.0120\n",
      "     63       \u001b[36m23.0113\u001b[0m       26.9715  0.0119\n",
      "     64       \u001b[36m23.0093\u001b[0m       26.9712  0.0119\n",
      "     65       \u001b[36m23.0087\u001b[0m       26.9714  0.0120\n",
      "     66       \u001b[36m23.0065\u001b[0m       26.9662  0.0117\n",
      "     67       \u001b[36m23.0062\u001b[0m       26.9752  0.0123\n",
      "     68       \u001b[36m23.0045\u001b[0m       26.9751  0.0120\n",
      "     69       \u001b[36m23.0039\u001b[0m       26.9725  0.0120\n",
      "     70       23.0043       26.9757  0.0123\n",
      "     71       \u001b[36m23.0018\u001b[0m       26.9770  0.0113\n",
      "     72       23.0033       26.9829  0.0124\n",
      "     73       \u001b[36m22.9976\u001b[0m       26.9706  0.0116\n",
      "     74       23.0002       26.9833  0.0118\n",
      "     75       \u001b[36m22.9958\u001b[0m       26.9684  0.0114\n",
      "     76       22.9986       26.9895  0.0119\n",
      "     77       22.9964       26.9706  0.0116\n",
      "     78       \u001b[36m22.9930\u001b[0m       26.9882  0.0117\n",
      "     79       \u001b[36m22.9926\u001b[0m       26.9877  0.0114\n",
      "     80       \u001b[36m22.9892\u001b[0m       26.9872  0.0112\n",
      "     81       22.9899       26.9947  0.0116\n",
      "     82       \u001b[36m22.9878\u001b[0m       26.9907  0.0117\n",
      "     83       \u001b[36m22.9855\u001b[0m       26.9921  0.0150\n",
      "     84       22.9859       26.9874  0.0131\n",
      "     85       \u001b[36m22.9830\u001b[0m       26.9980  0.0143\n",
      "     86       22.9837       26.9901  0.0116\n",
      "     87       \u001b[36m22.9830\u001b[0m       26.9768  0.0121\n",
      "     88       \u001b[36m22.9800\u001b[0m       26.9904  0.0122\n",
      "     89       22.9847       26.9890  0.0131\n",
      "     90       \u001b[36m22.9776\u001b[0m       26.9638  0.0128\n",
      "     91       22.9829       26.9912  0.0131\n",
      "     92       22.9856       26.9854  0.0131\n",
      "     93       22.9810       26.9758  0.0128\n",
      "     94       22.9860       27.0283  0.0128\n",
      "     95       \u001b[36m22.9765\u001b[0m       26.9664  0.0127\n",
      "     96       22.9811       26.9891  0.0129\n",
      "     97       \u001b[36m22.9733\u001b[0m       26.9902  0.0125\n",
      "     98       \u001b[36m22.9690\u001b[0m       27.0098  0.0121\n",
      "     99       22.9693       26.9800  0.0130\n",
      "    100       \u001b[36m22.9659\u001b[0m       26.9999  0.0123\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m39.1085\u001b[0m       \u001b[32m28.2689\u001b[0m  0.0118\n",
      "      2       \u001b[36m32.3383\u001b[0m       30.5542  0.0128\n",
      "      3       \u001b[36m31.3903\u001b[0m       \u001b[32m27.9785\u001b[0m  0.0122\n",
      "      4       \u001b[36m29.5458\u001b[0m       \u001b[32m26.4734\u001b[0m  0.0126\n",
      "      5       \u001b[36m29.5342\u001b[0m       26.8541  0.0143\n",
      "      6       \u001b[36m29.0537\u001b[0m       28.4830  0.0146\n",
      "      7       29.1550       27.8036  0.0123\n",
      "      8       \u001b[36m28.8408\u001b[0m       27.1549  0.0148\n",
      "      9       \u001b[36m28.7065\u001b[0m       27.4689  0.0127\n",
      "     10       \u001b[36m28.6825\u001b[0m       27.6215  0.0125\n",
      "     11       \u001b[36m28.6396\u001b[0m       27.3322  0.0115\n",
      "     12       \u001b[36m28.5513\u001b[0m       27.3489  0.0142\n",
      "     13       \u001b[36m28.5323\u001b[0m       27.4805  0.0133\n",
      "     14       \u001b[36m28.5201\u001b[0m       27.4269  0.0129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     15       \u001b[36m28.4991\u001b[0m       27.2852  0.0136\n",
      "     16       \u001b[36m28.4741\u001b[0m       27.3318  0.0120\n",
      "     17       \u001b[36m28.4702\u001b[0m       27.3854  0.0114\n",
      "     18       \u001b[36m28.4615\u001b[0m       27.2920  0.0121\n",
      "     19       \u001b[36m28.4495\u001b[0m       27.2883  0.0118\n",
      "     20       \u001b[36m28.4455\u001b[0m       27.3410  0.0123\n",
      "     21       28.4469       27.2611  0.0116\n",
      "     22       \u001b[36m28.4358\u001b[0m       27.3195  0.0123\n",
      "     23       28.4452       27.2374  0.0121\n",
      "     24       \u001b[36m28.4233\u001b[0m       27.2858  0.0121\n",
      "     25       28.4239       27.2564  0.0144\n",
      "     26       \u001b[36m28.4145\u001b[0m       27.2269  0.0148\n",
      "     27       \u001b[36m28.4108\u001b[0m       27.2801  0.0180\n",
      "     28       28.4130       27.2109  0.0148\n",
      "     29       \u001b[36m28.4026\u001b[0m       27.2360  0.0122\n",
      "     30       28.4041       27.2358  0.0131\n",
      "     31       \u001b[36m28.4005\u001b[0m       27.2087  0.0136\n",
      "     32       \u001b[36m28.3973\u001b[0m       27.2296  0.0164\n",
      "     33       28.3973       27.2125  0.0141\n",
      "     34       \u001b[36m28.3940\u001b[0m       27.2099  0.0124\n",
      "     35       \u001b[36m28.3927\u001b[0m       27.2143  0.0143\n",
      "     36       \u001b[36m28.3916\u001b[0m       27.2054  0.0124\n",
      "     37       \u001b[36m28.3895\u001b[0m       27.2090  0.0127\n",
      "     38       \u001b[36m28.3883\u001b[0m       27.2023  0.0122\n",
      "     39       \u001b[36m28.3877\u001b[0m       27.2056  0.0147\n",
      "     40       \u001b[36m28.3857\u001b[0m       27.2053  0.0136\n",
      "     41       \u001b[36m28.3855\u001b[0m       27.1975  0.0132\n",
      "     42       28.3894       27.2021  0.0124\n",
      "     43       28.3856       27.2022  0.0127\n",
      "     44       28.3937       27.2068  0.0122\n",
      "     45       28.4132       27.1900  0.0120\n",
      "     46       28.4015       27.2121  0.0131\n",
      "     47       28.4175       27.1758  0.0123\n",
      "     48       28.4102       27.2464  0.0120\n",
      "     49       28.4051       27.1108  0.0130\n",
      "     50       \u001b[36m28.3817\u001b[0m       27.2469  0.0143\n",
      "     51       28.3820       27.1480  0.0125\n",
      "     52       \u001b[36m28.3761\u001b[0m       27.1858  0.0130\n",
      "     53       28.3781       27.1573  0.0125\n",
      "     54       \u001b[36m28.3761\u001b[0m       27.1808  0.0126\n",
      "     55       \u001b[36m28.3732\u001b[0m       27.1644  0.0132\n",
      "     56       28.3738       27.1574  0.0127\n",
      "     57       \u001b[36m28.3718\u001b[0m       27.1619  0.0117\n",
      "     58       28.3719       27.1581  0.0120\n",
      "     59       \u001b[36m28.3706\u001b[0m       27.1565  0.0125\n",
      "     60       \u001b[36m28.3704\u001b[0m       27.1563  0.0127\n",
      "     61       \u001b[36m28.3693\u001b[0m       27.1585  0.0121\n",
      "     62       \u001b[36m28.3686\u001b[0m       27.1555  0.0123\n",
      "     63       \u001b[36m28.3682\u001b[0m       27.1557  0.0121\n",
      "     64       \u001b[36m28.3675\u001b[0m       27.1572  0.0123\n",
      "     65       \u001b[36m28.3667\u001b[0m       27.1517  0.0124\n",
      "     66       \u001b[36m28.3662\u001b[0m       27.1566  0.0119\n",
      "     67       \u001b[36m28.3660\u001b[0m       27.1532  0.0115\n",
      "     68       \u001b[36m28.3649\u001b[0m       27.1562  0.0127\n",
      "     69       \u001b[36m28.3647\u001b[0m       27.1550  0.0122\n",
      "     70       \u001b[36m28.3642\u001b[0m       27.1533  0.0129\n",
      "     71       \u001b[36m28.3633\u001b[0m       27.1555  0.0137\n",
      "     72       28.3633       27.1590  0.0116\n",
      "     73       \u001b[36m28.3625\u001b[0m       27.1563  0.0143\n",
      "     74       \u001b[36m28.3621\u001b[0m       27.1592  0.0161\n",
      "     75       \u001b[36m28.3618\u001b[0m       27.1609  0.0141\n",
      "     76       \u001b[36m28.3613\u001b[0m       27.1592  0.0146\n",
      "     77       \u001b[36m28.3601\u001b[0m       27.1650  0.0126\n",
      "     78       28.3614       27.1581  0.0129\n",
      "     79       \u001b[36m28.3590\u001b[0m       27.1638  0.0129\n",
      "     80       28.3595       27.1692  0.0127\n",
      "     81       28.3610       27.1578  0.0127\n",
      "     82       \u001b[36m28.3582\u001b[0m       27.1651  0.0131\n",
      "     83       28.3585       27.1810  0.0130\n",
      "     84       28.3632       27.1437  0.0146\n",
      "     85       28.3599       27.1828  0.0126\n",
      "     86       28.3615       27.1834  0.0127\n",
      "     87       28.3735       27.1457  0.0121\n",
      "     88       28.3712       27.1901  0.0137\n",
      "     89       28.3794       27.1730  0.0144\n",
      "     90       28.3932       27.1364  0.0119\n",
      "     91       28.3790       27.2090  0.0119\n",
      "     92       28.3969       27.1194  0.0116\n",
      "     93       28.3925       27.1823  0.0133\n",
      "     94       28.3617       27.1160  0.0146\n",
      "     95       28.3721       27.1028  0.0124\n",
      "     96       \u001b[36m28.3565\u001b[0m       27.1629  0.0116\n",
      "     97       28.3597       27.0825  0.0143\n",
      "     98       \u001b[36m28.3564\u001b[0m       27.1362  0.0123\n",
      "     99       \u001b[36m28.3536\u001b[0m       27.0981  0.0123\n",
      "    100       28.3556       27.1149  0.0182\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m43.3690\u001b[0m       \u001b[32m43.8970\u001b[0m  0.0116\n",
      "      2       \u001b[36m41.2620\u001b[0m       \u001b[32m41.3949\u001b[0m  0.0115\n",
      "      3       \u001b[36m39.3235\u001b[0m       \u001b[32m38.9662\u001b[0m  0.0177\n",
      "      4       \u001b[36m37.4961\u001b[0m       \u001b[32m36.6478\u001b[0m  0.0126\n",
      "      5       \u001b[36m35.8536\u001b[0m       \u001b[32m34.5777\u001b[0m  0.0126\n",
      "      6       \u001b[36m34.5224\u001b[0m       \u001b[32m32.9100\u001b[0m  0.0128\n",
      "      7       \u001b[36m33.5977\u001b[0m       \u001b[32m31.7484\u001b[0m  0.0123\n",
      "      8       \u001b[36m33.0715\u001b[0m       \u001b[32m31.0515\u001b[0m  0.0111\n",
      "      9       \u001b[36m32.8217\u001b[0m       \u001b[32m30.6726\u001b[0m  0.0113\n",
      "     10       \u001b[36m32.7086\u001b[0m       \u001b[32m30.4703\u001b[0m  0.0112\n",
      "     11       \u001b[36m32.6491\u001b[0m       \u001b[32m30.3578\u001b[0m  0.0110\n",
      "     12       \u001b[36m32.6096\u001b[0m       \u001b[32m30.2896\u001b[0m  0.0114\n",
      "     13       \u001b[36m32.5785\u001b[0m       \u001b[32m30.2441\u001b[0m  0.0110\n",
      "     14       \u001b[36m32.5523\u001b[0m       \u001b[32m30.2103\u001b[0m  0.0110\n",
      "     15       \u001b[36m32.5291\u001b[0m       \u001b[32m30.1836\u001b[0m  0.0113\n",
      "     16       \u001b[36m32.5086\u001b[0m       \u001b[32m30.1613\u001b[0m  0.0111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m32.4900\u001b[0m       \u001b[32m30.1418\u001b[0m  0.0114\n",
      "     18       \u001b[36m32.4734\u001b[0m       \u001b[32m30.1241\u001b[0m  0.0111\n",
      "     19       \u001b[36m32.4583\u001b[0m       \u001b[32m30.1084\u001b[0m  0.0110\n",
      "     20       \u001b[36m32.4443\u001b[0m       \u001b[32m30.0939\u001b[0m  0.0104\n",
      "     21       \u001b[36m32.4317\u001b[0m       \u001b[32m30.0802\u001b[0m  0.0107\n",
      "     22       \u001b[36m32.4199\u001b[0m       \u001b[32m30.0678\u001b[0m  0.0112\n",
      "     23       \u001b[36m32.4092\u001b[0m       \u001b[32m30.0563\u001b[0m  0.0112\n",
      "     24       \u001b[36m32.3993\u001b[0m       \u001b[32m30.0457\u001b[0m  0.0106\n",
      "     25       \u001b[36m32.3901\u001b[0m       \u001b[32m30.0355\u001b[0m  0.0110\n",
      "     26       \u001b[36m32.3817\u001b[0m       \u001b[32m30.0267\u001b[0m  0.0109\n",
      "     27       \u001b[36m32.3737\u001b[0m       \u001b[32m30.0181\u001b[0m  0.0115\n",
      "     28       \u001b[36m32.3664\u001b[0m       \u001b[32m30.0098\u001b[0m  0.0109\n",
      "     29       \u001b[36m32.3597\u001b[0m       \u001b[32m30.0019\u001b[0m  0.0110\n",
      "     30       \u001b[36m32.3535\u001b[0m       \u001b[32m29.9945\u001b[0m  0.0113\n",
      "     31       \u001b[36m32.3475\u001b[0m       \u001b[32m29.9873\u001b[0m  0.0112\n",
      "     32       \u001b[36m32.3420\u001b[0m       \u001b[32m29.9807\u001b[0m  0.0132\n",
      "     33       \u001b[36m32.3368\u001b[0m       \u001b[32m29.9750\u001b[0m  0.0118\n",
      "     34       \u001b[36m32.3318\u001b[0m       \u001b[32m29.9695\u001b[0m  0.0114\n",
      "     35       \u001b[36m32.3270\u001b[0m       \u001b[32m29.9639\u001b[0m  0.0110\n",
      "     36       \u001b[36m32.3224\u001b[0m       \u001b[32m29.9590\u001b[0m  0.0112\n",
      "     37       \u001b[36m32.3180\u001b[0m       \u001b[32m29.9536\u001b[0m  0.0116\n",
      "     38       \u001b[36m32.3138\u001b[0m       \u001b[32m29.9491\u001b[0m  0.0113\n",
      "     39       \u001b[36m32.3099\u001b[0m       \u001b[32m29.9448\u001b[0m  0.0113\n",
      "     40       \u001b[36m32.3061\u001b[0m       \u001b[32m29.9407\u001b[0m  0.0114\n",
      "     41       \u001b[36m32.3025\u001b[0m       \u001b[32m29.9365\u001b[0m  0.0112\n",
      "     42       \u001b[36m32.2990\u001b[0m       \u001b[32m29.9325\u001b[0m  0.0114\n",
      "     43       \u001b[36m32.2957\u001b[0m       \u001b[32m29.9289\u001b[0m  0.0114\n",
      "     44       \u001b[36m32.2926\u001b[0m       \u001b[32m29.9256\u001b[0m  0.0109\n",
      "     45       \u001b[36m32.2896\u001b[0m       \u001b[32m29.9221\u001b[0m  0.0107\n",
      "     46       \u001b[36m32.2868\u001b[0m       \u001b[32m29.9189\u001b[0m  0.0110\n",
      "     47       \u001b[36m32.2840\u001b[0m       \u001b[32m29.9160\u001b[0m  0.0114\n",
      "     48       \u001b[36m32.2814\u001b[0m       \u001b[32m29.9130\u001b[0m  0.0111\n",
      "     49       \u001b[36m32.2789\u001b[0m       \u001b[32m29.9105\u001b[0m  0.0109\n",
      "     50       \u001b[36m32.2766\u001b[0m       \u001b[32m29.9078\u001b[0m  0.0108\n",
      "     51       \u001b[36m32.2743\u001b[0m       \u001b[32m29.9056\u001b[0m  0.0109\n",
      "     52       \u001b[36m32.2721\u001b[0m       \u001b[32m29.9030\u001b[0m  0.0111\n",
      "     53       \u001b[36m32.2700\u001b[0m       \u001b[32m29.9012\u001b[0m  0.0111\n",
      "     54       \u001b[36m32.2680\u001b[0m       \u001b[32m29.8989\u001b[0m  0.0108\n",
      "     55       \u001b[36m32.2660\u001b[0m       \u001b[32m29.8971\u001b[0m  0.0105\n",
      "     56       \u001b[36m32.2641\u001b[0m       \u001b[32m29.8951\u001b[0m  0.0107\n",
      "     57       \u001b[36m32.2623\u001b[0m       \u001b[32m29.8936\u001b[0m  0.0113\n",
      "     58       \u001b[36m32.2606\u001b[0m       \u001b[32m29.8917\u001b[0m  0.0116\n",
      "     59       \u001b[36m32.2590\u001b[0m       \u001b[32m29.8902\u001b[0m  0.0111\n",
      "     60       \u001b[36m32.2573\u001b[0m       \u001b[32m29.8886\u001b[0m  0.0107\n",
      "     61       \u001b[36m32.2558\u001b[0m       \u001b[32m29.8873\u001b[0m  0.0108\n",
      "     62       \u001b[36m32.2543\u001b[0m       \u001b[32m29.8859\u001b[0m  0.0111\n",
      "     63       \u001b[36m32.2528\u001b[0m       \u001b[32m29.8845\u001b[0m  0.0111\n",
      "     64       \u001b[36m32.2515\u001b[0m       \u001b[32m29.8831\u001b[0m  0.0111\n",
      "     65       \u001b[36m32.2501\u001b[0m       \u001b[32m29.8820\u001b[0m  0.0108\n",
      "     66       \u001b[36m32.2488\u001b[0m       \u001b[32m29.8809\u001b[0m  0.0107\n",
      "     67       \u001b[36m32.2475\u001b[0m       \u001b[32m29.8798\u001b[0m  0.0111\n",
      "     68       \u001b[36m32.2463\u001b[0m       \u001b[32m29.8790\u001b[0m  0.0111\n",
      "     69       \u001b[36m32.2451\u001b[0m       \u001b[32m29.8779\u001b[0m  0.0110\n",
      "     70       \u001b[36m32.2440\u001b[0m       \u001b[32m29.8769\u001b[0m  0.0107\n",
      "     71       \u001b[36m32.2429\u001b[0m       \u001b[32m29.8763\u001b[0m  0.0108\n",
      "     72       \u001b[36m32.2418\u001b[0m       \u001b[32m29.8753\u001b[0m  0.0110\n",
      "     73       \u001b[36m32.2407\u001b[0m       \u001b[32m29.8743\u001b[0m  0.0111\n",
      "     74       \u001b[36m32.2397\u001b[0m       \u001b[32m29.8737\u001b[0m  0.0110\n",
      "     75       \u001b[36m32.2387\u001b[0m       \u001b[32m29.8729\u001b[0m  0.0107\n",
      "     76       \u001b[36m32.2377\u001b[0m       \u001b[32m29.8721\u001b[0m  0.0113\n",
      "     77       \u001b[36m32.2367\u001b[0m       \u001b[32m29.8712\u001b[0m  0.0110\n",
      "     78       \u001b[36m32.2359\u001b[0m       \u001b[32m29.8707\u001b[0m  0.0110\n",
      "     79       \u001b[36m32.2349\u001b[0m       \u001b[32m29.8698\u001b[0m  0.0109\n",
      "     80       \u001b[36m32.2340\u001b[0m       \u001b[32m29.8692\u001b[0m  0.0106\n",
      "     81       \u001b[36m32.2332\u001b[0m       \u001b[32m29.8682\u001b[0m  0.0121\n",
      "     82       \u001b[36m32.2323\u001b[0m       \u001b[32m29.8678\u001b[0m  0.0129\n",
      "     83       \u001b[36m32.2315\u001b[0m       \u001b[32m29.8673\u001b[0m  0.0171\n",
      "     84       \u001b[36m32.2307\u001b[0m       \u001b[32m29.8666\u001b[0m  0.0149\n",
      "     85       \u001b[36m32.2299\u001b[0m       \u001b[32m29.8662\u001b[0m  0.0119\n",
      "     86       \u001b[36m32.2291\u001b[0m       \u001b[32m29.8655\u001b[0m  0.0141\n",
      "     87       \u001b[36m32.2283\u001b[0m       \u001b[32m29.8649\u001b[0m  0.0177\n",
      "     88       \u001b[36m32.2276\u001b[0m       \u001b[32m29.8643\u001b[0m  0.0119\n",
      "     89       \u001b[36m32.2269\u001b[0m       \u001b[32m29.8639\u001b[0m  0.0117\n",
      "     90       \u001b[36m32.2261\u001b[0m       \u001b[32m29.8633\u001b[0m  0.0109\n",
      "     91       \u001b[36m32.2254\u001b[0m       \u001b[32m29.8629\u001b[0m  0.0132\n",
      "     92       \u001b[36m32.2248\u001b[0m       \u001b[32m29.8623\u001b[0m  0.0119\n",
      "     93       \u001b[36m32.2241\u001b[0m       \u001b[32m29.8619\u001b[0m  0.0118\n",
      "     94       \u001b[36m32.2234\u001b[0m       \u001b[32m29.8615\u001b[0m  0.0108\n",
      "     95       \u001b[36m32.2228\u001b[0m       \u001b[32m29.8613\u001b[0m  0.0107\n",
      "     96       \u001b[36m32.2221\u001b[0m       \u001b[32m29.8611\u001b[0m  0.0123\n",
      "     97       \u001b[36m32.2215\u001b[0m       \u001b[32m29.8605\u001b[0m  0.0116\n",
      "     98       \u001b[36m32.2209\u001b[0m       \u001b[32m29.8604\u001b[0m  0.0115\n",
      "     99       \u001b[36m32.2203\u001b[0m       \u001b[32m29.8600\u001b[0m  0.0108\n",
      "    100       \u001b[36m32.2197\u001b[0m       \u001b[32m29.8598\u001b[0m  0.0113\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m32.8603\u001b[0m       \u001b[32m30.9683\u001b[0m  0.0134\n",
      "      2       \u001b[36m31.1061\u001b[0m       \u001b[32m29.7190\u001b[0m  0.0116\n",
      "      3       \u001b[36m29.4281\u001b[0m       \u001b[32m28.5464\u001b[0m  0.0120\n",
      "      4       \u001b[36m27.8142\u001b[0m       \u001b[32m27.5101\u001b[0m  0.0116\n",
      "      5       \u001b[36m26.3487\u001b[0m       \u001b[32m26.7184\u001b[0m  0.0108\n",
      "      6       \u001b[36m25.1613\u001b[0m       \u001b[32m26.2548\u001b[0m  0.0105\n",
      "      7       \u001b[36m24.3432\u001b[0m       \u001b[32m26.0968\u001b[0m  0.0123\n",
      "      8       \u001b[36m23.8709\u001b[0m       26.1214  0.0117\n",
      "      9       \u001b[36m23.6337\u001b[0m       26.2042  0.0120\n",
      "     10       \u001b[36m23.5198\u001b[0m       26.2794  0.0124\n",
      "     11       \u001b[36m23.4604\u001b[0m       26.3305  0.0109\n",
      "     12       \u001b[36m23.4236\u001b[0m       26.3608  0.0113\n",
      "     13       \u001b[36m23.3966\u001b[0m       26.3771  0.0132\n",
      "     14       \u001b[36m23.3747\u001b[0m       26.3852  0.0118\n",
      "     15       \u001b[36m23.3561\u001b[0m       26.3887  0.0115\n",
      "     16       \u001b[36m23.3395\u001b[0m       26.3898  0.0108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m23.3245\u001b[0m       26.3898  0.0114\n",
      "     18       \u001b[36m23.3108\u001b[0m       26.3893  0.0131\n",
      "     19       \u001b[36m23.2983\u001b[0m       26.3885  0.0120\n",
      "     20       \u001b[36m23.2868\u001b[0m       26.3875  0.0115\n",
      "     21       \u001b[36m23.2760\u001b[0m       26.3865  0.0110\n",
      "     22       \u001b[36m23.2661\u001b[0m       26.3855  0.0108\n",
      "     23       \u001b[36m23.2568\u001b[0m       26.3846  0.0124\n",
      "     24       \u001b[36m23.2482\u001b[0m       26.3838  0.0119\n",
      "     25       \u001b[36m23.2401\u001b[0m       26.3833  0.0122\n",
      "     26       \u001b[36m23.2325\u001b[0m       26.3828  0.0109\n",
      "     27       \u001b[36m23.2254\u001b[0m       26.3825  0.0108\n",
      "     28       \u001b[36m23.2187\u001b[0m       26.3822  0.0133\n",
      "     29       \u001b[36m23.2125\u001b[0m       26.3820  0.0117\n",
      "     30       \u001b[36m23.2066\u001b[0m       26.3820  0.0115\n",
      "     31       \u001b[36m23.2011\u001b[0m       26.3820  0.0106\n",
      "     32       \u001b[36m23.1958\u001b[0m       26.3819  0.0107\n",
      "     33       \u001b[36m23.1908\u001b[0m       26.3819  0.0126\n",
      "     34       \u001b[36m23.1861\u001b[0m       26.3819  0.0115\n",
      "     35       \u001b[36m23.1815\u001b[0m       26.3818  0.0115\n",
      "     36       \u001b[36m23.1773\u001b[0m       26.3819  0.0109\n",
      "     37       \u001b[36m23.1732\u001b[0m       26.3819  0.0116\n",
      "     38       \u001b[36m23.1693\u001b[0m       26.3819  0.0130\n",
      "     39       \u001b[36m23.1656\u001b[0m       26.3819  0.0114\n",
      "     40       \u001b[36m23.1620\u001b[0m       26.3818  0.0121\n",
      "     41       \u001b[36m23.1586\u001b[0m       26.3817  0.0118\n",
      "     42       \u001b[36m23.1553\u001b[0m       26.3818  0.0110\n",
      "     43       \u001b[36m23.1522\u001b[0m       26.3819  0.0130\n",
      "     44       \u001b[36m23.1492\u001b[0m       26.3819  0.0120\n",
      "     45       \u001b[36m23.1464\u001b[0m       26.3821  0.0119\n",
      "     46       \u001b[36m23.1436\u001b[0m       26.3822  0.0113\n",
      "     47       \u001b[36m23.1410\u001b[0m       26.3823  0.0106\n",
      "     48       \u001b[36m23.1384\u001b[0m       26.3825  0.0124\n",
      "     49       \u001b[36m23.1360\u001b[0m       26.3826  0.0116\n",
      "     50       \u001b[36m23.1336\u001b[0m       26.3826  0.0119\n",
      "     51       \u001b[36m23.1314\u001b[0m       26.3827  0.0112\n",
      "     52       \u001b[36m23.1292\u001b[0m       26.3827  0.0106\n",
      "     53       \u001b[36m23.1271\u001b[0m       26.3828  0.0131\n",
      "     54       \u001b[36m23.1250\u001b[0m       26.3830  0.0118\n",
      "     55       \u001b[36m23.1231\u001b[0m       26.3831  0.0119\n",
      "     56       \u001b[36m23.1212\u001b[0m       26.3832  0.0107\n",
      "     57       \u001b[36m23.1193\u001b[0m       26.3833  0.0109\n",
      "     58       \u001b[36m23.1176\u001b[0m       26.3833  0.0137\n",
      "     59       \u001b[36m23.1159\u001b[0m       26.3833  0.0118\n",
      "     60       \u001b[36m23.1142\u001b[0m       26.3834  0.0121\n",
      "     61       \u001b[36m23.1126\u001b[0m       26.3834  0.0128\n",
      "     62       \u001b[36m23.1111\u001b[0m       26.3835  0.0200\n",
      "     63       \u001b[36m23.1096\u001b[0m       26.3834  0.0188\n",
      "     64       \u001b[36m23.1082\u001b[0m       26.3835  0.0202\n",
      "     65       \u001b[36m23.1068\u001b[0m       26.3835  0.0230\n",
      "     66       \u001b[36m23.1054\u001b[0m       26.3834  0.0157\n",
      "     67       \u001b[36m23.1041\u001b[0m       26.3834  0.0125\n",
      "     68       \u001b[36m23.1028\u001b[0m       26.3834  0.0118\n",
      "     69       \u001b[36m23.1016\u001b[0m       26.3834  0.0120\n",
      "     70       \u001b[36m23.1004\u001b[0m       26.3833  0.0119\n",
      "     71       \u001b[36m23.0992\u001b[0m       26.3834  0.0118\n",
      "     72       \u001b[36m23.0981\u001b[0m       26.3833  0.0119\n",
      "     73       \u001b[36m23.0970\u001b[0m       26.3832  0.0117\n",
      "     74       \u001b[36m23.0959\u001b[0m       26.3831  0.0118\n",
      "     75       \u001b[36m23.0949\u001b[0m       26.3831  0.0116\n",
      "     76       \u001b[36m23.0939\u001b[0m       26.3830  0.0118\n",
      "     77       \u001b[36m23.0929\u001b[0m       26.3830  0.0117\n",
      "     78       \u001b[36m23.0919\u001b[0m       26.3829  0.0114\n",
      "     79       \u001b[36m23.0910\u001b[0m       26.3829  0.0137\n",
      "     80       \u001b[36m23.0901\u001b[0m       26.3828  0.0118\n",
      "     81       \u001b[36m23.0892\u001b[0m       26.3827  0.0121\n",
      "     82       \u001b[36m23.0883\u001b[0m       26.3826  0.0111\n",
      "     83       \u001b[36m23.0874\u001b[0m       26.3824  0.0111\n",
      "     84       \u001b[36m23.0866\u001b[0m       26.3823  0.0111\n",
      "     85       \u001b[36m23.0858\u001b[0m       26.3822  0.0119\n",
      "     86       \u001b[36m23.0850\u001b[0m       26.3821  0.0116\n",
      "     87       \u001b[36m23.0842\u001b[0m       26.3819  0.0117\n",
      "     88       \u001b[36m23.0834\u001b[0m       26.3819  0.0120\n",
      "     89       \u001b[36m23.0826\u001b[0m       26.3817  0.0117\n",
      "     90       \u001b[36m23.0819\u001b[0m       26.3815  0.0116\n",
      "     91       \u001b[36m23.0812\u001b[0m       26.3815  0.0131\n",
      "     92       \u001b[36m23.0804\u001b[0m       26.3813  0.0109\n",
      "     93       \u001b[36m23.0797\u001b[0m       26.3812  0.0109\n",
      "     94       \u001b[36m23.0790\u001b[0m       26.3810  0.0108\n",
      "     95       \u001b[36m23.0784\u001b[0m       26.3810  0.0106\n",
      "     96       \u001b[36m23.0777\u001b[0m       26.3808  0.0107\n",
      "     97       \u001b[36m23.0771\u001b[0m       26.3806  0.0109\n",
      "     98       \u001b[36m23.0764\u001b[0m       26.3805  0.0104\n",
      "     99       \u001b[36m23.0758\u001b[0m       26.3804  0.0112\n",
      "    100       \u001b[36m23.0751\u001b[0m       26.3802  0.0108\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m40.4040\u001b[0m       \u001b[32m31.2613\u001b[0m  0.0109\n",
      "      2       \u001b[36m38.2250\u001b[0m       \u001b[32m29.8894\u001b[0m  0.0110\n",
      "      3       \u001b[36m36.2097\u001b[0m       \u001b[32m28.6615\u001b[0m  0.0114\n",
      "      4       \u001b[36m34.3410\u001b[0m       \u001b[32m27.6336\u001b[0m  0.0112\n",
      "      5       \u001b[36m32.6989\u001b[0m       \u001b[32m26.8701\u001b[0m  0.0106\n",
      "      6       \u001b[36m31.3269\u001b[0m       \u001b[32m26.4082\u001b[0m  0.0112\n",
      "      7       \u001b[36m30.2435\u001b[0m       \u001b[32m26.2684\u001b[0m  0.0117\n",
      "      8       \u001b[36m29.4894\u001b[0m       26.3981  0.0112\n",
      "      9       \u001b[36m29.0570\u001b[0m       26.6526  0.0118\n",
      "     10       \u001b[36m28.8535\u001b[0m       26.8925  0.0110\n",
      "     11       \u001b[36m28.7660\u001b[0m       27.0623  0.0114\n",
      "     12       \u001b[36m28.7245\u001b[0m       27.1636  0.0112\n",
      "     13       \u001b[36m28.6982\u001b[0m       27.2185  0.0117\n",
      "     14       \u001b[36m28.6775\u001b[0m       27.2462  0.0116\n",
      "     15       \u001b[36m28.6593\u001b[0m       27.2599  0.0112\n",
      "     16       \u001b[36m28.6433\u001b[0m       27.2657  0.0112\n",
      "     17       \u001b[36m28.6288\u001b[0m       27.2668  0.0110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     18       \u001b[36m28.6156\u001b[0m       27.2657  0.0116\n",
      "     19       \u001b[36m28.6036\u001b[0m       27.2640  0.0116\n",
      "     20       \u001b[36m28.5926\u001b[0m       27.2619  0.0111\n",
      "     21       \u001b[36m28.5825\u001b[0m       27.2599  0.0111\n",
      "     22       \u001b[36m28.5733\u001b[0m       27.2579  0.0116\n",
      "     23       \u001b[36m28.5648\u001b[0m       27.2559  0.0117\n",
      "     24       \u001b[36m28.5570\u001b[0m       27.2538  0.0112\n",
      "     25       \u001b[36m28.5497\u001b[0m       27.2519  0.0112\n",
      "     26       \u001b[36m28.5429\u001b[0m       27.2499  0.0113\n",
      "     27       \u001b[36m28.5365\u001b[0m       27.2481  0.0108\n",
      "     28       \u001b[36m28.5307\u001b[0m       27.2462  0.0113\n",
      "     29       \u001b[36m28.5252\u001b[0m       27.2446  0.0114\n",
      "     30       \u001b[36m28.5202\u001b[0m       27.2430  0.0112\n",
      "     31       \u001b[36m28.5154\u001b[0m       27.2416  0.0113\n",
      "     32       \u001b[36m28.5109\u001b[0m       27.2399  0.0112\n",
      "     33       \u001b[36m28.5067\u001b[0m       27.2383  0.0112\n",
      "     34       \u001b[36m28.5028\u001b[0m       27.2365  0.0111\n",
      "     35       \u001b[36m28.4990\u001b[0m       27.2344  0.0113\n",
      "     36       \u001b[36m28.4954\u001b[0m       27.2326  0.0114\n",
      "     37       \u001b[36m28.4920\u001b[0m       27.2311  0.0112\n",
      "     38       \u001b[36m28.4887\u001b[0m       27.2293  0.0120\n",
      "     39       \u001b[36m28.4856\u001b[0m       27.2278  0.0125\n",
      "     40       \u001b[36m28.4827\u001b[0m       27.2261  0.0141\n",
      "     41       \u001b[36m28.4799\u001b[0m       27.2245  0.0135\n",
      "     42       \u001b[36m28.4772\u001b[0m       27.2233  0.0116\n",
      "     43       \u001b[36m28.4748\u001b[0m       27.2220  0.0142\n",
      "     44       \u001b[36m28.4724\u001b[0m       27.2207  0.0139\n",
      "     45       \u001b[36m28.4702\u001b[0m       27.2193  0.0148\n",
      "     46       \u001b[36m28.4680\u001b[0m       27.2181  0.0126\n",
      "     47       \u001b[36m28.4660\u001b[0m       27.2167  0.0133\n",
      "     48       \u001b[36m28.4640\u001b[0m       27.2154  0.0110\n",
      "     49       \u001b[36m28.4621\u001b[0m       27.2143  0.0134\n",
      "     50       \u001b[36m28.4603\u001b[0m       27.2131  0.0117\n",
      "     51       \u001b[36m28.4586\u001b[0m       27.2117  0.0118\n",
      "     52       \u001b[36m28.4570\u001b[0m       27.2108  0.0110\n",
      "     53       \u001b[36m28.4554\u001b[0m       27.2098  0.0110\n",
      "     54       \u001b[36m28.4540\u001b[0m       27.2087  0.0129\n",
      "     55       \u001b[36m28.4525\u001b[0m       27.2076  0.0116\n",
      "     56       \u001b[36m28.4512\u001b[0m       27.2064  0.0118\n",
      "     57       \u001b[36m28.4498\u001b[0m       27.2052  0.0117\n",
      "     58       \u001b[36m28.4486\u001b[0m       27.2041  0.0111\n",
      "     59       \u001b[36m28.4473\u001b[0m       27.2028  0.0126\n",
      "     60       \u001b[36m28.4461\u001b[0m       27.2015  0.0119\n",
      "     61       \u001b[36m28.4450\u001b[0m       27.2004  0.0121\n",
      "     62       \u001b[36m28.4439\u001b[0m       27.1992  0.0107\n",
      "     63       \u001b[36m28.4428\u001b[0m       27.1981  0.0110\n",
      "     64       \u001b[36m28.4418\u001b[0m       27.1969  0.0127\n",
      "     65       \u001b[36m28.4408\u001b[0m       27.1958  0.0120\n",
      "     66       \u001b[36m28.4398\u001b[0m       27.1944  0.0119\n",
      "     67       \u001b[36m28.4389\u001b[0m       27.1933  0.0114\n",
      "     68       \u001b[36m28.4380\u001b[0m       27.1922  0.0108\n",
      "     69       \u001b[36m28.4371\u001b[0m       27.1910  0.0127\n",
      "     70       \u001b[36m28.4362\u001b[0m       27.1899  0.0121\n",
      "     71       \u001b[36m28.4354\u001b[0m       27.1888  0.0123\n",
      "     72       \u001b[36m28.4346\u001b[0m       27.1876  0.0110\n",
      "     73       \u001b[36m28.4338\u001b[0m       27.1865  0.0109\n",
      "     74       \u001b[36m28.4330\u001b[0m       27.1855  0.0131\n",
      "     75       \u001b[36m28.4322\u001b[0m       27.1845  0.0122\n",
      "     76       \u001b[36m28.4315\u001b[0m       27.1836  0.0119\n",
      "     77       \u001b[36m28.4307\u001b[0m       27.1826  0.0107\n",
      "     78       \u001b[36m28.4300\u001b[0m       27.1818  0.0116\n",
      "     79       \u001b[36m28.4294\u001b[0m       27.1808  0.0132\n",
      "     80       \u001b[36m28.4287\u001b[0m       27.1798  0.0121\n",
      "     81       \u001b[36m28.4281\u001b[0m       27.1789  0.0121\n",
      "     82       \u001b[36m28.4274\u001b[0m       27.1779  0.0108\n",
      "     83       \u001b[36m28.4268\u001b[0m       27.1770  0.0107\n",
      "     84       \u001b[36m28.4262\u001b[0m       27.1761  0.0132\n",
      "     85       \u001b[36m28.4256\u001b[0m       27.1752  0.0123\n",
      "     86       \u001b[36m28.4250\u001b[0m       27.1744  0.0117\n",
      "     87       \u001b[36m28.4245\u001b[0m       27.1733  0.0110\n",
      "     88       \u001b[36m28.4239\u001b[0m       27.1724  0.0109\n",
      "     89       \u001b[36m28.4234\u001b[0m       27.1716  0.0151\n",
      "     90       \u001b[36m28.4228\u001b[0m       27.1708  0.0119\n",
      "     91       \u001b[36m28.4223\u001b[0m       27.1701  0.0123\n",
      "     92       \u001b[36m28.4218\u001b[0m       27.1691  0.0114\n",
      "     93       \u001b[36m28.4213\u001b[0m       27.1683  0.0114\n",
      "     94       \u001b[36m28.4208\u001b[0m       27.1676  0.0131\n",
      "     95       \u001b[36m28.4203\u001b[0m       27.1670  0.0126\n",
      "     96       \u001b[36m28.4199\u001b[0m       27.1661  0.0120\n",
      "     97       \u001b[36m28.4194\u001b[0m       27.1653  0.0108\n",
      "     98       \u001b[36m28.4190\u001b[0m       27.1645  0.0107\n",
      "     99       \u001b[36m28.4185\u001b[0m       27.1638  0.0134\n",
      "    100       \u001b[36m28.4181\u001b[0m       27.1630  0.0117\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m40.6317\u001b[0m       \u001b[32m37.5104\u001b[0m  0.0122\n",
      "      2       \u001b[36m35.4160\u001b[0m       \u001b[32m31.5363\u001b[0m  0.0116\n",
      "      3       \u001b[36m34.5010\u001b[0m       \u001b[32m31.1309\u001b[0m  0.0132\n",
      "      4       \u001b[36m33.3731\u001b[0m       32.2417  0.0122\n",
      "      5       \u001b[36m33.3212\u001b[0m       31.2080  0.0121\n",
      "      6       \u001b[36m32.8573\u001b[0m       \u001b[32m30.4112\u001b[0m  0.0127\n",
      "      7       \u001b[36m32.7585\u001b[0m       \u001b[32m30.3643\u001b[0m  0.0118\n",
      "      8       \u001b[36m32.6455\u001b[0m       30.5646  0.0115\n",
      "      9       \u001b[36m32.5336\u001b[0m       30.4249  0.0141\n",
      "     10       \u001b[36m32.4479\u001b[0m       \u001b[32m30.2382\u001b[0m  0.0129\n",
      "     11       \u001b[36m32.3905\u001b[0m       \u001b[32m30.2267\u001b[0m  0.0125\n",
      "     12       \u001b[36m32.3466\u001b[0m       30.2549  0.0127\n",
      "     13       \u001b[36m32.3249\u001b[0m       30.3251  0.0120\n",
      "     14       \u001b[36m32.2997\u001b[0m       \u001b[32m30.1969\u001b[0m  0.0116\n",
      "     15       \u001b[36m32.2763\u001b[0m       \u001b[32m30.1892\u001b[0m  0.0139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     16       \u001b[36m32.2663\u001b[0m       30.1992  0.0132\n",
      "     17       \u001b[36m32.2510\u001b[0m       \u001b[32m30.1721\u001b[0m  0.0146\n",
      "     18       \u001b[36m32.2386\u001b[0m       \u001b[32m30.1277\u001b[0m  0.0165\n",
      "     19       \u001b[36m32.2281\u001b[0m       \u001b[32m30.1235\u001b[0m  0.0136\n",
      "     20       \u001b[36m32.2209\u001b[0m       \u001b[32m30.1209\u001b[0m  0.0122\n",
      "     21       \u001b[36m32.2123\u001b[0m       \u001b[32m30.1035\u001b[0m  0.0122\n",
      "     22       \u001b[36m32.2050\u001b[0m       \u001b[32m30.0843\u001b[0m  0.0184\n",
      "     23       \u001b[36m32.1985\u001b[0m       30.0910  0.0143\n",
      "     24       \u001b[36m32.1923\u001b[0m       \u001b[32m30.0719\u001b[0m  0.0138\n",
      "     25       \u001b[36m32.1865\u001b[0m       30.0761  0.0130\n",
      "     26       \u001b[36m32.1812\u001b[0m       \u001b[32m30.0566\u001b[0m  0.0126\n",
      "     27       \u001b[36m32.1765\u001b[0m       30.0740  0.0125\n",
      "     28       \u001b[36m32.1714\u001b[0m       \u001b[32m30.0438\u001b[0m  0.0125\n",
      "     29       \u001b[36m32.1679\u001b[0m       30.0685  0.0123\n",
      "     30       \u001b[36m32.1625\u001b[0m       \u001b[32m30.0387\u001b[0m  0.0126\n",
      "     31       \u001b[36m32.1600\u001b[0m       30.0565  0.0126\n",
      "     32       \u001b[36m32.1538\u001b[0m       \u001b[32m30.0382\u001b[0m  0.0131\n",
      "     33       \u001b[36m32.1516\u001b[0m       30.0392  0.0131\n",
      "     34       \u001b[36m32.1466\u001b[0m       30.0446  0.0124\n",
      "     35       \u001b[36m32.1433\u001b[0m       \u001b[32m30.0308\u001b[0m  0.0128\n",
      "     36       \u001b[36m32.1411\u001b[0m       30.0358  0.0123\n",
      "     37       \u001b[36m32.1352\u001b[0m       30.0391  0.0124\n",
      "     38       \u001b[36m32.1341\u001b[0m       \u001b[32m30.0229\u001b[0m  0.0145\n",
      "     39       \u001b[36m32.1328\u001b[0m       30.0334  0.0124\n",
      "     40       \u001b[36m32.1258\u001b[0m       30.0371  0.0133\n",
      "     41       32.1278       \u001b[32m30.0112\u001b[0m  0.0128\n",
      "     42       32.1294       30.0645  0.0120\n",
      "     43       32.1272       30.0179  0.0114\n",
      "     44       32.1276       30.0120  0.0142\n",
      "     45       \u001b[36m32.1214\u001b[0m       30.0657  0.0131\n",
      "     46       32.1273       30.0222  0.0125\n",
      "     47       32.1282       30.0719  0.0137\n",
      "     48       \u001b[36m32.1064\u001b[0m       \u001b[32m29.9742\u001b[0m  0.0132\n",
      "     49       32.1074       30.0253  0.0115\n",
      "     50       \u001b[36m32.1021\u001b[0m       30.0370  0.0114\n",
      "     51       \u001b[36m32.0978\u001b[0m       29.9936  0.0139\n",
      "     52       32.0978       30.0389  0.0130\n",
      "     53       \u001b[36m32.0929\u001b[0m       30.0175  0.0126\n",
      "     54       \u001b[36m32.0923\u001b[0m       30.0177  0.0123\n",
      "     55       \u001b[36m32.0908\u001b[0m       30.0379  0.0128\n",
      "     56       \u001b[36m32.0873\u001b[0m       30.0186  0.0118\n",
      "     57       32.0884       30.0183  0.0114\n",
      "     58       \u001b[36m32.0813\u001b[0m       30.0565  0.0144\n",
      "     59       32.0841       30.0176  0.0126\n",
      "     60       32.0879       30.0281  0.0131\n",
      "     61       \u001b[36m32.0752\u001b[0m       30.0685  0.0127\n",
      "     62       32.0858       30.0374  0.0113\n",
      "     63       32.1011       30.0716  0.0114\n",
      "     64       32.0831       30.0623  0.0141\n",
      "     65       32.0964       30.0469  0.0126\n",
      "     66       32.1188       30.2278  0.0124\n",
      "     67       32.1154       30.0374  0.0114\n",
      "     68       32.1064       30.1480  0.0116\n",
      "     69       \u001b[36m32.0748\u001b[0m       \u001b[32m29.9301\u001b[0m  0.0135\n",
      "     70       32.0775       30.1656  0.0125\n",
      "     71       32.0763       29.9985  0.0126\n",
      "     72       \u001b[36m32.0744\u001b[0m       30.1623  0.0125\n",
      "     73       32.0752       30.0087  0.0119\n",
      "     74       32.0788       30.2243  0.0122\n",
      "     75       32.0822       30.0101  0.0139\n",
      "     76       32.0934       30.3520  0.0130\n",
      "     77       32.1012       30.0365  0.0127\n",
      "     78       32.1137       30.4659  0.0119\n",
      "     79       32.1276       30.0522  0.0115\n",
      "     80       32.1473       30.4722  0.0139\n",
      "     81       32.1486       29.9733  0.0128\n",
      "     82       32.1409       30.1876  0.0123\n",
      "     83       32.1077       29.9314  0.0116\n",
      "     84       \u001b[36m32.0649\u001b[0m       30.0040  0.0116\n",
      "     85       32.0694       30.0811  0.0138\n",
      "     86       \u001b[36m32.0507\u001b[0m       29.9356  0.0123\n",
      "     87       32.0522       30.0770  0.0121\n",
      "     88       32.0526       29.9797  0.0130\n",
      "     89       \u001b[36m32.0429\u001b[0m       30.0171  0.0126\n",
      "     90       32.0466       30.0205  0.0125\n",
      "     91       \u001b[36m32.0400\u001b[0m       30.0028  0.0121\n",
      "     92       32.0408       30.0265  0.0164\n",
      "     93       \u001b[36m32.0386\u001b[0m       30.0096  0.0144\n",
      "     94       \u001b[36m32.0370\u001b[0m       30.0251  0.0130\n",
      "     95       \u001b[36m32.0362\u001b[0m       30.0155  0.0134\n",
      "     96       \u001b[36m32.0343\u001b[0m       30.0245  0.0137\n",
      "     97       \u001b[36m32.0337\u001b[0m       30.0185  0.0143\n",
      "     98       \u001b[36m32.0320\u001b[0m       30.0258  0.0229\n",
      "     99       \u001b[36m32.0313\u001b[0m       30.0231  0.0120\n",
      "    100       \u001b[36m32.0297\u001b[0m       30.0283  0.0136\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m33.6089\u001b[0m       \u001b[32m30.9681\u001b[0m  0.0119\n",
      "      2       \u001b[36m30.4118\u001b[0m       \u001b[32m27.9630\u001b[0m  0.0121\n",
      "      3       \u001b[36m26.2010\u001b[0m       \u001b[32m27.7256\u001b[0m  0.0120\n",
      "      4       \u001b[36m25.3672\u001b[0m       28.7754  0.0119\n",
      "      5       \u001b[36m24.6353\u001b[0m       \u001b[32m26.7241\u001b[0m  0.0116\n",
      "      6       \u001b[36m24.2298\u001b[0m       \u001b[32m26.4624\u001b[0m  0.0115\n",
      "      7       \u001b[36m24.0654\u001b[0m       26.6848  0.0122\n",
      "      8       \u001b[36m23.7685\u001b[0m       27.4258  0.0119\n",
      "      9       \u001b[36m23.6625\u001b[0m       27.4810  0.0117\n",
      "     10       \u001b[36m23.5405\u001b[0m       27.0433  0.0120\n",
      "     11       \u001b[36m23.4572\u001b[0m       26.8738  0.0125\n",
      "     12       \u001b[36m23.4018\u001b[0m       26.9581  0.0126\n",
      "     13       \u001b[36m23.3327\u001b[0m       27.0697  0.0128\n",
      "     14       \u001b[36m23.2766\u001b[0m       26.9797  0.0125\n",
      "     15       \u001b[36m23.2417\u001b[0m       26.8669  0.0118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     16       \u001b[36m23.2171\u001b[0m       26.8791  0.0121\n",
      "     17       \u001b[36m23.1878\u001b[0m       26.8948  0.0122\n",
      "     18       \u001b[36m23.1647\u001b[0m       26.8633  0.0121\n",
      "     19       \u001b[36m23.1466\u001b[0m       26.7982  0.0121\n",
      "     20       \u001b[36m23.1378\u001b[0m       26.8243  0.0120\n",
      "     21       \u001b[36m23.1254\u001b[0m       26.7785  0.0119\n",
      "     22       23.1302       26.8620  0.0149\n",
      "     23       23.1388       26.7373  0.0120\n",
      "     24       23.1704       26.9211  0.0118\n",
      "     25       23.1973       26.7107  0.0118\n",
      "     26       23.2302       26.8602  0.0123\n",
      "     27       23.2151       26.7280  0.0133\n",
      "     28       \u001b[36m23.1065\u001b[0m       26.6918  0.0125\n",
      "     29       \u001b[36m23.1050\u001b[0m       26.8474  0.0122\n",
      "     30       \u001b[36m23.0872\u001b[0m       26.7232  0.0117\n",
      "     31       \u001b[36m23.0863\u001b[0m       26.7028  0.0119\n",
      "     32       \u001b[36m23.0665\u001b[0m       26.7577  0.0122\n",
      "     33       \u001b[36m23.0591\u001b[0m       26.7202  0.0123\n",
      "     34       \u001b[36m23.0584\u001b[0m       26.7355  0.0123\n",
      "     35       \u001b[36m23.0510\u001b[0m       26.7172  0.0122\n",
      "     36       \u001b[36m23.0493\u001b[0m       26.7110  0.0120\n",
      "     37       \u001b[36m23.0444\u001b[0m       26.7202  0.0130\n",
      "     38       \u001b[36m23.0407\u001b[0m       26.7056  0.0121\n",
      "     39       \u001b[36m23.0390\u001b[0m       26.7094  0.0117\n",
      "     40       \u001b[36m23.0356\u001b[0m       26.7028  0.0116\n",
      "     41       \u001b[36m23.0336\u001b[0m       26.7058  0.0115\n",
      "     42       \u001b[36m23.0306\u001b[0m       26.6999  0.0121\n",
      "     43       \u001b[36m23.0288\u001b[0m       26.6994  0.0121\n",
      "     44       \u001b[36m23.0262\u001b[0m       26.6987  0.0118\n",
      "     45       \u001b[36m23.0243\u001b[0m       26.6983  0.0117\n",
      "     46       \u001b[36m23.0220\u001b[0m       26.6942  0.0117\n",
      "     47       \u001b[36m23.0203\u001b[0m       26.6953  0.0116\n",
      "     48       \u001b[36m23.0181\u001b[0m       26.6937  0.0124\n",
      "     49       \u001b[36m23.0165\u001b[0m       26.6959  0.0120\n",
      "     50       \u001b[36m23.0144\u001b[0m       26.6940  0.0119\n",
      "     51       \u001b[36m23.0129\u001b[0m       26.6965  0.0114\n",
      "     52       \u001b[36m23.0109\u001b[0m       26.6936  0.0114\n",
      "     53       \u001b[36m23.0096\u001b[0m       26.6980  0.0120\n",
      "     54       \u001b[36m23.0076\u001b[0m       26.6949  0.0118\n",
      "     55       \u001b[36m23.0064\u001b[0m       26.6972  0.0117\n",
      "     56       \u001b[36m23.0045\u001b[0m       26.6942  0.0116\n",
      "     57       \u001b[36m23.0034\u001b[0m       26.6989  0.0115\n",
      "     58       \u001b[36m23.0016\u001b[0m       26.6953  0.0122\n",
      "     59       \u001b[36m23.0005\u001b[0m       26.6995  0.0119\n",
      "     60       \u001b[36m22.9990\u001b[0m       26.6951  0.0117\n",
      "     61       \u001b[36m22.9977\u001b[0m       26.6979  0.0116\n",
      "     62       \u001b[36m22.9964\u001b[0m       26.6949  0.0116\n",
      "     63       \u001b[36m22.9949\u001b[0m       26.6968  0.0121\n",
      "     64       \u001b[36m22.9939\u001b[0m       26.6978  0.0115\n",
      "     65       \u001b[36m22.9925\u001b[0m       26.6975  0.0119\n",
      "     66       \u001b[36m22.9914\u001b[0m       26.7001  0.0115\n",
      "     67       \u001b[36m22.9905\u001b[0m       26.6999  0.0113\n",
      "     68       \u001b[36m22.9892\u001b[0m       26.6980  0.0176\n",
      "     69       \u001b[36m22.9890\u001b[0m       26.7027  0.0163\n",
      "     70       \u001b[36m22.9871\u001b[0m       26.7016  0.0125\n",
      "     71       \u001b[36m22.9857\u001b[0m       26.7028  0.0159\n",
      "     72       22.9866       26.7120  0.0164\n",
      "     73       \u001b[36m22.9849\u001b[0m       26.7054  0.0159\n",
      "     74       22.9871       26.7131  0.0161\n",
      "     75       22.9902       26.7181  0.0124\n",
      "     76       22.9913       26.7150  0.0123\n",
      "     77       23.0191       26.7758  0.0120\n",
      "     78       23.0321       26.6886  0.0119\n",
      "     79       23.0278       26.7912  0.0123\n",
      "     80       23.0249       26.6549  0.0122\n",
      "     81       22.9870       26.7443  0.0121\n",
      "     82       \u001b[36m22.9810\u001b[0m       26.7358  0.0119\n",
      "     83       22.9897       26.7293  0.0119\n",
      "     84       22.9910       26.7053  0.0126\n",
      "     85       23.0047       26.8157  0.0119\n",
      "     86       23.0080       26.7037  0.0121\n",
      "     87       23.0584       26.8876  0.0117\n",
      "     88       23.0898       26.7060  0.0116\n",
      "     89       23.1020       26.9267  0.0121\n",
      "     90       23.1082       26.6468  0.0119\n",
      "     91       23.0753       26.7703  0.0119\n",
      "     92       23.0332       26.7166  0.0123\n",
      "     93       22.9979       26.6767  0.0116\n",
      "     94       22.9844       26.7723  0.0121\n",
      "     95       \u001b[36m22.9781\u001b[0m       26.6875  0.0118\n",
      "     96       22.9858       26.7533  0.0122\n",
      "     97       \u001b[36m22.9765\u001b[0m       26.7366  0.0122\n",
      "     98       \u001b[36m22.9737\u001b[0m       26.7346  0.0123\n",
      "     99       \u001b[36m22.9694\u001b[0m       26.7418  0.0144\n",
      "    100       \u001b[36m22.9674\u001b[0m       26.7196  0.0122\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m39.3272\u001b[0m       \u001b[32m29.1145\u001b[0m  0.0121\n",
      "      2       \u001b[36m33.9140\u001b[0m       \u001b[32m27.2843\u001b[0m  0.0120\n",
      "      3       \u001b[36m31.2087\u001b[0m       30.4697  0.0117\n",
      "      4       \u001b[36m30.7051\u001b[0m       \u001b[32m27.0025\u001b[0m  0.0119\n",
      "      5       \u001b[36m29.8044\u001b[0m       \u001b[32m26.6357\u001b[0m  0.0117\n",
      "      6       \u001b[36m29.4550\u001b[0m       27.2925  0.0120\n",
      "      7       \u001b[36m29.2019\u001b[0m       28.4979  0.0117\n",
      "      8       \u001b[36m29.1607\u001b[0m       27.7612  0.0114\n",
      "      9       \u001b[36m28.8721\u001b[0m       27.2905  0.0120\n",
      "     10       \u001b[36m28.7644\u001b[0m       27.4859  0.0118\n",
      "     11       \u001b[36m28.7190\u001b[0m       27.7022  0.0120\n",
      "     12       \u001b[36m28.6818\u001b[0m       27.5688  0.0116\n",
      "     13       \u001b[36m28.6118\u001b[0m       27.3886  0.0115\n",
      "     14       \u001b[36m28.5690\u001b[0m       27.4807  0.0124\n",
      "     15       \u001b[36m28.5575\u001b[0m       27.5028  0.0123\n",
      "     16       \u001b[36m28.5329\u001b[0m       27.4120  0.0121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m28.5125\u001b[0m       27.3675  0.0118\n",
      "     18       \u001b[36m28.4976\u001b[0m       27.4140  0.0115\n",
      "     19       \u001b[36m28.4965\u001b[0m       27.3390  0.0122\n",
      "     20       \u001b[36m28.4849\u001b[0m       27.3946  0.0118\n",
      "     21       \u001b[36m28.4840\u001b[0m       27.3146  0.0120\n",
      "     22       \u001b[36m28.4675\u001b[0m       27.3392  0.0116\n",
      "     23       28.4724       27.4333  0.0116\n",
      "     24       28.5122       27.2669  0.0119\n",
      "     25       28.5080       27.5280  0.0118\n",
      "     26       28.5396       27.2238  0.0116\n",
      "     27       \u001b[36m28.4454\u001b[0m       27.3635  0.0117\n",
      "     28       28.4607       27.3597  0.0117\n",
      "     29       28.4474       27.3316  0.0119\n",
      "     30       28.4645       27.2971  0.0116\n",
      "     31       \u001b[36m28.4373\u001b[0m       27.3962  0.0116\n",
      "     32       28.4520       27.3185  0.0113\n",
      "     33       \u001b[36m28.4197\u001b[0m       27.3120  0.0112\n",
      "     34       28.4302       27.3707  0.0122\n",
      "     35       \u001b[36m28.4161\u001b[0m       27.3332  0.0121\n",
      "     36       28.4183       27.3325  0.0127\n",
      "     37       \u001b[36m28.4071\u001b[0m       27.3463  0.0119\n",
      "     38       28.4125       27.3470  0.0118\n",
      "     39       \u001b[36m28.4029\u001b[0m       27.3365  0.0119\n",
      "     40       28.4061       27.3428  0.0119\n",
      "     41       \u001b[36m28.4001\u001b[0m       27.3541  0.0118\n",
      "     42       28.4025       27.3380  0.0119\n",
      "     43       \u001b[36m28.3966\u001b[0m       27.3625  0.0118\n",
      "     44       28.3997       27.3414  0.0134\n",
      "     45       \u001b[36m28.3942\u001b[0m       27.3630  0.0175\n",
      "     46       28.3962       27.3458  0.0149\n",
      "     47       \u001b[36m28.3922\u001b[0m       27.3600  0.0132\n",
      "     48       28.3925       27.3517  0.0161\n",
      "     49       \u001b[36m28.3908\u001b[0m       27.3506  0.0156\n",
      "     50       \u001b[36m28.3892\u001b[0m       27.3604  0.0145\n",
      "     51       28.3896       27.3457  0.0177\n",
      "     52       \u001b[36m28.3876\u001b[0m       27.3623  0.0120\n",
      "     53       \u001b[36m28.3874\u001b[0m       27.3563  0.0118\n",
      "     54       \u001b[36m28.3870\u001b[0m       27.3492  0.0128\n",
      "     55       \u001b[36m28.3856\u001b[0m       27.3805  0.0119\n",
      "     56       28.3864       27.3394  0.0119\n",
      "     57       28.3886       27.3755  0.0119\n",
      "     58       28.3858       27.3619  0.0124\n",
      "     59       28.3935       27.3565  0.0120\n",
      "     60       28.4042       27.4240  0.0126\n",
      "     61       28.4083       27.3151  0.0120\n",
      "     62       28.4124       27.3751  0.0113\n",
      "     63       28.4144       27.4382  0.0122\n",
      "     64       28.4313       27.2598  0.0121\n",
      "     65       28.3996       27.5011  0.0115\n",
      "     66       28.3997       27.2924  0.0115\n",
      "     67       \u001b[36m28.3754\u001b[0m       27.3463  0.0117\n",
      "     68       28.3788       27.4101  0.0125\n",
      "     69       28.3814       27.3195  0.0122\n",
      "     70       \u001b[36m28.3718\u001b[0m       27.3504  0.0117\n",
      "     71       28.3779       27.3816  0.0117\n",
      "     72       28.3725       27.3362  0.0113\n",
      "     73       28.3728       27.3734  0.0120\n",
      "     74       28.3728       27.3586  0.0120\n",
      "     75       \u001b[36m28.3707\u001b[0m       27.3647  0.0121\n",
      "     76       28.3712       27.3649  0.0116\n",
      "     77       \u001b[36m28.3702\u001b[0m       27.3662  0.0116\n",
      "     78       \u001b[36m28.3698\u001b[0m       27.3670  0.0120\n",
      "     79       \u001b[36m28.3691\u001b[0m       27.3676  0.0117\n",
      "     80       \u001b[36m28.3681\u001b[0m       27.3697  0.0117\n",
      "     81       \u001b[36m28.3678\u001b[0m       27.3678  0.0116\n",
      "     82       \u001b[36m28.3676\u001b[0m       27.3712  0.0115\n",
      "     83       \u001b[36m28.3661\u001b[0m       27.3674  0.0120\n",
      "     84       28.3669       27.3706  0.0117\n",
      "     85       \u001b[36m28.3660\u001b[0m       27.3670  0.0116\n",
      "     86       \u001b[36m28.3644\u001b[0m       27.3729  0.0118\n",
      "     87       28.3671       27.3674  0.0119\n",
      "     88       28.3648       27.3708  0.0112\n",
      "     89       \u001b[36m28.3642\u001b[0m       27.3739  0.0115\n",
      "     90       28.3707       27.3658  0.0124\n",
      "     91       28.3662       27.3801  0.0119\n",
      "     92       28.3700       27.3831  0.0120\n",
      "     93       28.3840       27.3695  0.0117\n",
      "     94       28.3743       27.3879  0.0114\n",
      "     95       28.3929       27.4035  0.0122\n",
      "     96       28.4186       27.3973  0.0118\n",
      "     97       28.4015       27.3055  0.0116\n",
      "     98       28.4246       27.4334  0.0114\n",
      "     99       28.4389       27.5024  0.0114\n",
      "    100       28.4341       27.0913  0.0123\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m43.5951\u001b[0m       \u001b[32m44.7823\u001b[0m  0.0108\n",
      "      2       \u001b[36m42.1410\u001b[0m       \u001b[32m43.0675\u001b[0m  0.0110\n",
      "      3       \u001b[36m40.7808\u001b[0m       \u001b[32m41.3251\u001b[0m  0.0114\n",
      "      4       \u001b[36m39.4045\u001b[0m       \u001b[32m39.5207\u001b[0m  0.0112\n",
      "      5       \u001b[36m38.0176\u001b[0m       \u001b[32m37.6997\u001b[0m  0.0106\n",
      "      6       \u001b[36m36.6906\u001b[0m       \u001b[32m35.9856\u001b[0m  0.0109\n",
      "      7       \u001b[36m35.5317\u001b[0m       \u001b[32m34.4988\u001b[0m  0.0111\n",
      "      8       \u001b[36m34.6111\u001b[0m       \u001b[32m33.2806\u001b[0m  0.0116\n",
      "      9       \u001b[36m33.9260\u001b[0m       \u001b[32m32.3216\u001b[0m  0.0110\n",
      "     10       \u001b[36m33.4403\u001b[0m       \u001b[32m31.6053\u001b[0m  0.0110\n",
      "     11       \u001b[36m33.1161\u001b[0m       \u001b[32m31.1035\u001b[0m  0.0107\n",
      "     12       \u001b[36m32.9108\u001b[0m       \u001b[32m30.7688\u001b[0m  0.0112\n",
      "     13       \u001b[36m32.7818\u001b[0m       \u001b[32m30.5504\u001b[0m  0.0113\n",
      "     14       \u001b[36m32.6982\u001b[0m       \u001b[32m30.4074\u001b[0m  0.0109\n",
      "     15       \u001b[36m32.6401\u001b[0m       \u001b[32m30.3114\u001b[0m  0.0106\n",
      "     16       \u001b[36m32.5969\u001b[0m       \u001b[32m30.2448\u001b[0m  0.0107\n",
      "     17       \u001b[36m32.5631\u001b[0m       \u001b[32m30.1965\u001b[0m  0.0113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     18       \u001b[36m32.5357\u001b[0m       \u001b[32m30.1600\u001b[0m  0.0115\n",
      "     19       \u001b[36m32.5125\u001b[0m       \u001b[32m30.1316\u001b[0m  0.0107\n",
      "     20       \u001b[36m32.4923\u001b[0m       \u001b[32m30.1087\u001b[0m  0.0110\n",
      "     21       \u001b[36m32.4746\u001b[0m       \u001b[32m30.0896\u001b[0m  0.0106\n",
      "     22       \u001b[36m32.4589\u001b[0m       \u001b[32m30.0732\u001b[0m  0.0107\n",
      "     23       \u001b[36m32.4450\u001b[0m       \u001b[32m30.0590\u001b[0m  0.0134\n",
      "     24       \u001b[36m32.4325\u001b[0m       \u001b[32m30.0464\u001b[0m  0.0195\n",
      "     25       \u001b[36m32.4212\u001b[0m       \u001b[32m30.0352\u001b[0m  0.0170\n",
      "     26       \u001b[36m32.4109\u001b[0m       \u001b[32m30.0249\u001b[0m  0.0158\n",
      "     27       \u001b[36m32.4016\u001b[0m       \u001b[32m30.0156\u001b[0m  0.0130\n",
      "     28       \u001b[36m32.3931\u001b[0m       \u001b[32m30.0070\u001b[0m  0.0137\n",
      "     29       \u001b[36m32.3853\u001b[0m       \u001b[32m29.9988\u001b[0m  0.0212\n",
      "     30       \u001b[36m32.3781\u001b[0m       \u001b[32m29.9913\u001b[0m  0.0112\n",
      "     31       \u001b[36m32.3715\u001b[0m       \u001b[32m29.9842\u001b[0m  0.0114\n",
      "     32       \u001b[36m32.3653\u001b[0m       \u001b[32m29.9775\u001b[0m  0.0113\n",
      "     33       \u001b[36m32.3595\u001b[0m       \u001b[32m29.9711\u001b[0m  0.0114\n",
      "     34       \u001b[36m32.3539\u001b[0m       \u001b[32m29.9649\u001b[0m  0.0109\n",
      "     35       \u001b[36m32.3489\u001b[0m       \u001b[32m29.9591\u001b[0m  0.0109\n",
      "     36       \u001b[36m32.3442\u001b[0m       \u001b[32m29.9537\u001b[0m  0.0113\n",
      "     37       \u001b[36m32.3397\u001b[0m       \u001b[32m29.9488\u001b[0m  0.0110\n",
      "     38       \u001b[36m32.3355\u001b[0m       \u001b[32m29.9439\u001b[0m  0.0109\n",
      "     39       \u001b[36m32.3315\u001b[0m       \u001b[32m29.9393\u001b[0m  0.0109\n",
      "     40       \u001b[36m32.3276\u001b[0m       \u001b[32m29.9350\u001b[0m  0.0106\n",
      "     41       \u001b[36m32.3239\u001b[0m       \u001b[32m29.9309\u001b[0m  0.0108\n",
      "     42       \u001b[36m32.3205\u001b[0m       \u001b[32m29.9270\u001b[0m  0.0114\n",
      "     43       \u001b[36m32.3172\u001b[0m       \u001b[32m29.9232\u001b[0m  0.0109\n",
      "     44       \u001b[36m32.3139\u001b[0m       \u001b[32m29.9196\u001b[0m  0.0106\n",
      "     45       \u001b[36m32.3109\u001b[0m       \u001b[32m29.9162\u001b[0m  0.0106\n",
      "     46       \u001b[36m32.3080\u001b[0m       \u001b[32m29.9127\u001b[0m  0.0108\n",
      "     47       \u001b[36m32.3051\u001b[0m       \u001b[32m29.9094\u001b[0m  0.0113\n",
      "     48       \u001b[36m32.3025\u001b[0m       \u001b[32m29.9063\u001b[0m  0.0109\n",
      "     49       \u001b[36m32.2999\u001b[0m       \u001b[32m29.9034\u001b[0m  0.0107\n",
      "     50       \u001b[36m32.2975\u001b[0m       \u001b[32m29.9005\u001b[0m  0.0109\n",
      "     51       \u001b[36m32.2950\u001b[0m       \u001b[32m29.8978\u001b[0m  0.0111\n",
      "     52       \u001b[36m32.2928\u001b[0m       \u001b[32m29.8952\u001b[0m  0.0114\n",
      "     53       \u001b[36m32.2906\u001b[0m       \u001b[32m29.8923\u001b[0m  0.0108\n",
      "     54       \u001b[36m32.2884\u001b[0m       \u001b[32m29.8895\u001b[0m  0.0109\n",
      "     55       \u001b[36m32.2864\u001b[0m       \u001b[32m29.8871\u001b[0m  0.0120\n",
      "     56       \u001b[36m32.2844\u001b[0m       \u001b[32m29.8849\u001b[0m  0.0126\n",
      "     57       \u001b[36m32.2824\u001b[0m       \u001b[32m29.8828\u001b[0m  0.0117\n",
      "     58       \u001b[36m32.2805\u001b[0m       \u001b[32m29.8807\u001b[0m  0.0111\n",
      "     59       \u001b[36m32.2787\u001b[0m       \u001b[32m29.8787\u001b[0m  0.0108\n",
      "     60       \u001b[36m32.2770\u001b[0m       \u001b[32m29.8768\u001b[0m  0.0106\n",
      "     61       \u001b[36m32.2753\u001b[0m       \u001b[32m29.8750\u001b[0m  0.0110\n",
      "     62       \u001b[36m32.2736\u001b[0m       \u001b[32m29.8732\u001b[0m  0.0113\n",
      "     63       \u001b[36m32.2720\u001b[0m       \u001b[32m29.8716\u001b[0m  0.0112\n",
      "     64       \u001b[36m32.2704\u001b[0m       \u001b[32m29.8699\u001b[0m  0.0108\n",
      "     65       \u001b[36m32.2689\u001b[0m       \u001b[32m29.8683\u001b[0m  0.0117\n",
      "     66       \u001b[36m32.2674\u001b[0m       \u001b[32m29.8668\u001b[0m  0.0111\n",
      "     67       \u001b[36m32.2660\u001b[0m       \u001b[32m29.8653\u001b[0m  0.0111\n",
      "     68       \u001b[36m32.2645\u001b[0m       \u001b[32m29.8638\u001b[0m  0.0110\n",
      "     69       \u001b[36m32.2632\u001b[0m       \u001b[32m29.8626\u001b[0m  0.0108\n",
      "     70       \u001b[36m32.2618\u001b[0m       \u001b[32m29.8612\u001b[0m  0.0107\n",
      "     71       \u001b[36m32.2606\u001b[0m       \u001b[32m29.8600\u001b[0m  0.0113\n",
      "     72       \u001b[36m32.2593\u001b[0m       \u001b[32m29.8588\u001b[0m  0.0114\n",
      "     73       \u001b[36m32.2581\u001b[0m       \u001b[32m29.8576\u001b[0m  0.0109\n",
      "     74       \u001b[36m32.2568\u001b[0m       \u001b[32m29.8565\u001b[0m  0.0106\n",
      "     75       \u001b[36m32.2557\u001b[0m       \u001b[32m29.8554\u001b[0m  0.0106\n",
      "     76       \u001b[36m32.2545\u001b[0m       \u001b[32m29.8544\u001b[0m  0.0108\n",
      "     77       \u001b[36m32.2534\u001b[0m       \u001b[32m29.8534\u001b[0m  0.0110\n",
      "     78       \u001b[36m32.2523\u001b[0m       \u001b[32m29.8525\u001b[0m  0.0110\n",
      "     79       \u001b[36m32.2513\u001b[0m       \u001b[32m29.8515\u001b[0m  0.0104\n",
      "     80       \u001b[36m32.2502\u001b[0m       \u001b[32m29.8506\u001b[0m  0.0105\n",
      "     81       \u001b[36m32.2492\u001b[0m       \u001b[32m29.8498\u001b[0m  0.0108\n",
      "     82       \u001b[36m32.2482\u001b[0m       \u001b[32m29.8489\u001b[0m  0.0111\n",
      "     83       \u001b[36m32.2472\u001b[0m       \u001b[32m29.8481\u001b[0m  0.0113\n",
      "     84       \u001b[36m32.2463\u001b[0m       \u001b[32m29.8473\u001b[0m  0.0106\n",
      "     85       \u001b[36m32.2453\u001b[0m       \u001b[32m29.8466\u001b[0m  0.0104\n",
      "     86       \u001b[36m32.2444\u001b[0m       \u001b[32m29.8459\u001b[0m  0.0110\n",
      "     87       \u001b[36m32.2435\u001b[0m       \u001b[32m29.8451\u001b[0m  0.0112\n",
      "     88       \u001b[36m32.2426\u001b[0m       \u001b[32m29.8444\u001b[0m  0.0108\n",
      "     89       \u001b[36m32.2418\u001b[0m       \u001b[32m29.8438\u001b[0m  0.0105\n",
      "     90       \u001b[36m32.2409\u001b[0m       \u001b[32m29.8432\u001b[0m  0.0105\n",
      "     91       \u001b[36m32.2401\u001b[0m       \u001b[32m29.8425\u001b[0m  0.0111\n",
      "     92       \u001b[36m32.2393\u001b[0m       \u001b[32m29.8419\u001b[0m  0.0112\n",
      "     93       \u001b[36m32.2385\u001b[0m       \u001b[32m29.8413\u001b[0m  0.0110\n",
      "     94       \u001b[36m32.2377\u001b[0m       \u001b[32m29.8407\u001b[0m  0.0108\n",
      "     95       \u001b[36m32.2370\u001b[0m       \u001b[32m29.8402\u001b[0m  0.0106\n",
      "     96       \u001b[36m32.2362\u001b[0m       \u001b[32m29.8397\u001b[0m  0.0108\n",
      "     97       \u001b[36m32.2355\u001b[0m       \u001b[32m29.8392\u001b[0m  0.0113\n",
      "     98       \u001b[36m32.2348\u001b[0m       \u001b[32m29.8387\u001b[0m  0.0112\n",
      "     99       \u001b[36m32.2341\u001b[0m       \u001b[32m29.8382\u001b[0m  0.0112\n",
      "    100       \u001b[36m32.2334\u001b[0m       \u001b[32m29.8378\u001b[0m  0.0107\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m34.8994\u001b[0m       \u001b[32m32.2162\u001b[0m  0.0115\n",
      "      2       \u001b[36m32.6762\u001b[0m       \u001b[32m30.7617\u001b[0m  0.0108\n",
      "      3       \u001b[36m30.7978\u001b[0m       \u001b[32m29.4135\u001b[0m  0.0128\n",
      "      4       \u001b[36m28.9840\u001b[0m       \u001b[32m28.2098\u001b[0m  0.0122\n",
      "      5       \u001b[36m27.3478\u001b[0m       \u001b[32m27.2973\u001b[0m  0.0170\n",
      "      6       \u001b[36m26.0391\u001b[0m       \u001b[32m26.7398\u001b[0m  0.0148\n",
      "      7       \u001b[36m25.0833\u001b[0m       \u001b[32m26.4888\u001b[0m  0.0116\n",
      "      8       \u001b[36m24.4303\u001b[0m       \u001b[32m26.4463\u001b[0m  0.0121\n",
      "      9       \u001b[36m24.0204\u001b[0m       26.5036  0.0122\n",
      "     10       \u001b[36m23.7809\u001b[0m       26.5804  0.0136\n",
      "     11       \u001b[36m23.6430\u001b[0m       26.6400  0.0114\n",
      "     12       \u001b[36m23.5591\u001b[0m       26.6756  0.0115\n",
      "     13       \u001b[36m23.5034\u001b[0m       26.6927  0.0112\n",
      "     14       \u001b[36m23.4630\u001b[0m       26.6980  0.0110\n",
      "     15       \u001b[36m23.4313\u001b[0m       26.6961  0.0111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     16       \u001b[36m23.4049\u001b[0m       26.6903  0.0114\n",
      "     17       \u001b[36m23.3822\u001b[0m       26.6826  0.0112\n",
      "     18       \u001b[36m23.3623\u001b[0m       26.6743  0.0108\n",
      "     19       \u001b[36m23.3446\u001b[0m       26.6664  0.0108\n",
      "     20       \u001b[36m23.3289\u001b[0m       26.6587  0.0108\n",
      "     21       \u001b[36m23.3147\u001b[0m       26.6515  0.0111\n",
      "     22       \u001b[36m23.3018\u001b[0m       26.6447  0.0112\n",
      "     23       \u001b[36m23.2901\u001b[0m       26.6386  0.0111\n",
      "     24       \u001b[36m23.2793\u001b[0m       26.6332  0.0108\n",
      "     25       \u001b[36m23.2694\u001b[0m       26.6281  0.0107\n",
      "     26       \u001b[36m23.2602\u001b[0m       26.6234  0.0114\n",
      "     27       \u001b[36m23.2518\u001b[0m       26.6192  0.0109\n",
      "     28       \u001b[36m23.2440\u001b[0m       26.6153  0.0108\n",
      "     29       \u001b[36m23.2367\u001b[0m       26.6117  0.0105\n",
      "     30       \u001b[36m23.2299\u001b[0m       26.6083  0.0106\n",
      "     31       \u001b[36m23.2235\u001b[0m       26.6051  0.0108\n",
      "     32       \u001b[36m23.2176\u001b[0m       26.6021  0.0114\n",
      "     33       \u001b[36m23.2120\u001b[0m       26.5992  0.0110\n",
      "     34       \u001b[36m23.2068\u001b[0m       26.5966  0.0107\n",
      "     35       \u001b[36m23.2018\u001b[0m       26.5942  0.0107\n",
      "     36       \u001b[36m23.1971\u001b[0m       26.5919  0.0110\n",
      "     37       \u001b[36m23.1926\u001b[0m       26.5897  0.0113\n",
      "     38       \u001b[36m23.1884\u001b[0m       26.5876  0.0111\n",
      "     39       \u001b[36m23.1844\u001b[0m       26.5857  0.0110\n",
      "     40       \u001b[36m23.1806\u001b[0m       26.5839  0.0108\n",
      "     41       \u001b[36m23.1769\u001b[0m       26.5822  0.0109\n",
      "     42       \u001b[36m23.1735\u001b[0m       26.5805  0.0113\n",
      "     43       \u001b[36m23.1702\u001b[0m       26.5789  0.0112\n",
      "     44       \u001b[36m23.1670\u001b[0m       26.5773  0.0106\n",
      "     45       \u001b[36m23.1639\u001b[0m       26.5758  0.0108\n",
      "     46       \u001b[36m23.1610\u001b[0m       26.5744  0.0111\n",
      "     47       \u001b[36m23.1582\u001b[0m       26.5729  0.0108\n",
      "     48       \u001b[36m23.1555\u001b[0m       26.5716  0.0112\n",
      "     49       \u001b[36m23.1528\u001b[0m       26.5702  0.0108\n",
      "     50       \u001b[36m23.1503\u001b[0m       26.5690  0.0107\n",
      "     51       \u001b[36m23.1479\u001b[0m       26.5677  0.0112\n",
      "     52       \u001b[36m23.1455\u001b[0m       26.5665  0.0109\n",
      "     53       \u001b[36m23.1433\u001b[0m       26.5654  0.0112\n",
      "     54       \u001b[36m23.1411\u001b[0m       26.5643  0.0110\n",
      "     55       \u001b[36m23.1389\u001b[0m       26.5632  0.0107\n",
      "     56       \u001b[36m23.1369\u001b[0m       26.5621  0.0114\n",
      "     57       \u001b[36m23.1349\u001b[0m       26.5611  0.0110\n",
      "     58       \u001b[36m23.1330\u001b[0m       26.5602  0.0108\n",
      "     59       \u001b[36m23.1311\u001b[0m       26.5591  0.0108\n",
      "     60       \u001b[36m23.1293\u001b[0m       26.5581  0.0108\n",
      "     61       \u001b[36m23.1275\u001b[0m       26.5572  0.0113\n",
      "     62       \u001b[36m23.1258\u001b[0m       26.5561  0.0124\n",
      "     63       \u001b[36m23.1241\u001b[0m       26.5552  0.0120\n",
      "     64       \u001b[36m23.1225\u001b[0m       26.5542  0.0114\n",
      "     65       \u001b[36m23.1209\u001b[0m       26.5533  0.0113\n",
      "     66       \u001b[36m23.1194\u001b[0m       26.5523  0.0126\n",
      "     67       \u001b[36m23.1179\u001b[0m       26.5515  0.0111\n",
      "     68       \u001b[36m23.1165\u001b[0m       26.5506  0.0111\n",
      "     69       \u001b[36m23.1151\u001b[0m       26.5496  0.0110\n",
      "     70       \u001b[36m23.1137\u001b[0m       26.5488  0.0110\n",
      "     71       \u001b[36m23.1123\u001b[0m       26.5478  0.0114\n",
      "     72       \u001b[36m23.1110\u001b[0m       26.5470  0.0111\n",
      "     73       \u001b[36m23.1097\u001b[0m       26.5461  0.0112\n",
      "     74       \u001b[36m23.1085\u001b[0m       26.5450  0.0112\n",
      "     75       \u001b[36m23.1073\u001b[0m       26.5443  0.0111\n",
      "     76       \u001b[36m23.1061\u001b[0m       26.5435  0.0112\n",
      "     77       \u001b[36m23.1049\u001b[0m       26.5427  0.0110\n",
      "     78       \u001b[36m23.1037\u001b[0m       26.5419  0.0112\n",
      "     79       \u001b[36m23.1026\u001b[0m       26.5412  0.0107\n",
      "     80       \u001b[36m23.1015\u001b[0m       26.5403  0.0107\n",
      "     81       \u001b[36m23.1005\u001b[0m       26.5395  0.0108\n",
      "     82       \u001b[36m23.0994\u001b[0m       26.5388  0.0108\n",
      "     83       \u001b[36m23.0984\u001b[0m       26.5380  0.0110\n",
      "     84       \u001b[36m23.0974\u001b[0m       26.5373  0.0108\n",
      "     85       \u001b[36m23.0964\u001b[0m       26.5364  0.0115\n",
      "     86       \u001b[36m23.0954\u001b[0m       26.5357  0.0114\n",
      "     87       \u001b[36m23.0945\u001b[0m       26.5349  0.0111\n",
      "     88       \u001b[36m23.0935\u001b[0m       26.5341  0.0110\n",
      "     89       \u001b[36m23.0926\u001b[0m       26.5333  0.0150\n",
      "     90       \u001b[36m23.0917\u001b[0m       26.5325  0.0149\n",
      "     91       \u001b[36m23.0908\u001b[0m       26.5317  0.0118\n",
      "     92       \u001b[36m23.0900\u001b[0m       26.5310  0.0137\n",
      "     93       \u001b[36m23.0891\u001b[0m       26.5303  0.0114\n",
      "     94       \u001b[36m23.0883\u001b[0m       26.5296  0.0158\n",
      "     95       \u001b[36m23.0875\u001b[0m       26.5287  0.0120\n",
      "     96       \u001b[36m23.0867\u001b[0m       26.5280  0.0117\n",
      "     97       \u001b[36m23.0859\u001b[0m       26.5273  0.0111\n",
      "     98       \u001b[36m23.0852\u001b[0m       26.5266  0.0111\n",
      "     99       \u001b[36m23.0844\u001b[0m       26.5259  0.0113\n",
      "    100       \u001b[36m23.0837\u001b[0m       26.5252  0.0112\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m38.7435\u001b[0m       \u001b[32m30.2635\u001b[0m  0.0109\n",
      "      2       \u001b[36m36.7197\u001b[0m       \u001b[32m28.9416\u001b[0m  0.0110\n",
      "      3       \u001b[36m34.6758\u001b[0m       \u001b[32m27.7170\u001b[0m  0.0107\n",
      "      4       \u001b[36m32.7389\u001b[0m       \u001b[32m26.7686\u001b[0m  0.0112\n",
      "      5       \u001b[36m31.1250\u001b[0m       \u001b[32m26.2870\u001b[0m  0.0109\n",
      "      6       \u001b[36m29.9945\u001b[0m       \u001b[32m26.2666\u001b[0m  0.0111\n",
      "      7       \u001b[36m29.3530\u001b[0m       26.5067  0.0106\n",
      "      8       \u001b[36m29.0656\u001b[0m       26.7729  0.0109\n",
      "      9       \u001b[36m28.9485\u001b[0m       26.9614  0.0112\n",
      "     10       \u001b[36m28.8902\u001b[0m       27.0727  0.0108\n",
      "     11       \u001b[36m28.8498\u001b[0m       27.1347  0.0111\n",
      "     12       \u001b[36m28.8157\u001b[0m       27.1715  0.0109\n",
      "     13       \u001b[36m28.7853\u001b[0m       27.1921  0.0108\n",
      "     14       \u001b[36m28.7576\u001b[0m       27.2037  0.0110\n",
      "     15       \u001b[36m28.7324\u001b[0m       27.2121  0.0111\n",
      "     16       \u001b[36m28.7097\u001b[0m       27.2183  0.0116\n",
      "     17       \u001b[36m28.6891\u001b[0m       27.2234  0.0110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     18       \u001b[36m28.6705\u001b[0m       27.2277  0.0110\n",
      "     19       \u001b[36m28.6534\u001b[0m       27.2313  0.0112\n",
      "     20       \u001b[36m28.6380\u001b[0m       27.2351  0.0110\n",
      "     21       \u001b[36m28.6239\u001b[0m       27.2400  0.0111\n",
      "     22       \u001b[36m28.6112\u001b[0m       27.2439  0.0109\n",
      "     23       \u001b[36m28.5997\u001b[0m       27.2472  0.0108\n",
      "     24       \u001b[36m28.5890\u001b[0m       27.2506  0.0127\n",
      "     25       \u001b[36m28.5791\u001b[0m       27.2542  0.0109\n",
      "     26       \u001b[36m28.5702\u001b[0m       27.2569  0.0109\n",
      "     27       \u001b[36m28.5618\u001b[0m       27.2594  0.0109\n",
      "     28       \u001b[36m28.5540\u001b[0m       27.2617  0.0107\n",
      "     29       \u001b[36m28.5467\u001b[0m       27.2639  0.0109\n",
      "     30       \u001b[36m28.5400\u001b[0m       27.2666  0.0110\n",
      "     31       \u001b[36m28.5340\u001b[0m       27.2690  0.0110\n",
      "     32       \u001b[36m28.5284\u001b[0m       27.2712  0.0107\n",
      "     33       \u001b[36m28.5232\u001b[0m       27.2727  0.0110\n",
      "     34       \u001b[36m28.5182\u001b[0m       27.2739  0.0109\n",
      "     35       \u001b[36m28.5135\u001b[0m       27.2748  0.0111\n",
      "     36       \u001b[36m28.5092\u001b[0m       27.2758  0.0114\n",
      "     37       \u001b[36m28.5051\u001b[0m       27.2771  0.0112\n",
      "     38       \u001b[36m28.5013\u001b[0m       27.2778  0.0104\n",
      "     39       \u001b[36m28.4976\u001b[0m       27.2782  0.0110\n",
      "     40       \u001b[36m28.4942\u001b[0m       27.2785  0.0110\n",
      "     41       \u001b[36m28.4910\u001b[0m       27.2787  0.0109\n",
      "     42       \u001b[36m28.4880\u001b[0m       27.2785  0.0105\n",
      "     43       \u001b[36m28.4850\u001b[0m       27.2783  0.0105\n",
      "     44       \u001b[36m28.4823\u001b[0m       27.2780  0.0108\n",
      "     45       \u001b[36m28.4796\u001b[0m       27.2780  0.0110\n",
      "     46       \u001b[36m28.4771\u001b[0m       27.2777  0.0112\n",
      "     47       \u001b[36m28.4747\u001b[0m       27.2774  0.0104\n",
      "     48       \u001b[36m28.4724\u001b[0m       27.2771  0.0105\n",
      "     49       \u001b[36m28.4702\u001b[0m       27.2767  0.0109\n",
      "     50       \u001b[36m28.4681\u001b[0m       27.2764  0.0110\n",
      "     51       \u001b[36m28.4661\u001b[0m       27.2760  0.0110\n",
      "     52       \u001b[36m28.4641\u001b[0m       27.2762  0.0107\n",
      "     53       \u001b[36m28.4623\u001b[0m       27.2762  0.0107\n",
      "     54       \u001b[36m28.4606\u001b[0m       27.2760  0.0108\n",
      "     55       \u001b[36m28.4590\u001b[0m       27.2756  0.0110\n",
      "     56       \u001b[36m28.4574\u001b[0m       27.2753  0.0112\n",
      "     57       \u001b[36m28.4558\u001b[0m       27.2748  0.0106\n",
      "     58       \u001b[36m28.4543\u001b[0m       27.2745  0.0107\n",
      "     59       \u001b[36m28.4528\u001b[0m       27.2739  0.0109\n",
      "     60       \u001b[36m28.4514\u001b[0m       27.2734  0.0109\n",
      "     61       \u001b[36m28.4501\u001b[0m       27.2729  0.0110\n",
      "     62       \u001b[36m28.4488\u001b[0m       27.2726  0.0106\n",
      "     63       \u001b[36m28.4476\u001b[0m       27.2721  0.0105\n",
      "     64       \u001b[36m28.4464\u001b[0m       27.2715  0.0111\n",
      "     65       \u001b[36m28.4453\u001b[0m       27.2710  0.0114\n",
      "     66       \u001b[36m28.4441\u001b[0m       27.2705  0.0112\n",
      "     67       \u001b[36m28.4430\u001b[0m       27.2701  0.0105\n",
      "     68       \u001b[36m28.4420\u001b[0m       27.2695  0.0107\n",
      "     69       \u001b[36m28.4410\u001b[0m       27.2691  0.0111\n",
      "     70       \u001b[36m28.4400\u001b[0m       27.2685  0.0110\n",
      "     71       \u001b[36m28.4391\u001b[0m       27.2681  0.0109\n",
      "     72       \u001b[36m28.4382\u001b[0m       27.2676  0.0169\n",
      "     73       \u001b[36m28.4373\u001b[0m       27.2670  0.0164\n",
      "     74       \u001b[36m28.4364\u001b[0m       27.2665  0.0120\n",
      "     75       \u001b[36m28.4356\u001b[0m       27.2660  0.0120\n",
      "     76       \u001b[36m28.4348\u001b[0m       27.2654  0.0113\n",
      "     77       \u001b[36m28.4340\u001b[0m       27.2649  0.0114\n",
      "     78       \u001b[36m28.4332\u001b[0m       27.2643  0.0131\n",
      "     79       \u001b[36m28.4324\u001b[0m       27.2638  0.0113\n",
      "     80       \u001b[36m28.4317\u001b[0m       27.2634  0.0117\n",
      "     81       \u001b[36m28.4309\u001b[0m       27.2630  0.0112\n",
      "     82       \u001b[36m28.4302\u001b[0m       27.2627  0.0111\n",
      "     83       \u001b[36m28.4295\u001b[0m       27.2622  0.0115\n",
      "     84       \u001b[36m28.4288\u001b[0m       27.2620  0.0115\n",
      "     85       \u001b[36m28.4282\u001b[0m       27.2614  0.0111\n",
      "     86       \u001b[36m28.4275\u001b[0m       27.2609  0.0110\n",
      "     87       \u001b[36m28.4268\u001b[0m       27.2606  0.0115\n",
      "     88       \u001b[36m28.4262\u001b[0m       27.2603  0.0111\n",
      "     89       \u001b[36m28.4256\u001b[0m       27.2600  0.0109\n",
      "     90       \u001b[36m28.4250\u001b[0m       27.2597  0.0113\n",
      "     91       \u001b[36m28.4244\u001b[0m       27.2592  0.0111\n",
      "     92       \u001b[36m28.4238\u001b[0m       27.2590  0.0111\n",
      "     93       \u001b[36m28.4233\u001b[0m       27.2588  0.0112\n",
      "     94       \u001b[36m28.4227\u001b[0m       27.2581  0.0109\n",
      "     95       \u001b[36m28.4222\u001b[0m       27.2579  0.0111\n",
      "     96       \u001b[36m28.4216\u001b[0m       27.2575  0.0109\n",
      "     97       \u001b[36m28.4211\u001b[0m       27.2571  0.0108\n",
      "     98       \u001b[36m28.4206\u001b[0m       27.2566  0.0109\n",
      "     99       \u001b[36m28.4201\u001b[0m       27.2562  0.0111\n",
      "    100       \u001b[36m28.4196\u001b[0m       27.2560  0.0110\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m39.6278\u001b[0m       \u001b[32m36.0363\u001b[0m  0.0117\n",
      "      2       \u001b[36m34.6734\u001b[0m       \u001b[32m31.0092\u001b[0m  0.0115\n",
      "      3       \u001b[36m33.9317\u001b[0m       31.4151  0.0122\n",
      "      4       \u001b[36m33.2678\u001b[0m       32.1194  0.0116\n",
      "      5       \u001b[36m33.0151\u001b[0m       \u001b[32m30.4187\u001b[0m  0.0117\n",
      "      6       \u001b[36m32.7255\u001b[0m       \u001b[32m30.1288\u001b[0m  0.0116\n",
      "      7       \u001b[36m32.6286\u001b[0m       30.6315  0.0114\n",
      "      8       \u001b[36m32.5791\u001b[0m       30.5627  0.0117\n",
      "      9       \u001b[36m32.4340\u001b[0m       30.1362  0.0118\n",
      "     10       \u001b[36m32.3876\u001b[0m       30.2594  0.0116\n",
      "     11       \u001b[36m32.3579\u001b[0m       30.3292  0.0115\n",
      "     12       \u001b[36m32.3208\u001b[0m       30.2357  0.0112\n",
      "     13       \u001b[36m32.2926\u001b[0m       30.2417  0.0118\n",
      "     14       \u001b[36m32.2792\u001b[0m       30.1764  0.0117\n",
      "     15       \u001b[36m32.2648\u001b[0m       30.2375  0.0114\n",
      "     16       \u001b[36m32.2527\u001b[0m       30.1721  0.0113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m32.2363\u001b[0m       30.1490  0.0118\n",
      "     18       \u001b[36m32.2303\u001b[0m       30.1657  0.0121\n",
      "     19       \u001b[36m32.2209\u001b[0m       30.1464  0.0120\n",
      "     20       \u001b[36m32.2122\u001b[0m       30.1311  0.0121\n",
      "     21       \u001b[36m32.2052\u001b[0m       30.1342  0.0112\n",
      "     22       \u001b[36m32.1991\u001b[0m       30.1336  0.0112\n",
      "     23       \u001b[36m32.1921\u001b[0m       \u001b[32m30.1211\u001b[0m  0.0120\n",
      "     24       \u001b[36m32.1863\u001b[0m       30.1259  0.0119\n",
      "     25       \u001b[36m32.1808\u001b[0m       \u001b[32m30.1151\u001b[0m  0.0116\n",
      "     26       \u001b[36m32.1754\u001b[0m       \u001b[32m30.1144\u001b[0m  0.0117\n",
      "     27       \u001b[36m32.1701\u001b[0m       \u001b[32m30.1039\u001b[0m  0.0119\n",
      "     28       \u001b[36m32.1654\u001b[0m       30.1107  0.0115\n",
      "     29       \u001b[36m32.1603\u001b[0m       \u001b[32m30.0988\u001b[0m  0.0115\n",
      "     30       \u001b[36m32.1563\u001b[0m       30.1024  0.0119\n",
      "     31       \u001b[36m32.1515\u001b[0m       30.1031  0.0116\n",
      "     32       \u001b[36m32.1475\u001b[0m       \u001b[32m30.0985\u001b[0m  0.0116\n",
      "     33       \u001b[36m32.1436\u001b[0m       30.1072  0.0113\n",
      "     34       \u001b[36m32.1391\u001b[0m       30.1004  0.0112\n",
      "     35       \u001b[36m32.1366\u001b[0m       \u001b[32m30.0946\u001b[0m  0.0120\n",
      "     36       \u001b[36m32.1325\u001b[0m       30.1198  0.0116\n",
      "     37       \u001b[36m32.1290\u001b[0m       30.0986  0.0114\n",
      "     38       32.1291       \u001b[32m30.0763\u001b[0m  0.0118\n",
      "     39       \u001b[36m32.1253\u001b[0m       30.1818  0.0113\n",
      "     40       32.1313       30.0841  0.0113\n",
      "     41       32.1336       \u001b[32m30.0674\u001b[0m  0.0119\n",
      "     42       32.1309       30.2831  0.0117\n",
      "     43       32.1444       \u001b[32m30.0474\u001b[0m  0.0118\n",
      "     44       32.1373       30.1602  0.0115\n",
      "     45       \u001b[36m32.1112\u001b[0m       \u001b[32m30.0324\u001b[0m  0.0119\n",
      "     46       \u001b[36m32.1096\u001b[0m       30.0961  0.0118\n",
      "     47       \u001b[36m32.1034\u001b[0m       30.0648  0.0116\n",
      "     48       \u001b[36m32.1007\u001b[0m       30.0832  0.0117\n",
      "     49       \u001b[36m32.0968\u001b[0m       30.0745  0.0248\n",
      "     50       \u001b[36m32.0952\u001b[0m       30.0939  0.0200\n",
      "     51       \u001b[36m32.0914\u001b[0m       30.0942  0.0124\n",
      "     52       \u001b[36m32.0898\u001b[0m       30.0903  0.0190\n",
      "     53       \u001b[36m32.0869\u001b[0m       30.1057  0.0157\n",
      "     54       \u001b[36m32.0841\u001b[0m       30.1055  0.0152\n",
      "     55       \u001b[36m32.0825\u001b[0m       30.1112  0.0158\n",
      "     56       \u001b[36m32.0796\u001b[0m       30.1267  0.0147\n",
      "     57       \u001b[36m32.0771\u001b[0m       30.1186  0.0147\n",
      "     58       \u001b[36m32.0757\u001b[0m       30.1213  0.0131\n",
      "     59       \u001b[36m32.0724\u001b[0m       30.1475  0.0134\n",
      "     60       \u001b[36m32.0705\u001b[0m       30.1345  0.0125\n",
      "     61       \u001b[36m32.0703\u001b[0m       30.1269  0.0117\n",
      "     62       \u001b[36m32.0664\u001b[0m       30.1985  0.0117\n",
      "     63       \u001b[36m32.0661\u001b[0m       30.1608  0.0131\n",
      "     64       32.0697       30.0949  0.0123\n",
      "     65       32.0708       30.3041  0.0121\n",
      "     66       32.0789       30.1632  0.0120\n",
      "     67       32.0896       30.0805  0.0119\n",
      "     68       32.1045       30.3166  0.0118\n",
      "     69       32.1084       30.0406  0.0124\n",
      "     70       32.0982       30.3531  0.0121\n",
      "     71       32.0765       \u001b[32m30.0184\u001b[0m  0.0114\n",
      "     72       32.0747       30.2522  0.0113\n",
      "     73       \u001b[36m32.0615\u001b[0m       30.0187  0.0119\n",
      "     74       32.0621       30.2547  0.0120\n",
      "     75       \u001b[36m32.0590\u001b[0m       30.0308  0.0119\n",
      "     76       \u001b[36m32.0559\u001b[0m       30.2423  0.0118\n",
      "     77       \u001b[36m32.0541\u001b[0m       30.0506  0.0116\n",
      "     78       \u001b[36m32.0522\u001b[0m       30.2616  0.0113\n",
      "     79       \u001b[36m32.0506\u001b[0m       30.0511  0.0114\n",
      "     80       \u001b[36m32.0492\u001b[0m       30.2622  0.0119\n",
      "     81       \u001b[36m32.0463\u001b[0m       30.0563  0.0117\n",
      "     82       \u001b[36m32.0449\u001b[0m       30.2557  0.0113\n",
      "     83       \u001b[36m32.0417\u001b[0m       30.0655  0.0111\n",
      "     84       \u001b[36m32.0407\u001b[0m       30.2431  0.0124\n",
      "     85       \u001b[36m32.0366\u001b[0m       30.0828  0.0130\n",
      "     86       \u001b[36m32.0362\u001b[0m       30.2197  0.0116\n",
      "     87       \u001b[36m32.0315\u001b[0m       30.1019  0.0117\n",
      "     88       32.0322       30.1948  0.0117\n",
      "     89       \u001b[36m32.0266\u001b[0m       30.1322  0.0115\n",
      "     90       32.0279       30.1691  0.0121\n",
      "     91       \u001b[36m32.0229\u001b[0m       30.1664  0.0120\n",
      "     92       32.0241       30.1499  0.0118\n",
      "     93       \u001b[36m32.0208\u001b[0m       30.1934  0.0117\n",
      "     94       \u001b[36m32.0206\u001b[0m       30.1496  0.0116\n",
      "     95       \u001b[36m32.0196\u001b[0m       30.1936  0.0117\n",
      "     96       \u001b[36m32.0164\u001b[0m       30.1790  0.0114\n",
      "     97       32.0174       30.1742  0.0115\n",
      "     98       \u001b[36m32.0144\u001b[0m       30.2055  0.0119\n",
      "     99       \u001b[36m32.0136\u001b[0m       30.1822  0.0117\n",
      "    100       \u001b[36m32.0134\u001b[0m       30.1824  0.0116\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m31.7688\u001b[0m       \u001b[32m28.8479\u001b[0m  0.0115\n",
      "      2       \u001b[36m27.1516\u001b[0m       \u001b[32m28.1505\u001b[0m  0.0113\n",
      "      3       \u001b[36m25.0262\u001b[0m       \u001b[32m27.6629\u001b[0m  0.0124\n",
      "      4       \u001b[36m24.1476\u001b[0m       \u001b[32m26.2974\u001b[0m  0.0117\n",
      "      5       \u001b[36m24.1013\u001b[0m       26.4990  0.0113\n",
      "      6       \u001b[36m23.6982\u001b[0m       27.4106  0.0112\n",
      "      7       \u001b[36m23.5408\u001b[0m       27.3933  0.0119\n",
      "      8       \u001b[36m23.4127\u001b[0m       26.9547  0.0116\n",
      "      9       \u001b[36m23.4104\u001b[0m       27.0095  0.0115\n",
      "     10       \u001b[36m23.3061\u001b[0m       27.1946  0.0116\n",
      "     11       \u001b[36m23.2606\u001b[0m       26.9890  0.0114\n",
      "     12       \u001b[36m23.2305\u001b[0m       26.9039  0.0128\n",
      "     13       \u001b[36m23.2071\u001b[0m       27.0329  0.0120\n",
      "     14       \u001b[36m23.1755\u001b[0m       27.0355  0.0118\n",
      "     15       \u001b[36m23.1561\u001b[0m       26.8965  0.0116\n",
      "     16       \u001b[36m23.1441\u001b[0m       26.8650  0.0116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m23.1297\u001b[0m       26.9117  0.0125\n",
      "     18       \u001b[36m23.1134\u001b[0m       26.8924  0.0120\n",
      "     19       \u001b[36m23.1044\u001b[0m       26.8430  0.0117\n",
      "     20       \u001b[36m23.0962\u001b[0m       26.8460  0.0114\n",
      "     21       \u001b[36m23.0867\u001b[0m       26.8474  0.0113\n",
      "     22       \u001b[36m23.0780\u001b[0m       26.8197  0.0122\n",
      "     23       \u001b[36m23.0729\u001b[0m       26.8056  0.0119\n",
      "     24       \u001b[36m23.0656\u001b[0m       26.7978  0.0115\n",
      "     25       \u001b[36m23.0609\u001b[0m       26.7761  0.0114\n",
      "     26       \u001b[36m23.0556\u001b[0m       26.7613  0.0113\n",
      "     27       \u001b[36m23.0518\u001b[0m       26.7470  0.0118\n",
      "     28       \u001b[36m23.0471\u001b[0m       26.7297  0.0117\n",
      "     29       \u001b[36m23.0431\u001b[0m       26.7199  0.0132\n",
      "     30       \u001b[36m23.0390\u001b[0m       26.7014  0.0163\n",
      "     31       \u001b[36m23.0329\u001b[0m       26.7032  0.0129\n",
      "     32       \u001b[36m23.0293\u001b[0m       26.6883  0.0140\n",
      "     33       \u001b[36m23.0258\u001b[0m       26.6891  0.0138\n",
      "     34       \u001b[36m23.0228\u001b[0m       26.6837  0.0140\n",
      "     35       \u001b[36m23.0215\u001b[0m       26.6720  0.0127\n",
      "     36       \u001b[36m23.0179\u001b[0m       26.6871  0.0158\n",
      "     37       23.0208       26.6688  0.0127\n",
      "     38       23.0245       26.6634  0.0122\n",
      "     39       23.0181       26.6815  0.0117\n",
      "     40       23.0202       26.6626  0.0137\n",
      "     41       23.0235       26.6782  0.0124\n",
      "     42       23.0225       26.6846  0.0124\n",
      "     43       23.0197       26.6372  0.0117\n",
      "     44       \u001b[36m23.0028\u001b[0m       26.6790  0.0115\n",
      "     45       \u001b[36m22.9978\u001b[0m       26.6778  0.0145\n",
      "     46       22.9984       26.6599  0.0126\n",
      "     47       \u001b[36m22.9966\u001b[0m       26.6619  0.0120\n",
      "     48       \u001b[36m22.9931\u001b[0m       26.6673  0.0122\n",
      "     49       \u001b[36m22.9905\u001b[0m       26.6628  0.0123\n",
      "     50       \u001b[36m22.9890\u001b[0m       26.6647  0.0113\n",
      "     51       \u001b[36m22.9870\u001b[0m       26.6639  0.0113\n",
      "     52       \u001b[36m22.9850\u001b[0m       26.6604  0.0139\n",
      "     53       \u001b[36m22.9837\u001b[0m       26.6672  0.0126\n",
      "     54       \u001b[36m22.9816\u001b[0m       26.6599  0.0124\n",
      "     55       \u001b[36m22.9804\u001b[0m       26.6703  0.0113\n",
      "     56       \u001b[36m22.9787\u001b[0m       26.6614  0.0113\n",
      "     57       \u001b[36m22.9772\u001b[0m       26.6723  0.0138\n",
      "     58       \u001b[36m22.9758\u001b[0m       26.6593  0.0121\n",
      "     59       \u001b[36m22.9743\u001b[0m       26.6720  0.0122\n",
      "     60       \u001b[36m22.9726\u001b[0m       26.6626  0.0114\n",
      "     61       \u001b[36m22.9717\u001b[0m       26.6702  0.0112\n",
      "     62       \u001b[36m22.9698\u001b[0m       26.6664  0.0132\n",
      "     63       \u001b[36m22.9686\u001b[0m       26.6671  0.0125\n",
      "     64       \u001b[36m22.9678\u001b[0m       26.6612  0.0121\n",
      "     65       \u001b[36m22.9655\u001b[0m       26.6746  0.0117\n",
      "     66       \u001b[36m22.9648\u001b[0m       26.6527  0.0116\n",
      "     67       22.9653       26.6721  0.0135\n",
      "     68       \u001b[36m22.9620\u001b[0m       26.6777  0.0118\n",
      "     69       22.9641       26.6497  0.0121\n",
      "     70       22.9690       26.6750  0.0121\n",
      "     71       22.9673       26.7207  0.0125\n",
      "     72       22.9827       26.6178  0.0116\n",
      "     73       23.0019       26.6797  0.0116\n",
      "     74       22.9887       26.7333  0.0135\n",
      "     75       22.9961       26.6586  0.0122\n",
      "     76       23.0034       26.6138  0.0125\n",
      "     77       22.9639       26.7243  0.0112\n",
      "     78       \u001b[36m22.9563\u001b[0m       26.6639  0.0113\n",
      "     79       \u001b[36m22.9537\u001b[0m       26.6652  0.0140\n",
      "     80       \u001b[36m22.9535\u001b[0m       26.6629  0.0130\n",
      "     81       \u001b[36m22.9499\u001b[0m       26.6686  0.0124\n",
      "     82       \u001b[36m22.9487\u001b[0m       26.6789  0.0114\n",
      "     83       \u001b[36m22.9469\u001b[0m       26.6538  0.0113\n",
      "     84       \u001b[36m22.9459\u001b[0m       26.6539  0.0136\n",
      "     85       \u001b[36m22.9447\u001b[0m       26.6686  0.0121\n",
      "     86       \u001b[36m22.9435\u001b[0m       26.6592  0.0123\n",
      "     87       \u001b[36m22.9424\u001b[0m       26.6514  0.0116\n",
      "     88       \u001b[36m22.9412\u001b[0m       26.6601  0.0118\n",
      "     89       \u001b[36m22.9401\u001b[0m       26.6563  0.0134\n",
      "     90       \u001b[36m22.9390\u001b[0m       26.6561  0.0123\n",
      "     91       \u001b[36m22.9379\u001b[0m       26.6553  0.0122\n",
      "     92       \u001b[36m22.9369\u001b[0m       26.6571  0.0117\n",
      "     93       \u001b[36m22.9356\u001b[0m       26.6556  0.0118\n",
      "     94       \u001b[36m22.9347\u001b[0m       26.6538  0.0135\n",
      "     95       \u001b[36m22.9338\u001b[0m       26.6557  0.0122\n",
      "     96       \u001b[36m22.9324\u001b[0m       26.6563  0.0122\n",
      "     97       \u001b[36m22.9317\u001b[0m       26.6554  0.0111\n",
      "     98       \u001b[36m22.9310\u001b[0m       26.6578  0.0115\n",
      "     99       \u001b[36m22.9299\u001b[0m       26.6545  0.0139\n",
      "    100       \u001b[36m22.9298\u001b[0m       26.6466  0.0124\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m38.7326\u001b[0m       \u001b[32m27.9210\u001b[0m  0.0115\n",
      "      2       \u001b[36m31.8186\u001b[0m       31.4259  0.0113\n",
      "      3       \u001b[36m31.3259\u001b[0m       \u001b[32m27.2495\u001b[0m  0.0114\n",
      "      4       \u001b[36m29.8122\u001b[0m       \u001b[32m26.4964\u001b[0m  0.0134\n",
      "      5       \u001b[36m29.3774\u001b[0m       27.2866  0.0157\n",
      "      6       \u001b[36m29.1654\u001b[0m       28.6154  0.0163\n",
      "      7       \u001b[36m29.0835\u001b[0m       27.4410  0.0129\n",
      "      8       \u001b[36m28.7902\u001b[0m       27.0530  0.0137\n",
      "      9       \u001b[36m28.6828\u001b[0m       27.5572  0.0130\n",
      "     10       28.6838       27.5580  0.0179\n",
      "     11       \u001b[36m28.5951\u001b[0m       27.1587  0.0122\n",
      "     12       \u001b[36m28.5372\u001b[0m       27.3456  0.0130\n",
      "     13       \u001b[36m28.5364\u001b[0m       27.5071  0.0121\n",
      "     14       \u001b[36m28.5178\u001b[0m       27.3199  0.0117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     15       \u001b[36m28.4856\u001b[0m       27.3445  0.0119\n",
      "     16       \u001b[36m28.4818\u001b[0m       27.4077  0.0121\n",
      "     17       \u001b[36m28.4733\u001b[0m       27.3244  0.0120\n",
      "     18       \u001b[36m28.4586\u001b[0m       27.3419  0.0120\n",
      "     19       28.4607       27.3508  0.0118\n",
      "     20       28.4651       27.3443  0.0118\n",
      "     21       28.4724       27.3932  0.0122\n",
      "     22       28.5224       27.3566  0.0118\n",
      "     23       28.5269       27.4260  0.0119\n",
      "     24       28.4902       27.2159  0.0116\n",
      "     25       \u001b[36m28.4341\u001b[0m       27.2591  0.0113\n",
      "     26       \u001b[36m28.4322\u001b[0m       27.3951  0.0122\n",
      "     27       28.4328       27.2035  0.0118\n",
      "     28       \u001b[36m28.4085\u001b[0m       27.3020  0.0118\n",
      "     29       28.4196       27.3343  0.0115\n",
      "     30       28.4117       27.2749  0.0117\n",
      "     31       28.4099       27.3031  0.0117\n",
      "     32       \u001b[36m28.4043\u001b[0m       27.3329  0.0113\n",
      "     33       \u001b[36m28.4034\u001b[0m       27.2816  0.0114\n",
      "     34       \u001b[36m28.3979\u001b[0m       27.3258  0.0123\n",
      "     35       28.3991       27.3019  0.0115\n",
      "     36       \u001b[36m28.3950\u001b[0m       27.3138  0.0115\n",
      "     37       \u001b[36m28.3942\u001b[0m       27.3298  0.0118\n",
      "     38       \u001b[36m28.3935\u001b[0m       27.2985  0.0117\n",
      "     39       \u001b[36m28.3923\u001b[0m       27.3413  0.0138\n",
      "     40       \u001b[36m28.3910\u001b[0m       27.3128  0.0117\n",
      "     41       28.3915       27.3171  0.0120\n",
      "     42       28.3970       27.3562  0.0118\n",
      "     43       28.3930       27.2858  0.0117\n",
      "     44       28.4036       27.3639  0.0113\n",
      "     45       28.4304       27.3650  0.0113\n",
      "     46       28.4261       27.2313  0.0121\n",
      "     47       28.4476       27.4170  0.0115\n",
      "     48       28.4915       27.4035  0.0114\n",
      "     49       28.4884       27.0743  0.0111\n",
      "     50       28.3950       27.4389  0.0114\n",
      "     51       28.4029       27.2147  0.0122\n",
      "     52       \u001b[36m28.3812\u001b[0m       27.2554  0.0117\n",
      "     53       28.3920       27.2863  0.0115\n",
      "     54       \u001b[36m28.3804\u001b[0m       27.2861  0.0116\n",
      "     55       \u001b[36m28.3798\u001b[0m       27.3282  0.0117\n",
      "     56       \u001b[36m28.3787\u001b[0m       27.2705  0.0119\n",
      "     57       \u001b[36m28.3771\u001b[0m       27.2909  0.0113\n",
      "     58       \u001b[36m28.3766\u001b[0m       27.3181  0.0113\n",
      "     59       \u001b[36m28.3745\u001b[0m       27.2808  0.0121\n",
      "     60       \u001b[36m28.3737\u001b[0m       27.3034  0.0118\n",
      "     61       \u001b[36m28.3733\u001b[0m       27.2903  0.0115\n",
      "     62       \u001b[36m28.3720\u001b[0m       27.3033  0.0115\n",
      "     63       \u001b[36m28.3717\u001b[0m       27.2909  0.0113\n",
      "     64       \u001b[36m28.3705\u001b[0m       27.2977  0.0118\n",
      "     65       \u001b[36m28.3704\u001b[0m       27.3004  0.0117\n",
      "     66       \u001b[36m28.3690\u001b[0m       27.2933  0.0115\n",
      "     67       \u001b[36m28.3690\u001b[0m       27.3013  0.0113\n",
      "     68       \u001b[36m28.3680\u001b[0m       27.2948  0.0112\n",
      "     69       \u001b[36m28.3674\u001b[0m       27.2970  0.0120\n",
      "     70       \u001b[36m28.3667\u001b[0m       27.2995  0.0118\n",
      "     71       \u001b[36m28.3666\u001b[0m       27.2937  0.0117\n",
      "     72       \u001b[36m28.3651\u001b[0m       27.2969  0.0113\n",
      "     73       28.3657       27.2952  0.0109\n",
      "     74       \u001b[36m28.3638\u001b[0m       27.2912  0.0119\n",
      "     75       28.3643       27.2959  0.0115\n",
      "     76       \u001b[36m28.3634\u001b[0m       27.2894  0.0117\n",
      "     77       \u001b[36m28.3626\u001b[0m       27.2919  0.0116\n",
      "     78       28.3628       27.2861  0.0117\n",
      "     79       \u001b[36m28.3624\u001b[0m       27.2869  0.0114\n",
      "     80       \u001b[36m28.3612\u001b[0m       27.2800  0.0112\n",
      "     81       \u001b[36m28.3609\u001b[0m       27.2849  0.0120\n",
      "     82       28.3642       27.2900  0.0118\n",
      "     83       28.3617       27.2643  0.0115\n",
      "     84       28.3655       27.2859  0.0121\n",
      "     85       28.3771       27.2809  0.0175\n",
      "     86       28.3865       27.2604  0.0135\n",
      "     87       28.4006       27.2399  0.0121\n",
      "     88       28.4271       27.3245  0.0128\n",
      "     89       28.4363       27.2431  0.0119\n",
      "     90       28.4489       27.1139  0.0127\n",
      "     91       28.3931       27.3353  0.0128\n",
      "     92       28.3663       27.1779  0.0122\n",
      "     93       28.3707       27.2176  0.0117\n",
      "     94       \u001b[36m28.3607\u001b[0m       27.2224  0.0117\n",
      "     95       28.3631       27.2879  0.0116\n",
      "     96       \u001b[36m28.3581\u001b[0m       27.2144  0.0123\n",
      "     97       \u001b[36m28.3580\u001b[0m       27.2234  0.0116\n",
      "     98       \u001b[36m28.3560\u001b[0m       27.2383  0.0115\n",
      "     99       \u001b[36m28.3559\u001b[0m       27.2402  0.0114\n",
      "    100       \u001b[36m28.3545\u001b[0m       27.2364  0.0114\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m43.9754\u001b[0m       \u001b[32m44.8409\u001b[0m  0.0114\n",
      "      2       \u001b[36m42.1996\u001b[0m       \u001b[32m42.6832\u001b[0m  0.0112\n",
      "      3       \u001b[36m40.4870\u001b[0m       \u001b[32m40.5002\u001b[0m  0.0109\n",
      "      4       \u001b[36m38.7929\u001b[0m       \u001b[32m38.3889\u001b[0m  0.0110\n",
      "      5       \u001b[36m37.2167\u001b[0m       \u001b[32m36.4792\u001b[0m  0.0107\n",
      "      6       \u001b[36m35.8534\u001b[0m       \u001b[32m34.7843\u001b[0m  0.0106\n",
      "      7       \u001b[36m34.7332\u001b[0m       \u001b[32m33.3213\u001b[0m  0.0110\n",
      "      8       \u001b[36m33.8792\u001b[0m       \u001b[32m32.1693\u001b[0m  0.0112\n",
      "      9       \u001b[36m33.3100\u001b[0m       \u001b[32m31.3706\u001b[0m  0.0108\n",
      "     10       \u001b[36m32.9822\u001b[0m       \u001b[32m30.8791\u001b[0m  0.0109\n",
      "     11       \u001b[36m32.8100\u001b[0m       \u001b[32m30.5942\u001b[0m  0.0107\n",
      "     12       \u001b[36m32.7178\u001b[0m       \u001b[32m30.4302\u001b[0m  0.0114\n",
      "     13       \u001b[36m32.6613\u001b[0m       \u001b[32m30.3322\u001b[0m  0.0111\n",
      "     14       \u001b[36m32.6211\u001b[0m       \u001b[32m30.2705\u001b[0m  0.0115\n",
      "     15       \u001b[36m32.5896\u001b[0m       \u001b[32m30.2291\u001b[0m  0.0112\n",
      "     16       \u001b[36m32.5632\u001b[0m       \u001b[32m30.1994\u001b[0m  0.0110\n",
      "     17       \u001b[36m32.5406\u001b[0m       \u001b[32m30.1766\u001b[0m  0.0117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     18       \u001b[36m32.5210\u001b[0m       \u001b[32m30.1582\u001b[0m  0.0118\n",
      "     19       \u001b[36m32.5035\u001b[0m       \u001b[32m30.1428\u001b[0m  0.0113\n",
      "     20       \u001b[36m32.4878\u001b[0m       \u001b[32m30.1294\u001b[0m  0.0114\n",
      "     21       \u001b[36m32.4737\u001b[0m       \u001b[32m30.1175\u001b[0m  0.0109\n",
      "     22       \u001b[36m32.4608\u001b[0m       \u001b[32m30.1066\u001b[0m  0.0111\n",
      "     23       \u001b[36m32.4490\u001b[0m       \u001b[32m30.0966\u001b[0m  0.0114\n",
      "     24       \u001b[36m32.4383\u001b[0m       \u001b[32m30.0873\u001b[0m  0.0112\n",
      "     25       \u001b[36m32.4284\u001b[0m       \u001b[32m30.0785\u001b[0m  0.0112\n",
      "     26       \u001b[36m32.4192\u001b[0m       \u001b[32m30.0704\u001b[0m  0.0110\n",
      "     27       \u001b[36m32.4108\u001b[0m       \u001b[32m30.0628\u001b[0m  0.0110\n",
      "     28       \u001b[36m32.4030\u001b[0m       \u001b[32m30.0558\u001b[0m  0.0112\n",
      "     29       \u001b[36m32.3956\u001b[0m       \u001b[32m30.0491\u001b[0m  0.0109\n",
      "     30       \u001b[36m32.3887\u001b[0m       \u001b[32m30.0430\u001b[0m  0.0107\n",
      "     31       \u001b[36m32.3823\u001b[0m       \u001b[32m30.0371\u001b[0m  0.0111\n",
      "     32       \u001b[36m32.3762\u001b[0m       \u001b[32m30.0315\u001b[0m  0.0108\n",
      "     33       \u001b[36m32.3704\u001b[0m       \u001b[32m30.0261\u001b[0m  0.0114\n",
      "     34       \u001b[36m32.3650\u001b[0m       \u001b[32m30.0209\u001b[0m  0.0110\n",
      "     35       \u001b[36m32.3600\u001b[0m       \u001b[32m30.0156\u001b[0m  0.0113\n",
      "     36       \u001b[36m32.3552\u001b[0m       \u001b[32m30.0110\u001b[0m  0.0109\n",
      "     37       \u001b[36m32.3508\u001b[0m       \u001b[32m30.0063\u001b[0m  0.0111\n",
      "     38       \u001b[36m32.3465\u001b[0m       \u001b[32m30.0017\u001b[0m  0.0115\n",
      "     39       \u001b[36m32.3424\u001b[0m       \u001b[32m29.9978\u001b[0m  0.0110\n",
      "     40       \u001b[36m32.3386\u001b[0m       \u001b[32m29.9937\u001b[0m  0.0110\n",
      "     41       \u001b[36m32.3349\u001b[0m       \u001b[32m29.9900\u001b[0m  0.0107\n",
      "     42       \u001b[36m32.3314\u001b[0m       \u001b[32m29.9864\u001b[0m  0.0107\n",
      "     43       \u001b[36m32.3281\u001b[0m       \u001b[32m29.9827\u001b[0m  0.0114\n",
      "     44       \u001b[36m32.3249\u001b[0m       \u001b[32m29.9794\u001b[0m  0.0110\n",
      "     45       \u001b[36m32.3217\u001b[0m       \u001b[32m29.9763\u001b[0m  0.0109\n",
      "     46       \u001b[36m32.3188\u001b[0m       \u001b[32m29.9732\u001b[0m  0.0105\n",
      "     47       \u001b[36m32.3160\u001b[0m       \u001b[32m29.9702\u001b[0m  0.0109\n",
      "     48       \u001b[36m32.3131\u001b[0m       \u001b[32m29.9674\u001b[0m  0.0121\n",
      "     49       \u001b[36m32.3105\u001b[0m       \u001b[32m29.9645\u001b[0m  0.0116\n",
      "     50       \u001b[36m32.3079\u001b[0m       \u001b[32m29.9617\u001b[0m  0.0115\n",
      "     51       \u001b[36m32.3056\u001b[0m       \u001b[32m29.9591\u001b[0m  0.0112\n",
      "     52       \u001b[36m32.3032\u001b[0m       \u001b[32m29.9569\u001b[0m  0.0112\n",
      "     53       \u001b[36m32.3009\u001b[0m       \u001b[32m29.9544\u001b[0m  0.0115\n",
      "     54       \u001b[36m32.2987\u001b[0m       \u001b[32m29.9523\u001b[0m  0.0115\n",
      "     55       \u001b[36m32.2967\u001b[0m       \u001b[32m29.9503\u001b[0m  0.0113\n",
      "     56       \u001b[36m32.2946\u001b[0m       \u001b[32m29.9480\u001b[0m  0.0112\n",
      "     57       \u001b[36m32.2925\u001b[0m       \u001b[32m29.9458\u001b[0m  0.0113\n",
      "     58       \u001b[36m32.2907\u001b[0m       \u001b[32m29.9438\u001b[0m  0.0114\n",
      "     59       \u001b[36m32.2888\u001b[0m       \u001b[32m29.9418\u001b[0m  0.0114\n",
      "     60       \u001b[36m32.2870\u001b[0m       \u001b[32m29.9399\u001b[0m  0.0112\n",
      "     61       \u001b[36m32.2853\u001b[0m       \u001b[32m29.9379\u001b[0m  0.0111\n",
      "     62       \u001b[36m32.2837\u001b[0m       \u001b[32m29.9362\u001b[0m  0.0111\n",
      "     63       \u001b[36m32.2819\u001b[0m       \u001b[32m29.9345\u001b[0m  0.0112\n",
      "     64       \u001b[36m32.2804\u001b[0m       \u001b[32m29.9328\u001b[0m  0.0108\n",
      "     65       \u001b[36m32.2788\u001b[0m       \u001b[32m29.9313\u001b[0m  0.0111\n",
      "     66       \u001b[36m32.2774\u001b[0m       \u001b[32m29.9297\u001b[0m  0.0107\n",
      "     67       \u001b[36m32.2759\u001b[0m       \u001b[32m29.9280\u001b[0m  0.0190\n",
      "     68       \u001b[36m32.2744\u001b[0m       \u001b[32m29.9266\u001b[0m  0.0180\n",
      "     69       \u001b[36m32.2731\u001b[0m       \u001b[32m29.9251\u001b[0m  0.0173\n",
      "     70       \u001b[36m32.2717\u001b[0m       \u001b[32m29.9236\u001b[0m  0.0115\n",
      "     71       \u001b[36m32.2703\u001b[0m       \u001b[32m29.9223\u001b[0m  0.0112\n",
      "     72       \u001b[36m32.2691\u001b[0m       \u001b[32m29.9209\u001b[0m  0.0120\n",
      "     73       \u001b[36m32.2678\u001b[0m       \u001b[32m29.9198\u001b[0m  0.0138\n",
      "     74       \u001b[36m32.2666\u001b[0m       \u001b[32m29.9185\u001b[0m  0.0142\n",
      "     75       \u001b[36m32.2654\u001b[0m       \u001b[32m29.9173\u001b[0m  0.0111\n",
      "     76       \u001b[36m32.2643\u001b[0m       \u001b[32m29.9160\u001b[0m  0.0113\n",
      "     77       \u001b[36m32.2630\u001b[0m       \u001b[32m29.9148\u001b[0m  0.0111\n",
      "     78       \u001b[36m32.2619\u001b[0m       \u001b[32m29.9138\u001b[0m  0.0110\n",
      "     79       \u001b[36m32.2608\u001b[0m       \u001b[32m29.9127\u001b[0m  0.0110\n",
      "     80       \u001b[36m32.2598\u001b[0m       \u001b[32m29.9115\u001b[0m  0.0110\n",
      "     81       \u001b[36m32.2586\u001b[0m       \u001b[32m29.9107\u001b[0m  0.0111\n",
      "     82       \u001b[36m32.2576\u001b[0m       \u001b[32m29.9095\u001b[0m  0.0110\n",
      "     83       \u001b[36m32.2565\u001b[0m       \u001b[32m29.9086\u001b[0m  0.0109\n",
      "     84       \u001b[36m32.2556\u001b[0m       \u001b[32m29.9076\u001b[0m  0.0111\n",
      "     85       \u001b[36m32.2545\u001b[0m       \u001b[32m29.9067\u001b[0m  0.0112\n",
      "     86       \u001b[36m32.2537\u001b[0m       \u001b[32m29.9058\u001b[0m  0.0110\n",
      "     87       \u001b[36m32.2527\u001b[0m       \u001b[32m29.9049\u001b[0m  0.0112\n",
      "     88       \u001b[36m32.2518\u001b[0m       \u001b[32m29.9040\u001b[0m  0.0107\n",
      "     89       \u001b[36m32.2509\u001b[0m       \u001b[32m29.9032\u001b[0m  0.0109\n",
      "     90       \u001b[36m32.2500\u001b[0m       \u001b[32m29.9024\u001b[0m  0.0108\n",
      "     91       \u001b[36m32.2491\u001b[0m       \u001b[32m29.9018\u001b[0m  0.0111\n",
      "     92       \u001b[36m32.2482\u001b[0m       \u001b[32m29.9010\u001b[0m  0.0110\n",
      "     93       \u001b[36m32.2474\u001b[0m       \u001b[32m29.9001\u001b[0m  0.0107\n",
      "     94       \u001b[36m32.2466\u001b[0m       \u001b[32m29.8994\u001b[0m  0.0107\n",
      "     95       \u001b[36m32.2458\u001b[0m       \u001b[32m29.8986\u001b[0m  0.0117\n",
      "     96       \u001b[36m32.2450\u001b[0m       \u001b[32m29.8978\u001b[0m  0.0109\n",
      "     97       \u001b[36m32.2442\u001b[0m       \u001b[32m29.8969\u001b[0m  0.0108\n",
      "     98       \u001b[36m32.2435\u001b[0m       \u001b[32m29.8962\u001b[0m  0.0108\n",
      "     99       \u001b[36m32.2427\u001b[0m       \u001b[32m29.8955\u001b[0m  0.0105\n",
      "    100       \u001b[36m32.2420\u001b[0m       \u001b[32m29.8948\u001b[0m  0.0107\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m34.0995\u001b[0m       \u001b[32m31.8178\u001b[0m  0.0109\n",
      "      2       \u001b[36m32.1026\u001b[0m       \u001b[32m30.2764\u001b[0m  0.0116\n",
      "      3       \u001b[36m30.0538\u001b[0m       \u001b[32m28.7828\u001b[0m  0.0112\n",
      "      4       \u001b[36m28.0672\u001b[0m       \u001b[32m27.5447\u001b[0m  0.0111\n",
      "      5       \u001b[36m26.3966\u001b[0m       \u001b[32m26.7573\u001b[0m  0.0107\n",
      "      6       \u001b[36m25.1956\u001b[0m       \u001b[32m26.4146\u001b[0m  0.0108\n",
      "      7       \u001b[36m24.4388\u001b[0m       \u001b[32m26.3620\u001b[0m  0.0108\n",
      "      8       \u001b[36m24.0129\u001b[0m       26.4238  0.0111\n",
      "      9       \u001b[36m23.7875\u001b[0m       26.4934  0.0109\n",
      "     10       \u001b[36m23.6642\u001b[0m       26.5386  0.0109\n",
      "     11       \u001b[36m23.5887\u001b[0m       26.5581  0.0107\n",
      "     12       \u001b[36m23.5359\u001b[0m       26.5621  0.0108\n",
      "     13       \u001b[36m23.4950\u001b[0m       26.5579  0.0109\n",
      "     14       \u001b[36m23.4616\u001b[0m       26.5498  0.0109\n",
      "     15       \u001b[36m23.4333\u001b[0m       26.5403  0.0109\n",
      "     16       \u001b[36m23.4088\u001b[0m       26.5310  0.0109\n",
      "     17       \u001b[36m23.3873\u001b[0m       26.5217  0.0107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     18       \u001b[36m23.3682\u001b[0m       26.5136  0.0112\n",
      "     19       \u001b[36m23.3511\u001b[0m       26.5064  0.0112\n",
      "     20       \u001b[36m23.3358\u001b[0m       26.5001  0.0109\n",
      "     21       \u001b[36m23.3220\u001b[0m       26.4945  0.0107\n",
      "     22       \u001b[36m23.3096\u001b[0m       26.4898  0.0107\n",
      "     23       \u001b[36m23.2982\u001b[0m       26.4854  0.0110\n",
      "     24       \u001b[36m23.2877\u001b[0m       26.4816  0.0110\n",
      "     25       \u001b[36m23.2780\u001b[0m       26.4779  0.0110\n",
      "     26       \u001b[36m23.2690\u001b[0m       26.4749  0.0108\n",
      "     27       \u001b[36m23.2606\u001b[0m       26.4722  0.0108\n",
      "     28       \u001b[36m23.2529\u001b[0m       26.4696  0.0109\n",
      "     29       \u001b[36m23.2456\u001b[0m       26.4676  0.0113\n",
      "     30       \u001b[36m23.2388\u001b[0m       26.4656  0.0112\n",
      "     31       \u001b[36m23.2324\u001b[0m       26.4638  0.0111\n",
      "     32       \u001b[36m23.2264\u001b[0m       26.4624  0.0111\n",
      "     33       \u001b[36m23.2206\u001b[0m       26.4611  0.0117\n",
      "     34       \u001b[36m23.2152\u001b[0m       26.4599  0.0116\n",
      "     35       \u001b[36m23.2102\u001b[0m       26.4590  0.0114\n",
      "     36       \u001b[36m23.2053\u001b[0m       26.4581  0.0113\n",
      "     37       \u001b[36m23.2007\u001b[0m       26.4574  0.0110\n",
      "     38       \u001b[36m23.1963\u001b[0m       26.4566  0.0112\n",
      "     39       \u001b[36m23.1922\u001b[0m       26.4559  0.0113\n",
      "     40       \u001b[36m23.1882\u001b[0m       26.4552  0.0111\n",
      "     41       \u001b[36m23.1844\u001b[0m       26.4546  0.0111\n",
      "     42       \u001b[36m23.1807\u001b[0m       26.4540  0.0110\n",
      "     43       \u001b[36m23.1772\u001b[0m       26.4534  0.0113\n",
      "     44       \u001b[36m23.1738\u001b[0m       26.4531  0.0113\n",
      "     45       \u001b[36m23.1706\u001b[0m       26.4526  0.0110\n",
      "     46       \u001b[36m23.1675\u001b[0m       26.4522  0.0108\n",
      "     47       \u001b[36m23.1645\u001b[0m       26.4515  0.0109\n",
      "     48       \u001b[36m23.1616\u001b[0m       26.4511  0.0107\n",
      "     49       \u001b[36m23.1588\u001b[0m       26.4506  0.0109\n",
      "     50       \u001b[36m23.1561\u001b[0m       26.4501  0.0218\n",
      "     51       \u001b[36m23.1535\u001b[0m       26.4497  0.0135\n",
      "     52       \u001b[36m23.1510\u001b[0m       26.4493  0.0125\n",
      "     53       \u001b[36m23.1485\u001b[0m       26.4488  0.0128\n",
      "     54       \u001b[36m23.1462\u001b[0m       26.4483  0.0129\n",
      "     55       \u001b[36m23.1439\u001b[0m       26.4479  0.0205\n",
      "     56       \u001b[36m23.1417\u001b[0m       26.4474  0.0173\n",
      "     57       \u001b[36m23.1396\u001b[0m       26.4470  0.0123\n",
      "     58       \u001b[36m23.1375\u001b[0m       26.4465  0.0116\n",
      "     59       \u001b[36m23.1355\u001b[0m       26.4462  0.0113\n",
      "     60       \u001b[36m23.1336\u001b[0m       26.4457  0.0113\n",
      "     61       \u001b[36m23.1317\u001b[0m       26.4452  0.0120\n",
      "     62       \u001b[36m23.1299\u001b[0m       26.4448  0.0113\n",
      "     63       \u001b[36m23.1281\u001b[0m       26.4445  0.0112\n",
      "     64       \u001b[36m23.1264\u001b[0m       26.4441  0.0112\n",
      "     65       \u001b[36m23.1247\u001b[0m       26.4437  0.0110\n",
      "     66       \u001b[36m23.1231\u001b[0m       26.4433  0.0114\n",
      "     67       \u001b[36m23.1215\u001b[0m       26.4428  0.0111\n",
      "     68       \u001b[36m23.1199\u001b[0m       26.4424  0.0110\n",
      "     69       \u001b[36m23.1184\u001b[0m       26.4421  0.0109\n",
      "     70       \u001b[36m23.1169\u001b[0m       26.4416  0.0113\n",
      "     71       \u001b[36m23.1155\u001b[0m       26.4412  0.0114\n",
      "     72       \u001b[36m23.1141\u001b[0m       26.4408  0.0116\n",
      "     73       \u001b[36m23.1128\u001b[0m       26.4404  0.0113\n",
      "     74       \u001b[36m23.1115\u001b[0m       26.4400  0.0110\n",
      "     75       \u001b[36m23.1102\u001b[0m       26.4395  0.0113\n",
      "     76       \u001b[36m23.1089\u001b[0m       26.4391  0.0113\n",
      "     77       \u001b[36m23.1077\u001b[0m       26.4387  0.0112\n",
      "     78       \u001b[36m23.1065\u001b[0m       26.4383  0.0115\n",
      "     79       \u001b[36m23.1053\u001b[0m       26.4379  0.0112\n",
      "     80       \u001b[36m23.1042\u001b[0m       26.4375  0.0109\n",
      "     81       \u001b[36m23.1031\u001b[0m       26.4371  0.0113\n",
      "     82       \u001b[36m23.1020\u001b[0m       26.4367  0.0111\n",
      "     83       \u001b[36m23.1009\u001b[0m       26.4364  0.0111\n",
      "     84       \u001b[36m23.0998\u001b[0m       26.4360  0.0109\n",
      "     85       \u001b[36m23.0988\u001b[0m       26.4357  0.0112\n",
      "     86       \u001b[36m23.0978\u001b[0m       26.4352  0.0113\n",
      "     87       \u001b[36m23.0968\u001b[0m       26.4350  0.0117\n",
      "     88       \u001b[36m23.0959\u001b[0m       26.4346  0.0108\n",
      "     89       \u001b[36m23.0949\u001b[0m       26.4341  0.0109\n",
      "     90       \u001b[36m23.0940\u001b[0m       26.4338  0.0106\n",
      "     91       \u001b[36m23.0931\u001b[0m       26.4334  0.0111\n",
      "     92       \u001b[36m23.0922\u001b[0m       26.4330  0.0111\n",
      "     93       \u001b[36m23.0914\u001b[0m       26.4327  0.0108\n",
      "     94       \u001b[36m23.0905\u001b[0m       26.4323  0.0108\n",
      "     95       \u001b[36m23.0897\u001b[0m       26.4320  0.0108\n",
      "     96       \u001b[36m23.0889\u001b[0m       26.4316  0.0119\n",
      "     97       \u001b[36m23.0881\u001b[0m       26.4312  0.0111\n",
      "     98       \u001b[36m23.0874\u001b[0m       26.4308  0.0110\n",
      "     99       \u001b[36m23.0866\u001b[0m       26.4305  0.0107\n",
      "    100       \u001b[36m23.0859\u001b[0m       26.4301  0.0108\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m41.1970\u001b[0m       \u001b[32m31.9264\u001b[0m  0.0117\n",
      "      2       \u001b[36m39.1376\u001b[0m       \u001b[32m30.5807\u001b[0m  0.0114\n",
      "      3       \u001b[36m37.2010\u001b[0m       \u001b[32m29.3228\u001b[0m  0.0114\n",
      "      4       \u001b[36m35.3543\u001b[0m       \u001b[32m28.1911\u001b[0m  0.0120\n",
      "      5       \u001b[36m33.6518\u001b[0m       \u001b[32m27.2429\u001b[0m  0.0138\n",
      "      6       \u001b[36m32.1188\u001b[0m       \u001b[32m26.5627\u001b[0m  0.0112\n",
      "      7       \u001b[36m30.8084\u001b[0m       \u001b[32m26.2373\u001b[0m  0.0119\n",
      "      8       \u001b[36m29.8279\u001b[0m       26.2769  0.0113\n",
      "      9       \u001b[36m29.2386\u001b[0m       26.5340  0.0114\n",
      "     10       \u001b[36m28.9572\u001b[0m       26.8107  0.0113\n",
      "     11       \u001b[36m28.8405\u001b[0m       27.0116  0.0111\n",
      "     12       \u001b[36m28.7874\u001b[0m       27.1308  0.0109\n",
      "     13       \u001b[36m28.7547\u001b[0m       27.1957  0.0111\n",
      "     14       \u001b[36m28.7294\u001b[0m       27.2293  0.0112\n",
      "     15       \u001b[36m28.7075\u001b[0m       27.2473  0.0111\n",
      "     16       \u001b[36m28.6880\u001b[0m       27.2559  0.0120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m28.6704\u001b[0m       27.2609  0.0111\n",
      "     18       \u001b[36m28.6545\u001b[0m       27.2648  0.0113\n",
      "     19       \u001b[36m28.6405\u001b[0m       27.2666  0.0114\n",
      "     20       \u001b[36m28.6278\u001b[0m       27.2677  0.0110\n",
      "     21       \u001b[36m28.6163\u001b[0m       27.2685  0.0130\n",
      "     22       \u001b[36m28.6058\u001b[0m       27.2693  0.0114\n",
      "     23       \u001b[36m28.5962\u001b[0m       27.2698  0.0125\n",
      "     24       \u001b[36m28.5874\u001b[0m       27.2699  0.0118\n",
      "     25       \u001b[36m28.5793\u001b[0m       27.2703  0.0115\n",
      "     26       \u001b[36m28.5717\u001b[0m       27.2703  0.0112\n",
      "     27       \u001b[36m28.5647\u001b[0m       27.2704  0.0108\n",
      "     28       \u001b[36m28.5581\u001b[0m       27.2706  0.0110\n",
      "     29       \u001b[36m28.5521\u001b[0m       27.2705  0.0114\n",
      "     30       \u001b[36m28.5464\u001b[0m       27.2703  0.0136\n",
      "     31       \u001b[36m28.5411\u001b[0m       27.2706  0.0137\n",
      "     32       \u001b[36m28.5362\u001b[0m       27.2705  0.0119\n",
      "     33       \u001b[36m28.5316\u001b[0m       27.2701  0.0125\n",
      "     34       \u001b[36m28.5271\u001b[0m       27.2701  0.0132\n",
      "     35       \u001b[36m28.5230\u001b[0m       27.2695  0.0186\n",
      "     36       \u001b[36m28.5190\u001b[0m       27.2695  0.0118\n",
      "     37       \u001b[36m28.5153\u001b[0m       27.2691  0.0150\n",
      "     38       \u001b[36m28.5117\u001b[0m       27.2687  0.0109\n",
      "     39       \u001b[36m28.5083\u001b[0m       27.2684  0.0111\n",
      "     40       \u001b[36m28.5051\u001b[0m       27.2683  0.0110\n",
      "     41       \u001b[36m28.5021\u001b[0m       27.2680  0.0113\n",
      "     42       \u001b[36m28.4992\u001b[0m       27.2675  0.0111\n",
      "     43       \u001b[36m28.4964\u001b[0m       27.2675  0.0111\n",
      "     44       \u001b[36m28.4939\u001b[0m       27.2669  0.0114\n",
      "     45       \u001b[36m28.4913\u001b[0m       27.2665  0.0111\n",
      "     46       \u001b[36m28.4889\u001b[0m       27.2656  0.0114\n",
      "     47       \u001b[36m28.4865\u001b[0m       27.2648  0.0117\n",
      "     48       \u001b[36m28.4842\u001b[0m       27.2642  0.0111\n",
      "     49       \u001b[36m28.4820\u001b[0m       27.2637  0.0109\n",
      "     50       \u001b[36m28.4799\u001b[0m       27.2629  0.0106\n",
      "     51       \u001b[36m28.4778\u001b[0m       27.2624  0.0111\n",
      "     52       \u001b[36m28.4759\u001b[0m       27.2618  0.0116\n",
      "     53       \u001b[36m28.4740\u001b[0m       27.2608  0.0111\n",
      "     54       \u001b[36m28.4721\u001b[0m       27.2604  0.0108\n",
      "     55       \u001b[36m28.4703\u001b[0m       27.2594  0.0107\n",
      "     56       \u001b[36m28.4686\u001b[0m       27.2587  0.0111\n",
      "     57       \u001b[36m28.4669\u001b[0m       27.2577  0.0109\n",
      "     58       \u001b[36m28.4652\u001b[0m       27.2570  0.0109\n",
      "     59       \u001b[36m28.4637\u001b[0m       27.2563  0.0108\n",
      "     60       \u001b[36m28.4621\u001b[0m       27.2556  0.0112\n",
      "     61       \u001b[36m28.4606\u001b[0m       27.2545  0.0125\n",
      "     62       \u001b[36m28.4592\u001b[0m       27.2540  0.0116\n",
      "     63       \u001b[36m28.4578\u001b[0m       27.2534  0.0125\n",
      "     64       \u001b[36m28.4564\u001b[0m       27.2527  0.0113\n",
      "     65       \u001b[36m28.4551\u001b[0m       27.2518  0.0113\n",
      "     66       \u001b[36m28.4538\u001b[0m       27.2512  0.0128\n",
      "     67       \u001b[36m28.4526\u001b[0m       27.2504  0.0113\n",
      "     68       \u001b[36m28.4514\u001b[0m       27.2494  0.0110\n",
      "     69       \u001b[36m28.4502\u001b[0m       27.2488  0.0112\n",
      "     70       \u001b[36m28.4490\u001b[0m       27.2479  0.0112\n",
      "     71       \u001b[36m28.4479\u001b[0m       27.2472  0.0112\n",
      "     72       \u001b[36m28.4468\u001b[0m       27.2464  0.0112\n",
      "     73       \u001b[36m28.4457\u001b[0m       27.2458  0.0111\n",
      "     74       \u001b[36m28.4447\u001b[0m       27.2450  0.0110\n",
      "     75       \u001b[36m28.4437\u001b[0m       27.2443  0.0107\n",
      "     76       \u001b[36m28.4427\u001b[0m       27.2436  0.0113\n",
      "     77       \u001b[36m28.4417\u001b[0m       27.2432  0.0114\n",
      "     78       \u001b[36m28.4408\u001b[0m       27.2424  0.0110\n",
      "     79       \u001b[36m28.4399\u001b[0m       27.2419  0.0108\n",
      "     80       \u001b[36m28.4390\u001b[0m       27.2413  0.0107\n",
      "     81       \u001b[36m28.4381\u001b[0m       27.2406  0.0107\n",
      "     82       \u001b[36m28.4373\u001b[0m       27.2400  0.0112\n",
      "     83       \u001b[36m28.4364\u001b[0m       27.2394  0.0108\n",
      "     84       \u001b[36m28.4356\u001b[0m       27.2388  0.0108\n",
      "     85       \u001b[36m28.4348\u001b[0m       27.2382  0.0107\n",
      "     86       \u001b[36m28.4340\u001b[0m       27.2377  0.0110\n",
      "     87       \u001b[36m28.4333\u001b[0m       27.2371  0.0110\n",
      "     88       \u001b[36m28.4326\u001b[0m       27.2366  0.0109\n",
      "     89       \u001b[36m28.4318\u001b[0m       27.2361  0.0108\n",
      "     90       \u001b[36m28.4311\u001b[0m       27.2358  0.0108\n",
      "     91       \u001b[36m28.4305\u001b[0m       27.2354  0.0110\n",
      "     92       \u001b[36m28.4298\u001b[0m       27.2348  0.0111\n",
      "     93       \u001b[36m28.4291\u001b[0m       27.2344  0.0112\n",
      "     94       \u001b[36m28.4285\u001b[0m       27.2339  0.0110\n",
      "     95       \u001b[36m28.4278\u001b[0m       27.2333  0.0137\n",
      "     96       \u001b[36m28.4272\u001b[0m       27.2329  0.0113\n",
      "     97       \u001b[36m28.4266\u001b[0m       27.2326  0.0114\n",
      "     98       \u001b[36m28.4260\u001b[0m       27.2321  0.0113\n",
      "     99       \u001b[36m28.4254\u001b[0m       27.2315  0.0124\n",
      "    100       \u001b[36m28.4248\u001b[0m       27.2313  0.0116\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m39.9022\u001b[0m       \u001b[32m33.7820\u001b[0m  0.0122\n",
      "      2       \u001b[36m34.7396\u001b[0m       \u001b[32m30.8259\u001b[0m  0.0118\n",
      "      3       \u001b[36m33.6453\u001b[0m       32.4619  0.0119\n",
      "      4       \u001b[36m33.6237\u001b[0m       31.6118  0.0117\n",
      "      5       \u001b[36m32.9147\u001b[0m       \u001b[32m30.2358\u001b[0m  0.0121\n",
      "      6       \u001b[36m32.8111\u001b[0m       30.3257  0.0116\n",
      "      7       \u001b[36m32.6709\u001b[0m       30.7630  0.0116\n",
      "      8       \u001b[36m32.5670\u001b[0m       30.3823  0.0119\n",
      "      9       \u001b[36m32.4305\u001b[0m       \u001b[32m30.2047\u001b[0m  0.0125\n",
      "     10       \u001b[36m32.4057\u001b[0m       30.2577  0.0120\n",
      "     11       \u001b[36m32.3621\u001b[0m       30.3503  0.0163\n",
      "     12       \u001b[36m32.3246\u001b[0m       \u001b[32m30.2041\u001b[0m  0.0184\n",
      "     13       \u001b[36m32.2908\u001b[0m       \u001b[32m30.1241\u001b[0m  0.0125\n",
      "     14       \u001b[36m32.2813\u001b[0m       30.2713  0.0183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     15       \u001b[36m32.2681\u001b[0m       30.1587  0.0161\n",
      "     16       \u001b[36m32.2463\u001b[0m       30.1378  0.0134\n",
      "     17       \u001b[36m32.2401\u001b[0m       30.1260  0.0140\n",
      "     18       \u001b[36m32.2292\u001b[0m       30.1601  0.0121\n",
      "     19       \u001b[36m32.2177\u001b[0m       \u001b[32m30.0761\u001b[0m  0.0134\n",
      "     20       \u001b[36m32.2089\u001b[0m       30.1491  0.0122\n",
      "     21       \u001b[36m32.2029\u001b[0m       30.0805  0.0125\n",
      "     22       \u001b[36m32.1943\u001b[0m       30.1435  0.0129\n",
      "     23       \u001b[36m32.1882\u001b[0m       \u001b[32m30.0607\u001b[0m  0.0124\n",
      "     24       \u001b[36m32.1834\u001b[0m       30.1560  0.0122\n",
      "     25       \u001b[36m32.1761\u001b[0m       \u001b[32m30.0402\u001b[0m  0.0159\n",
      "     26       \u001b[36m32.1722\u001b[0m       30.1483  0.0124\n",
      "     27       \u001b[36m32.1659\u001b[0m       \u001b[32m30.0329\u001b[0m  0.0120\n",
      "     28       \u001b[36m32.1620\u001b[0m       30.1364  0.0117\n",
      "     29       \u001b[36m32.1555\u001b[0m       \u001b[32m30.0290\u001b[0m  0.0146\n",
      "     30       \u001b[36m32.1527\u001b[0m       30.1276  0.0157\n",
      "     31       \u001b[36m32.1450\u001b[0m       \u001b[32m30.0236\u001b[0m  0.0131\n",
      "     32       \u001b[36m32.1432\u001b[0m       30.1092  0.0138\n",
      "     33       \u001b[36m32.1342\u001b[0m       30.0343  0.0126\n",
      "     34       \u001b[36m32.1328\u001b[0m       30.0825  0.0134\n",
      "     35       \u001b[36m32.1240\u001b[0m       30.0532  0.0126\n",
      "     36       \u001b[36m32.1231\u001b[0m       30.0540  0.0126\n",
      "     37       \u001b[36m32.1166\u001b[0m       30.0794  0.0143\n",
      "     38       \u001b[36m32.1142\u001b[0m       30.0552  0.0133\n",
      "     39       \u001b[36m32.1120\u001b[0m       30.0751  0.0118\n",
      "     40       \u001b[36m32.1055\u001b[0m       30.0938  0.0126\n",
      "     41       32.1058       30.0560  0.0136\n",
      "     42       \u001b[36m32.1053\u001b[0m       30.1096  0.0118\n",
      "     43       \u001b[36m32.0980\u001b[0m       30.1161  0.0134\n",
      "     44       32.1037       30.0514  0.0118\n",
      "     45       32.1153       30.2191  0.0117\n",
      "     46       32.1120       30.0771  0.0128\n",
      "     47       32.1095       30.0447  0.0119\n",
      "     48       32.0999       30.1447  0.0128\n",
      "     49       \u001b[36m32.0947\u001b[0m       30.0441  0.0116\n",
      "     50       \u001b[36m32.0892\u001b[0m       30.1453  0.0116\n",
      "     51       \u001b[36m32.0788\u001b[0m       30.0635  0.0129\n",
      "     52       32.0792       30.1358  0.0145\n",
      "     53       \u001b[36m32.0714\u001b[0m       30.1014  0.0142\n",
      "     54       32.0729       30.1169  0.0145\n",
      "     55       \u001b[36m32.0661\u001b[0m       30.1497  0.0128\n",
      "     56       \u001b[36m32.0658\u001b[0m       30.1090  0.0121\n",
      "     57       \u001b[36m32.0642\u001b[0m       30.1645  0.0139\n",
      "     58       \u001b[36m32.0582\u001b[0m       30.1539  0.0132\n",
      "     59       32.0588       30.1386  0.0144\n",
      "     60       \u001b[36m32.0553\u001b[0m       30.2015  0.0122\n",
      "     61       \u001b[36m32.0525\u001b[0m       30.1658  0.0130\n",
      "     62       32.0535       30.1610  0.0122\n",
      "     63       \u001b[36m32.0484\u001b[0m       30.2444  0.0118\n",
      "     64       \u001b[36m32.0478\u001b[0m       30.1698  0.0129\n",
      "     65       32.0500       30.1786  0.0142\n",
      "     66       \u001b[36m32.0429\u001b[0m       30.3111  0.0150\n",
      "     67       32.0475       30.1583  0.0158\n",
      "     68       32.0549       30.1680  0.0127\n",
      "     69       \u001b[36m32.0424\u001b[0m       30.4838  0.0125\n",
      "     70       32.0680       30.1259  0.0136\n",
      "     71       32.0888       30.2207  0.0119\n",
      "     72       32.0524       30.4483  0.0144\n",
      "     73       32.0885       30.1216  0.0120\n",
      "     74       32.1132       30.4315  0.0121\n",
      "     75       32.0620       30.1291  0.0148\n",
      "     76       32.0605       30.2425  0.0128\n",
      "     77       \u001b[36m32.0342\u001b[0m       30.1236  0.0139\n",
      "     78       32.0396       30.2438  0.0129\n",
      "     79       \u001b[36m32.0268\u001b[0m       30.1700  0.0120\n",
      "     80       32.0311       30.2081  0.0124\n",
      "     81       \u001b[36m32.0201\u001b[0m       30.2007  0.0121\n",
      "     82       32.0220       30.2095  0.0122\n",
      "     83       \u001b[36m32.0169\u001b[0m       30.2125  0.0156\n",
      "     84       \u001b[36m32.0164\u001b[0m       30.2252  0.0159\n",
      "     85       \u001b[36m32.0141\u001b[0m       30.2306  0.0134\n",
      "     86       \u001b[36m32.0121\u001b[0m       30.2324  0.0142\n",
      "     87       \u001b[36m32.0120\u001b[0m       30.2291  0.0150\n",
      "     88       \u001b[36m32.0085\u001b[0m       30.2597  0.0179\n",
      "     89       \u001b[36m32.0083\u001b[0m       30.2345  0.0131\n",
      "     90       \u001b[36m32.0066\u001b[0m       30.2630  0.0121\n",
      "     91       \u001b[36m32.0043\u001b[0m       30.2620  0.0121\n",
      "     92       \u001b[36m32.0039\u001b[0m       30.2593  0.0122\n",
      "     93       \u001b[36m32.0019\u001b[0m       30.2814  0.0118\n",
      "     94       \u001b[36m31.9997\u001b[0m       30.2838  0.0118\n",
      "     95       32.0000       30.2702  0.0117\n",
      "     96       \u001b[36m31.9974\u001b[0m       30.3150  0.0119\n",
      "     97       \u001b[36m31.9956\u001b[0m       30.2903  0.0120\n",
      "     98       31.9963       30.2897  0.0120\n",
      "     99       \u001b[36m31.9927\u001b[0m       30.3419  0.0120\n",
      "    100       \u001b[36m31.9915\u001b[0m       30.3112  0.0135\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m31.6195\u001b[0m       \u001b[32m27.5795\u001b[0m  0.0119\n",
      "      2       \u001b[36m25.8261\u001b[0m       29.5930  0.0117\n",
      "      3       \u001b[36m24.5398\u001b[0m       \u001b[32m26.3720\u001b[0m  0.0113\n",
      "      4       \u001b[36m24.3929\u001b[0m       26.4798  0.0131\n",
      "      5       \u001b[36m23.8410\u001b[0m       27.7265  0.0124\n",
      "      6       \u001b[36m23.6449\u001b[0m       27.6215  0.0132\n",
      "      7       \u001b[36m23.4516\u001b[0m       26.8781  0.0122\n",
      "      8       23.4553       27.0258  0.0128\n",
      "      9       \u001b[36m23.3108\u001b[0m       27.3290  0.0135\n",
      "     10       \u001b[36m23.2500\u001b[0m       26.9505  0.0119\n",
      "     11       \u001b[36m23.2307\u001b[0m       26.8823  0.0131\n",
      "     12       \u001b[36m23.1959\u001b[0m       27.0431  0.0133\n",
      "     13       \u001b[36m23.1587\u001b[0m       26.8992  0.0140\n",
      "     14       \u001b[36m23.1417\u001b[0m       26.7839  0.0131\n",
      "     15       \u001b[36m23.1316\u001b[0m       26.8728  0.0123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     16       \u001b[36m23.1127\u001b[0m       26.8446  0.0150\n",
      "     17       \u001b[36m23.0986\u001b[0m       26.7670  0.0126\n",
      "     18       \u001b[36m23.0925\u001b[0m       26.7977  0.0149\n",
      "     19       \u001b[36m23.0806\u001b[0m       26.8069  0.0123\n",
      "     20       \u001b[36m23.0731\u001b[0m       26.7518  0.0117\n",
      "     21       \u001b[36m23.0669\u001b[0m       26.7701  0.0143\n",
      "     22       \u001b[36m23.0619\u001b[0m       26.7540  0.0122\n",
      "     23       \u001b[36m23.0559\u001b[0m       26.7521  0.0123\n",
      "     24       \u001b[36m23.0537\u001b[0m       26.7345  0.0119\n",
      "     25       \u001b[36m23.0483\u001b[0m       26.7354  0.0132\n",
      "     26       \u001b[36m23.0420\u001b[0m       26.7209  0.0135\n",
      "     27       \u001b[36m23.0360\u001b[0m       26.7054  0.0144\n",
      "     28       \u001b[36m23.0302\u001b[0m       26.7309  0.0147\n",
      "     29       \u001b[36m23.0291\u001b[0m       26.6914  0.0127\n",
      "     30       23.0295       26.7336  0.0122\n",
      "     31       \u001b[36m23.0275\u001b[0m       26.7079  0.0137\n",
      "     32       23.0337       26.6649  0.0143\n",
      "     33       23.0454       26.8268  0.0146\n",
      "     34       23.0540       26.6867  0.0125\n",
      "     35       23.0516       26.6448  0.0123\n",
      "     36       \u001b[36m23.0178\u001b[0m       26.7821  0.0144\n",
      "     37       \u001b[36m23.0083\u001b[0m       26.6857  0.0123\n",
      "     38       \u001b[36m23.0051\u001b[0m       26.7026  0.0133\n",
      "     39       \u001b[36m23.0016\u001b[0m       26.7075  0.0118\n",
      "     40       \u001b[36m22.9972\u001b[0m       26.6883  0.0123\n",
      "     41       \u001b[36m22.9958\u001b[0m       26.7133  0.0123\n",
      "     42       \u001b[36m22.9919\u001b[0m       26.6912  0.0131\n",
      "     43       \u001b[36m22.9903\u001b[0m       26.6918  0.0124\n",
      "     44       \u001b[36m22.9881\u001b[0m       26.6985  0.0118\n",
      "     45       \u001b[36m22.9858\u001b[0m       26.6925  0.0136\n",
      "     46       \u001b[36m22.9842\u001b[0m       26.6899  0.0126\n",
      "     47       \u001b[36m22.9817\u001b[0m       26.6914  0.0122\n",
      "     48       \u001b[36m22.9803\u001b[0m       26.6859  0.0140\n",
      "     49       \u001b[36m22.9785\u001b[0m       26.6985  0.0121\n",
      "     50       \u001b[36m22.9762\u001b[0m       26.6859  0.0140\n",
      "     51       \u001b[36m22.9754\u001b[0m       26.6909  0.0142\n",
      "     52       \u001b[36m22.9727\u001b[0m       26.6979  0.0124\n",
      "     53       \u001b[36m22.9717\u001b[0m       26.6793  0.0126\n",
      "     54       \u001b[36m22.9715\u001b[0m       26.6916  0.0121\n",
      "     55       \u001b[36m22.9677\u001b[0m       26.7065  0.0155\n",
      "     56       22.9713       26.6568  0.0157\n",
      "     57       22.9747       26.7128  0.0135\n",
      "     58       \u001b[36m22.9665\u001b[0m       26.7167  0.0128\n",
      "     59       22.9802       26.6672  0.0128\n",
      "     60       22.9886       26.7188  0.0129\n",
      "     61       22.9716       26.6968  0.0145\n",
      "     62       22.9816       26.6955  0.0121\n",
      "     63       22.9838       26.7490  0.0121\n",
      "     64       22.9720       26.6522  0.0117\n",
      "     65       22.9744       26.7265  0.0118\n",
      "     66       \u001b[36m22.9595\u001b[0m       26.7187  0.0119\n",
      "     67       \u001b[36m22.9579\u001b[0m       26.6908  0.0118\n",
      "     68       \u001b[36m22.9551\u001b[0m       26.7214  0.0119\n",
      "     69       \u001b[36m22.9485\u001b[0m       26.6954  0.0116\n",
      "     70       \u001b[36m22.9480\u001b[0m       26.7160  0.0118\n",
      "     71       \u001b[36m22.9442\u001b[0m       26.7106  0.0120\n",
      "     72       22.9446       26.7042  0.0119\n",
      "     73       \u001b[36m22.9419\u001b[0m       26.7213  0.0117\n",
      "     74       \u001b[36m22.9404\u001b[0m       26.7004  0.0118\n",
      "     75       \u001b[36m22.9403\u001b[0m       26.7225  0.0122\n",
      "     76       \u001b[36m22.9367\u001b[0m       26.7161  0.0113\n",
      "     77       \u001b[36m22.9365\u001b[0m       26.7115  0.0113\n",
      "     78       \u001b[36m22.9356\u001b[0m       26.7307  0.0117\n",
      "     79       \u001b[36m22.9326\u001b[0m       26.7126  0.0121\n",
      "     80       \u001b[36m22.9325\u001b[0m       26.7204  0.0117\n",
      "     81       \u001b[36m22.9324\u001b[0m       26.7273  0.0119\n",
      "     82       \u001b[36m22.9295\u001b[0m       26.7255  0.0117\n",
      "     83       22.9312       26.7146  0.0124\n",
      "     84       22.9353       26.7389  0.0116\n",
      "     85       22.9298       26.7379  0.0117\n",
      "     86       22.9331       26.7075  0.0114\n",
      "     87       22.9381       26.7614  0.0114\n",
      "     88       22.9423       26.7487  0.0118\n",
      "     89       22.9454       26.6849  0.0114\n",
      "     90       22.9606       26.8194  0.0116\n",
      "     91       22.9669       26.7286  0.0112\n",
      "     92       22.9655       26.6728  0.0115\n",
      "     93       22.9750       26.8562  0.0119\n",
      "     94       22.9491       26.6744  0.0114\n",
      "     95       22.9509       26.7594  0.0114\n",
      "     96       22.9366       26.7432  0.0114\n",
      "     97       \u001b[36m22.9224\u001b[0m       26.6820  0.0113\n",
      "     98       22.9238       26.7787  0.0117\n",
      "     99       \u001b[36m22.9136\u001b[0m       26.7089  0.0116\n",
      "    100       22.9163       26.7589  0.0121\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m38.0221\u001b[0m       \u001b[32m26.9927\u001b[0m  0.0128\n",
      "      2       \u001b[36m31.3160\u001b[0m       30.2949  0.0124\n",
      "      3       \u001b[36m30.0704\u001b[0m       \u001b[32m26.5319\u001b[0m  0.0125\n",
      "      4       \u001b[36m29.6389\u001b[0m       26.7921  0.0124\n",
      "      5       \u001b[36m29.1021\u001b[0m       28.8282  0.0118\n",
      "      6       29.1351       27.5058  0.0118\n",
      "      7       \u001b[36m28.7666\u001b[0m       27.1382  0.0115\n",
      "      8       \u001b[36m28.6689\u001b[0m       27.7537  0.0115\n",
      "      9       28.7066       27.5137  0.0119\n",
      "     10       \u001b[36m28.5707\u001b[0m       27.2584  0.0116\n",
      "     11       \u001b[36m28.5244\u001b[0m       27.6305  0.0118\n",
      "     12       28.5405       27.4635  0.0119\n",
      "     13       \u001b[36m28.4852\u001b[0m       27.2686  0.0119\n",
      "     14       \u001b[36m28.4724\u001b[0m       27.4764  0.0113\n",
      "     15       28.4731       27.3207  0.0118\n",
      "     16       \u001b[36m28.4512\u001b[0m       27.2723  0.0116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m28.4450\u001b[0m       27.3800  0.0120\n",
      "     18       \u001b[36m28.4434\u001b[0m       27.2423  0.0118\n",
      "     19       \u001b[36m28.4315\u001b[0m       27.2955  0.0116\n",
      "     20       \u001b[36m28.4314\u001b[0m       27.2976  0.0115\n",
      "     21       28.4371       27.2475  0.0115\n",
      "     22       28.4413       27.3500  0.0111\n",
      "     23       28.4545       27.2047  0.0122\n",
      "     24       28.4370       27.3044  0.0119\n",
      "     25       \u001b[36m28.4140\u001b[0m       27.2260  0.0120\n",
      "     26       28.4180       27.2157  0.0118\n",
      "     27       \u001b[36m28.4022\u001b[0m       27.2871  0.0116\n",
      "     28       28.4037       27.2173  0.0126\n",
      "     29       \u001b[36m28.3961\u001b[0m       27.2522  0.0125\n",
      "     30       28.3993       27.2169  0.0125\n",
      "     31       \u001b[36m28.3933\u001b[0m       27.2432  0.0119\n",
      "     32       \u001b[36m28.3919\u001b[0m       27.2282  0.0118\n",
      "     33       \u001b[36m28.3911\u001b[0m       27.2092  0.0118\n",
      "     34       \u001b[36m28.3880\u001b[0m       27.2358  0.0196\n",
      "     35       \u001b[36m28.3875\u001b[0m       27.2135  0.0240\n",
      "     36       \u001b[36m28.3855\u001b[0m       27.2176  0.0173\n",
      "     37       \u001b[36m28.3843\u001b[0m       27.2217  0.0154\n",
      "     38       \u001b[36m28.3832\u001b[0m       27.2127  0.0193\n",
      "     39       \u001b[36m28.3822\u001b[0m       27.2192  0.0173\n",
      "     40       \u001b[36m28.3799\u001b[0m       27.2198  0.0126\n",
      "     41       28.3807       27.2170  0.0119\n",
      "     42       \u001b[36m28.3784\u001b[0m       27.2209  0.0118\n",
      "     43       \u001b[36m28.3769\u001b[0m       27.2270  0.0115\n",
      "     44       28.3787       27.2165  0.0119\n",
      "     45       \u001b[36m28.3761\u001b[0m       27.2259  0.0117\n",
      "     46       \u001b[36m28.3744\u001b[0m       27.2360  0.0118\n",
      "     47       28.3798       27.2160  0.0118\n",
      "     48       28.3789       27.2302  0.0117\n",
      "     49       28.3773       27.2466  0.0119\n",
      "     50       28.3910       27.2261  0.0118\n",
      "     51       28.4062       27.2182  0.0117\n",
      "     52       28.4033       27.2665  0.0128\n",
      "     53       28.4217       27.1910  0.0124\n",
      "     54       28.4145       27.2679  0.0120\n",
      "     55       28.4080       27.1597  0.0127\n",
      "     56       28.4018       27.1783  0.0128\n",
      "     57       \u001b[36m28.3667\u001b[0m       27.2573  0.0118\n",
      "     58       28.3846       27.1202  0.0117\n",
      "     59       \u001b[36m28.3665\u001b[0m       27.2482  0.0118\n",
      "     60       28.3697       27.1762  0.0122\n",
      "     61       28.3684       27.1990  0.0119\n",
      "     62       \u001b[36m28.3645\u001b[0m       27.2072  0.0118\n",
      "     63       28.3659       27.1769  0.0116\n",
      "     64       \u001b[36m28.3622\u001b[0m       27.2138  0.0115\n",
      "     65       \u001b[36m28.3621\u001b[0m       27.1899  0.0118\n",
      "     66       28.3626       27.1955  0.0117\n",
      "     67       \u001b[36m28.3584\u001b[0m       27.2123  0.0132\n",
      "     68       28.3612       27.1799  0.0120\n",
      "     69       \u001b[36m28.3581\u001b[0m       27.2159  0.0117\n",
      "     70       \u001b[36m28.3579\u001b[0m       27.1880  0.0116\n",
      "     71       28.3587       27.2012  0.0116\n",
      "     72       \u001b[36m28.3558\u001b[0m       27.2073  0.0116\n",
      "     73       28.3567       27.1924  0.0115\n",
      "     74       28.3564       27.2059  0.0119\n",
      "     75       \u001b[36m28.3540\u001b[0m       27.2017  0.0116\n",
      "     76       28.3561       27.1855  0.0118\n",
      "     77       \u001b[36m28.3535\u001b[0m       27.2069  0.0115\n",
      "     78       \u001b[36m28.3531\u001b[0m       27.1880  0.0117\n",
      "     79       \u001b[36m28.3525\u001b[0m       27.1990  0.0116\n",
      "     80       28.3529       27.1791  0.0116\n",
      "     81       \u001b[36m28.3510\u001b[0m       27.1968  0.0114\n",
      "     82       \u001b[36m28.3505\u001b[0m       27.1863  0.0119\n",
      "     83       28.3520       27.1771  0.0118\n",
      "     84       \u001b[36m28.3497\u001b[0m       27.1850  0.0121\n",
      "     85       \u001b[36m28.3486\u001b[0m       27.1933  0.0125\n",
      "     86       28.3506       27.1692  0.0124\n",
      "     87       28.3516       27.1736  0.0119\n",
      "     88       \u001b[36m28.3474\u001b[0m       27.1858  0.0115\n",
      "     89       28.3495       27.2009  0.0122\n",
      "     90       28.3591       27.1348  0.0121\n",
      "     91       28.3583       27.1921  0.0117\n",
      "     92       28.3593       27.2126  0.0115\n",
      "     93       28.3754       27.1921  0.0112\n",
      "     94       28.4233       27.0612  0.0118\n",
      "     95       28.3933       27.3601  0.0115\n",
      "     96       28.4245       27.0405  0.0114\n",
      "     97       28.4201       27.1299  0.0114\n",
      "     98       28.3713       27.2699  0.0115\n",
      "     99       28.3890       26.9130  0.0126\n",
      "    100       28.3590       27.4054  0.0123\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m43.1922\u001b[0m       \u001b[32m43.5700\u001b[0m  0.0110\n",
      "      2       \u001b[36m41.0572\u001b[0m       \u001b[32m40.9357\u001b[0m  0.0111\n",
      "      3       \u001b[36m39.0262\u001b[0m       \u001b[32m38.3937\u001b[0m  0.0110\n",
      "      4       \u001b[36m37.1576\u001b[0m       \u001b[32m36.1195\u001b[0m  0.0114\n",
      "      5       \u001b[36m35.6050\u001b[0m       \u001b[32m34.2252\u001b[0m  0.0109\n",
      "      6       \u001b[36m34.4482\u001b[0m       \u001b[32m32.7632\u001b[0m  0.0109\n",
      "      7       \u001b[36m33.6890\u001b[0m       \u001b[32m31.7666\u001b[0m  0.0109\n",
      "      8       \u001b[36m33.2558\u001b[0m       \u001b[32m31.1634\u001b[0m  0.0109\n",
      "      9       \u001b[36m33.0267\u001b[0m       \u001b[32m30.8213\u001b[0m  0.0111\n",
      "     10       \u001b[36m32.9011\u001b[0m       \u001b[32m30.6264\u001b[0m  0.0115\n",
      "     11       \u001b[36m32.8209\u001b[0m       \u001b[32m30.5093\u001b[0m  0.0144\n",
      "     12       \u001b[36m32.7628\u001b[0m       \u001b[32m30.4324\u001b[0m  0.0121\n",
      "     13       \u001b[36m32.7166\u001b[0m       \u001b[32m30.3777\u001b[0m  0.0113\n",
      "     14       \u001b[36m32.6781\u001b[0m       \u001b[32m30.3359\u001b[0m  0.0122\n",
      "     15       \u001b[36m32.6455\u001b[0m       \u001b[32m30.3019\u001b[0m  0.0120\n",
      "     16       \u001b[36m32.6176\u001b[0m       \u001b[32m30.2734\u001b[0m  0.0114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m32.5927\u001b[0m       \u001b[32m30.2489\u001b[0m  0.0147\n",
      "     18       \u001b[36m32.5702\u001b[0m       \u001b[32m30.2269\u001b[0m  0.0123\n",
      "     19       \u001b[36m32.5500\u001b[0m       \u001b[32m30.2072\u001b[0m  0.0114\n",
      "     20       \u001b[36m32.5316\u001b[0m       \u001b[32m30.1887\u001b[0m  0.0114\n",
      "     21       \u001b[36m32.5149\u001b[0m       \u001b[32m30.1719\u001b[0m  0.0115\n",
      "     22       \u001b[36m32.4998\u001b[0m       \u001b[32m30.1564\u001b[0m  0.0110\n",
      "     23       \u001b[36m32.4857\u001b[0m       \u001b[32m30.1419\u001b[0m  0.0115\n",
      "     24       \u001b[36m32.4728\u001b[0m       \u001b[32m30.1284\u001b[0m  0.0112\n",
      "     25       \u001b[36m32.4607\u001b[0m       \u001b[32m30.1157\u001b[0m  0.0110\n",
      "     26       \u001b[36m32.4498\u001b[0m       \u001b[32m30.1040\u001b[0m  0.0108\n",
      "     27       \u001b[36m32.4393\u001b[0m       \u001b[32m30.0928\u001b[0m  0.0108\n",
      "     28       \u001b[36m32.4297\u001b[0m       \u001b[32m30.0824\u001b[0m  0.0111\n",
      "     29       \u001b[36m32.4207\u001b[0m       \u001b[32m30.0729\u001b[0m  0.0114\n",
      "     30       \u001b[36m32.4124\u001b[0m       \u001b[32m30.0637\u001b[0m  0.0110\n",
      "     31       \u001b[36m32.4044\u001b[0m       \u001b[32m30.0551\u001b[0m  0.0109\n",
      "     32       \u001b[36m32.3970\u001b[0m       \u001b[32m30.0469\u001b[0m  0.0114\n",
      "     33       \u001b[36m32.3900\u001b[0m       \u001b[32m30.0392\u001b[0m  0.0115\n",
      "     34       \u001b[36m32.3834\u001b[0m       \u001b[32m30.0318\u001b[0m  0.0130\n",
      "     35       \u001b[36m32.3771\u001b[0m       \u001b[32m30.0247\u001b[0m  0.0119\n",
      "     36       \u001b[36m32.3713\u001b[0m       \u001b[32m30.0182\u001b[0m  0.0109\n",
      "     37       \u001b[36m32.3658\u001b[0m       \u001b[32m30.0119\u001b[0m  0.0109\n",
      "     38       \u001b[36m32.3606\u001b[0m       \u001b[32m30.0061\u001b[0m  0.0115\n",
      "     39       \u001b[36m32.3557\u001b[0m       \u001b[32m30.0007\u001b[0m  0.0117\n",
      "     40       \u001b[36m32.3510\u001b[0m       \u001b[32m29.9954\u001b[0m  0.0114\n",
      "     41       \u001b[36m32.3466\u001b[0m       \u001b[32m29.9905\u001b[0m  0.0110\n",
      "     42       \u001b[36m32.3423\u001b[0m       \u001b[32m29.9858\u001b[0m  0.0110\n",
      "     43       \u001b[36m32.3382\u001b[0m       \u001b[32m29.9814\u001b[0m  0.0113\n",
      "     44       \u001b[36m32.3342\u001b[0m       \u001b[32m29.9773\u001b[0m  0.0114\n",
      "     45       \u001b[36m32.3304\u001b[0m       \u001b[32m29.9732\u001b[0m  0.0114\n",
      "     46       \u001b[36m32.3268\u001b[0m       \u001b[32m29.9695\u001b[0m  0.0110\n",
      "     47       \u001b[36m32.3233\u001b[0m       \u001b[32m29.9658\u001b[0m  0.0108\n",
      "     48       \u001b[36m32.3200\u001b[0m       \u001b[32m29.9624\u001b[0m  0.0109\n",
      "     49       \u001b[36m32.3167\u001b[0m       \u001b[32m29.9592\u001b[0m  0.0110\n",
      "     50       \u001b[36m32.3136\u001b[0m       \u001b[32m29.9562\u001b[0m  0.0111\n",
      "     51       \u001b[36m32.3106\u001b[0m       \u001b[32m29.9535\u001b[0m  0.0107\n",
      "     52       \u001b[36m32.3078\u001b[0m       \u001b[32m29.9509\u001b[0m  0.0105\n",
      "     53       \u001b[36m32.3050\u001b[0m       \u001b[32m29.9485\u001b[0m  0.0109\n",
      "     54       \u001b[36m32.3023\u001b[0m       \u001b[32m29.9464\u001b[0m  0.0115\n",
      "     55       \u001b[36m32.2997\u001b[0m       \u001b[32m29.9439\u001b[0m  0.0108\n",
      "     56       \u001b[36m32.2971\u001b[0m       \u001b[32m29.9417\u001b[0m  0.0109\n",
      "     57       \u001b[36m32.2947\u001b[0m       \u001b[32m29.9396\u001b[0m  0.0114\n",
      "     58       \u001b[36m32.2923\u001b[0m       \u001b[32m29.9376\u001b[0m  0.0119\n",
      "     59       \u001b[36m32.2901\u001b[0m       \u001b[32m29.9357\u001b[0m  0.0117\n",
      "     60       \u001b[36m32.2879\u001b[0m       \u001b[32m29.9332\u001b[0m  0.0117\n",
      "     61       \u001b[36m32.2857\u001b[0m       \u001b[32m29.9313\u001b[0m  0.0111\n",
      "     62       \u001b[36m32.2837\u001b[0m       \u001b[32m29.9294\u001b[0m  0.0108\n",
      "     63       \u001b[36m32.2817\u001b[0m       \u001b[32m29.9277\u001b[0m  0.0113\n",
      "     64       \u001b[36m32.2798\u001b[0m       \u001b[32m29.9258\u001b[0m  0.0115\n",
      "     65       \u001b[36m32.2779\u001b[0m       \u001b[32m29.9239\u001b[0m  0.0113\n",
      "     66       \u001b[36m32.2761\u001b[0m       \u001b[32m29.9226\u001b[0m  0.0110\n",
      "     67       \u001b[36m32.2743\u001b[0m       \u001b[32m29.9208\u001b[0m  0.0109\n",
      "     68       \u001b[36m32.2726\u001b[0m       \u001b[32m29.9196\u001b[0m  0.0112\n",
      "     69       \u001b[36m32.2709\u001b[0m       \u001b[32m29.9178\u001b[0m  0.0111\n",
      "     70       \u001b[36m32.2693\u001b[0m       \u001b[32m29.9169\u001b[0m  0.0116\n",
      "     71       \u001b[36m32.2677\u001b[0m       \u001b[32m29.9150\u001b[0m  0.0112\n",
      "     72       \u001b[36m32.2661\u001b[0m       \u001b[32m29.9140\u001b[0m  0.0111\n",
      "     73       \u001b[36m32.2646\u001b[0m       \u001b[32m29.9127\u001b[0m  0.0111\n",
      "     74       \u001b[36m32.2631\u001b[0m       \u001b[32m29.9115\u001b[0m  0.0111\n",
      "     75       \u001b[36m32.2617\u001b[0m       \u001b[32m29.9103\u001b[0m  0.0110\n",
      "     76       \u001b[36m32.2603\u001b[0m       \u001b[32m29.9093\u001b[0m  0.0106\n",
      "     77       \u001b[36m32.2589\u001b[0m       \u001b[32m29.9079\u001b[0m  0.0105\n",
      "     78       \u001b[36m32.2576\u001b[0m       \u001b[32m29.9071\u001b[0m  0.0109\n",
      "     79       \u001b[36m32.2563\u001b[0m       \u001b[32m29.9059\u001b[0m  0.0109\n",
      "     80       \u001b[36m32.2550\u001b[0m       \u001b[32m29.9052\u001b[0m  0.0112\n",
      "     81       \u001b[36m32.2538\u001b[0m       \u001b[32m29.9038\u001b[0m  0.0114\n",
      "     82       \u001b[36m32.2525\u001b[0m       \u001b[32m29.9029\u001b[0m  0.0112\n",
      "     83       \u001b[36m32.2513\u001b[0m       \u001b[32m29.9020\u001b[0m  0.0117\n",
      "     84       \u001b[36m32.2502\u001b[0m       \u001b[32m29.9009\u001b[0m  0.0120\n",
      "     85       \u001b[36m32.2490\u001b[0m       \u001b[32m29.8999\u001b[0m  0.0116\n",
      "     86       \u001b[36m32.2479\u001b[0m       \u001b[32m29.8993\u001b[0m  0.0113\n",
      "     87       \u001b[36m32.2468\u001b[0m       \u001b[32m29.8979\u001b[0m  0.0112\n",
      "     88       \u001b[36m32.2458\u001b[0m       \u001b[32m29.8973\u001b[0m  0.0121\n",
      "     89       \u001b[36m32.2448\u001b[0m       \u001b[32m29.8963\u001b[0m  0.0116\n",
      "     90       \u001b[36m32.2437\u001b[0m       \u001b[32m29.8953\u001b[0m  0.0115\n",
      "     91       \u001b[36m32.2427\u001b[0m       \u001b[32m29.8946\u001b[0m  0.0113\n",
      "     92       \u001b[36m32.2418\u001b[0m       \u001b[32m29.8935\u001b[0m  0.0113\n",
      "     93       \u001b[36m32.2408\u001b[0m       \u001b[32m29.8929\u001b[0m  0.0118\n",
      "     94       \u001b[36m32.2399\u001b[0m       \u001b[32m29.8917\u001b[0m  0.0161\n",
      "     95       \u001b[36m32.2390\u001b[0m       \u001b[32m29.8912\u001b[0m  0.0122\n",
      "     96       \u001b[36m32.2381\u001b[0m       \u001b[32m29.8905\u001b[0m  0.0111\n",
      "     97       \u001b[36m32.2372\u001b[0m       \u001b[32m29.8893\u001b[0m  0.0111\n",
      "     98       \u001b[36m32.2364\u001b[0m       \u001b[32m29.8889\u001b[0m  0.0116\n",
      "     99       \u001b[36m32.2355\u001b[0m       \u001b[32m29.8882\u001b[0m  0.0138\n",
      "    100       \u001b[36m32.2347\u001b[0m       \u001b[32m29.8871\u001b[0m  0.0113\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m33.8371\u001b[0m       \u001b[32m31.3971\u001b[0m  0.0115\n",
      "      2       \u001b[36m31.6363\u001b[0m       \u001b[32m29.7931\u001b[0m  0.0110\n",
      "      3       \u001b[36m29.5406\u001b[0m       \u001b[32m28.3429\u001b[0m  0.0114\n",
      "      4       \u001b[36m27.5812\u001b[0m       \u001b[32m27.2546\u001b[0m  0.0111\n",
      "      5       \u001b[36m25.9943\u001b[0m       \u001b[32m26.6935\u001b[0m  0.0110\n",
      "      6       \u001b[36m24.9424\u001b[0m       \u001b[32m26.5690\u001b[0m  0.0108\n",
      "      7       \u001b[36m24.3593\u001b[0m       26.6330  0.0110\n",
      "      8       \u001b[36m24.0586\u001b[0m       26.7109  0.0110\n",
      "      9       \u001b[36m23.8931\u001b[0m       26.7504  0.0108\n",
      "     10       \u001b[36m23.7874\u001b[0m       26.7563  0.0107\n",
      "     11       \u001b[36m23.7106\u001b[0m       26.7440  0.0108\n",
      "     12       \u001b[36m23.6502\u001b[0m       26.7243  0.0114\n",
      "     13       \u001b[36m23.6006\u001b[0m       26.7027  0.0111\n",
      "     14       \u001b[36m23.5585\u001b[0m       26.6817  0.0108\n",
      "     15       \u001b[36m23.5220\u001b[0m       26.6623  0.0109\n",
      "     16       \u001b[36m23.4901\u001b[0m       26.6446  0.0110\n",
      "     17       \u001b[36m23.4616\u001b[0m       26.6289  0.0114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     18       \u001b[36m23.4363\u001b[0m       26.6150  0.0118\n",
      "     19       \u001b[36m23.4136\u001b[0m       26.6030  0.0124\n",
      "     20       \u001b[36m23.3932\u001b[0m       26.5921  0.0114\n",
      "     21       \u001b[36m23.3746\u001b[0m       26.5824  0.0108\n",
      "     22       \u001b[36m23.3577\u001b[0m       26.5741  0.0106\n",
      "     23       \u001b[36m23.3422\u001b[0m       \u001b[32m26.5669\u001b[0m  0.0115\n",
      "     24       \u001b[36m23.3280\u001b[0m       \u001b[32m26.5603\u001b[0m  0.0114\n",
      "     25       \u001b[36m23.3149\u001b[0m       \u001b[32m26.5544\u001b[0m  0.0106\n",
      "     26       \u001b[36m23.3027\u001b[0m       \u001b[32m26.5491\u001b[0m  0.0109\n",
      "     27       \u001b[36m23.2915\u001b[0m       \u001b[32m26.5446\u001b[0m  0.0105\n",
      "     28       \u001b[36m23.2810\u001b[0m       \u001b[32m26.5406\u001b[0m  0.0110\n",
      "     29       \u001b[36m23.2713\u001b[0m       \u001b[32m26.5370\u001b[0m  0.0108\n",
      "     30       \u001b[36m23.2622\u001b[0m       \u001b[32m26.5342\u001b[0m  0.0108\n",
      "     31       \u001b[36m23.2538\u001b[0m       \u001b[32m26.5314\u001b[0m  0.0105\n",
      "     32       \u001b[36m23.2459\u001b[0m       \u001b[32m26.5290\u001b[0m  0.0119\n",
      "     33       \u001b[36m23.2385\u001b[0m       \u001b[32m26.5269\u001b[0m  0.0118\n",
      "     34       \u001b[36m23.2316\u001b[0m       \u001b[32m26.5251\u001b[0m  0.0109\n",
      "     35       \u001b[36m23.2250\u001b[0m       \u001b[32m26.5233\u001b[0m  0.0106\n",
      "     36       \u001b[36m23.2188\u001b[0m       \u001b[32m26.5217\u001b[0m  0.0108\n",
      "     37       \u001b[36m23.2129\u001b[0m       \u001b[32m26.5204\u001b[0m  0.0107\n",
      "     38       \u001b[36m23.2073\u001b[0m       \u001b[32m26.5192\u001b[0m  0.0108\n",
      "     39       \u001b[36m23.2020\u001b[0m       \u001b[32m26.5181\u001b[0m  0.0112\n",
      "     40       \u001b[36m23.1970\u001b[0m       \u001b[32m26.5171\u001b[0m  0.0111\n",
      "     41       \u001b[36m23.1922\u001b[0m       \u001b[32m26.5163\u001b[0m  0.0108\n",
      "     42       \u001b[36m23.1876\u001b[0m       \u001b[32m26.5156\u001b[0m  0.0109\n",
      "     43       \u001b[36m23.1833\u001b[0m       \u001b[32m26.5149\u001b[0m  0.0110\n",
      "     44       \u001b[36m23.1791\u001b[0m       \u001b[32m26.5144\u001b[0m  0.0110\n",
      "     45       \u001b[36m23.1751\u001b[0m       \u001b[32m26.5140\u001b[0m  0.0110\n",
      "     46       \u001b[36m23.1713\u001b[0m       \u001b[32m26.5135\u001b[0m  0.0110\n",
      "     47       \u001b[36m23.1677\u001b[0m       \u001b[32m26.5132\u001b[0m  0.0107\n",
      "     48       \u001b[36m23.1642\u001b[0m       \u001b[32m26.5128\u001b[0m  0.0110\n",
      "     49       \u001b[36m23.1609\u001b[0m       \u001b[32m26.5126\u001b[0m  0.0111\n",
      "     50       \u001b[36m23.1577\u001b[0m       \u001b[32m26.5123\u001b[0m  0.0117\n",
      "     51       \u001b[36m23.1547\u001b[0m       \u001b[32m26.5121\u001b[0m  0.0114\n",
      "     52       \u001b[36m23.1517\u001b[0m       \u001b[32m26.5118\u001b[0m  0.0113\n",
      "     53       \u001b[36m23.1489\u001b[0m       \u001b[32m26.5115\u001b[0m  0.0116\n",
      "     54       \u001b[36m23.1461\u001b[0m       \u001b[32m26.5113\u001b[0m  0.0112\n",
      "     55       \u001b[36m23.1435\u001b[0m       \u001b[32m26.5111\u001b[0m  0.0112\n",
      "     56       \u001b[36m23.1410\u001b[0m       \u001b[32m26.5108\u001b[0m  0.0113\n",
      "     57       \u001b[36m23.1385\u001b[0m       \u001b[32m26.5107\u001b[0m  0.0110\n",
      "     58       \u001b[36m23.1362\u001b[0m       \u001b[32m26.5105\u001b[0m  0.0112\n",
      "     59       \u001b[36m23.1339\u001b[0m       \u001b[32m26.5104\u001b[0m  0.0113\n",
      "     60       \u001b[36m23.1317\u001b[0m       \u001b[32m26.5102\u001b[0m  0.0113\n",
      "     61       \u001b[36m23.1296\u001b[0m       \u001b[32m26.5101\u001b[0m  0.0109\n",
      "     62       \u001b[36m23.1276\u001b[0m       26.5101  0.0108\n",
      "     63       \u001b[36m23.1256\u001b[0m       \u001b[32m26.5100\u001b[0m  0.0110\n",
      "     64       \u001b[36m23.1237\u001b[0m       26.5100  0.0112\n",
      "     65       \u001b[36m23.1218\u001b[0m       26.5100  0.0107\n",
      "     66       \u001b[36m23.1201\u001b[0m       \u001b[32m26.5100\u001b[0m  0.0113\n",
      "     67       \u001b[36m23.1183\u001b[0m       \u001b[32m26.5099\u001b[0m  0.0111\n",
      "     68       \u001b[36m23.1166\u001b[0m       \u001b[32m26.5098\u001b[0m  0.0110\n",
      "     69       \u001b[36m23.1150\u001b[0m       \u001b[32m26.5097\u001b[0m  0.0111\n",
      "     70       \u001b[36m23.1134\u001b[0m       \u001b[32m26.5096\u001b[0m  0.0112\n",
      "     71       \u001b[36m23.1118\u001b[0m       \u001b[32m26.5095\u001b[0m  0.0118\n",
      "     72       \u001b[36m23.1103\u001b[0m       \u001b[32m26.5093\u001b[0m  0.0108\n",
      "     73       \u001b[36m23.1089\u001b[0m       \u001b[32m26.5092\u001b[0m  0.0110\n",
      "     74       \u001b[36m23.1074\u001b[0m       \u001b[32m26.5090\u001b[0m  0.0110\n",
      "     75       \u001b[36m23.1061\u001b[0m       \u001b[32m26.5089\u001b[0m  0.0109\n",
      "     76       \u001b[36m23.1047\u001b[0m       \u001b[32m26.5087\u001b[0m  0.0108\n",
      "     77       \u001b[36m23.1035\u001b[0m       \u001b[32m26.5085\u001b[0m  0.0123\n",
      "     78       \u001b[36m23.1022\u001b[0m       \u001b[32m26.5082\u001b[0m  0.0141\n",
      "     79       \u001b[36m23.1010\u001b[0m       \u001b[32m26.5080\u001b[0m  0.0143\n",
      "     80       \u001b[36m23.0997\u001b[0m       \u001b[32m26.5077\u001b[0m  0.0130\n",
      "     81       \u001b[36m23.0986\u001b[0m       \u001b[32m26.5074\u001b[0m  0.0131\n",
      "     82       \u001b[36m23.0974\u001b[0m       \u001b[32m26.5071\u001b[0m  0.0140\n",
      "     83       \u001b[36m23.0963\u001b[0m       \u001b[32m26.5069\u001b[0m  0.0126\n",
      "     84       \u001b[36m23.0952\u001b[0m       \u001b[32m26.5067\u001b[0m  0.0164\n",
      "     85       \u001b[36m23.0942\u001b[0m       \u001b[32m26.5064\u001b[0m  0.0118\n",
      "     86       \u001b[36m23.0931\u001b[0m       \u001b[32m26.5059\u001b[0m  0.0116\n",
      "     87       \u001b[36m23.0921\u001b[0m       \u001b[32m26.5056\u001b[0m  0.0117\n",
      "     88       \u001b[36m23.0911\u001b[0m       \u001b[32m26.5053\u001b[0m  0.0114\n",
      "     89       \u001b[36m23.0901\u001b[0m       \u001b[32m26.5050\u001b[0m  0.0121\n",
      "     90       \u001b[36m23.0892\u001b[0m       \u001b[32m26.5047\u001b[0m  0.0138\n",
      "     91       \u001b[36m23.0883\u001b[0m       \u001b[32m26.5041\u001b[0m  0.0111\n",
      "     92       \u001b[36m23.0874\u001b[0m       \u001b[32m26.5038\u001b[0m  0.0112\n",
      "     93       \u001b[36m23.0865\u001b[0m       \u001b[32m26.5035\u001b[0m  0.0120\n",
      "     94       \u001b[36m23.0856\u001b[0m       \u001b[32m26.5032\u001b[0m  0.0139\n",
      "     95       \u001b[36m23.0848\u001b[0m       \u001b[32m26.5029\u001b[0m  0.0113\n",
      "     96       \u001b[36m23.0840\u001b[0m       \u001b[32m26.5024\u001b[0m  0.0123\n",
      "     97       \u001b[36m23.0831\u001b[0m       \u001b[32m26.5021\u001b[0m  0.0145\n",
      "     98       \u001b[36m23.0823\u001b[0m       \u001b[32m26.5018\u001b[0m  0.0117\n",
      "     99       \u001b[36m23.0815\u001b[0m       \u001b[32m26.5014\u001b[0m  0.0132\n",
      "    100       \u001b[36m23.0808\u001b[0m       \u001b[32m26.5009\u001b[0m  0.0119\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m38.9976\u001b[0m       \u001b[32m30.2683\u001b[0m  0.0219\n",
      "      2       \u001b[36m36.7342\u001b[0m       \u001b[32m28.8089\u001b[0m  0.0124\n",
      "      3       \u001b[36m34.5355\u001b[0m       \u001b[32m27.5530\u001b[0m  0.0112\n",
      "      4       \u001b[36m32.5510\u001b[0m       \u001b[32m26.6980\u001b[0m  0.0117\n",
      "      5       \u001b[36m30.9566\u001b[0m       \u001b[32m26.3594\u001b[0m  0.0113\n",
      "      6       \u001b[36m29.8587\u001b[0m       26.4792  0.0116\n",
      "      7       \u001b[36m29.2578\u001b[0m       26.7953  0.0120\n",
      "      8       \u001b[36m28.9954\u001b[0m       27.0765  0.0121\n",
      "      9       \u001b[36m28.8890\u001b[0m       27.2431  0.0130\n",
      "     10       \u001b[36m28.8349\u001b[0m       27.3244  0.0120\n",
      "     11       \u001b[36m28.7972\u001b[0m       27.3554  0.0115\n",
      "     12       \u001b[36m28.7657\u001b[0m       27.3627  0.0130\n",
      "     13       \u001b[36m28.7384\u001b[0m       27.3586  0.0126\n",
      "     14       \u001b[36m28.7139\u001b[0m       27.3494  0.0115\n",
      "     15       \u001b[36m28.6919\u001b[0m       27.3399  0.0124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     16       \u001b[36m28.6723\u001b[0m       27.3305  0.0122\n",
      "     17       \u001b[36m28.6548\u001b[0m       27.3220  0.0130\n",
      "     18       \u001b[36m28.6390\u001b[0m       27.3144  0.0123\n",
      "     19       \u001b[36m28.6247\u001b[0m       27.3077  0.0119\n",
      "     20       \u001b[36m28.6118\u001b[0m       27.3019  0.0115\n",
      "     21       \u001b[36m28.6002\u001b[0m       27.2989  0.0113\n",
      "     22       \u001b[36m28.5898\u001b[0m       27.2959  0.0119\n",
      "     23       \u001b[36m28.5802\u001b[0m       27.2927  0.0126\n",
      "     24       \u001b[36m28.5715\u001b[0m       27.2897  0.0120\n",
      "     25       \u001b[36m28.5634\u001b[0m       27.2868  0.0117\n",
      "     26       \u001b[36m28.5559\u001b[0m       27.2843  0.0117\n",
      "     27       \u001b[36m28.5490\u001b[0m       27.2818  0.0117\n",
      "     28       \u001b[36m28.5426\u001b[0m       27.2797  0.0123\n",
      "     29       \u001b[36m28.5366\u001b[0m       27.2777  0.0117\n",
      "     30       \u001b[36m28.5311\u001b[0m       27.2760  0.0119\n",
      "     31       \u001b[36m28.5259\u001b[0m       27.2743  0.0121\n",
      "     32       \u001b[36m28.5211\u001b[0m       27.2727  0.0123\n",
      "     33       \u001b[36m28.5165\u001b[0m       27.2710  0.0115\n",
      "     34       \u001b[36m28.5123\u001b[0m       27.2692  0.0133\n",
      "     35       \u001b[36m28.5083\u001b[0m       27.2678  0.0140\n",
      "     36       \u001b[36m28.5045\u001b[0m       27.2668  0.0119\n",
      "     37       \u001b[36m28.5010\u001b[0m       27.2655  0.0116\n",
      "     38       \u001b[36m28.4976\u001b[0m       27.2643  0.0110\n",
      "     39       \u001b[36m28.4945\u001b[0m       27.2629  0.0106\n",
      "     40       \u001b[36m28.4914\u001b[0m       27.2611  0.0106\n",
      "     41       \u001b[36m28.4885\u001b[0m       27.2597  0.0110\n",
      "     42       \u001b[36m28.4858\u001b[0m       27.2583  0.0109\n",
      "     43       \u001b[36m28.4831\u001b[0m       27.2567  0.0110\n",
      "     44       \u001b[36m28.4806\u001b[0m       27.2555  0.0108\n",
      "     45       \u001b[36m28.4782\u001b[0m       27.2543  0.0108\n",
      "     46       \u001b[36m28.4760\u001b[0m       27.2530  0.0108\n",
      "     47       \u001b[36m28.4738\u001b[0m       27.2514  0.0113\n",
      "     48       \u001b[36m28.4716\u001b[0m       27.2499  0.0109\n",
      "     49       \u001b[36m28.4696\u001b[0m       27.2486  0.0107\n",
      "     50       \u001b[36m28.4677\u001b[0m       27.2474  0.0107\n",
      "     51       \u001b[36m28.4658\u001b[0m       27.2460  0.0109\n",
      "     52       \u001b[36m28.4640\u001b[0m       27.2448  0.0109\n",
      "     53       \u001b[36m28.4622\u001b[0m       27.2438  0.0108\n",
      "     54       \u001b[36m28.4606\u001b[0m       27.2428  0.0119\n",
      "     55       \u001b[36m28.4590\u001b[0m       27.2418  0.0228\n",
      "     56       \u001b[36m28.4575\u001b[0m       27.2408  0.0124\n",
      "     57       \u001b[36m28.4561\u001b[0m       27.2396  0.0116\n",
      "     58       \u001b[36m28.4546\u001b[0m       27.2384  0.0112\n",
      "     59       \u001b[36m28.4532\u001b[0m       27.2373  0.0112\n",
      "     60       \u001b[36m28.4519\u001b[0m       27.2362  0.0145\n",
      "     61       \u001b[36m28.4506\u001b[0m       27.2352  0.0127\n",
      "     62       \u001b[36m28.4494\u001b[0m       27.2343  0.0122\n",
      "     63       \u001b[36m28.4482\u001b[0m       27.2331  0.0117\n",
      "     64       \u001b[36m28.4470\u001b[0m       27.2321  0.0113\n",
      "     65       \u001b[36m28.4459\u001b[0m       27.2310  0.0118\n",
      "     66       \u001b[36m28.4448\u001b[0m       27.2301  0.0117\n",
      "     67       \u001b[36m28.4437\u001b[0m       27.2291  0.0115\n",
      "     68       \u001b[36m28.4427\u001b[0m       27.2282  0.0115\n",
      "     69       \u001b[36m28.4417\u001b[0m       27.2274  0.0112\n",
      "     70       \u001b[36m28.4408\u001b[0m       27.2264  0.0117\n",
      "     71       \u001b[36m28.4398\u001b[0m       27.2256  0.0118\n",
      "     72       \u001b[36m28.4389\u001b[0m       27.2246  0.0116\n",
      "     73       \u001b[36m28.4381\u001b[0m       27.2238  0.0112\n",
      "     74       \u001b[36m28.4372\u001b[0m       27.2229  0.0115\n",
      "     75       \u001b[36m28.4364\u001b[0m       27.2222  0.0113\n",
      "     76       \u001b[36m28.4356\u001b[0m       27.2213  0.0115\n",
      "     77       \u001b[36m28.4348\u001b[0m       27.2203  0.0113\n",
      "     78       \u001b[36m28.4340\u001b[0m       27.2195  0.0110\n",
      "     79       \u001b[36m28.4332\u001b[0m       27.2187  0.0109\n",
      "     80       \u001b[36m28.4324\u001b[0m       27.2178  0.0115\n",
      "     81       \u001b[36m28.4317\u001b[0m       27.2169  0.0117\n",
      "     82       \u001b[36m28.4310\u001b[0m       27.2162  0.0115\n",
      "     83       \u001b[36m28.4303\u001b[0m       27.2154  0.0115\n",
      "     84       \u001b[36m28.4296\u001b[0m       27.2146  0.0112\n",
      "     85       \u001b[36m28.4290\u001b[0m       27.2138  0.0109\n",
      "     86       \u001b[36m28.4283\u001b[0m       27.2132  0.0110\n",
      "     87       \u001b[36m28.4277\u001b[0m       27.2126  0.0109\n",
      "     88       \u001b[36m28.4271\u001b[0m       27.2119  0.0108\n",
      "     89       \u001b[36m28.4265\u001b[0m       27.2112  0.0107\n",
      "     90       \u001b[36m28.4259\u001b[0m       27.2104  0.0111\n",
      "     91       \u001b[36m28.4253\u001b[0m       27.2098  0.0109\n",
      "     92       \u001b[36m28.4248\u001b[0m       27.2093  0.0124\n",
      "     93       \u001b[36m28.4242\u001b[0m       27.2086  0.0107\n",
      "     94       \u001b[36m28.4237\u001b[0m       27.2080  0.0107\n",
      "     95       \u001b[36m28.4231\u001b[0m       27.2074  0.0108\n",
      "     96       \u001b[36m28.4226\u001b[0m       27.2068  0.0115\n",
      "     97       \u001b[36m28.4221\u001b[0m       27.2061  0.0110\n",
      "     98       \u001b[36m28.4216\u001b[0m       27.2055  0.0108\n",
      "     99       \u001b[36m28.4211\u001b[0m       27.2051  0.0107\n",
      "    100       \u001b[36m28.4207\u001b[0m       27.2045  0.0109\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m41.9759\u001b[0m       \u001b[32m40.5129\u001b[0m  0.0124\n",
      "      2       \u001b[36m37.2889\u001b[0m       \u001b[32m32.0126\u001b[0m  0.0117\n",
      "      3       \u001b[36m34.6764\u001b[0m       \u001b[32m31.3477\u001b[0m  0.0119\n",
      "      4       \u001b[36m33.8595\u001b[0m       32.1790  0.0116\n",
      "      5       \u001b[36m33.6302\u001b[0m       31.8102  0.0115\n",
      "      6       \u001b[36m33.1207\u001b[0m       \u001b[32m30.5662\u001b[0m  0.0113\n",
      "      7       \u001b[36m32.9593\u001b[0m       \u001b[32m30.3302\u001b[0m  0.0124\n",
      "      8       \u001b[36m32.8096\u001b[0m       30.6989  0.0117\n",
      "      9       \u001b[36m32.7059\u001b[0m       30.6650  0.0120\n",
      "     10       \u001b[36m32.5683\u001b[0m       \u001b[32m30.2585\u001b[0m  0.0120\n",
      "     11       \u001b[36m32.4746\u001b[0m       \u001b[32m30.2115\u001b[0m  0.0125\n",
      "     12       \u001b[36m32.4180\u001b[0m       30.3250  0.0122\n",
      "     13       \u001b[36m32.3726\u001b[0m       30.2987  0.0126\n",
      "     14       \u001b[36m32.3285\u001b[0m       \u001b[32m30.1295\u001b[0m  0.0121\n",
      "     15       \u001b[36m32.3014\u001b[0m       30.1530  0.0118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     16       \u001b[36m32.2873\u001b[0m       30.1696  0.0124\n",
      "     17       \u001b[36m32.2690\u001b[0m       30.1426  0.0124\n",
      "     18       \u001b[36m32.2556\u001b[0m       \u001b[32m30.0778\u001b[0m  0.0118\n",
      "     19       \u001b[36m32.2469\u001b[0m       30.1364  0.0114\n",
      "     20       \u001b[36m32.2379\u001b[0m       \u001b[32m30.0751\u001b[0m  0.0115\n",
      "     21       \u001b[36m32.2233\u001b[0m       30.0830  0.0120\n",
      "     22       \u001b[36m32.2190\u001b[0m       \u001b[32m30.0659\u001b[0m  0.0116\n",
      "     23       \u001b[36m32.2098\u001b[0m       30.0829  0.0124\n",
      "     24       \u001b[36m32.2041\u001b[0m       \u001b[32m30.0373\u001b[0m  0.0123\n",
      "     25       \u001b[36m32.1954\u001b[0m       30.0539  0.0126\n",
      "     26       \u001b[36m32.1922\u001b[0m       30.0392  0.0124\n",
      "     27       \u001b[36m32.1859\u001b[0m       30.0455  0.0126\n",
      "     28       \u001b[36m32.1820\u001b[0m       \u001b[32m30.0152\u001b[0m  0.0121\n",
      "     29       \u001b[36m32.1775\u001b[0m       30.0368  0.0120\n",
      "     30       \u001b[36m32.1740\u001b[0m       \u001b[32m30.0100\u001b[0m  0.0126\n",
      "     31       \u001b[36m32.1702\u001b[0m       30.0264  0.0123\n",
      "     32       \u001b[36m32.1664\u001b[0m       \u001b[32m30.0022\u001b[0m  0.0120\n",
      "     33       \u001b[36m32.1636\u001b[0m       30.0181  0.0120\n",
      "     34       \u001b[36m32.1588\u001b[0m       \u001b[32m29.9953\u001b[0m  0.0190\n",
      "     35       \u001b[36m32.1568\u001b[0m       30.0076  0.0161\n",
      "     36       \u001b[36m32.1515\u001b[0m       \u001b[32m29.9899\u001b[0m  0.0134\n",
      "     37       \u001b[36m32.1498\u001b[0m       29.9974  0.0144\n",
      "     38       \u001b[36m32.1453\u001b[0m       \u001b[32m29.9876\u001b[0m  0.0134\n",
      "     39       \u001b[36m32.1438\u001b[0m       29.9883  0.0170\n",
      "     40       \u001b[36m32.1398\u001b[0m       29.9905  0.0157\n",
      "     41       \u001b[36m32.1379\u001b[0m       \u001b[32m29.9851\u001b[0m  0.0121\n",
      "     42       \u001b[36m32.1357\u001b[0m       29.9875  0.0120\n",
      "     43       \u001b[36m32.1323\u001b[0m       29.9859  0.0119\n",
      "     44       \u001b[36m32.1312\u001b[0m       \u001b[32m29.9801\u001b[0m  0.0123\n",
      "     45       \u001b[36m32.1291\u001b[0m       29.9854  0.0118\n",
      "     46       \u001b[36m32.1263\u001b[0m       \u001b[32m29.9778\u001b[0m  0.0118\n",
      "     47       32.1266       \u001b[32m29.9706\u001b[0m  0.0124\n",
      "     48       32.1266       30.0003  0.0120\n",
      "     49       \u001b[36m32.1254\u001b[0m       29.9730  0.0120\n",
      "     50       32.1258       29.9712  0.0124\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m31.8559\u001b[0m       \u001b[32m28.2353\u001b[0m  0.0117\n",
      "      2       \u001b[36m25.9510\u001b[0m       30.2843  0.0127\n",
      "      3       \u001b[36m24.7651\u001b[0m       \u001b[32m26.5925\u001b[0m  0.0125\n",
      "      4       \u001b[36m24.1230\u001b[0m       \u001b[32m26.4239\u001b[0m  0.0126\n",
      "      5       \u001b[36m23.8025\u001b[0m       27.2046  0.0126\n",
      "      6       \u001b[36m23.5832\u001b[0m       27.7527  0.0118\n",
      "      7       \u001b[36m23.4398\u001b[0m       26.9581  0.0117\n",
      "      8       \u001b[36m23.3620\u001b[0m       26.7480  0.0124\n",
      "      9       \u001b[36m23.2907\u001b[0m       26.9234  0.0122\n",
      "     10       \u001b[36m23.2372\u001b[0m       26.7992  0.0123\n",
      "     11       \u001b[36m23.1867\u001b[0m       26.6613  0.0123\n",
      "     12       \u001b[36m23.1777\u001b[0m       26.7241  0.0119\n",
      "     13       \u001b[36m23.1460\u001b[0m       26.7431  0.0123\n",
      "     14       \u001b[36m23.1216\u001b[0m       26.6000  0.0122\n",
      "     15       \u001b[36m23.1150\u001b[0m       26.6078  0.0125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     16       \u001b[36m23.1003\u001b[0m       26.6371  0.0121\n",
      "     17       \u001b[36m23.0886\u001b[0m       26.5803  0.0118\n",
      "     18       \u001b[36m23.0802\u001b[0m       26.5536  0.0123\n",
      "     19       \u001b[36m23.0746\u001b[0m       26.5834  0.0117\n",
      "     20       \u001b[36m23.0657\u001b[0m       26.5592  0.0117\n",
      "     21       \u001b[36m23.0615\u001b[0m       26.5525  0.0118\n",
      "     22       \u001b[36m23.0572\u001b[0m       26.5509  0.0116\n",
      "     23       \u001b[36m23.0518\u001b[0m       26.5507  0.0126\n",
      "     24       \u001b[36m23.0486\u001b[0m       26.5367  0.0119\n",
      "     25       \u001b[36m23.0441\u001b[0m       26.5361  0.0118\n",
      "     26       \u001b[36m23.0411\u001b[0m       26.5296  0.0121\n",
      "     27       \u001b[36m23.0377\u001b[0m       26.5251  0.0117\n",
      "     28       \u001b[36m23.0349\u001b[0m       26.5271  0.0126\n",
      "     29       \u001b[36m23.0329\u001b[0m       26.5232  0.0119\n",
      "     30       \u001b[36m23.0296\u001b[0m       26.5231  0.0118\n",
      "     31       \u001b[36m23.0279\u001b[0m       26.5260  0.0113\n",
      "     32       \u001b[36m23.0263\u001b[0m       26.5185  0.0116\n",
      "     33       \u001b[36m23.0234\u001b[0m       26.5187  0.0122\n",
      "     34       23.0245       26.5271  0.0120\n",
      "     35       \u001b[36m23.0220\u001b[0m       26.5147  0.0115\n",
      "     36       \u001b[36m23.0219\u001b[0m       26.5199  0.0122\n",
      "     37       23.0345       26.5435  0.0118\n",
      "     38       23.0399       26.5279  0.0113\n",
      "     39       23.0508       26.5467  0.0113\n",
      "     40       23.0574       26.4865  0.0121\n",
      "     41       \u001b[36m23.0169\u001b[0m       26.5609  0.0119\n",
      "     42       \u001b[36m23.0110\u001b[0m       26.4834  0.0118\n",
      "     43       23.0133       26.5189  0.0118\n",
      "     44       23.0123       26.5141  0.0115\n",
      "     45       \u001b[36m23.0039\u001b[0m       26.5037  0.0115\n",
      "     46       \u001b[36m23.0038\u001b[0m       26.5072  0.0119\n",
      "     47       \u001b[36m23.0015\u001b[0m       26.5147  0.0122\n",
      "     48       \u001b[36m23.0006\u001b[0m       26.5109  0.0118\n",
      "     49       \u001b[36m22.9986\u001b[0m       26.5122  0.0115\n",
      "     50       \u001b[36m22.9976\u001b[0m       26.5162  0.0113\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m39.0480\u001b[0m       \u001b[32m29.3173\u001b[0m  0.0128\n",
      "      2       \u001b[36m33.6685\u001b[0m       \u001b[32m27.3020\u001b[0m  0.0127\n",
      "      3       \u001b[36m31.1223\u001b[0m       30.1998  0.0118\n",
      "      4       \u001b[36m30.2284\u001b[0m       \u001b[32m26.8876\u001b[0m  0.0126\n",
      "      5       \u001b[36m29.7131\u001b[0m       \u001b[32m26.6057\u001b[0m  0.0118\n",
      "      6       \u001b[36m29.2310\u001b[0m       27.7826  0.0118\n",
      "      7       \u001b[36m29.1884\u001b[0m       28.5535  0.0116\n",
      "      8       \u001b[36m29.0236\u001b[0m       27.4961  0.0115\n",
      "      9       \u001b[36m28.7745\u001b[0m       27.2184  0.0120\n",
      "     10       \u001b[36m28.6978\u001b[0m       27.5177  0.0153\n",
      "     11       \u001b[36m28.6782\u001b[0m       27.5902  0.0152\n",
      "     12       \u001b[36m28.6010\u001b[0m       27.3611  0.0120\n",
      "     13       \u001b[36m28.5363\u001b[0m       27.3394  0.0122\n",
      "     14       \u001b[36m28.5183\u001b[0m       27.4843  0.0138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     15       \u001b[36m28.5122\u001b[0m       27.3266  0.0151\n",
      "     16       \u001b[36m28.4798\u001b[0m       27.2674  0.0122\n",
      "     17       \u001b[36m28.4715\u001b[0m       27.3364  0.0123\n",
      "     18       \u001b[36m28.4684\u001b[0m       27.3426  0.0119\n",
      "     19       \u001b[36m28.4543\u001b[0m       27.3232  0.0117\n",
      "     20       \u001b[36m28.4483\u001b[0m       27.3475  0.0126\n",
      "     21       \u001b[36m28.4450\u001b[0m       27.3563  0.0118\n",
      "     22       \u001b[36m28.4391\u001b[0m       27.3588  0.0115\n",
      "     23       \u001b[36m28.4378\u001b[0m       27.3674  0.0116\n",
      "     24       28.4381       27.4270  0.0114\n",
      "     25       28.4535       27.4102  0.0119\n",
      "     26       28.4554       27.4160  0.0117\n",
      "     27       28.4574       27.3829  0.0118\n",
      "     28       \u001b[36m28.4343\u001b[0m       27.4515  0.0114\n",
      "     29       \u001b[36m28.4147\u001b[0m       27.3964  0.0115\n",
      "     30       28.4189       27.4729  0.0117\n",
      "     31       \u001b[36m28.4131\u001b[0m       27.4527  0.0116\n",
      "     32       \u001b[36m28.4088\u001b[0m       27.4363  0.0116\n",
      "     33       \u001b[36m28.4044\u001b[0m       27.4566  0.0114\n",
      "     34       28.4064       27.4503  0.0117\n",
      "     35       \u001b[36m28.4016\u001b[0m       27.4229  0.0113\n",
      "     36       \u001b[36m28.3999\u001b[0m       27.4655  0.0116\n",
      "     37       \u001b[36m28.3989\u001b[0m       27.4586  0.0117\n",
      "     38       \u001b[36m28.3978\u001b[0m       27.4501  0.0114\n",
      "     39       \u001b[36m28.3953\u001b[0m       27.4649  0.0113\n",
      "     40       \u001b[36m28.3948\u001b[0m       27.4669  0.0112\n",
      "     41       \u001b[36m28.3935\u001b[0m       27.4578  0.0120\n",
      "     42       \u001b[36m28.3928\u001b[0m       27.4692  0.0117\n",
      "     43       \u001b[36m28.3913\u001b[0m       27.4670  0.0116\n",
      "     44       \u001b[36m28.3903\u001b[0m       27.4705  0.0113\n",
      "     45       \u001b[36m28.3894\u001b[0m       27.4738  0.0114\n",
      "     46       \u001b[36m28.3877\u001b[0m       27.4789  0.0119\n",
      "     47       \u001b[36m28.3877\u001b[0m       27.4754  0.0134\n",
      "     48       \u001b[36m28.3864\u001b[0m       27.4721  0.0123\n",
      "     49       \u001b[36m28.3853\u001b[0m       27.4806  0.0119\n",
      "     50       \u001b[36m28.3852\u001b[0m       27.4668  0.0113\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m41.8082\u001b[0m       \u001b[32m41.3210\u001b[0m  0.0111\n",
      "      2       \u001b[36m38.8466\u001b[0m       \u001b[32m37.6034\u001b[0m  0.0109\n",
      "      3       \u001b[36m36.0857\u001b[0m       \u001b[32m34.0935\u001b[0m  0.0109\n",
      "      4       \u001b[36m33.9502\u001b[0m       \u001b[32m31.7248\u001b[0m  0.0109\n",
      "      5       \u001b[36m32.9785\u001b[0m       \u001b[32m30.7423\u001b[0m  0.0106\n",
      "      6       \u001b[36m32.7297\u001b[0m       \u001b[32m30.4055\u001b[0m  0.0109\n",
      "      7       \u001b[36m32.6463\u001b[0m       \u001b[32m30.2653\u001b[0m  0.0108\n",
      "      8       \u001b[36m32.5882\u001b[0m       \u001b[32m30.1854\u001b[0m  0.0109\n",
      "      9       \u001b[36m32.5405\u001b[0m       \u001b[32m30.1299\u001b[0m  0.0112\n",
      "     10       \u001b[36m32.5015\u001b[0m       \u001b[32m30.0877\u001b[0m  0.0108\n",
      "     11       \u001b[36m32.4691\u001b[0m       \u001b[32m30.0537\u001b[0m  0.0107\n",
      "     12       \u001b[36m32.4437\u001b[0m       \u001b[32m30.0259\u001b[0m  0.0130\n",
      "     13       \u001b[36m32.4221\u001b[0m       \u001b[32m30.0040\u001b[0m  0.0109\n",
      "     14       \u001b[36m32.4042\u001b[0m       \u001b[32m29.9861\u001b[0m  0.0112\n",
      "     15       \u001b[36m32.3892\u001b[0m       \u001b[32m29.9714\u001b[0m  0.0107\n",
      "     16       \u001b[36m32.3764\u001b[0m       \u001b[32m29.9597\u001b[0m  0.0109\n",
      "     17       \u001b[36m32.3653\u001b[0m       \u001b[32m29.9500\u001b[0m  0.0109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     18       \u001b[36m32.3558\u001b[0m       \u001b[32m29.9413\u001b[0m  0.0114\n",
      "     19       \u001b[36m32.3478\u001b[0m       \u001b[32m29.9336\u001b[0m  0.0113\n",
      "     20       \u001b[36m32.3406\u001b[0m       \u001b[32m29.9262\u001b[0m  0.0108\n",
      "     21       \u001b[36m32.3340\u001b[0m       \u001b[32m29.9202\u001b[0m  0.0107\n",
      "     22       \u001b[36m32.3280\u001b[0m       \u001b[32m29.9153\u001b[0m  0.0106\n",
      "     23       \u001b[36m32.3228\u001b[0m       \u001b[32m29.9108\u001b[0m  0.0111\n",
      "     24       \u001b[36m32.3179\u001b[0m       \u001b[32m29.9057\u001b[0m  0.0116\n",
      "     25       \u001b[36m32.3134\u001b[0m       \u001b[32m29.9004\u001b[0m  0.0106\n",
      "     26       \u001b[36m32.3096\u001b[0m       \u001b[32m29.8954\u001b[0m  0.0103\n",
      "     27       \u001b[36m32.3059\u001b[0m       \u001b[32m29.8909\u001b[0m  0.0104\n",
      "     28       \u001b[36m32.3027\u001b[0m       \u001b[32m29.8865\u001b[0m  0.0112\n",
      "     29       \u001b[36m32.2997\u001b[0m       \u001b[32m29.8829\u001b[0m  0.0110\n",
      "     30       \u001b[36m32.2970\u001b[0m       \u001b[32m29.8801\u001b[0m  0.0106\n",
      "     31       \u001b[36m32.2944\u001b[0m       \u001b[32m29.8776\u001b[0m  0.0107\n",
      "     32       \u001b[36m32.2918\u001b[0m       \u001b[32m29.8748\u001b[0m  0.0106\n",
      "     33       \u001b[36m32.2896\u001b[0m       \u001b[32m29.8725\u001b[0m  0.0109\n",
      "     34       \u001b[36m32.2872\u001b[0m       \u001b[32m29.8691\u001b[0m  0.0110\n",
      "     35       \u001b[36m32.2853\u001b[0m       \u001b[32m29.8674\u001b[0m  0.0108\n",
      "     36       \u001b[36m32.2830\u001b[0m       \u001b[32m29.8649\u001b[0m  0.0106\n",
      "     37       \u001b[36m32.2812\u001b[0m       \u001b[32m29.8636\u001b[0m  0.0107\n",
      "     38       \u001b[36m32.2792\u001b[0m       \u001b[32m29.8608\u001b[0m  0.0112\n",
      "     39       \u001b[36m32.2775\u001b[0m       \u001b[32m29.8593\u001b[0m  0.0111\n",
      "     40       \u001b[36m32.2757\u001b[0m       \u001b[32m29.8571\u001b[0m  0.0126\n",
      "     41       \u001b[36m32.2739\u001b[0m       \u001b[32m29.8555\u001b[0m  0.0130\n",
      "     42       \u001b[36m32.2722\u001b[0m       \u001b[32m29.8537\u001b[0m  0.0145\n",
      "     43       \u001b[36m32.2706\u001b[0m       \u001b[32m29.8523\u001b[0m  0.0133\n",
      "     44       \u001b[36m32.2691\u001b[0m       \u001b[32m29.8510\u001b[0m  0.0117\n",
      "     45       \u001b[36m32.2675\u001b[0m       \u001b[32m29.8492\u001b[0m  0.0111\n",
      "     46       \u001b[36m32.2662\u001b[0m       \u001b[32m29.8484\u001b[0m  0.0118\n",
      "     47       \u001b[36m32.2647\u001b[0m       \u001b[32m29.8469\u001b[0m  0.0147\n",
      "     48       \u001b[36m32.2632\u001b[0m       \u001b[32m29.8446\u001b[0m  0.0144\n",
      "     49       \u001b[36m32.2622\u001b[0m       \u001b[32m29.8442\u001b[0m  0.0122\n",
      "     50       \u001b[36m32.2609\u001b[0m       \u001b[32m29.8431\u001b[0m  0.0109\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m33.4212\u001b[0m       \u001b[32m30.5885\u001b[0m  0.0111\n",
      "      2       \u001b[36m29.9905\u001b[0m       \u001b[32m28.2004\u001b[0m  0.0110\n",
      "      3       \u001b[36m26.8414\u001b[0m       \u001b[32m26.5789\u001b[0m  0.0112\n",
      "      4       \u001b[36m24.6312\u001b[0m       \u001b[32m26.1990\u001b[0m  0.0111\n",
      "      5       \u001b[36m23.7459\u001b[0m       26.3563  0.0111\n",
      "      6       \u001b[36m23.5222\u001b[0m       26.4514  0.0108\n",
      "      7       \u001b[36m23.4434\u001b[0m       26.4748  0.0112\n",
      "      8       \u001b[36m23.3936\u001b[0m       26.4753  0.0111\n",
      "      9       \u001b[36m23.3545\u001b[0m       26.4717  0.0111\n",
      "     10       \u001b[36m23.3229\u001b[0m       26.4681  0.0111\n",
      "     11       \u001b[36m23.2971\u001b[0m       26.4645  0.0109\n",
      "     12       \u001b[36m23.2751\u001b[0m       26.4617  0.0110\n",
      "     13       \u001b[36m23.2562\u001b[0m       26.4592  0.0110\n",
      "     14       \u001b[36m23.2402\u001b[0m       26.4581  0.0112\n",
      "     15       \u001b[36m23.2262\u001b[0m       26.4571  0.0113\n",
      "     16       \u001b[36m23.2142\u001b[0m       26.4562  0.0109\n",
      "     17       \u001b[36m23.2035\u001b[0m       26.4550  0.0112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     18       \u001b[36m23.1940\u001b[0m       26.4540  0.0114\n",
      "     19       \u001b[36m23.1857\u001b[0m       26.4528  0.0111\n",
      "     20       \u001b[36m23.1782\u001b[0m       26.4522  0.0117\n",
      "     21       \u001b[36m23.1716\u001b[0m       26.4511  0.0114\n",
      "     22       \u001b[36m23.1656\u001b[0m       26.4502  0.0110\n",
      "     23       \u001b[36m23.1602\u001b[0m       26.4494  0.0110\n",
      "     24       \u001b[36m23.1553\u001b[0m       26.4486  0.0109\n",
      "     25       \u001b[36m23.1509\u001b[0m       26.4479  0.0109\n",
      "     26       \u001b[36m23.1468\u001b[0m       26.4473  0.0108\n",
      "     27       \u001b[36m23.1431\u001b[0m       26.4464  0.0109\n",
      "     28       \u001b[36m23.1396\u001b[0m       26.4457  0.0110\n",
      "     29       \u001b[36m23.1364\u001b[0m       26.4451  0.0110\n",
      "     30       \u001b[36m23.1335\u001b[0m       26.4442  0.0111\n",
      "     31       \u001b[36m23.1308\u001b[0m       26.4435  0.0108\n",
      "     32       \u001b[36m23.1282\u001b[0m       26.4427  0.0109\n",
      "     33       \u001b[36m23.1258\u001b[0m       26.4423  0.0111\n",
      "     34       \u001b[36m23.1235\u001b[0m       26.4417  0.0111\n",
      "     35       \u001b[36m23.1214\u001b[0m       26.4412  0.0111\n",
      "     36       \u001b[36m23.1194\u001b[0m       26.4405  0.0110\n",
      "     37       \u001b[36m23.1174\u001b[0m       26.4400  0.0109\n",
      "     38       \u001b[36m23.1156\u001b[0m       26.4393  0.0113\n",
      "     39       \u001b[36m23.1139\u001b[0m       26.4386  0.0115\n",
      "     40       \u001b[36m23.1123\u001b[0m       26.4381  0.0112\n",
      "     41       \u001b[36m23.1107\u001b[0m       26.4376  0.0109\n",
      "     42       \u001b[36m23.1092\u001b[0m       26.4370  0.0108\n",
      "     43       \u001b[36m23.1078\u001b[0m       26.4364  0.0110\n",
      "     44       \u001b[36m23.1064\u001b[0m       26.4358  0.0113\n",
      "     45       \u001b[36m23.1051\u001b[0m       26.4353  0.0109\n",
      "     46       \u001b[36m23.1038\u001b[0m       26.4349  0.0109\n",
      "     47       \u001b[36m23.1026\u001b[0m       26.4345  0.0108\n",
      "     48       \u001b[36m23.1014\u001b[0m       26.4340  0.0111\n",
      "     49       \u001b[36m23.1003\u001b[0m       26.4335  0.0111\n",
      "     50       \u001b[36m23.0992\u001b[0m       26.4329  0.0110\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m41.4234\u001b[0m       \u001b[32m31.4164\u001b[0m  0.0109\n",
      "      2       \u001b[36m37.9944\u001b[0m       \u001b[32m29.6027\u001b[0m  0.0111\n",
      "      3       \u001b[36m35.5006\u001b[0m       \u001b[32m28.2385\u001b[0m  0.0114\n",
      "      4       \u001b[36m33.5143\u001b[0m       \u001b[32m27.1861\u001b[0m  0.0116\n",
      "      5       \u001b[36m31.8487\u001b[0m       \u001b[32m26.4391\u001b[0m  0.0111\n",
      "      6       \u001b[36m30.4433\u001b[0m       \u001b[32m26.0970\u001b[0m  0.0121\n",
      "      7       \u001b[36m29.4153\u001b[0m       26.2339  0.0131\n",
      "      8       \u001b[36m28.8735\u001b[0m       26.6069  0.0119\n",
      "      9       \u001b[36m28.6790\u001b[0m       26.8925  0.0121\n",
      "     10       \u001b[36m28.6153\u001b[0m       27.0457  0.0125\n",
      "     11       \u001b[36m28.5863\u001b[0m       27.1112  0.0109\n",
      "     12       \u001b[36m28.5647\u001b[0m       27.1378  0.0112\n",
      "     13       \u001b[36m28.5461\u001b[0m       27.1477  0.0112\n",
      "     14       \u001b[36m28.5304\u001b[0m       27.1510  0.0110\n",
      "     15       \u001b[36m28.5173\u001b[0m       27.1542  0.0110\n",
      "     16       \u001b[36m28.5062\u001b[0m       27.1573  0.0108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m28.4967\u001b[0m       27.1586  0.0114\n",
      "     18       \u001b[36m28.4883\u001b[0m       27.1609  0.0111\n",
      "     19       \u001b[36m28.4811\u001b[0m       27.1633  0.0114\n",
      "     20       \u001b[36m28.4747\u001b[0m       27.1659  0.0121\n",
      "     21       \u001b[36m28.4691\u001b[0m       27.1686  0.0108\n",
      "     22       \u001b[36m28.4642\u001b[0m       27.1723  0.0108\n",
      "     23       \u001b[36m28.4600\u001b[0m       27.1754  0.0118\n",
      "     24       \u001b[36m28.4562\u001b[0m       27.1783  0.0145\n",
      "     25       \u001b[36m28.4529\u001b[0m       27.1808  0.0128\n",
      "     26       \u001b[36m28.4499\u001b[0m       27.1829  0.0116\n",
      "     27       \u001b[36m28.4473\u001b[0m       27.1854  0.0124\n",
      "     28       \u001b[36m28.4450\u001b[0m       27.1868  0.0125\n",
      "     29       \u001b[36m28.4427\u001b[0m       27.1882  0.0135\n",
      "     30       \u001b[36m28.4408\u001b[0m       27.1896  0.0118\n",
      "     31       \u001b[36m28.4391\u001b[0m       27.1908  0.0125\n",
      "     32       \u001b[36m28.4375\u001b[0m       27.1921  0.0109\n",
      "     33       \u001b[36m28.4362\u001b[0m       27.1931  0.0110\n",
      "     34       \u001b[36m28.4348\u001b[0m       27.1935  0.0109\n",
      "     35       \u001b[36m28.4336\u001b[0m       27.1945  0.0110\n",
      "     36       \u001b[36m28.4325\u001b[0m       27.1953  0.0110\n",
      "     37       \u001b[36m28.4315\u001b[0m       27.1962  0.0108\n",
      "     38       \u001b[36m28.4305\u001b[0m       27.1965  0.0108\n",
      "     39       \u001b[36m28.4295\u001b[0m       27.1973  0.0108\n",
      "     40       \u001b[36m28.4287\u001b[0m       27.1978  0.0112\n",
      "     41       \u001b[36m28.4278\u001b[0m       27.1979  0.0109\n",
      "     42       \u001b[36m28.4271\u001b[0m       27.1983  0.0109\n",
      "     43       \u001b[36m28.4263\u001b[0m       27.1985  0.0109\n",
      "     44       \u001b[36m28.4256\u001b[0m       27.1983  0.0107\n",
      "     45       \u001b[36m28.4249\u001b[0m       27.1985  0.0108\n",
      "     46       \u001b[36m28.4242\u001b[0m       27.1982  0.0108\n",
      "     47       \u001b[36m28.4235\u001b[0m       27.1984  0.0110\n",
      "     48       \u001b[36m28.4229\u001b[0m       27.1983  0.0107\n",
      "     49       \u001b[36m28.4223\u001b[0m       27.1984  0.0107\n",
      "     50       \u001b[36m28.4217\u001b[0m       27.1982  0.0110\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m42.2210\u001b[0m       \u001b[32m42.9339\u001b[0m  0.0115\n",
      "      2       \u001b[36m37.1596\u001b[0m       \u001b[32m32.1179\u001b[0m  0.0116\n",
      "      3       \u001b[36m35.1167\u001b[0m       32.2235  0.0112\n",
      "      4       \u001b[36m34.2066\u001b[0m       32.4067  0.0115\n",
      "      5       \u001b[36m33.5400\u001b[0m       \u001b[32m31.5382\u001b[0m  0.0121\n",
      "      6       \u001b[36m33.2341\u001b[0m       \u001b[32m30.5088\u001b[0m  0.0114\n",
      "      7       \u001b[36m32.9852\u001b[0m       30.7572  0.0113\n",
      "      8       \u001b[36m32.8800\u001b[0m       30.7343  0.0119\n",
      "      9       \u001b[36m32.7140\u001b[0m       \u001b[32m30.3596\u001b[0m  0.0114\n",
      "     10       \u001b[36m32.6126\u001b[0m       \u001b[32m30.3048\u001b[0m  0.0113\n",
      "     11       \u001b[36m32.5335\u001b[0m       30.4169  0.0115\n",
      "     12       \u001b[36m32.4722\u001b[0m       30.3212  0.0116\n",
      "     13       \u001b[36m32.4077\u001b[0m       \u001b[32m30.2954\u001b[0m  0.0131\n",
      "     14       \u001b[36m32.3690\u001b[0m       30.3603  0.0116\n",
      "     15       \u001b[36m32.3375\u001b[0m       30.3452  0.0116\n",
      "     16       \u001b[36m32.3068\u001b[0m       30.3651  0.0114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m32.2878\u001b[0m       30.3763  0.0120\n",
      "     18       \u001b[36m32.2687\u001b[0m       30.3699  0.0115\n",
      "     19       \u001b[36m32.2544\u001b[0m       30.3982  0.0115\n",
      "     20       \u001b[36m32.2430\u001b[0m       30.3794  0.0120\n",
      "     21       \u001b[36m32.2309\u001b[0m       30.3864  0.0116\n",
      "     22       \u001b[36m32.2223\u001b[0m       30.3456  0.0115\n",
      "     23       \u001b[36m32.2123\u001b[0m       30.3636  0.0115\n",
      "     24       \u001b[36m32.2069\u001b[0m       \u001b[32m30.2876\u001b[0m  0.0114\n",
      "     25       \u001b[36m32.2004\u001b[0m       30.4374  0.0117\n",
      "     26       32.2109       \u001b[32m30.1787\u001b[0m  0.0116\n",
      "     27       32.2235       30.4553  0.0112\n",
      "     28       32.2141       30.2666  0.0111\n",
      "     29       \u001b[36m32.1851\u001b[0m       30.2280  0.0113\n",
      "     30       \u001b[36m32.1833\u001b[0m       30.3612  0.0118\n",
      "     31       \u001b[36m32.1761\u001b[0m       30.2552  0.0115\n",
      "     32       \u001b[36m32.1675\u001b[0m       30.3375  0.0116\n",
      "     33       \u001b[36m32.1647\u001b[0m       30.3622  0.0112\n",
      "     34       \u001b[36m32.1564\u001b[0m       30.3275  0.0114\n",
      "     35       \u001b[36m32.1546\u001b[0m       30.3797  0.0116\n",
      "     36       \u001b[36m32.1501\u001b[0m       30.3517  0.0115\n",
      "     37       \u001b[36m32.1450\u001b[0m       30.3790  0.0118\n",
      "     38       \u001b[36m32.1437\u001b[0m       30.3848  0.0114\n",
      "     39       \u001b[36m32.1382\u001b[0m       30.3884  0.0115\n",
      "     40       \u001b[36m32.1363\u001b[0m       30.4032  0.0118\n",
      "     41       \u001b[36m32.1332\u001b[0m       30.3933  0.0115\n",
      "     42       \u001b[36m32.1298\u001b[0m       30.4193  0.0115\n",
      "     43       \u001b[36m32.1272\u001b[0m       30.4195  0.0112\n",
      "     44       \u001b[36m32.1242\u001b[0m       30.4316  0.0112\n",
      "     45       \u001b[36m32.1216\u001b[0m       30.4372  0.0119\n",
      "     46       \u001b[36m32.1191\u001b[0m       30.4481  0.0122\n",
      "     47       \u001b[36m32.1161\u001b[0m       30.4530  0.0119\n",
      "     48       \u001b[36m32.1145\u001b[0m       30.4536  0.0112\n",
      "     49       \u001b[36m32.1120\u001b[0m       30.4613  0.0117\n",
      "     50       \u001b[36m32.1104\u001b[0m       30.4618  0.0118\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m32.4858\u001b[0m       \u001b[32m29.5385\u001b[0m  0.0122\n",
      "      2       \u001b[36m26.9170\u001b[0m       \u001b[32m29.0364\u001b[0m  0.0119\n",
      "      3       \u001b[36m25.2150\u001b[0m       \u001b[32m26.8925\u001b[0m  0.0122\n",
      "      4       \u001b[36m24.3532\u001b[0m       \u001b[32m26.5877\u001b[0m  0.0145\n",
      "      5       \u001b[36m24.2531\u001b[0m       26.9793  0.0162\n",
      "      6       \u001b[36m23.6830\u001b[0m       27.4699  0.0125\n",
      "      7       \u001b[36m23.6091\u001b[0m       26.9232  0.0129\n",
      "      8       \u001b[36m23.5136\u001b[0m       26.9687  0.0127\n",
      "      9       \u001b[36m23.4004\u001b[0m       27.1847  0.0130\n",
      "     10       \u001b[36m23.2986\u001b[0m       26.9332  0.0138\n",
      "     11       \u001b[36m23.2600\u001b[0m       26.8505  0.0133\n",
      "     12       \u001b[36m23.2289\u001b[0m       27.0346  0.0120\n",
      "     13       \u001b[36m23.1832\u001b[0m       26.9214  0.0117\n",
      "     14       \u001b[36m23.1579\u001b[0m       26.7390  0.0117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     15       \u001b[36m23.1467\u001b[0m       26.7803  0.0220\n",
      "     16       \u001b[36m23.1243\u001b[0m       26.7724  0.0172\n",
      "     17       \u001b[36m23.1110\u001b[0m       26.6804  0.0116\n",
      "     18       \u001b[36m23.1026\u001b[0m       26.6590  0.0116\n",
      "     19       \u001b[36m23.0919\u001b[0m       26.6485  0.0111\n",
      "     20       \u001b[36m23.0846\u001b[0m       26.6272  0.0119\n",
      "     21       \u001b[36m23.0777\u001b[0m       26.6211  0.0114\n",
      "     22       \u001b[36m23.0713\u001b[0m       26.6240  0.0115\n",
      "     23       \u001b[36m23.0654\u001b[0m       26.6150  0.0114\n",
      "     24       \u001b[36m23.0601\u001b[0m       26.6085  0.0113\n",
      "     25       \u001b[36m23.0554\u001b[0m       26.6228  0.0118\n",
      "     26       \u001b[36m23.0504\u001b[0m       26.6221  0.0115\n",
      "     27       \u001b[36m23.0464\u001b[0m       26.6215  0.0115\n",
      "     28       \u001b[36m23.0426\u001b[0m       26.6342  0.0114\n",
      "     29       \u001b[36m23.0388\u001b[0m       26.6273  0.0113\n",
      "     30       \u001b[36m23.0359\u001b[0m       26.6182  0.0117\n",
      "     31       \u001b[36m23.0326\u001b[0m       26.6203  0.0118\n",
      "     32       \u001b[36m23.0298\u001b[0m       26.6176  0.0113\n",
      "     33       \u001b[36m23.0272\u001b[0m       26.6197  0.0124\n",
      "     34       \u001b[36m23.0247\u001b[0m       26.6206  0.0139\n",
      "     35       \u001b[36m23.0223\u001b[0m       26.6138  0.0120\n",
      "     36       \u001b[36m23.0202\u001b[0m       26.6163  0.0115\n",
      "     37       \u001b[36m23.0180\u001b[0m       26.6131  0.0116\n",
      "     38       \u001b[36m23.0164\u001b[0m       26.6093  0.0116\n",
      "     39       \u001b[36m23.0144\u001b[0m       26.6081  0.0114\n",
      "     40       \u001b[36m23.0130\u001b[0m       26.6085  0.0119\n",
      "     41       \u001b[36m23.0113\u001b[0m       26.6059  0.0115\n",
      "     42       \u001b[36m23.0099\u001b[0m       26.6065  0.0115\n",
      "     43       \u001b[36m23.0088\u001b[0m       26.6022  0.0114\n",
      "     44       \u001b[36m23.0078\u001b[0m       26.6115  0.0114\n",
      "     45       \u001b[36m23.0054\u001b[0m       26.6057  0.0117\n",
      "     46       \u001b[36m23.0043\u001b[0m       26.6154  0.0114\n",
      "     47       \u001b[36m23.0027\u001b[0m       26.6146  0.0115\n",
      "     48       \u001b[36m23.0014\u001b[0m       26.6151  0.0113\n",
      "     49       \u001b[36m22.9999\u001b[0m       26.6157  0.0117\n",
      "     50       \u001b[36m22.9987\u001b[0m       26.6104  0.0120\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m39.2431\u001b[0m       \u001b[32m28.4215\u001b[0m  0.0115\n",
      "      2       \u001b[36m32.6289\u001b[0m       31.3266  0.0113\n",
      "      3       \u001b[36m31.7390\u001b[0m       \u001b[32m27.5459\u001b[0m  0.0114\n",
      "      4       \u001b[36m29.8951\u001b[0m       \u001b[32m26.4687\u001b[0m  0.0116\n",
      "      5       \u001b[36m29.6502\u001b[0m       27.2776  0.0115\n",
      "      6       \u001b[36m29.2720\u001b[0m       28.6096  0.0116\n",
      "      7       \u001b[36m29.1125\u001b[0m       27.4437  0.0112\n",
      "      8       \u001b[36m28.8796\u001b[0m       27.2466  0.0119\n",
      "      9       \u001b[36m28.7304\u001b[0m       27.8022  0.0116\n",
      "     10       \u001b[36m28.7264\u001b[0m       27.5509  0.0128\n",
      "     11       \u001b[36m28.6192\u001b[0m       27.3617  0.0114\n",
      "     12       \u001b[36m28.5481\u001b[0m       27.6322  0.0112\n",
      "     13       \u001b[36m28.5409\u001b[0m       27.6067  0.0121\n",
      "     14       \u001b[36m28.5143\u001b[0m       27.4488  0.0115\n",
      "     15       \u001b[36m28.4890\u001b[0m       27.4789  0.0117\n",
      "     16       \u001b[36m28.4779\u001b[0m       27.5558  0.0111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m28.4725\u001b[0m       27.5096  0.0113\n",
      "     18       \u001b[36m28.4605\u001b[0m       27.4577  0.0116\n",
      "     19       \u001b[36m28.4531\u001b[0m       27.5078  0.0113\n",
      "     20       \u001b[36m28.4503\u001b[0m       27.4843  0.0113\n",
      "     21       \u001b[36m28.4408\u001b[0m       27.4825  0.0113\n",
      "     22       \u001b[36m28.4382\u001b[0m       27.4793  0.0112\n",
      "     23       \u001b[36m28.4323\u001b[0m       27.4706  0.0115\n",
      "     24       \u001b[36m28.4285\u001b[0m       27.4678  0.0120\n",
      "     25       \u001b[36m28.4249\u001b[0m       27.4710  0.0116\n",
      "     26       \u001b[36m28.4225\u001b[0m       27.4575  0.0113\n",
      "     27       \u001b[36m28.4177\u001b[0m       27.4473  0.0112\n",
      "     28       \u001b[36m28.4151\u001b[0m       27.4636  0.0144\n",
      "     29       \u001b[36m28.4140\u001b[0m       27.4434  0.0134\n",
      "     30       \u001b[36m28.4097\u001b[0m       27.4390  0.0123\n",
      "     31       \u001b[36m28.4084\u001b[0m       27.4363  0.0112\n",
      "     32       \u001b[36m28.4050\u001b[0m       27.4222  0.0126\n",
      "     33       \u001b[36m28.4045\u001b[0m       27.4309  0.0219\n",
      "     34       \u001b[36m28.4011\u001b[0m       27.4093  0.0146\n",
      "     35       \u001b[36m28.4006\u001b[0m       27.4191  0.0148\n",
      "     36       28.4027       27.4154  0.0142\n",
      "     37       \u001b[36m28.4004\u001b[0m       27.3934  0.0182\n",
      "     38       28.4054       27.4260  0.0135\n",
      "     39       28.4130       27.4141  0.0128\n",
      "     40       28.4250       27.3477  0.0129\n",
      "     41       28.4606       27.4027  0.0120\n",
      "     42       28.4427       27.4682  0.0119\n",
      "     43       28.4642       27.2019  0.0141\n",
      "     44       28.4040       27.3851  0.0125\n",
      "     45       28.4081       27.3331  0.0126\n",
      "     46       \u001b[36m28.3926\u001b[0m       27.3168  0.0120\n",
      "     47       28.3978       27.3402  0.0120\n",
      "     48       \u001b[36m28.3895\u001b[0m       27.3015  0.0153\n",
      "     49       \u001b[36m28.3860\u001b[0m       27.3552  0.0124\n",
      "     50       28.3866       27.3368  0.0125\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m41.3853\u001b[0m       \u001b[32m40.7249\u001b[0m  0.0114\n",
      "      2       \u001b[36m38.5025\u001b[0m       \u001b[32m37.2549\u001b[0m  0.0113\n",
      "      3       \u001b[36m35.9952\u001b[0m       \u001b[32m34.1985\u001b[0m  0.0129\n",
      "      4       \u001b[36m34.0993\u001b[0m       \u001b[32m32.0052\u001b[0m  0.0108\n",
      "      5       \u001b[36m33.0677\u001b[0m       \u001b[32m30.8581\u001b[0m  0.0107\n",
      "      6       \u001b[36m32.7162\u001b[0m       \u001b[32m30.4106\u001b[0m  0.0128\n",
      "      7       \u001b[36m32.6198\u001b[0m       \u001b[32m30.2403\u001b[0m  0.0141\n",
      "      8       \u001b[36m32.5749\u001b[0m       \u001b[32m30.1595\u001b[0m  0.0121\n",
      "      9       \u001b[36m32.5405\u001b[0m       \u001b[32m30.1129\u001b[0m  0.0119\n",
      "     10       \u001b[36m32.5124\u001b[0m       \u001b[32m30.0797\u001b[0m  0.0111\n",
      "     11       \u001b[36m32.4883\u001b[0m       \u001b[32m30.0536\u001b[0m  0.0112\n",
      "     12       \u001b[36m32.4681\u001b[0m       \u001b[32m30.0312\u001b[0m  0.0133\n",
      "     13       \u001b[36m32.4503\u001b[0m       \u001b[32m30.0120\u001b[0m  0.0119\n",
      "     14       \u001b[36m32.4349\u001b[0m       \u001b[32m29.9950\u001b[0m  0.0115\n",
      "     15       \u001b[36m32.4212\u001b[0m       \u001b[32m29.9792\u001b[0m  0.0113\n",
      "     16       \u001b[36m32.4094\u001b[0m       \u001b[32m29.9655\u001b[0m  0.0110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m32.3987\u001b[0m       \u001b[32m29.9531\u001b[0m  0.0122\n",
      "     18       \u001b[36m32.3891\u001b[0m       \u001b[32m29.9418\u001b[0m  0.0139\n",
      "     19       \u001b[36m32.3802\u001b[0m       \u001b[32m29.9318\u001b[0m  0.0124\n",
      "     20       \u001b[36m32.3721\u001b[0m       \u001b[32m29.9226\u001b[0m  0.0120\n",
      "     21       \u001b[36m32.3646\u001b[0m       \u001b[32m29.9141\u001b[0m  0.0129\n",
      "     22       \u001b[36m32.3576\u001b[0m       \u001b[32m29.9063\u001b[0m  0.0125\n",
      "     23       \u001b[36m32.3513\u001b[0m       \u001b[32m29.8996\u001b[0m  0.0118\n",
      "     24       \u001b[36m32.3454\u001b[0m       \u001b[32m29.8932\u001b[0m  0.0120\n",
      "     25       \u001b[36m32.3399\u001b[0m       \u001b[32m29.8865\u001b[0m  0.0120\n",
      "     26       \u001b[36m32.3349\u001b[0m       \u001b[32m29.8816\u001b[0m  0.0113\n",
      "     27       \u001b[36m32.3304\u001b[0m       \u001b[32m29.8757\u001b[0m  0.0123\n",
      "     28       \u001b[36m32.3261\u001b[0m       \u001b[32m29.8708\u001b[0m  0.0116\n",
      "     29       \u001b[36m32.3222\u001b[0m       \u001b[32m29.8664\u001b[0m  0.0113\n",
      "     30       \u001b[36m32.3183\u001b[0m       \u001b[32m29.8618\u001b[0m  0.0118\n",
      "     31       \u001b[36m32.3147\u001b[0m       \u001b[32m29.8581\u001b[0m  0.0116\n",
      "     32       \u001b[36m32.3114\u001b[0m       \u001b[32m29.8545\u001b[0m  0.0122\n",
      "     33       \u001b[36m32.3081\u001b[0m       \u001b[32m29.8512\u001b[0m  0.0114\n",
      "     34       \u001b[36m32.3051\u001b[0m       \u001b[32m29.8482\u001b[0m  0.0112\n",
      "     35       \u001b[36m32.3020\u001b[0m       \u001b[32m29.8451\u001b[0m  0.0142\n",
      "     36       \u001b[36m32.2990\u001b[0m       \u001b[32m29.8424\u001b[0m  0.0111\n",
      "     37       \u001b[36m32.2967\u001b[0m       \u001b[32m29.8399\u001b[0m  0.0120\n",
      "     38       \u001b[36m32.2939\u001b[0m       \u001b[32m29.8372\u001b[0m  0.0112\n",
      "     39       \u001b[36m32.2916\u001b[0m       \u001b[32m29.8351\u001b[0m  0.0113\n",
      "     40       \u001b[36m32.2893\u001b[0m       \u001b[32m29.8331\u001b[0m  0.0118\n",
      "     41       \u001b[36m32.2870\u001b[0m       \u001b[32m29.8310\u001b[0m  0.0109\n",
      "     42       \u001b[36m32.2850\u001b[0m       \u001b[32m29.8290\u001b[0m  0.0120\n",
      "     43       \u001b[36m32.2831\u001b[0m       \u001b[32m29.8273\u001b[0m  0.0116\n",
      "     44       \u001b[36m32.2810\u001b[0m       \u001b[32m29.8255\u001b[0m  0.0119\n",
      "     45       \u001b[36m32.2791\u001b[0m       \u001b[32m29.8233\u001b[0m  0.0148\n",
      "     46       \u001b[36m32.2775\u001b[0m       \u001b[32m29.8222\u001b[0m  0.0118\n",
      "     47       \u001b[36m32.2757\u001b[0m       \u001b[32m29.8209\u001b[0m  0.0123\n",
      "     48       \u001b[36m32.2739\u001b[0m       \u001b[32m29.8188\u001b[0m  0.0122\n",
      "     49       \u001b[36m32.2725\u001b[0m       \u001b[32m29.8183\u001b[0m  0.0119\n",
      "     50       \u001b[36m32.2709\u001b[0m       \u001b[32m29.8167\u001b[0m  0.0121\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m32.8901\u001b[0m       \u001b[32m30.1164\u001b[0m  0.0108\n",
      "      2       \u001b[36m29.3867\u001b[0m       \u001b[32m27.7892\u001b[0m  0.0117\n",
      "      3       \u001b[36m26.3296\u001b[0m       \u001b[32m26.3384\u001b[0m  0.0116\n",
      "      4       \u001b[36m24.3486\u001b[0m       \u001b[32m26.0849\u001b[0m  0.0117\n",
      "      5       \u001b[36m23.6135\u001b[0m       26.2468  0.0119\n",
      "      6       \u001b[36m23.4281\u001b[0m       26.3352  0.0110\n",
      "      7       \u001b[36m23.3606\u001b[0m       26.3613  0.0117\n",
      "      8       \u001b[36m23.3184\u001b[0m       26.3698  0.0163\n",
      "      9       \u001b[36m23.2866\u001b[0m       26.3728  0.0143\n",
      "     10       \u001b[36m23.2615\u001b[0m       26.3761  0.0169\n",
      "     11       \u001b[36m23.2408\u001b[0m       26.3791  0.0180\n",
      "     12       \u001b[36m23.2239\u001b[0m       26.3820  0.0139\n",
      "     13       \u001b[36m23.2098\u001b[0m       26.3855  0.0126\n",
      "     14       \u001b[36m23.1980\u001b[0m       26.3890  0.0126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     15       \u001b[36m23.1877\u001b[0m       26.3924  0.0115\n",
      "     16       \u001b[36m23.1792\u001b[0m       26.3958  0.0118\n",
      "     17       \u001b[36m23.1717\u001b[0m       26.3985  0.0112\n",
      "     18       \u001b[36m23.1649\u001b[0m       26.4006  0.0115\n",
      "     19       \u001b[36m23.1589\u001b[0m       26.4031  0.0109\n",
      "     20       \u001b[36m23.1536\u001b[0m       26.4047  0.0109\n",
      "     21       \u001b[36m23.1489\u001b[0m       26.4060  0.0111\n",
      "     22       \u001b[36m23.1447\u001b[0m       26.4072  0.0113\n",
      "     23       \u001b[36m23.1408\u001b[0m       26.4083  0.0112\n",
      "     24       \u001b[36m23.1373\u001b[0m       26.4092  0.0111\n",
      "     25       \u001b[36m23.1341\u001b[0m       26.4107  0.0118\n",
      "     26       \u001b[36m23.1311\u001b[0m       26.4111  0.0113\n",
      "     27       \u001b[36m23.1284\u001b[0m       26.4115  0.0116\n",
      "     28       \u001b[36m23.1259\u001b[0m       26.4114  0.0111\n",
      "     29       \u001b[36m23.1236\u001b[0m       26.4114  0.0110\n",
      "     30       \u001b[36m23.1215\u001b[0m       26.4115  0.0110\n",
      "     31       \u001b[36m23.1194\u001b[0m       26.4111  0.0111\n",
      "     32       \u001b[36m23.1175\u001b[0m       26.4109  0.0117\n",
      "     33       \u001b[36m23.1156\u001b[0m       26.4107  0.0109\n",
      "     34       \u001b[36m23.1138\u001b[0m       26.4104  0.0113\n",
      "     35       \u001b[36m23.1121\u001b[0m       26.4103  0.0108\n",
      "     36       \u001b[36m23.1104\u001b[0m       26.4101  0.0117\n",
      "     37       \u001b[36m23.1089\u001b[0m       26.4099  0.0122\n",
      "     38       \u001b[36m23.1074\u001b[0m       26.4097  0.0120\n",
      "     39       \u001b[36m23.1060\u001b[0m       26.4090  0.0106\n",
      "     40       \u001b[36m23.1047\u001b[0m       26.4087  0.0106\n",
      "     41       \u001b[36m23.1033\u001b[0m       26.4086  0.0126\n",
      "     42       \u001b[36m23.1020\u001b[0m       26.4079  0.0115\n",
      "     43       \u001b[36m23.1009\u001b[0m       26.4077  0.0114\n",
      "     44       \u001b[36m23.0997\u001b[0m       26.4075  0.0108\n",
      "     45       \u001b[36m23.0985\u001b[0m       26.4068  0.0108\n",
      "     46       \u001b[36m23.0974\u001b[0m       26.4064  0.0132\n",
      "     47       \u001b[36m23.0964\u001b[0m       26.4059  0.0119\n",
      "     48       \u001b[36m23.0954\u001b[0m       26.4051  0.0117\n",
      "     49       \u001b[36m23.0944\u001b[0m       26.4049  0.0107\n",
      "     50       \u001b[36m23.0935\u001b[0m       26.4041  0.0105\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m39.1899\u001b[0m       \u001b[32m29.8730\u001b[0m  0.0137\n",
      "      2       \u001b[36m35.9724\u001b[0m       \u001b[32m28.0607\u001b[0m  0.0121\n",
      "      3       \u001b[36m33.2237\u001b[0m       \u001b[32m26.7389\u001b[0m  0.0119\n",
      "      4       \u001b[36m30.9573\u001b[0m       \u001b[32m26.1123\u001b[0m  0.0127\n",
      "      5       \u001b[36m29.4291\u001b[0m       26.2744  0.0110\n",
      "      6       \u001b[36m28.7599\u001b[0m       26.7377  0.0114\n",
      "      7       \u001b[36m28.5988\u001b[0m       27.0169  0.0135\n",
      "      8       \u001b[36m28.5660\u001b[0m       27.1213  0.0120\n",
      "      9       \u001b[36m28.5490\u001b[0m       27.1514  0.0124\n",
      "     10       \u001b[36m28.5342\u001b[0m       27.1569  0.0110\n",
      "     11       \u001b[36m28.5214\u001b[0m       27.1569  0.0108\n",
      "     12       \u001b[36m28.5107\u001b[0m       27.1550  0.0128\n",
      "     13       \u001b[36m28.5018\u001b[0m       27.1531  0.0122\n",
      "     14       \u001b[36m28.4944\u001b[0m       27.1511  0.0117\n",
      "     15       \u001b[36m28.4883\u001b[0m       27.1528  0.0106\n",
      "     16       \u001b[36m28.4832\u001b[0m       27.1529  0.0108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m28.4785\u001b[0m       27.1524  0.0142\n",
      "     18       \u001b[36m28.4742\u001b[0m       27.1533  0.0123\n",
      "     19       \u001b[36m28.4705\u001b[0m       27.1503  0.0116\n",
      "     20       \u001b[36m28.4667\u001b[0m       27.1524  0.0107\n",
      "     21       \u001b[36m28.4636\u001b[0m       27.1527  0.0106\n",
      "     22       \u001b[36m28.4608\u001b[0m       27.1503  0.0127\n",
      "     23       \u001b[36m28.4577\u001b[0m       27.1521  0.0115\n",
      "     24       \u001b[36m28.4552\u001b[0m       27.1533  0.0116\n",
      "     25       \u001b[36m28.4528\u001b[0m       27.1531  0.0106\n",
      "     26       \u001b[36m28.4505\u001b[0m       27.1517  0.0108\n",
      "     27       \u001b[36m28.4481\u001b[0m       27.1537  0.0133\n",
      "     28       \u001b[36m28.4463\u001b[0m       27.1542  0.0120\n",
      "     29       \u001b[36m28.4446\u001b[0m       27.1527  0.0122\n",
      "     30       \u001b[36m28.4427\u001b[0m       27.1550  0.0113\n",
      "     31       \u001b[36m28.4412\u001b[0m       27.1552  0.0109\n",
      "     32       \u001b[36m28.4398\u001b[0m       27.1534  0.0135\n",
      "     33       \u001b[36m28.4382\u001b[0m       27.1550  0.0119\n",
      "     34       \u001b[36m28.4371\u001b[0m       27.1532  0.0119\n",
      "     35       \u001b[36m28.4356\u001b[0m       27.1551  0.0106\n",
      "     36       \u001b[36m28.4346\u001b[0m       27.1535  0.0106\n",
      "     37       \u001b[36m28.4333\u001b[0m       27.1545  0.0166\n",
      "     38       \u001b[36m28.4323\u001b[0m       27.1535  0.0146\n",
      "     39       \u001b[36m28.4312\u001b[0m       27.1546  0.0130\n",
      "     40       \u001b[36m28.4304\u001b[0m       27.1540  0.0113\n",
      "     41       \u001b[36m28.4294\u001b[0m       27.1550  0.0114\n",
      "     42       \u001b[36m28.4287\u001b[0m       27.1541  0.0150\n",
      "     43       \u001b[36m28.4277\u001b[0m       27.1554  0.0129\n",
      "     44       \u001b[36m28.4271\u001b[0m       27.1542  0.0127\n",
      "     45       \u001b[36m28.4262\u001b[0m       27.1561  0.0110\n",
      "     46       \u001b[36m28.4257\u001b[0m       27.1545  0.0110\n",
      "     47       \u001b[36m28.4248\u001b[0m       27.1557  0.0132\n",
      "     48       \u001b[36m28.4243\u001b[0m       27.1545  0.0116\n",
      "     49       \u001b[36m28.4235\u001b[0m       27.1543  0.0117\n",
      "     50       \u001b[36m28.4228\u001b[0m       27.1557  0.0111\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m40.9593\u001b[0m       \u001b[32m34.9363\u001b[0m  0.0120\n",
      "      2       \u001b[36m35.3816\u001b[0m       \u001b[32m31.1929\u001b[0m  0.0139\n",
      "      3       \u001b[36m33.8794\u001b[0m       33.0925  0.0128\n",
      "      4       \u001b[36m33.7382\u001b[0m       \u001b[32m31.1414\u001b[0m  0.0126\n",
      "      5       \u001b[36m32.9783\u001b[0m       \u001b[32m30.2510\u001b[0m  0.0126\n",
      "      6       \u001b[36m32.9004\u001b[0m       30.7774  0.0128\n",
      "      7       \u001b[36m32.7742\u001b[0m       30.8160  0.0124\n",
      "      8       \u001b[36m32.5710\u001b[0m       \u001b[32m30.1699\u001b[0m  0.0121\n",
      "      9       \u001b[36m32.4888\u001b[0m       30.2353  0.0150\n",
      "     10       \u001b[36m32.4264\u001b[0m       30.4674  0.0128\n",
      "     11       \u001b[36m32.3783\u001b[0m       \u001b[32m30.1681\u001b[0m  0.0124\n",
      "     12       \u001b[36m32.3286\u001b[0m       \u001b[32m30.1285\u001b[0m  0.0120\n",
      "     13       \u001b[36m32.3138\u001b[0m       30.2766  0.0119\n",
      "     14       \u001b[36m32.2966\u001b[0m       30.1755  0.0141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     15       \u001b[36m32.2731\u001b[0m       30.1364  0.0138\n",
      "     16       \u001b[36m32.2651\u001b[0m       30.1490  0.0134\n",
      "     17       \u001b[36m32.2569\u001b[0m       30.1532  0.0140\n",
      "     18       \u001b[36m32.2430\u001b[0m       \u001b[32m30.0660\u001b[0m  0.0115\n",
      "     19       \u001b[36m32.2382\u001b[0m       30.1425  0.0115\n",
      "     20       \u001b[36m32.2336\u001b[0m       \u001b[32m30.0651\u001b[0m  0.0142\n",
      "     21       \u001b[36m32.2274\u001b[0m       30.1258  0.0127\n",
      "     22       \u001b[36m32.2238\u001b[0m       \u001b[32m30.0489\u001b[0m  0.0127\n",
      "     23       \u001b[36m32.2235\u001b[0m       30.1566  0.0142\n",
      "     24       \u001b[36m32.2198\u001b[0m       \u001b[32m30.0404\u001b[0m  0.0134\n",
      "     25       32.2253       30.1635  0.0130\n",
      "     26       32.2219       \u001b[32m30.0214\u001b[0m  0.0124\n",
      "     27       32.2253       30.0755  0.0125\n",
      "     28       \u001b[36m32.2110\u001b[0m       \u001b[32m29.9796\u001b[0m  0.0124\n",
      "     29       \u001b[36m32.1963\u001b[0m       29.9923  0.0128\n",
      "     30       \u001b[36m32.1871\u001b[0m       29.9950  0.0130\n",
      "     31       \u001b[36m32.1758\u001b[0m       \u001b[32m29.9657\u001b[0m  0.0128\n",
      "     32       \u001b[36m32.1719\u001b[0m       29.9912  0.0117\n",
      "     33       \u001b[36m32.1674\u001b[0m       \u001b[32m29.9638\u001b[0m  0.0115\n",
      "     34       \u001b[36m32.1629\u001b[0m       29.9770  0.0144\n",
      "     35       \u001b[36m32.1596\u001b[0m       \u001b[32m29.9637\u001b[0m  0.0124\n",
      "     36       \u001b[36m32.1553\u001b[0m       29.9659  0.0129\n",
      "     37       \u001b[36m32.1518\u001b[0m       \u001b[32m29.9578\u001b[0m  0.0131\n",
      "     38       \u001b[36m32.1480\u001b[0m       29.9611  0.0124\n",
      "     39       \u001b[36m32.1445\u001b[0m       29.9589  0.0120\n",
      "     40       \u001b[36m32.1413\u001b[0m       29.9594  0.0118\n",
      "     41       \u001b[36m32.1383\u001b[0m       29.9638  0.0112\n",
      "     42       \u001b[36m32.1353\u001b[0m       \u001b[32m29.9568\u001b[0m  0.0114\n",
      "     43       \u001b[36m32.1322\u001b[0m       29.9691  0.0120\n",
      "     44       \u001b[36m32.1304\u001b[0m       29.9601  0.0120\n",
      "     45       \u001b[36m32.1275\u001b[0m       29.9571  0.0120\n",
      "     46       \u001b[36m32.1256\u001b[0m       29.9912  0.0115\n",
      "     47       32.1292       29.9685  0.0116\n",
      "     48       32.1356       29.9784  0.0120\n",
      "     49       32.1340       30.0169  0.0118\n",
      "     50       32.1431       30.0097  0.0119\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m31.8667\u001b[0m       \u001b[32m27.9142\u001b[0m  0.0117\n",
      "      2       \u001b[36m25.9494\u001b[0m       29.3190  0.0116\n",
      "      3       \u001b[36m24.4956\u001b[0m       \u001b[32m26.3638\u001b[0m  0.0128\n",
      "      4       \u001b[36m24.3123\u001b[0m       26.5284  0.0118\n",
      "      5       \u001b[36m23.7162\u001b[0m       27.6587  0.0118\n",
      "      6       \u001b[36m23.5808\u001b[0m       27.2325  0.0121\n",
      "      7       \u001b[36m23.4807\u001b[0m       26.9377  0.0114\n",
      "      8       \u001b[36m23.3897\u001b[0m       27.1874  0.0113\n",
      "      9       \u001b[36m23.2703\u001b[0m       26.9541  0.0117\n",
      "     10       \u001b[36m23.2258\u001b[0m       26.6737  0.0121\n",
      "     11       \u001b[36m23.2133\u001b[0m       26.9190  0.0120\n",
      "     12       \u001b[36m23.1647\u001b[0m       26.9104  0.0145\n",
      "     13       \u001b[36m23.1408\u001b[0m       26.6632  0.0182\n",
      "     14       \u001b[36m23.1365\u001b[0m       26.7316  0.0128\n",
      "     15       \u001b[36m23.1133\u001b[0m       26.7457  0.0121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     16       \u001b[36m23.1031\u001b[0m       26.6613  0.0150\n",
      "     17       \u001b[36m23.0957\u001b[0m       26.6807  0.0181\n",
      "     18       \u001b[36m23.0854\u001b[0m       26.6947  0.0122\n",
      "     19       \u001b[36m23.0768\u001b[0m       26.6707  0.0131\n",
      "     20       \u001b[36m23.0728\u001b[0m       26.6785  0.0120\n",
      "     21       \u001b[36m23.0661\u001b[0m       26.6800  0.0119\n",
      "     22       \u001b[36m23.0608\u001b[0m       26.6656  0.0121\n",
      "     23       \u001b[36m23.0573\u001b[0m       26.6766  0.0121\n",
      "     24       \u001b[36m23.0522\u001b[0m       26.6707  0.0116\n",
      "     25       \u001b[36m23.0487\u001b[0m       26.6691  0.0116\n",
      "     26       \u001b[36m23.0450\u001b[0m       26.6698  0.0116\n",
      "     27       \u001b[36m23.0419\u001b[0m       26.6621  0.0122\n",
      "     28       \u001b[36m23.0384\u001b[0m       26.6496  0.0118\n",
      "     29       \u001b[36m23.0362\u001b[0m       26.6527  0.0116\n",
      "     30       \u001b[36m23.0326\u001b[0m       26.6394  0.0115\n",
      "     31       \u001b[36m23.0315\u001b[0m       26.6439  0.0115\n",
      "     32       \u001b[36m23.0276\u001b[0m       26.6296  0.0130\n",
      "     33       23.0281       26.6456  0.0117\n",
      "     34       \u001b[36m23.0239\u001b[0m       26.6154  0.0114\n",
      "     35       23.0259       26.6494  0.0116\n",
      "     36       \u001b[36m23.0223\u001b[0m       26.6070  0.0116\n",
      "     37       23.0240       26.6551  0.0117\n",
      "     38       \u001b[36m23.0191\u001b[0m       26.5924  0.0117\n",
      "     39       \u001b[36m23.0181\u001b[0m       26.6636  0.0125\n",
      "     40       \u001b[36m23.0115\u001b[0m       26.5919  0.0117\n",
      "     41       23.0126       26.6480  0.0112\n",
      "     42       \u001b[36m23.0077\u001b[0m       26.6043  0.0113\n",
      "     43       \u001b[36m23.0063\u001b[0m       26.6341  0.0124\n",
      "     44       \u001b[36m23.0042\u001b[0m       26.6001  0.0122\n",
      "     45       \u001b[36m23.0023\u001b[0m       26.6426  0.0122\n",
      "     46       23.0055       26.5894  0.0114\n",
      "     47       23.0128       26.6510  0.0116\n",
      "     48       23.0041       26.5970  0.0128\n",
      "     49       23.0083       26.5968  0.0119\n",
      "     50       \u001b[36m22.9962\u001b[0m       26.6550  0.0118\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m39.8069\u001b[0m       \u001b[32m27.6392\u001b[0m  0.0118\n",
      "      2       \u001b[36m32.5867\u001b[0m       30.8838  0.0116\n",
      "      3       \u001b[36m30.6347\u001b[0m       \u001b[32m26.4807\u001b[0m  0.0122\n",
      "      4       \u001b[36m29.9501\u001b[0m       26.8128  0.0119\n",
      "      5       \u001b[36m29.3449\u001b[0m       28.4683  0.0120\n",
      "      6       \u001b[36m29.1832\u001b[0m       27.3208  0.0119\n",
      "      7       \u001b[36m28.8858\u001b[0m       27.2037  0.0118\n",
      "      8       \u001b[36m28.7806\u001b[0m       27.7440  0.0116\n",
      "      9       \u001b[36m28.7602\u001b[0m       27.1082  0.0120\n",
      "     10       \u001b[36m28.6154\u001b[0m       27.0887  0.0116\n",
      "     11       \u001b[36m28.5852\u001b[0m       27.5719  0.0116\n",
      "     12       \u001b[36m28.5850\u001b[0m       27.2157  0.0115\n",
      "     13       \u001b[36m28.5207\u001b[0m       27.1253  0.0115\n",
      "     14       \u001b[36m28.5109\u001b[0m       27.3315  0.0119\n",
      "     15       \u001b[36m28.5049\u001b[0m       27.2181  0.0119\n",
      "     16       \u001b[36m28.4785\u001b[0m       27.2133  0.0116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m28.4764\u001b[0m       27.2364  0.0117\n",
      "     18       \u001b[36m28.4671\u001b[0m       27.1846  0.0114\n",
      "     19       \u001b[36m28.4576\u001b[0m       27.2099  0.0158\n",
      "     20       \u001b[36m28.4547\u001b[0m       27.1966  0.0117\n",
      "     21       \u001b[36m28.4478\u001b[0m       27.1842  0.0119\n",
      "     22       \u001b[36m28.4438\u001b[0m       27.1890  0.0115\n",
      "     23       \u001b[36m28.4398\u001b[0m       27.1914  0.0116\n",
      "     24       \u001b[36m28.4368\u001b[0m       27.1828  0.0119\n",
      "     25       \u001b[36m28.4326\u001b[0m       27.1824  0.0114\n",
      "     26       \u001b[36m28.4301\u001b[0m       27.1850  0.0116\n",
      "     27       \u001b[36m28.4272\u001b[0m       27.1764  0.0112\n",
      "     28       \u001b[36m28.4244\u001b[0m       27.1865  0.0114\n",
      "     29       \u001b[36m28.4226\u001b[0m       27.1783  0.0119\n",
      "     30       \u001b[36m28.4197\u001b[0m       27.1819  0.0115\n",
      "     31       \u001b[36m28.4186\u001b[0m       27.1797  0.0120\n",
      "     32       \u001b[36m28.4161\u001b[0m       27.1818  0.0117\n",
      "     33       \u001b[36m28.4148\u001b[0m       27.1810  0.0113\n",
      "     34       \u001b[36m28.4130\u001b[0m       27.1805  0.0120\n",
      "     35       \u001b[36m28.4113\u001b[0m       27.1831  0.0117\n",
      "     36       \u001b[36m28.4099\u001b[0m       27.1822  0.0116\n",
      "     37       \u001b[36m28.4085\u001b[0m       27.1845  0.0118\n",
      "     38       \u001b[36m28.4072\u001b[0m       27.1850  0.0114\n",
      "     39       \u001b[36m28.4060\u001b[0m       27.1880  0.0121\n",
      "     40       \u001b[36m28.4045\u001b[0m       27.1887  0.0151\n",
      "     41       \u001b[36m28.4036\u001b[0m       27.1912  0.0167\n",
      "     42       \u001b[36m28.4023\u001b[0m       27.1912  0.0125\n",
      "     43       \u001b[36m28.4014\u001b[0m       27.1974  0.0122\n",
      "     44       \u001b[36m28.4001\u001b[0m       27.1935  0.0131\n",
      "     45       \u001b[36m28.3992\u001b[0m       27.1994  0.0216\n",
      "     46       \u001b[36m28.3979\u001b[0m       27.1941  0.0120\n",
      "     47       \u001b[36m28.3972\u001b[0m       27.2031  0.0133\n",
      "     48       \u001b[36m28.3963\u001b[0m       27.2016  0.0123\n",
      "     49       \u001b[36m28.3951\u001b[0m       27.2043  0.0121\n",
      "     50       28.3959       27.2071  0.0118\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m42.6726\u001b[0m       \u001b[32m41.4968\u001b[0m  0.0111\n",
      "      2       \u001b[36m38.8902\u001b[0m       \u001b[32m37.2980\u001b[0m  0.0115\n",
      "      3       \u001b[36m36.0050\u001b[0m       \u001b[32m33.9155\u001b[0m  0.0114\n",
      "      4       \u001b[36m34.0133\u001b[0m       \u001b[32m31.6644\u001b[0m  0.0112\n",
      "      5       \u001b[36m33.0328\u001b[0m       \u001b[32m30.6389\u001b[0m  0.0112\n",
      "      6       \u001b[36m32.7274\u001b[0m       \u001b[32m30.2779\u001b[0m  0.0111\n",
      "      7       \u001b[36m32.6291\u001b[0m       \u001b[32m30.1422\u001b[0m  0.0114\n",
      "      8       \u001b[36m32.5727\u001b[0m       \u001b[32m30.0755\u001b[0m  0.0113\n",
      "      9       \u001b[36m32.5325\u001b[0m       \u001b[32m30.0352\u001b[0m  0.0109\n",
      "     10       \u001b[36m32.5013\u001b[0m       \u001b[32m30.0059\u001b[0m  0.0108\n",
      "     11       \u001b[36m32.4752\u001b[0m       \u001b[32m29.9819\u001b[0m  0.0107\n",
      "     12       \u001b[36m32.4543\u001b[0m       \u001b[32m29.9624\u001b[0m  0.0109\n",
      "     13       \u001b[36m32.4365\u001b[0m       \u001b[32m29.9468\u001b[0m  0.0110\n",
      "     14       \u001b[36m32.4215\u001b[0m       \u001b[32m29.9330\u001b[0m  0.0110\n",
      "     15       \u001b[36m32.4079\u001b[0m       \u001b[32m29.9211\u001b[0m  0.0106\n",
      "     16       \u001b[36m32.3964\u001b[0m       \u001b[32m29.9100\u001b[0m  0.0108\n",
      "     17       \u001b[36m32.3860\u001b[0m       \u001b[32m29.9006\u001b[0m  0.0109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     18       \u001b[36m32.3769\u001b[0m       \u001b[32m29.8920\u001b[0m  0.0121\n",
      "     19       \u001b[36m32.3688\u001b[0m       \u001b[32m29.8840\u001b[0m  0.0110\n",
      "     20       \u001b[36m32.3615\u001b[0m       \u001b[32m29.8780\u001b[0m  0.0113\n",
      "     21       \u001b[36m32.3549\u001b[0m       \u001b[32m29.8710\u001b[0m  0.0106\n",
      "     22       \u001b[36m32.3489\u001b[0m       \u001b[32m29.8654\u001b[0m  0.0109\n",
      "     23       \u001b[36m32.3434\u001b[0m       \u001b[32m29.8612\u001b[0m  0.0109\n",
      "     24       \u001b[36m32.3384\u001b[0m       \u001b[32m29.8565\u001b[0m  0.0115\n",
      "     25       \u001b[36m32.3336\u001b[0m       \u001b[32m29.8535\u001b[0m  0.0113\n",
      "     26       \u001b[36m32.3291\u001b[0m       \u001b[32m29.8494\u001b[0m  0.0110\n",
      "     27       \u001b[36m32.3250\u001b[0m       \u001b[32m29.8463\u001b[0m  0.0108\n",
      "     28       \u001b[36m32.3212\u001b[0m       \u001b[32m29.8433\u001b[0m  0.0111\n",
      "     29       \u001b[36m32.3177\u001b[0m       \u001b[32m29.8411\u001b[0m  0.0120\n",
      "     30       \u001b[36m32.3142\u001b[0m       \u001b[32m29.8387\u001b[0m  0.0111\n",
      "     31       \u001b[36m32.3111\u001b[0m       \u001b[32m29.8364\u001b[0m  0.0109\n",
      "     32       \u001b[36m32.3081\u001b[0m       \u001b[32m29.8342\u001b[0m  0.0108\n",
      "     33       \u001b[36m32.3053\u001b[0m       \u001b[32m29.8318\u001b[0m  0.0107\n",
      "     34       \u001b[36m32.3024\u001b[0m       \u001b[32m29.8305\u001b[0m  0.0110\n",
      "     35       \u001b[36m32.3000\u001b[0m       \u001b[32m29.8284\u001b[0m  0.0108\n",
      "     36       \u001b[36m32.2974\u001b[0m       \u001b[32m29.8270\u001b[0m  0.0108\n",
      "     37       \u001b[36m32.2951\u001b[0m       \u001b[32m29.8256\u001b[0m  0.0112\n",
      "     38       \u001b[36m32.2929\u001b[0m       \u001b[32m29.8244\u001b[0m  0.0114\n",
      "     39       \u001b[36m32.2906\u001b[0m       \u001b[32m29.8232\u001b[0m  0.0111\n",
      "     40       \u001b[36m32.2887\u001b[0m       \u001b[32m29.8222\u001b[0m  0.0111\n",
      "     41       \u001b[36m32.2865\u001b[0m       \u001b[32m29.8212\u001b[0m  0.0108\n",
      "     42       \u001b[36m32.2849\u001b[0m       \u001b[32m29.8202\u001b[0m  0.0107\n",
      "     43       \u001b[36m32.2830\u001b[0m       \u001b[32m29.8190\u001b[0m  0.0108\n",
      "     44       \u001b[36m32.2814\u001b[0m       \u001b[32m29.8182\u001b[0m  0.0110\n",
      "     45       \u001b[36m32.2795\u001b[0m       \u001b[32m29.8169\u001b[0m  0.0108\n",
      "     46       \u001b[36m32.2780\u001b[0m       \u001b[32m29.8158\u001b[0m  0.0107\n",
      "     47       \u001b[36m32.2764\u001b[0m       \u001b[32m29.8151\u001b[0m  0.0105\n",
      "     48       \u001b[36m32.2749\u001b[0m       \u001b[32m29.8138\u001b[0m  0.0110\n",
      "     49       \u001b[36m32.2734\u001b[0m       \u001b[32m29.8137\u001b[0m  0.0109\n",
      "     50       \u001b[36m32.2721\u001b[0m       \u001b[32m29.8120\u001b[0m  0.0109\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m31.9991\u001b[0m       \u001b[32m29.6820\u001b[0m  0.0110\n",
      "      2       \u001b[36m29.0399\u001b[0m       \u001b[32m27.8707\u001b[0m  0.0108\n",
      "      3       \u001b[36m26.5908\u001b[0m       \u001b[32m26.6629\u001b[0m  0.0112\n",
      "      4       \u001b[36m24.8183\u001b[0m       \u001b[32m26.2366\u001b[0m  0.0111\n",
      "      5       \u001b[36m23.9026\u001b[0m       26.2993  0.0110\n",
      "      6       \u001b[36m23.5771\u001b[0m       26.4102  0.0107\n",
      "      7       \u001b[36m23.4665\u001b[0m       26.4519  0.0108\n",
      "      8       \u001b[36m23.4091\u001b[0m       26.4573  0.0111\n",
      "      9       \u001b[36m23.3684\u001b[0m       26.4509  0.0113\n",
      "     10       \u001b[36m23.3356\u001b[0m       26.4420  0.0110\n",
      "     11       \u001b[36m23.3078\u001b[0m       26.4336  0.0110\n",
      "     12       \u001b[36m23.2840\u001b[0m       26.4266  0.0108\n",
      "     13       \u001b[36m23.2634\u001b[0m       26.4213  0.0110\n",
      "     14       \u001b[36m23.2455\u001b[0m       26.4170  0.0112\n",
      "     15       \u001b[36m23.2300\u001b[0m       26.4140  0.0109\n",
      "     16       \u001b[36m23.2164\u001b[0m       26.4116  0.0112\n",
      "     17       \u001b[36m23.2047\u001b[0m       26.4102  0.0107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     18       \u001b[36m23.1943\u001b[0m       26.4086  0.0117\n",
      "     19       \u001b[36m23.1851\u001b[0m       26.4070  0.0110\n",
      "     20       \u001b[36m23.1768\u001b[0m       26.4061  0.0109\n",
      "     21       \u001b[36m23.1694\u001b[0m       26.4053  0.0109\n",
      "     22       \u001b[36m23.1628\u001b[0m       26.4048  0.0143\n",
      "     23       \u001b[36m23.1568\u001b[0m       26.4041  0.0165\n",
      "     24       \u001b[36m23.1514\u001b[0m       26.4039  0.0186\n",
      "     25       \u001b[36m23.1464\u001b[0m       26.4039  0.0173\n",
      "     26       \u001b[36m23.1419\u001b[0m       26.4040  0.0123\n",
      "     27       \u001b[36m23.1377\u001b[0m       26.4040  0.0159\n",
      "     28       \u001b[36m23.1339\u001b[0m       26.4041  0.0134\n",
      "     29       \u001b[36m23.1304\u001b[0m       26.4044  0.0112\n",
      "     30       \u001b[36m23.1271\u001b[0m       26.4045  0.0109\n",
      "     31       \u001b[36m23.1241\u001b[0m       26.4044  0.0111\n",
      "     32       \u001b[36m23.1213\u001b[0m       26.4046  0.0112\n",
      "     33       \u001b[36m23.1187\u001b[0m       26.4048  0.0112\n",
      "     34       \u001b[36m23.1163\u001b[0m       26.4049  0.0111\n",
      "     35       \u001b[36m23.1140\u001b[0m       26.4051  0.0110\n",
      "     36       \u001b[36m23.1119\u001b[0m       26.4051  0.0112\n",
      "     37       \u001b[36m23.1099\u001b[0m       26.4053  0.0111\n",
      "     38       \u001b[36m23.1080\u001b[0m       26.4051  0.0112\n",
      "     39       \u001b[36m23.1063\u001b[0m       26.4053  0.0112\n",
      "     40       \u001b[36m23.1045\u001b[0m       26.4052  0.0109\n",
      "     41       \u001b[36m23.1028\u001b[0m       26.4054  0.0112\n",
      "     42       \u001b[36m23.1012\u001b[0m       26.4056  0.0112\n",
      "     43       \u001b[36m23.0998\u001b[0m       26.4057  0.0111\n",
      "     44       \u001b[36m23.0983\u001b[0m       26.4060  0.0109\n",
      "     45       \u001b[36m23.0969\u001b[0m       26.4062  0.0105\n",
      "     46       \u001b[36m23.0956\u001b[0m       26.4062  0.0111\n",
      "     47       \u001b[36m23.0944\u001b[0m       26.4061  0.0108\n",
      "     48       \u001b[36m23.0932\u001b[0m       26.4062  0.0116\n",
      "     49       \u001b[36m23.0921\u001b[0m       26.4062  0.0110\n",
      "     50       \u001b[36m23.0910\u001b[0m       26.4064  0.0109\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m39.8164\u001b[0m       \u001b[32m29.5281\u001b[0m  0.0110\n",
      "      2       \u001b[36m34.9378\u001b[0m       \u001b[32m26.8910\u001b[0m  0.0113\n",
      "      3       \u001b[36m31.0499\u001b[0m       \u001b[32m26.1701\u001b[0m  0.0109\n",
      "      4       \u001b[36m29.2176\u001b[0m       26.8209  0.0111\n",
      "      5       \u001b[36m28.8628\u001b[0m       27.1837  0.0107\n",
      "      6       \u001b[36m28.7896\u001b[0m       27.2700  0.0109\n",
      "      7       \u001b[36m28.7368\u001b[0m       27.2866  0.0110\n",
      "      8       \u001b[36m28.6928\u001b[0m       27.2859  0.0111\n",
      "      9       \u001b[36m28.6565\u001b[0m       27.2881  0.0114\n",
      "     10       \u001b[36m28.6269\u001b[0m       27.2896  0.0116\n",
      "     11       \u001b[36m28.6028\u001b[0m       27.2917  0.0113\n",
      "     12       \u001b[36m28.5826\u001b[0m       27.2923  0.0120\n",
      "     13       \u001b[36m28.5653\u001b[0m       27.2940  0.0116\n",
      "     14       \u001b[36m28.5507\u001b[0m       27.2975  0.0117\n",
      "     15       \u001b[36m28.5386\u001b[0m       27.2977  0.0114\n",
      "     16       \u001b[36m28.5279\u001b[0m       27.3012  0.0111\n",
      "     17       \u001b[36m28.5190\u001b[0m       27.3040  0.0114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     18       \u001b[36m28.5111\u001b[0m       27.3052  0.0116\n",
      "     19       \u001b[36m28.5039\u001b[0m       27.3058  0.0112\n",
      "     20       \u001b[36m28.4975\u001b[0m       27.3063  0.0112\n",
      "     21       \u001b[36m28.4916\u001b[0m       27.3055  0.0111\n",
      "     22       \u001b[36m28.4862\u001b[0m       27.3065  0.0109\n",
      "     23       \u001b[36m28.4814\u001b[0m       27.3074  0.0111\n",
      "     24       \u001b[36m28.4770\u001b[0m       27.3065  0.0113\n",
      "     25       \u001b[36m28.4729\u001b[0m       27.3052  0.0111\n",
      "     26       \u001b[36m28.4691\u001b[0m       27.3052  0.0107\n",
      "     27       \u001b[36m28.4658\u001b[0m       27.3055  0.0110\n",
      "     28       \u001b[36m28.4627\u001b[0m       27.3048  0.0108\n",
      "     29       \u001b[36m28.4599\u001b[0m       27.3050  0.0109\n",
      "     30       \u001b[36m28.4574\u001b[0m       27.3020  0.0106\n",
      "     31       \u001b[36m28.4549\u001b[0m       27.3024  0.0108\n",
      "     32       \u001b[36m28.4527\u001b[0m       27.2998  0.0107\n",
      "     33       \u001b[36m28.4504\u001b[0m       27.2983  0.0115\n",
      "     34       \u001b[36m28.4484\u001b[0m       27.2977  0.0113\n",
      "     35       \u001b[36m28.4465\u001b[0m       27.2956  0.0117\n",
      "     36       \u001b[36m28.4448\u001b[0m       27.2952  0.0111\n",
      "     37       \u001b[36m28.4432\u001b[0m       27.2931  0.0115\n",
      "     38       \u001b[36m28.4416\u001b[0m       27.2922  0.0117\n",
      "     39       \u001b[36m28.4402\u001b[0m       27.2900  0.0119\n",
      "     40       \u001b[36m28.4388\u001b[0m       27.2893  0.0111\n",
      "     41       \u001b[36m28.4375\u001b[0m       27.2874  0.0111\n",
      "     42       \u001b[36m28.4362\u001b[0m       27.2869  0.0108\n",
      "     43       \u001b[36m28.4350\u001b[0m       27.2850  0.0112\n",
      "     44       \u001b[36m28.4338\u001b[0m       27.2838  0.0112\n",
      "     45       \u001b[36m28.4327\u001b[0m       27.2834  0.0111\n",
      "     46       \u001b[36m28.4317\u001b[0m       27.2824  0.0109\n",
      "     47       \u001b[36m28.4307\u001b[0m       27.2813  0.0110\n",
      "     48       \u001b[36m28.4298\u001b[0m       27.2801  0.0112\n",
      "     49       \u001b[36m28.4288\u001b[0m       27.2794  0.0114\n",
      "     50       \u001b[36m28.4280\u001b[0m       27.2794  0.0108\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m40.9928\u001b[0m       \u001b[32m40.8780\u001b[0m  0.0117\n",
      "      2       \u001b[36m36.4950\u001b[0m       \u001b[32m32.1330\u001b[0m  0.0116\n",
      "      3       \u001b[36m35.0027\u001b[0m       32.5202  0.0142\n",
      "      4       \u001b[36m34.2710\u001b[0m       32.6146  0.0153\n",
      "      5       \u001b[36m33.5448\u001b[0m       \u001b[32m30.9840\u001b[0m  0.0122\n",
      "      6       \u001b[36m33.0995\u001b[0m       \u001b[32m30.6353\u001b[0m  0.0117\n",
      "      7       \u001b[36m33.0044\u001b[0m       \u001b[32m30.6330\u001b[0m  0.0125\n",
      "      8       \u001b[36m32.8048\u001b[0m       30.6917  0.0136\n",
      "      9       \u001b[36m32.6786\u001b[0m       \u001b[32m30.2267\u001b[0m  0.0127\n",
      "     10       \u001b[36m32.5701\u001b[0m       \u001b[32m30.1756\u001b[0m  0.0125\n",
      "     11       \u001b[36m32.5173\u001b[0m       30.2971  0.0120\n",
      "     12       \u001b[36m32.4480\u001b[0m       30.1963  0.0116\n",
      "     13       \u001b[36m32.3968\u001b[0m       \u001b[32m30.1716\u001b[0m  0.0116\n",
      "     14       \u001b[36m32.3653\u001b[0m       30.2184  0.0120\n",
      "     15       \u001b[36m32.3380\u001b[0m       \u001b[32m30.1213\u001b[0m  0.0120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     16       \u001b[36m32.3134\u001b[0m       30.1405  0.0123\n",
      "     17       \u001b[36m32.2990\u001b[0m       30.1272  0.0119\n",
      "     18       \u001b[36m32.2800\u001b[0m       \u001b[32m30.0961\u001b[0m  0.0119\n",
      "     19       \u001b[36m32.2703\u001b[0m       30.1348  0.0117\n",
      "     20       \u001b[36m32.2604\u001b[0m       30.1076  0.0118\n",
      "     21       \u001b[36m32.2494\u001b[0m       30.1078  0.0120\n",
      "     22       \u001b[36m32.2431\u001b[0m       30.1038  0.0117\n",
      "     23       \u001b[36m32.2349\u001b[0m       \u001b[32m30.0890\u001b[0m  0.0116\n",
      "     24       \u001b[36m32.2289\u001b[0m       30.0893  0.0113\n",
      "     25       \u001b[36m32.2231\u001b[0m       \u001b[32m30.0877\u001b[0m  0.0114\n",
      "     26       \u001b[36m32.2174\u001b[0m       30.0934  0.0118\n",
      "     27       \u001b[36m32.2126\u001b[0m       30.0895  0.0118\n",
      "     28       \u001b[36m32.2080\u001b[0m       30.0930  0.0118\n",
      "     29       \u001b[36m32.2044\u001b[0m       30.0939  0.0117\n",
      "     30       \u001b[36m32.2003\u001b[0m       30.0946  0.0113\n",
      "     31       \u001b[36m32.1966\u001b[0m       30.0953  0.0120\n",
      "     32       \u001b[36m32.1930\u001b[0m       30.0956  0.0121\n",
      "     33       \u001b[36m32.1899\u001b[0m       30.0991  0.0118\n",
      "     34       \u001b[36m32.1865\u001b[0m       30.0981  0.0119\n",
      "     35       \u001b[36m32.1834\u001b[0m       30.1018  0.0118\n",
      "     36       \u001b[36m32.1802\u001b[0m       30.1038  0.0130\n",
      "     37       \u001b[36m32.1773\u001b[0m       30.1094  0.0121\n",
      "     38       \u001b[36m32.1750\u001b[0m       30.1126  0.0120\n",
      "     39       \u001b[36m32.1716\u001b[0m       30.1200  0.0114\n",
      "     40       \u001b[36m32.1695\u001b[0m       30.1232  0.0115\n",
      "     41       \u001b[36m32.1665\u001b[0m       30.1338  0.0123\n",
      "     42       \u001b[36m32.1644\u001b[0m       30.1367  0.0129\n",
      "     43       \u001b[36m32.1616\u001b[0m       30.1492  0.0120\n",
      "     44       \u001b[36m32.1595\u001b[0m       30.1484  0.0116\n",
      "     45       \u001b[36m32.1571\u001b[0m       30.1638  0.0114\n",
      "     46       \u001b[36m32.1545\u001b[0m       30.1530  0.0121\n",
      "     47       \u001b[36m32.1536\u001b[0m       30.1826  0.0120\n",
      "     48       \u001b[36m32.1500\u001b[0m       30.1516  0.0119\n",
      "     49       32.1511       30.2107  0.0116\n",
      "     50       \u001b[36m32.1479\u001b[0m       30.1266  0.0116\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m33.1192\u001b[0m       \u001b[32m29.4884\u001b[0m  0.0124\n",
      "      2       \u001b[36m27.5662\u001b[0m       29.8076  0.0117\n",
      "      3       \u001b[36m25.4327\u001b[0m       \u001b[32m26.8945\u001b[0m  0.0116\n",
      "      4       \u001b[36m24.8618\u001b[0m       \u001b[32m26.4604\u001b[0m  0.0117\n",
      "      5       \u001b[36m24.2385\u001b[0m       27.2401  0.0118\n",
      "      6       \u001b[36m23.8922\u001b[0m       27.5693  0.0115\n",
      "      7       \u001b[36m23.5755\u001b[0m       26.9240  0.0114\n",
      "      8       \u001b[36m23.5655\u001b[0m       26.9744  0.0120\n",
      "      9       \u001b[36m23.4272\u001b[0m       27.2241  0.0117\n",
      "     10       \u001b[36m23.3378\u001b[0m       26.9335  0.0115\n",
      "     11       \u001b[36m23.2865\u001b[0m       26.8181  0.0115\n",
      "     12       \u001b[36m23.2545\u001b[0m       26.9553  0.0115\n",
      "     13       \u001b[36m23.2129\u001b[0m       26.9028  0.0119\n",
      "     14       \u001b[36m23.1885\u001b[0m       26.8187  0.0116\n",
      "     15       \u001b[36m23.1652\u001b[0m       26.8162  0.0116\n",
      "     16       \u001b[36m23.1488\u001b[0m       26.8079  0.0113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m23.1367\u001b[0m       26.8199  0.0116\n",
      "     18       \u001b[36m23.1230\u001b[0m       26.7891  0.0119\n",
      "     19       \u001b[36m23.1137\u001b[0m       26.7576  0.0118\n",
      "     20       \u001b[36m23.1060\u001b[0m       26.7755  0.0116\n",
      "     21       \u001b[36m23.0963\u001b[0m       26.7644  0.0113\n",
      "     22       \u001b[36m23.0898\u001b[0m       26.7607  0.0113\n",
      "     23       \u001b[36m23.0836\u001b[0m       26.7667  0.0118\n",
      "     24       \u001b[36m23.0780\u001b[0m       26.7632  0.0117\n",
      "     25       \u001b[36m23.0728\u001b[0m       26.7632  0.0116\n",
      "     26       \u001b[36m23.0680\u001b[0m       26.7673  0.0115\n",
      "     27       \u001b[36m23.0636\u001b[0m       26.7720  0.0115\n",
      "     28       \u001b[36m23.0594\u001b[0m       26.7667  0.0121\n",
      "     29       \u001b[36m23.0559\u001b[0m       26.7709  0.0117\n",
      "     30       \u001b[36m23.0523\u001b[0m       26.7746  0.0120\n",
      "     31       \u001b[36m23.0491\u001b[0m       26.7682  0.0132\n",
      "     32       \u001b[36m23.0458\u001b[0m       26.7662  0.0159\n",
      "     33       \u001b[36m23.0430\u001b[0m       26.7654  0.0141\n",
      "     34       \u001b[36m23.0399\u001b[0m       26.7615  0.0133\n",
      "     35       \u001b[36m23.0372\u001b[0m       26.7626  0.0125\n",
      "     36       \u001b[36m23.0346\u001b[0m       26.7502  0.0125\n",
      "     37       \u001b[36m23.0321\u001b[0m       26.7514  0.0141\n",
      "     38       \u001b[36m23.0298\u001b[0m       26.7473  0.0133\n",
      "     39       \u001b[36m23.0277\u001b[0m       26.7456  0.0141\n",
      "     40       \u001b[36m23.0262\u001b[0m       26.7399  0.0123\n",
      "     41       \u001b[36m23.0246\u001b[0m       26.7405  0.0118\n",
      "     42       \u001b[36m23.0213\u001b[0m       26.7357  0.0123\n",
      "     43       \u001b[36m23.0194\u001b[0m       26.7305  0.0124\n",
      "     44       \u001b[36m23.0181\u001b[0m       26.7279  0.0118\n",
      "     45       \u001b[36m23.0172\u001b[0m       26.7248  0.0116\n",
      "     46       \u001b[36m23.0171\u001b[0m       26.7186  0.0117\n",
      "     47       23.0239       26.7215  0.0123\n",
      "     48       23.0181       26.7140  0.0123\n",
      "     49       23.0237       26.7388  0.0120\n",
      "     50       23.0173       26.6589  0.0117\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m39.0678\u001b[0m       \u001b[32m28.2423\u001b[0m  0.0116\n",
      "      2       \u001b[36m32.1112\u001b[0m       31.6333  0.0120\n",
      "      3       \u001b[36m31.6792\u001b[0m       \u001b[32m27.1318\u001b[0m  0.0117\n",
      "      4       \u001b[36m29.9785\u001b[0m       \u001b[32m26.4434\u001b[0m  0.0118\n",
      "      5       \u001b[36m29.4793\u001b[0m       27.5830  0.0118\n",
      "      6       \u001b[36m29.3389\u001b[0m       28.8298  0.0115\n",
      "      7       \u001b[36m29.1082\u001b[0m       27.4071  0.0113\n",
      "      8       \u001b[36m28.8618\u001b[0m       27.2505  0.0121\n",
      "      9       \u001b[36m28.7547\u001b[0m       27.9113  0.0123\n",
      "     10       \u001b[36m28.7517\u001b[0m       27.5846  0.0121\n",
      "     11       \u001b[36m28.6096\u001b[0m       27.3558  0.0122\n",
      "     12       \u001b[36m28.5566\u001b[0m       27.6254  0.0123\n",
      "     13       \u001b[36m28.5542\u001b[0m       27.6323  0.0127\n",
      "     14       \u001b[36m28.5176\u001b[0m       27.4394  0.0125\n",
      "     15       \u001b[36m28.4872\u001b[0m       27.5439  0.0133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     16       28.4899       27.5253  0.0124\n",
      "     17       \u001b[36m28.4706\u001b[0m       27.4262  0.0117\n",
      "     18       \u001b[36m28.4570\u001b[0m       27.4854  0.0121\n",
      "     19       \u001b[36m28.4534\u001b[0m       27.4600  0.0120\n",
      "     20       \u001b[36m28.4428\u001b[0m       27.4220  0.0118\n",
      "     21       \u001b[36m28.4351\u001b[0m       27.4601  0.0117\n",
      "     22       \u001b[36m28.4318\u001b[0m       27.4342  0.0117\n",
      "     23       \u001b[36m28.4247\u001b[0m       27.4369  0.0118\n",
      "     24       \u001b[36m28.4212\u001b[0m       27.4459  0.0116\n",
      "     25       \u001b[36m28.4176\u001b[0m       27.4284  0.0117\n",
      "     26       \u001b[36m28.4133\u001b[0m       27.4302  0.0116\n",
      "     27       \u001b[36m28.4118\u001b[0m       27.4224  0.0118\n",
      "     28       \u001b[36m28.4076\u001b[0m       27.4114  0.0143\n",
      "     29       \u001b[36m28.4059\u001b[0m       27.4133  0.0119\n",
      "     30       28.4066       27.3895  0.0116\n",
      "     31       \u001b[36m28.4015\u001b[0m       27.3897  0.0115\n",
      "     32       28.4032       27.4048  0.0113\n",
      "     33       28.4142       27.3661  0.0117\n",
      "     34       28.4159       27.3472  0.0115\n",
      "     35       28.4303       27.4358  0.0117\n",
      "     36       28.4821       27.2730  0.0117\n",
      "     37       28.4502       27.4183  0.0116\n",
      "     38       28.4879       27.2252  0.0118\n",
      "     39       28.4210       27.4148  0.0115\n",
      "     40       28.4126       27.2345  0.0116\n",
      "     41       \u001b[36m28.3989\u001b[0m       27.3902  0.0117\n",
      "     42       28.4076       27.2398  0.0125\n",
      "     43       \u001b[36m28.3890\u001b[0m       27.3997  0.0132\n",
      "     44       28.3951       27.2669  0.0123\n",
      "     45       \u001b[36m28.3870\u001b[0m       27.3391  0.0122\n",
      "     46       28.3901       27.2681  0.0126\n",
      "     47       \u001b[36m28.3849\u001b[0m       27.3202  0.0130\n",
      "     48       28.3858       27.2742  0.0127\n",
      "     49       \u001b[36m28.3832\u001b[0m       27.2888  0.0125\n",
      "     50       28.3833       27.2740  0.0122\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m43.1804\u001b[0m       \u001b[32m42.8201\u001b[0m  0.0121\n",
      "      2       \u001b[36m40.1350\u001b[0m       \u001b[32m39.1188\u001b[0m  0.0112\n",
      "      3       \u001b[36m37.3341\u001b[0m       \u001b[32m35.7162\u001b[0m  0.0113\n",
      "      4       \u001b[36m35.0248\u001b[0m       \u001b[32m33.0201\u001b[0m  0.0112\n",
      "      5       \u001b[36m33.5245\u001b[0m       \u001b[32m31.3338\u001b[0m  0.0111\n",
      "      6       \u001b[36m32.8570\u001b[0m       \u001b[32m30.5714\u001b[0m  0.0110\n",
      "      7       \u001b[36m32.6443\u001b[0m       \u001b[32m30.2748\u001b[0m  0.0112\n",
      "      8       \u001b[36m32.5657\u001b[0m       \u001b[32m30.1529\u001b[0m  0.0113\n",
      "      9       \u001b[36m32.5215\u001b[0m       \u001b[32m30.0931\u001b[0m  0.0123\n",
      "     10       \u001b[36m32.4900\u001b[0m       \u001b[32m30.0569\u001b[0m  0.0147\n",
      "     11       \u001b[36m32.4657\u001b[0m       \u001b[32m30.0314\u001b[0m  0.0118\n",
      "     12       \u001b[36m32.4462\u001b[0m       \u001b[32m30.0113\u001b[0m  0.0116\n",
      "     13       \u001b[36m32.4300\u001b[0m       \u001b[32m29.9942\u001b[0m  0.0114\n",
      "     14       \u001b[36m32.4161\u001b[0m       \u001b[32m29.9796\u001b[0m  0.0113\n",
      "     15       \u001b[36m32.4042\u001b[0m       \u001b[32m29.9667\u001b[0m  0.0128\n",
      "     16       \u001b[36m32.3937\u001b[0m       \u001b[32m29.9548\u001b[0m  0.0116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m32.3844\u001b[0m       \u001b[32m29.9446\u001b[0m  0.0121\n",
      "     18       \u001b[36m32.3758\u001b[0m       \u001b[32m29.9352\u001b[0m  0.0112\n",
      "     19       \u001b[36m32.3679\u001b[0m       \u001b[32m29.9265\u001b[0m  0.0112\n",
      "     20       \u001b[36m32.3607\u001b[0m       \u001b[32m29.9183\u001b[0m  0.0110\n",
      "     21       \u001b[36m32.3541\u001b[0m       \u001b[32m29.9111\u001b[0m  0.0107\n",
      "     22       \u001b[36m32.3480\u001b[0m       \u001b[32m29.9044\u001b[0m  0.0109\n",
      "     23       \u001b[36m32.3423\u001b[0m       \u001b[32m29.8981\u001b[0m  0.0112\n",
      "     24       \u001b[36m32.3370\u001b[0m       \u001b[32m29.8925\u001b[0m  0.0109\n",
      "     25       \u001b[36m32.3321\u001b[0m       \u001b[32m29.8872\u001b[0m  0.0108\n",
      "     26       \u001b[36m32.3273\u001b[0m       \u001b[32m29.8822\u001b[0m  0.0108\n",
      "     27       \u001b[36m32.3230\u001b[0m       \u001b[32m29.8774\u001b[0m  0.0112\n",
      "     28       \u001b[36m32.3191\u001b[0m       \u001b[32m29.8730\u001b[0m  0.0111\n",
      "     29       \u001b[36m32.3153\u001b[0m       \u001b[32m29.8688\u001b[0m  0.0109\n",
      "     30       \u001b[36m32.3117\u001b[0m       \u001b[32m29.8651\u001b[0m  0.0106\n",
      "     31       \u001b[36m32.3084\u001b[0m       \u001b[32m29.8615\u001b[0m  0.0108\n",
      "     32       \u001b[36m32.3052\u001b[0m       \u001b[32m29.8581\u001b[0m  0.0108\n",
      "     33       \u001b[36m32.3022\u001b[0m       \u001b[32m29.8548\u001b[0m  0.0110\n",
      "     34       \u001b[36m32.2995\u001b[0m       \u001b[32m29.8518\u001b[0m  0.0110\n",
      "     35       \u001b[36m32.2968\u001b[0m       \u001b[32m29.8490\u001b[0m  0.0107\n",
      "     36       \u001b[36m32.2942\u001b[0m       \u001b[32m29.8466\u001b[0m  0.0107\n",
      "     37       \u001b[36m32.2918\u001b[0m       \u001b[32m29.8440\u001b[0m  0.0111\n",
      "     38       \u001b[36m32.2895\u001b[0m       \u001b[32m29.8418\u001b[0m  0.0114\n",
      "     39       \u001b[36m32.2872\u001b[0m       \u001b[32m29.8395\u001b[0m  0.0110\n",
      "     40       \u001b[36m32.2851\u001b[0m       \u001b[32m29.8375\u001b[0m  0.0108\n",
      "     41       \u001b[36m32.2831\u001b[0m       \u001b[32m29.8355\u001b[0m  0.0108\n",
      "     42       \u001b[36m32.2811\u001b[0m       \u001b[32m29.8335\u001b[0m  0.0111\n",
      "     43       \u001b[36m32.2793\u001b[0m       \u001b[32m29.8318\u001b[0m  0.0110\n",
      "     44       \u001b[36m32.2775\u001b[0m       \u001b[32m29.8301\u001b[0m  0.0112\n",
      "     45       \u001b[36m32.2759\u001b[0m       \u001b[32m29.8288\u001b[0m  0.0116\n",
      "     46       \u001b[36m32.2742\u001b[0m       \u001b[32m29.8274\u001b[0m  0.0106\n",
      "     47       \u001b[36m32.2727\u001b[0m       \u001b[32m29.8262\u001b[0m  0.0114\n",
      "     48       \u001b[36m32.2711\u001b[0m       \u001b[32m29.8252\u001b[0m  0.0119\n",
      "     49       \u001b[36m32.2697\u001b[0m       \u001b[32m29.8240\u001b[0m  0.0115\n",
      "     50       \u001b[36m32.2683\u001b[0m       \u001b[32m29.8229\u001b[0m  0.0109\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m31.5630\u001b[0m       \u001b[32m29.4030\u001b[0m  0.0124\n",
      "      2       \u001b[36m28.6571\u001b[0m       \u001b[32m27.5361\u001b[0m  0.0117\n",
      "      3       \u001b[36m26.0901\u001b[0m       \u001b[32m26.4000\u001b[0m  0.0116\n",
      "      4       \u001b[36m24.3624\u001b[0m       \u001b[32m26.2081\u001b[0m  0.0112\n",
      "      5       \u001b[36m23.6400\u001b[0m       26.3660  0.0109\n",
      "      6       \u001b[36m23.4286\u001b[0m       26.4646  0.0108\n",
      "      7       \u001b[36m23.3587\u001b[0m       26.4914  0.0113\n",
      "      8       \u001b[36m23.3223\u001b[0m       26.4903  0.0111\n",
      "      9       \u001b[36m23.2965\u001b[0m       26.4813  0.0111\n",
      "     10       \u001b[36m23.2758\u001b[0m       26.4732  0.0110\n",
      "     11       \u001b[36m23.2584\u001b[0m       26.4651  0.0109\n",
      "     12       \u001b[36m23.2434\u001b[0m       26.4572  0.0110\n",
      "     13       \u001b[36m23.2307\u001b[0m       26.4520  0.0110\n",
      "     14       \u001b[36m23.2194\u001b[0m       26.4473  0.0109\n",
      "     15       \u001b[36m23.2095\u001b[0m       26.4428  0.0108\n",
      "     16       \u001b[36m23.2005\u001b[0m       26.4388  0.0108\n",
      "     17       \u001b[36m23.1925\u001b[0m       26.4358  0.0108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     18       \u001b[36m23.1853\u001b[0m       26.4332  0.0114\n",
      "     19       \u001b[36m23.1788\u001b[0m       26.4306  0.0108\n",
      "     20       \u001b[36m23.1730\u001b[0m       26.4292  0.0108\n",
      "     21       \u001b[36m23.1678\u001b[0m       26.4278  0.0108\n",
      "     22       \u001b[36m23.1629\u001b[0m       26.4262  0.0109\n",
      "     23       \u001b[36m23.1585\u001b[0m       26.4250  0.0110\n",
      "     24       \u001b[36m23.1543\u001b[0m       26.4239  0.0112\n",
      "     25       \u001b[36m23.1505\u001b[0m       26.4229  0.0110\n",
      "     26       \u001b[36m23.1468\u001b[0m       26.4220  0.0109\n",
      "     27       \u001b[36m23.1435\u001b[0m       26.4214  0.0109\n",
      "     28       \u001b[36m23.1404\u001b[0m       26.4208  0.0113\n",
      "     29       \u001b[36m23.1374\u001b[0m       26.4202  0.0114\n",
      "     30       \u001b[36m23.1345\u001b[0m       26.4194  0.0110\n",
      "     31       \u001b[36m23.1319\u001b[0m       26.4187  0.0106\n",
      "     32       \u001b[36m23.1293\u001b[0m       26.4181  0.0108\n",
      "     33       \u001b[36m23.1270\u001b[0m       26.4178  0.0108\n",
      "     34       \u001b[36m23.1247\u001b[0m       26.4175  0.0115\n",
      "     35       \u001b[36m23.1226\u001b[0m       26.4172  0.0107\n",
      "     36       \u001b[36m23.1205\u001b[0m       26.4170  0.0106\n",
      "     37       \u001b[36m23.1186\u001b[0m       26.4167  0.0105\n",
      "     38       \u001b[36m23.1168\u001b[0m       26.4164  0.0114\n",
      "     39       \u001b[36m23.1150\u001b[0m       26.4162  0.0111\n",
      "     40       \u001b[36m23.1133\u001b[0m       26.4159  0.0111\n",
      "     41       \u001b[36m23.1116\u001b[0m       26.4156  0.0107\n",
      "     42       \u001b[36m23.1101\u001b[0m       26.4153  0.0124\n",
      "     43       \u001b[36m23.1086\u001b[0m       26.4151  0.0170\n",
      "     44       \u001b[36m23.1071\u001b[0m       26.4147  0.0160\n",
      "     45       \u001b[36m23.1057\u001b[0m       26.4144  0.0123\n",
      "     46       \u001b[36m23.1044\u001b[0m       26.4142  0.0110\n",
      "     47       \u001b[36m23.1031\u001b[0m       26.4139  0.0112\n",
      "     48       \u001b[36m23.1018\u001b[0m       26.4137  0.0117\n",
      "     49       \u001b[36m23.1006\u001b[0m       26.4134  0.0131\n",
      "     50       \u001b[36m23.0994\u001b[0m       26.4132  0.0118\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m42.1238\u001b[0m       \u001b[32m31.7659\u001b[0m  0.0115\n",
      "      2       \u001b[36m38.4749\u001b[0m       \u001b[32m29.4753\u001b[0m  0.0110\n",
      "      3       \u001b[36m35.1626\u001b[0m       \u001b[32m27.4831\u001b[0m  0.0113\n",
      "      4       \u001b[36m32.2239\u001b[0m       \u001b[32m26.2941\u001b[0m  0.0110\n",
      "      5       \u001b[36m30.0852\u001b[0m       26.3477  0.0107\n",
      "      6       \u001b[36m29.0841\u001b[0m       26.9534  0.0106\n",
      "      7       \u001b[36m28.8351\u001b[0m       27.2737  0.0111\n",
      "      8       \u001b[36m28.7625\u001b[0m       27.3575  0.0110\n",
      "      9       \u001b[36m28.7122\u001b[0m       27.3660  0.0107\n",
      "     10       \u001b[36m28.6710\u001b[0m       27.3569  0.0108\n",
      "     11       \u001b[36m28.6376\u001b[0m       27.3465  0.0107\n",
      "     12       \u001b[36m28.6108\u001b[0m       27.3359  0.0107\n",
      "     13       \u001b[36m28.5884\u001b[0m       27.3277  0.0111\n",
      "     14       \u001b[36m28.5701\u001b[0m       27.3215  0.0108\n",
      "     15       \u001b[36m28.5546\u001b[0m       27.3163  0.0105\n",
      "     16       \u001b[36m28.5414\u001b[0m       27.3122  0.0107\n",
      "     17       \u001b[36m28.5301\u001b[0m       27.3081  0.0108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     18       \u001b[36m28.5202\u001b[0m       27.3044  0.0115\n",
      "     19       \u001b[36m28.5116\u001b[0m       27.3015  0.0109\n",
      "     20       \u001b[36m28.5041\u001b[0m       27.2986  0.0113\n",
      "     21       \u001b[36m28.4974\u001b[0m       27.2959  0.0107\n",
      "     22       \u001b[36m28.4915\u001b[0m       27.2933  0.0107\n",
      "     23       \u001b[36m28.4862\u001b[0m       27.2904  0.0111\n",
      "     24       \u001b[36m28.4813\u001b[0m       27.2876  0.0112\n",
      "     25       \u001b[36m28.4769\u001b[0m       27.2850  0.0113\n",
      "     26       \u001b[36m28.4728\u001b[0m       27.2826  0.0106\n",
      "     27       \u001b[36m28.4691\u001b[0m       27.2804  0.0108\n",
      "     28       \u001b[36m28.4656\u001b[0m       27.2789  0.0108\n",
      "     29       \u001b[36m28.4625\u001b[0m       27.2775  0.0111\n",
      "     30       \u001b[36m28.4596\u001b[0m       27.2760  0.0133\n",
      "     31       \u001b[36m28.4570\u001b[0m       27.2739  0.0107\n",
      "     32       \u001b[36m28.4544\u001b[0m       27.2726  0.0106\n",
      "     33       \u001b[36m28.4521\u001b[0m       27.2700  0.0115\n",
      "     34       \u001b[36m28.4498\u001b[0m       27.2687  0.0110\n",
      "     35       \u001b[36m28.4478\u001b[0m       27.2674  0.0111\n",
      "     36       \u001b[36m28.4459\u001b[0m       27.2651  0.0105\n",
      "     37       \u001b[36m28.4439\u001b[0m       27.2637  0.0108\n",
      "     38       \u001b[36m28.4421\u001b[0m       27.2625  0.0109\n",
      "     39       \u001b[36m28.4405\u001b[0m       27.2604  0.0111\n",
      "     40       \u001b[36m28.4388\u001b[0m       27.2592  0.0109\n",
      "     41       \u001b[36m28.4372\u001b[0m       27.2584  0.0107\n",
      "     42       \u001b[36m28.4358\u001b[0m       27.2568  0.0111\n",
      "     43       \u001b[36m28.4343\u001b[0m       27.2565  0.0111\n",
      "     44       \u001b[36m28.4331\u001b[0m       27.2554  0.0111\n",
      "     45       \u001b[36m28.4318\u001b[0m       27.2553  0.0110\n",
      "     46       \u001b[36m28.4307\u001b[0m       27.2541  0.0107\n",
      "     47       \u001b[36m28.4295\u001b[0m       27.2535  0.0125\n",
      "     48       \u001b[36m28.4284\u001b[0m       27.2527  0.0110\n",
      "     49       \u001b[36m28.4273\u001b[0m       27.2522  0.0111\n",
      "     50       \u001b[36m28.4263\u001b[0m       27.2512  0.0109\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m40.2907\u001b[0m       \u001b[32m37.8752\u001b[0m  0.0116\n",
      "      2       \u001b[36m36.4265\u001b[0m       \u001b[32m31.6018\u001b[0m  0.0115\n",
      "      3       \u001b[36m34.2597\u001b[0m       33.2367  0.0122\n",
      "      4       \u001b[36m34.0031\u001b[0m       31.7890  0.0115\n",
      "      5       \u001b[36m33.1905\u001b[0m       \u001b[32m30.3809\u001b[0m  0.0117\n",
      "      6       \u001b[36m32.9820\u001b[0m       30.6666  0.0116\n",
      "      7       \u001b[36m32.8328\u001b[0m       30.8762  0.0113\n",
      "      8       \u001b[36m32.6724\u001b[0m       \u001b[32m30.1658\u001b[0m  0.0120\n",
      "      9       \u001b[36m32.5536\u001b[0m       30.2497  0.0117\n",
      "     10       \u001b[36m32.4917\u001b[0m       30.5352  0.0114\n",
      "     11       \u001b[36m32.4383\u001b[0m       30.2075  0.0116\n",
      "     12       \u001b[36m32.3788\u001b[0m       30.1833  0.0114\n",
      "     13       \u001b[36m32.3625\u001b[0m       30.3207  0.0110\n",
      "     14       \u001b[36m32.3360\u001b[0m       \u001b[32m30.1602\u001b[0m  0.0120\n",
      "     15       \u001b[36m32.3102\u001b[0m       30.1727  0.0117\n",
      "     16       \u001b[36m32.3050\u001b[0m       30.1964  0.0112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m32.2892\u001b[0m       \u001b[32m30.1283\u001b[0m  0.0122\n",
      "     18       \u001b[36m32.2791\u001b[0m       30.1437  0.0114\n",
      "     19       \u001b[36m32.2735\u001b[0m       \u001b[32m30.1137\u001b[0m  0.0115\n",
      "     20       \u001b[36m32.2631\u001b[0m       \u001b[32m30.1042\u001b[0m  0.0137\n",
      "     21       \u001b[36m32.2579\u001b[0m       \u001b[32m30.0887\u001b[0m  0.0172\n",
      "     22       \u001b[36m32.2501\u001b[0m       \u001b[32m30.0762\u001b[0m  0.0121\n",
      "     23       \u001b[36m32.2453\u001b[0m       30.0796  0.0114\n",
      "     24       \u001b[36m32.2398\u001b[0m       \u001b[32m30.0614\u001b[0m  0.0113\n",
      "     25       \u001b[36m32.2343\u001b[0m       30.0651  0.0224\n",
      "     26       \u001b[36m32.2305\u001b[0m       \u001b[32m30.0543\u001b[0m  0.0145\n",
      "     27       \u001b[36m32.2250\u001b[0m       \u001b[32m30.0510\u001b[0m  0.0144\n",
      "     28       \u001b[36m32.2215\u001b[0m       \u001b[32m30.0447\u001b[0m  0.0155\n",
      "     29       \u001b[36m32.2165\u001b[0m       30.0466  0.0175\n",
      "     30       \u001b[36m32.2126\u001b[0m       \u001b[32m30.0444\u001b[0m  0.0146\n",
      "     31       \u001b[36m32.2089\u001b[0m       30.0484  0.0124\n",
      "     32       \u001b[36m32.2054\u001b[0m       \u001b[32m30.0397\u001b[0m  0.0121\n",
      "     33       \u001b[36m32.2017\u001b[0m       30.0522  0.0126\n",
      "     34       \u001b[36m32.1979\u001b[0m       30.0456  0.0120\n",
      "     35       \u001b[36m32.1949\u001b[0m       30.0664  0.0122\n",
      "     36       \u001b[36m32.1918\u001b[0m       30.0438  0.0120\n",
      "     37       \u001b[36m32.1896\u001b[0m       30.0877  0.0118\n",
      "     38       \u001b[36m32.1860\u001b[0m       30.0453  0.0122\n",
      "     39       32.1866       30.1263  0.0121\n",
      "     40       \u001b[36m32.1825\u001b[0m       30.0464  0.0120\n",
      "     41       32.1875       30.1786  0.0120\n",
      "     42       32.1829       30.0470  0.0116\n",
      "     43       32.1962       30.3006  0.0121\n",
      "     44       32.1962       30.0605  0.0115\n",
      "     45       32.2281       30.4768  0.0118\n",
      "     46       32.2427       \u001b[32m29.9898\u001b[0m  0.0126\n",
      "     47       32.2612       30.3109  0.0120\n",
      "     48       32.2067       \u001b[32m29.8967\u001b[0m  0.0124\n",
      "     49       \u001b[36m32.1715\u001b[0m       30.1205  0.0121\n",
      "     50       32.1932       30.1084  0.0119\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m32.1168\u001b[0m       \u001b[32m29.0612\u001b[0m  0.0121\n",
      "      2       \u001b[36m26.6481\u001b[0m       \u001b[32m28.4874\u001b[0m  0.0116\n",
      "      3       \u001b[36m24.7102\u001b[0m       \u001b[32m26.4873\u001b[0m  0.0122\n",
      "      4       \u001b[36m24.6145\u001b[0m       26.6847  0.0116\n",
      "      5       \u001b[36m23.8650\u001b[0m       28.0213  0.0116\n",
      "      6       \u001b[36m23.6307\u001b[0m       26.7626  0.0115\n",
      "      7       \u001b[36m23.5847\u001b[0m       26.6380  0.0119\n",
      "      8       \u001b[36m23.4359\u001b[0m       27.1882  0.0123\n",
      "      9       \u001b[36m23.3345\u001b[0m       26.8206  0.0124\n",
      "     10       \u001b[36m23.2777\u001b[0m       26.5704  0.0121\n",
      "     11       \u001b[36m23.2533\u001b[0m       26.8581  0.0124\n",
      "     12       \u001b[36m23.1978\u001b[0m       26.7678  0.0141\n",
      "     13       \u001b[36m23.1751\u001b[0m       26.6026  0.0126\n",
      "     14       \u001b[36m23.1633\u001b[0m       26.7066  0.0126\n",
      "     15       \u001b[36m23.1397\u001b[0m       26.6845  0.0120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     16       \u001b[36m23.1275\u001b[0m       26.6284  0.0120\n",
      "     17       \u001b[36m23.1149\u001b[0m       26.6345  0.0121\n",
      "     18       \u001b[36m23.1050\u001b[0m       26.6645  0.0118\n",
      "     19       \u001b[36m23.0917\u001b[0m       26.6186  0.0121\n",
      "     20       \u001b[36m23.0867\u001b[0m       26.6313  0.0121\n",
      "     21       \u001b[36m23.0768\u001b[0m       26.6208  0.0118\n",
      "     22       \u001b[36m23.0712\u001b[0m       26.6224  0.0121\n",
      "     23       \u001b[36m23.0649\u001b[0m       26.6155  0.0117\n",
      "     24       \u001b[36m23.0595\u001b[0m       26.6181  0.0119\n",
      "     25       \u001b[36m23.0559\u001b[0m       26.6084  0.0117\n",
      "     26       \u001b[36m23.0490\u001b[0m       26.6164  0.0122\n",
      "     27       23.0508       26.6115  0.0128\n",
      "     28       23.0517       26.6216  0.0124\n",
      "     29       23.0548       26.6176  0.0125\n",
      "     30       23.0605       26.5985  0.0119\n",
      "     31       \u001b[36m23.0423\u001b[0m       26.6408  0.0121\n",
      "     32       \u001b[36m23.0370\u001b[0m       26.6005  0.0122\n",
      "     33       \u001b[36m23.0349\u001b[0m       26.6410  0.0119\n",
      "     34       \u001b[36m23.0274\u001b[0m       26.6188  0.0117\n",
      "     35       \u001b[36m23.0241\u001b[0m       26.6051  0.0117\n",
      "     36       23.0253       26.6533  0.0116\n",
      "     37       \u001b[36m23.0211\u001b[0m       26.5845  0.0121\n",
      "     38       \u001b[36m23.0196\u001b[0m       26.6580  0.0117\n",
      "     39       \u001b[36m23.0161\u001b[0m       26.5907  0.0120\n",
      "     40       \u001b[36m23.0125\u001b[0m       26.6568  0.0123\n",
      "     41       \u001b[36m23.0118\u001b[0m       26.6022  0.0123\n",
      "     42       23.0141       26.6560  0.0129\n",
      "     43       \u001b[36m23.0087\u001b[0m       26.6107  0.0128\n",
      "     44       23.0117       26.6357  0.0129\n",
      "     45       \u001b[36m23.0010\u001b[0m       26.6391  0.0123\n",
      "     46       23.0066       26.6205  0.0126\n",
      "     47       23.0087       26.6828  0.0123\n",
      "     48       \u001b[36m23.0002\u001b[0m       26.6167  0.0121\n",
      "     49       23.0087       26.6589  0.0118\n",
      "     50       23.0015       26.6773  0.0133\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m38.6639\u001b[0m       \u001b[32m27.2967\u001b[0m  0.0142\n",
      "      2       \u001b[36m31.5151\u001b[0m       30.5043  0.0125\n",
      "      3       \u001b[36m30.1400\u001b[0m       \u001b[32m26.6366\u001b[0m  0.0124\n",
      "      4       \u001b[36m29.5924\u001b[0m       27.0664  0.0127\n",
      "      5       \u001b[36m29.2736\u001b[0m       28.4221  0.0140\n",
      "      6       \u001b[36m29.0944\u001b[0m       27.1102  0.0126\n",
      "      7       \u001b[36m28.8096\u001b[0m       27.2562  0.0128\n",
      "      8       \u001b[36m28.7480\u001b[0m       27.6383  0.0123\n",
      "      9       \u001b[36m28.6636\u001b[0m       27.1026  0.0124\n",
      "     10       \u001b[36m28.5631\u001b[0m       27.4067  0.0122\n",
      "     11       \u001b[36m28.5595\u001b[0m       27.4949  0.0124\n",
      "     12       \u001b[36m28.5142\u001b[0m       27.1756  0.0116\n",
      "     13       \u001b[36m28.4825\u001b[0m       27.3938  0.0117\n",
      "     14       28.4854       27.3449  0.0119\n",
      "     15       \u001b[36m28.4578\u001b[0m       27.2675  0.0117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     16       \u001b[36m28.4541\u001b[0m       27.3285  0.0127\n",
      "     17       \u001b[36m28.4455\u001b[0m       27.2523  0.0118\n",
      "     18       \u001b[36m28.4348\u001b[0m       27.2833  0.0115\n",
      "     19       28.4395       27.2341  0.0117\n",
      "     20       \u001b[36m28.4308\u001b[0m       27.2358  0.0119\n",
      "     21       28.4384       27.2832  0.0128\n",
      "     22       28.4499       27.2111  0.0124\n",
      "     23       28.4655       27.2078  0.0124\n",
      "     24       28.4645       27.2651  0.0122\n",
      "     25       28.4395       27.2396  0.0126\n",
      "     26       28.4437       27.1661  0.0126\n",
      "     27       \u001b[36m28.4074\u001b[0m       27.2795  0.0121\n",
      "     28       28.4106       27.2084  0.0119\n",
      "     29       \u001b[36m28.4008\u001b[0m       27.2287  0.0119\n",
      "     30       28.4034       27.1973  0.0119\n",
      "     31       \u001b[36m28.3961\u001b[0m       27.2311  0.0120\n",
      "     32       \u001b[36m28.3953\u001b[0m       27.2027  0.0120\n",
      "     33       \u001b[36m28.3922\u001b[0m       27.1914  0.0119\n",
      "     34       \u001b[36m28.3908\u001b[0m       27.1993  0.0115\n",
      "     35       \u001b[36m28.3890\u001b[0m       27.1973  0.0116\n",
      "     36       \u001b[36m28.3867\u001b[0m       27.1886  0.0119\n",
      "     37       \u001b[36m28.3862\u001b[0m       27.1739  0.0120\n",
      "     38       \u001b[36m28.3838\u001b[0m       27.1892  0.0122\n",
      "     39       \u001b[36m28.3835\u001b[0m       27.1736  0.0121\n",
      "     40       \u001b[36m28.3818\u001b[0m       27.1732  0.0117\n",
      "     41       \u001b[36m28.3809\u001b[0m       27.1694  0.0130\n",
      "     42       \u001b[36m28.3794\u001b[0m       27.1725  0.0124\n",
      "     43       28.3795       27.1655  0.0118\n",
      "     44       \u001b[36m28.3768\u001b[0m       27.1677  0.0116\n",
      "     45       28.3773       27.1643  0.0115\n",
      "     46       \u001b[36m28.3756\u001b[0m       27.1666  0.0119\n",
      "     47       28.3756       27.1627  0.0118\n",
      "     48       \u001b[36m28.3738\u001b[0m       27.1661  0.0117\n",
      "     49       28.3748       27.1565  0.0116\n",
      "     50       \u001b[36m28.3714\u001b[0m       27.1672  0.0115\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m43.1114\u001b[0m       \u001b[32m42.4930\u001b[0m  0.0113\n",
      "      2       \u001b[36m39.7347\u001b[0m       \u001b[32m38.5300\u001b[0m  0.0122\n",
      "      3       \u001b[36m36.8396\u001b[0m       \u001b[32m34.9533\u001b[0m  0.0120\n",
      "      4       \u001b[36m34.6148\u001b[0m       \u001b[32m32.3769\u001b[0m  0.0120\n",
      "      5       \u001b[36m33.3974\u001b[0m       \u001b[32m31.0583\u001b[0m  0.0113\n",
      "      6       \u001b[36m32.9569\u001b[0m       \u001b[32m30.5642\u001b[0m  0.0110\n",
      "      7       \u001b[36m32.8092\u001b[0m       \u001b[32m30.3782\u001b[0m  0.0113\n",
      "      8       \u001b[36m32.7310\u001b[0m       \u001b[32m30.2870\u001b[0m  0.0114\n",
      "      9       \u001b[36m32.6733\u001b[0m       \u001b[32m30.2287\u001b[0m  0.0108\n",
      "     10       \u001b[36m32.6265\u001b[0m       \u001b[32m30.1846\u001b[0m  0.0108\n",
      "     11       \u001b[36m32.5887\u001b[0m       \u001b[32m30.1484\u001b[0m  0.0105\n",
      "     12       \u001b[36m32.5553\u001b[0m       \u001b[32m30.1178\u001b[0m  0.0112\n",
      "     13       \u001b[36m32.5275\u001b[0m       \u001b[32m30.0921\u001b[0m  0.0114\n",
      "     14       \u001b[36m32.5019\u001b[0m       \u001b[32m30.0682\u001b[0m  0.0114\n",
      "     15       \u001b[36m32.4801\u001b[0m       \u001b[32m30.0471\u001b[0m  0.0108\n",
      "     16       \u001b[36m32.4613\u001b[0m       \u001b[32m30.0284\u001b[0m  0.0108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m32.4447\u001b[0m       \u001b[32m30.0115\u001b[0m  0.0122\n",
      "     18       \u001b[36m32.4300\u001b[0m       \u001b[32m29.9964\u001b[0m  0.0113\n",
      "     19       \u001b[36m32.4171\u001b[0m       \u001b[32m29.9831\u001b[0m  0.0114\n",
      "     20       \u001b[36m32.4047\u001b[0m       \u001b[32m29.9704\u001b[0m  0.0114\n",
      "     21       \u001b[36m32.3942\u001b[0m       \u001b[32m29.9590\u001b[0m  0.0115\n",
      "     22       \u001b[36m32.3841\u001b[0m       \u001b[32m29.9486\u001b[0m  0.0123\n",
      "     23       \u001b[36m32.3753\u001b[0m       \u001b[32m29.9392\u001b[0m  0.0120\n",
      "     24       \u001b[36m32.3669\u001b[0m       \u001b[32m29.9306\u001b[0m  0.0149\n",
      "     25       \u001b[36m32.3596\u001b[0m       \u001b[32m29.9228\u001b[0m  0.0141\n",
      "     26       \u001b[36m32.3522\u001b[0m       \u001b[32m29.9155\u001b[0m  0.0122\n",
      "     27       \u001b[36m32.3459\u001b[0m       \u001b[32m29.9091\u001b[0m  0.0112\n",
      "     28       \u001b[36m32.3397\u001b[0m       \u001b[32m29.9030\u001b[0m  0.0168\n",
      "     29       \u001b[36m32.3343\u001b[0m       \u001b[32m29.8976\u001b[0m  0.0145\n",
      "     30       \u001b[36m32.3291\u001b[0m       \u001b[32m29.8921\u001b[0m  0.0133\n",
      "     31       \u001b[36m32.3241\u001b[0m       \u001b[32m29.8875\u001b[0m  0.0123\n",
      "     32       \u001b[36m32.3194\u001b[0m       \u001b[32m29.8834\u001b[0m  0.0113\n",
      "     33       \u001b[36m32.3153\u001b[0m       \u001b[32m29.8795\u001b[0m  0.0134\n",
      "     34       \u001b[36m32.3111\u001b[0m       \u001b[32m29.8758\u001b[0m  0.0130\n",
      "     35       \u001b[36m32.3075\u001b[0m       \u001b[32m29.8727\u001b[0m  0.0127\n",
      "     36       \u001b[36m32.3037\u001b[0m       \u001b[32m29.8694\u001b[0m  0.0123\n",
      "     37       \u001b[36m32.3007\u001b[0m       \u001b[32m29.8670\u001b[0m  0.0112\n",
      "     38       \u001b[36m32.2974\u001b[0m       \u001b[32m29.8636\u001b[0m  0.0124\n",
      "     39       \u001b[36m32.2944\u001b[0m       \u001b[32m29.8610\u001b[0m  0.0113\n",
      "     40       \u001b[36m32.2917\u001b[0m       \u001b[32m29.8587\u001b[0m  0.0124\n",
      "     41       \u001b[36m32.2887\u001b[0m       \u001b[32m29.8563\u001b[0m  0.0117\n",
      "     42       \u001b[36m32.2862\u001b[0m       \u001b[32m29.8542\u001b[0m  0.0108\n",
      "     43       \u001b[36m32.2836\u001b[0m       \u001b[32m29.8522\u001b[0m  0.0147\n",
      "     44       \u001b[36m32.2813\u001b[0m       \u001b[32m29.8502\u001b[0m  0.0119\n",
      "     45       \u001b[36m32.2789\u001b[0m       \u001b[32m29.8483\u001b[0m  0.0121\n",
      "     46       \u001b[36m32.2769\u001b[0m       \u001b[32m29.8468\u001b[0m  0.0117\n",
      "     47       \u001b[36m32.2747\u001b[0m       \u001b[32m29.8448\u001b[0m  0.0119\n",
      "     48       \u001b[36m32.2729\u001b[0m       \u001b[32m29.8434\u001b[0m  0.0120\n",
      "     49       \u001b[36m32.2712\u001b[0m       \u001b[32m29.8422\u001b[0m  0.0119\n",
      "     50       \u001b[36m32.2693\u001b[0m       \u001b[32m29.8409\u001b[0m  0.0127\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m33.1307\u001b[0m       \u001b[32m30.2575\u001b[0m  0.0114\n",
      "      2       \u001b[36m29.6143\u001b[0m       \u001b[32m27.9734\u001b[0m  0.0120\n",
      "      3       \u001b[36m26.5336\u001b[0m       \u001b[32m26.6682\u001b[0m  0.0115\n",
      "      4       \u001b[36m24.5917\u001b[0m       \u001b[32m26.4648\u001b[0m  0.0121\n",
      "      5       \u001b[36m23.8267\u001b[0m       26.5717  0.0124\n",
      "      6       \u001b[36m23.5886\u001b[0m       26.6152  0.0121\n",
      "      7       \u001b[36m23.4898\u001b[0m       26.6080  0.0122\n",
      "      8       \u001b[36m23.4295\u001b[0m       26.5869  0.0123\n",
      "      9       \u001b[36m23.3849\u001b[0m       26.5667  0.0115\n",
      "     10       \u001b[36m23.3498\u001b[0m       26.5486  0.0115\n",
      "     11       \u001b[36m23.3210\u001b[0m       26.5334  0.0115\n",
      "     12       \u001b[36m23.2973\u001b[0m       26.5210  0.0126\n",
      "     13       \u001b[36m23.2772\u001b[0m       26.5099  0.0121\n",
      "     14       \u001b[36m23.2601\u001b[0m       26.5005  0.0124\n",
      "     15       \u001b[36m23.2453\u001b[0m       26.4929  0.0113\n",
      "     16       \u001b[36m23.2323\u001b[0m       26.4862  0.0121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m23.2209\u001b[0m       26.4804  0.0131\n",
      "     18       \u001b[36m23.2108\u001b[0m       26.4754  0.0129\n",
      "     19       \u001b[36m23.2018\u001b[0m       26.4713  0.0114\n",
      "     20       \u001b[36m23.1936\u001b[0m       26.4674  0.0118\n",
      "     21       \u001b[36m23.1864\u001b[0m       \u001b[32m26.4641\u001b[0m  0.0113\n",
      "     22       \u001b[36m23.1797\u001b[0m       \u001b[32m26.4613\u001b[0m  0.0121\n",
      "     23       \u001b[36m23.1737\u001b[0m       \u001b[32m26.4587\u001b[0m  0.0113\n",
      "     24       \u001b[36m23.1683\u001b[0m       \u001b[32m26.4564\u001b[0m  0.0118\n",
      "     25       \u001b[36m23.1633\u001b[0m       \u001b[32m26.4543\u001b[0m  0.0115\n",
      "     26       \u001b[36m23.1587\u001b[0m       \u001b[32m26.4524\u001b[0m  0.0120\n",
      "     27       \u001b[36m23.1544\u001b[0m       \u001b[32m26.4506\u001b[0m  0.0141\n",
      "     28       \u001b[36m23.1504\u001b[0m       \u001b[32m26.4491\u001b[0m  0.0137\n",
      "     29       \u001b[36m23.1467\u001b[0m       \u001b[32m26.4477\u001b[0m  0.0128\n",
      "     30       \u001b[36m23.1433\u001b[0m       \u001b[32m26.4462\u001b[0m  0.0116\n",
      "     31       \u001b[36m23.1401\u001b[0m       \u001b[32m26.4449\u001b[0m  0.0136\n",
      "     32       \u001b[36m23.1370\u001b[0m       \u001b[32m26.4437\u001b[0m  0.0125\n",
      "     33       \u001b[36m23.1342\u001b[0m       \u001b[32m26.4423\u001b[0m  0.0114\n",
      "     34       \u001b[36m23.1315\u001b[0m       \u001b[32m26.4411\u001b[0m  0.0118\n",
      "     35       \u001b[36m23.1290\u001b[0m       \u001b[32m26.4398\u001b[0m  0.0123\n",
      "     36       \u001b[36m23.1265\u001b[0m       \u001b[32m26.4387\u001b[0m  0.0114\n",
      "     37       \u001b[36m23.1242\u001b[0m       \u001b[32m26.4376\u001b[0m  0.0117\n",
      "     38       \u001b[36m23.1221\u001b[0m       \u001b[32m26.4367\u001b[0m  0.0125\n",
      "     39       \u001b[36m23.1200\u001b[0m       \u001b[32m26.4357\u001b[0m  0.0126\n",
      "     40       \u001b[36m23.1180\u001b[0m       \u001b[32m26.4348\u001b[0m  0.0114\n",
      "     41       \u001b[36m23.1161\u001b[0m       \u001b[32m26.4338\u001b[0m  0.0121\n",
      "     42       \u001b[36m23.1143\u001b[0m       \u001b[32m26.4329\u001b[0m  0.0127\n",
      "     43       \u001b[36m23.1126\u001b[0m       \u001b[32m26.4320\u001b[0m  0.0126\n",
      "     44       \u001b[36m23.1109\u001b[0m       \u001b[32m26.4313\u001b[0m  0.0113\n",
      "     45       \u001b[36m23.1093\u001b[0m       \u001b[32m26.4305\u001b[0m  0.0122\n",
      "     46       \u001b[36m23.1078\u001b[0m       \u001b[32m26.4298\u001b[0m  0.0113\n",
      "     47       \u001b[36m23.1063\u001b[0m       \u001b[32m26.4291\u001b[0m  0.0136\n",
      "     48       \u001b[36m23.1049\u001b[0m       \u001b[32m26.4284\u001b[0m  0.0121\n",
      "     49       \u001b[36m23.1036\u001b[0m       \u001b[32m26.4278\u001b[0m  0.0121\n",
      "     50       \u001b[36m23.1023\u001b[0m       \u001b[32m26.4271\u001b[0m  0.0117\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m38.7434\u001b[0m       \u001b[32m29.4355\u001b[0m  0.0120\n",
      "      2       \u001b[36m35.3017\u001b[0m       \u001b[32m27.3658\u001b[0m  0.0120\n",
      "      3       \u001b[36m32.0115\u001b[0m       \u001b[32m26.1890\u001b[0m  0.0113\n",
      "      4       \u001b[36m29.6722\u001b[0m       26.5105  0.0138\n",
      "      5       \u001b[36m28.8938\u001b[0m       27.0727  0.0151\n",
      "      6       \u001b[36m28.7625\u001b[0m       27.2708  0.0126\n",
      "      7       \u001b[36m28.7107\u001b[0m       27.3059  0.0127\n",
      "      8       \u001b[36m28.6680\u001b[0m       27.3013  0.0123\n",
      "      9       \u001b[36m28.6329\u001b[0m       27.2906  0.0119\n",
      "     10       \u001b[36m28.6044\u001b[0m       27.2750  0.0138\n",
      "     11       \u001b[36m28.5809\u001b[0m       27.2668  0.0115\n",
      "     12       \u001b[36m28.5620\u001b[0m       27.2587  0.0122\n",
      "     13       \u001b[36m28.5462\u001b[0m       27.2532  0.0113\n",
      "     14       \u001b[36m28.5329\u001b[0m       27.2482  0.0112\n",
      "     15       \u001b[36m28.5218\u001b[0m       27.2434  0.0112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     16       \u001b[36m28.5121\u001b[0m       27.2417  0.0114\n",
      "     17       \u001b[36m28.5040\u001b[0m       27.2380  0.0114\n",
      "     18       \u001b[36m28.4969\u001b[0m       27.2359  0.0111\n",
      "     19       \u001b[36m28.4906\u001b[0m       27.2342  0.0108\n",
      "     20       \u001b[36m28.4850\u001b[0m       27.2331  0.0107\n",
      "     21       \u001b[36m28.4801\u001b[0m       27.2318  0.0111\n",
      "     22       \u001b[36m28.4757\u001b[0m       27.2304  0.0116\n",
      "     23       \u001b[36m28.4717\u001b[0m       27.2296  0.0109\n",
      "     24       \u001b[36m28.4681\u001b[0m       27.2286  0.0108\n",
      "     25       \u001b[36m28.4648\u001b[0m       27.2280  0.0107\n",
      "     26       \u001b[36m28.4617\u001b[0m       27.2273  0.0110\n",
      "     27       \u001b[36m28.4589\u001b[0m       27.2268  0.0115\n",
      "     28       \u001b[36m28.4562\u001b[0m       27.2262  0.0110\n",
      "     29       \u001b[36m28.4538\u001b[0m       27.2264  0.0106\n",
      "     30       \u001b[36m28.4516\u001b[0m       27.2257  0.0108\n",
      "     31       \u001b[36m28.4495\u001b[0m       27.2254  0.0110\n",
      "     32       \u001b[36m28.4476\u001b[0m       27.2247  0.0115\n",
      "     33       \u001b[36m28.4458\u001b[0m       27.2240  0.0108\n",
      "     34       \u001b[36m28.4441\u001b[0m       27.2235  0.0113\n",
      "     35       \u001b[36m28.4425\u001b[0m       27.2230  0.0116\n",
      "     36       \u001b[36m28.4409\u001b[0m       27.2228  0.0119\n",
      "     37       \u001b[36m28.4395\u001b[0m       27.2224  0.0119\n",
      "     38       \u001b[36m28.4381\u001b[0m       27.2225  0.0119\n",
      "     39       \u001b[36m28.4368\u001b[0m       27.2218  0.0117\n",
      "     40       \u001b[36m28.4356\u001b[0m       27.2216  0.0115\n",
      "     41       \u001b[36m28.4344\u001b[0m       27.2213  0.0117\n",
      "     42       \u001b[36m28.4333\u001b[0m       27.2209  0.0115\n",
      "     43       \u001b[36m28.4322\u001b[0m       27.2202  0.0113\n",
      "     44       \u001b[36m28.4311\u001b[0m       27.2205  0.0112\n",
      "     45       \u001b[36m28.4302\u001b[0m       27.2195  0.0148\n",
      "     46       \u001b[36m28.4291\u001b[0m       27.2192  0.0125\n",
      "     47       \u001b[36m28.4282\u001b[0m       27.2183  0.0118\n",
      "     48       \u001b[36m28.4272\u001b[0m       27.2181  0.0113\n",
      "     49       \u001b[36m28.4264\u001b[0m       27.2165  0.0119\n",
      "     50       \u001b[36m28.4253\u001b[0m       27.2161  0.0109\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m40.2685\u001b[0m       \u001b[32m32.2341\u001b[0m  0.0127\n",
      "      2       \u001b[36m34.5466\u001b[0m       \u001b[32m31.1877\u001b[0m  0.0119\n",
      "      3       \u001b[36m33.6041\u001b[0m       31.7561  0.0121\n",
      "      4       \u001b[36m33.0259\u001b[0m       \u001b[32m30.2341\u001b[0m  0.0117\n",
      "      5       \u001b[36m32.7948\u001b[0m       30.6314  0.0119\n",
      "      6       \u001b[36m32.6964\u001b[0m       30.3878  0.0116\n",
      "      7       \u001b[36m32.5281\u001b[0m       \u001b[32m30.0822\u001b[0m  0.0131\n",
      "      8       \u001b[36m32.4604\u001b[0m       30.3459  0.0130\n",
      "      9       \u001b[36m32.4047\u001b[0m       30.1198  0.0137\n",
      "     10       \u001b[36m32.3455\u001b[0m       \u001b[32m30.0530\u001b[0m  0.0125\n",
      "     11       \u001b[36m32.3237\u001b[0m       30.1779  0.0122\n",
      "     12       \u001b[36m32.2914\u001b[0m       \u001b[32m30.0107\u001b[0m  0.0134\n",
      "     13       \u001b[36m32.2676\u001b[0m       30.0716  0.0129\n",
      "     14       \u001b[36m32.2628\u001b[0m       30.0538  0.0140\n",
      "     15       \u001b[36m32.2394\u001b[0m       \u001b[32m29.9967\u001b[0m  0.0131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     16       \u001b[36m32.2341\u001b[0m       30.0446  0.0133\n",
      "     17       \u001b[36m32.2245\u001b[0m       29.9988  0.0147\n",
      "     18       \u001b[36m32.2141\u001b[0m       30.0115  0.0132\n",
      "     19       \u001b[36m32.2084\u001b[0m       30.0065  0.0123\n",
      "     20       \u001b[36m32.1995\u001b[0m       \u001b[32m29.9905\u001b[0m  0.0124\n",
      "     21       \u001b[36m32.1943\u001b[0m       30.0011  0.0123\n",
      "     22       \u001b[36m32.1875\u001b[0m       \u001b[32m29.9876\u001b[0m  0.0124\n",
      "     23       \u001b[36m32.1818\u001b[0m       29.9910  0.0143\n",
      "     24       \u001b[36m32.1769\u001b[0m       29.9897  0.0135\n",
      "     25       \u001b[36m32.1711\u001b[0m       \u001b[32m29.9812\u001b[0m  0.0122\n",
      "     26       \u001b[36m32.1673\u001b[0m       29.9822  0.0132\n",
      "     27       \u001b[36m32.1629\u001b[0m       29.9841  0.0123\n",
      "     28       \u001b[36m32.1602\u001b[0m       \u001b[32m29.9700\u001b[0m  0.0135\n",
      "     29       32.1604       30.0137  0.0138\n",
      "     30       32.1721       30.0417  0.0133\n",
      "     31       32.1953       \u001b[32m29.9667\u001b[0m  0.0126\n",
      "     32       32.1898       29.9935  0.0168\n",
      "     33       32.1935       30.0362  0.0159\n",
      "     34       32.1797       \u001b[32m29.9536\u001b[0m  0.0135\n",
      "     35       32.1650       30.0343  0.0134\n",
      "     36       \u001b[36m32.1423\u001b[0m       \u001b[32m29.9432\u001b[0m  0.0130\n",
      "     37       \u001b[36m32.1385\u001b[0m       30.0114  0.0138\n",
      "     38       \u001b[36m32.1303\u001b[0m       \u001b[32m29.9360\u001b[0m  0.0124\n",
      "     39       \u001b[36m32.1280\u001b[0m       29.9800  0.0133\n",
      "     40       \u001b[36m32.1243\u001b[0m       29.9543  0.0148\n",
      "     41       \u001b[36m32.1196\u001b[0m       29.9545  0.0138\n",
      "     42       \u001b[36m32.1139\u001b[0m       29.9624  0.0123\n",
      "     43       \u001b[36m32.1102\u001b[0m       29.9473  0.0129\n",
      "     44       \u001b[36m32.1048\u001b[0m       29.9690  0.0127\n",
      "     45       \u001b[36m32.1020\u001b[0m       29.9482  0.0134\n",
      "     46       \u001b[36m32.0982\u001b[0m       29.9635  0.0121\n",
      "     47       \u001b[36m32.0943\u001b[0m       29.9582  0.0130\n",
      "     48       \u001b[36m32.0925\u001b[0m       29.9535  0.0126\n",
      "     49       \u001b[36m32.0880\u001b[0m       29.9772  0.0118\n",
      "     50       \u001b[36m32.0841\u001b[0m       29.9497  0.0145\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m31.8811\u001b[0m       \u001b[32m27.1786\u001b[0m  0.0121\n",
      "      2       \u001b[36m25.4636\u001b[0m       \u001b[32m26.8050\u001b[0m  0.0126\n",
      "      3       \u001b[36m24.3908\u001b[0m       \u001b[32m26.4512\u001b[0m  0.0131\n",
      "      4       \u001b[36m23.9258\u001b[0m       28.0390  0.0131\n",
      "      5       \u001b[36m23.6510\u001b[0m       27.3934  0.0133\n",
      "      6       \u001b[36m23.5608\u001b[0m       27.0051  0.0125\n",
      "      7       \u001b[36m23.3956\u001b[0m       27.4993  0.0137\n",
      "      8       \u001b[36m23.2950\u001b[0m       26.8336  0.0120\n",
      "      9       \u001b[36m23.2666\u001b[0m       26.8413  0.0115\n",
      "     10       \u001b[36m23.2087\u001b[0m       27.1612  0.0116\n",
      "     11       \u001b[36m23.1592\u001b[0m       26.6581  0.0119\n",
      "     12       23.1627       26.7551  0.0118\n",
      "     13       \u001b[36m23.1259\u001b[0m       26.7636  0.0116\n",
      "     14       \u001b[36m23.1109\u001b[0m       26.6323  0.0119\n",
      "     15       \u001b[36m23.1035\u001b[0m       26.6875  0.0118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     16       \u001b[36m23.0831\u001b[0m       26.6023  0.0116\n",
      "     17       \u001b[36m23.0793\u001b[0m       26.6197  0.0113\n",
      "     18       \u001b[36m23.0675\u001b[0m       26.6234  0.0120\n",
      "     19       \u001b[36m23.0609\u001b[0m       26.5836  0.0119\n",
      "     20       \u001b[36m23.0604\u001b[0m       26.5662  0.0181\n",
      "     21       23.0793       26.6935  0.0125\n",
      "     22       23.1137       26.5541  0.0116\n",
      "     23       23.1715       26.5266  0.0122\n",
      "     24       23.1625       26.6437  0.0118\n",
      "     25       23.0985       26.5861  0.0124\n",
      "     26       \u001b[36m23.0589\u001b[0m       26.5528  0.0128\n",
      "     27       23.0637       26.5668  0.0121\n",
      "     28       \u001b[36m23.0432\u001b[0m       26.5396  0.0120\n",
      "     29       \u001b[36m23.0392\u001b[0m       26.5404  0.0123\n",
      "     30       \u001b[36m23.0293\u001b[0m       26.5187  0.0131\n",
      "     31       \u001b[36m23.0269\u001b[0m       26.5249  0.0128\n",
      "     32       \u001b[36m23.0209\u001b[0m       26.5037  0.0120\n",
      "     33       \u001b[36m23.0183\u001b[0m       26.5156  0.0126\n",
      "     34       \u001b[36m23.0147\u001b[0m       26.5173  0.0145\n",
      "     35       \u001b[36m23.0120\u001b[0m       26.5202  0.0145\n",
      "     36       \u001b[36m23.0090\u001b[0m       26.5119  0.0117\n",
      "     37       \u001b[36m23.0069\u001b[0m       26.5133  0.0157\n",
      "     38       \u001b[36m23.0043\u001b[0m       26.5081  0.0135\n",
      "     39       \u001b[36m23.0023\u001b[0m       26.5101  0.0135\n",
      "     40       \u001b[36m23.0000\u001b[0m       26.5076  0.0120\n",
      "     41       \u001b[36m22.9981\u001b[0m       26.5077  0.0120\n",
      "     42       \u001b[36m22.9960\u001b[0m       26.5085  0.0123\n",
      "     43       \u001b[36m22.9940\u001b[0m       26.5054  0.0120\n",
      "     44       \u001b[36m22.9924\u001b[0m       26.5103  0.0135\n",
      "     45       \u001b[36m22.9907\u001b[0m       26.5047  0.0128\n",
      "     46       \u001b[36m22.9887\u001b[0m       26.5132  0.0121\n",
      "     47       \u001b[36m22.9869\u001b[0m       26.5098  0.0133\n",
      "     48       \u001b[36m22.9853\u001b[0m       26.5181  0.0149\n",
      "     49       \u001b[36m22.9836\u001b[0m       26.5053  0.0130\n",
      "     50       \u001b[36m22.9832\u001b[0m       26.5230  0.0146\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m38.0216\u001b[0m       \u001b[32m28.2242\u001b[0m  0.0127\n",
      "      2       \u001b[36m31.7669\u001b[0m       \u001b[32m27.4227\u001b[0m  0.0119\n",
      "      3       \u001b[36m30.1380\u001b[0m       \u001b[32m26.4136\u001b[0m  0.0119\n",
      "      4       \u001b[36m29.4700\u001b[0m       28.6920  0.0144\n",
      "      5       \u001b[36m29.4684\u001b[0m       27.9568  0.0179\n",
      "      6       \u001b[36m28.9362\u001b[0m       27.4196  0.0157\n",
      "      7       \u001b[36m28.7747\u001b[0m       28.0485  0.0145\n",
      "      8       28.8121       27.2046  0.0145\n",
      "      9       \u001b[36m28.6205\u001b[0m       27.2668  0.0161\n",
      "     10       \u001b[36m28.5826\u001b[0m       27.8924  0.0146\n",
      "     11       28.5921       27.3391  0.0131\n",
      "     12       \u001b[36m28.5161\u001b[0m       27.3010  0.0124\n",
      "     13       \u001b[36m28.5123\u001b[0m       27.5119  0.0122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     14       \u001b[36m28.4951\u001b[0m       27.2845  0.0122\n",
      "     15       \u001b[36m28.4689\u001b[0m       27.4116  0.0123\n",
      "     16       28.4757       27.2982  0.0117\n",
      "     17       \u001b[36m28.4493\u001b[0m       27.3343  0.0128\n",
      "     18       28.4523       27.3507  0.0121\n",
      "     19       \u001b[36m28.4403\u001b[0m       27.2863  0.0118\n",
      "     20       \u001b[36m28.4348\u001b[0m       27.3422  0.0118\n",
      "     21       \u001b[36m28.4326\u001b[0m       27.2936  0.0119\n",
      "     22       \u001b[36m28.4235\u001b[0m       27.3198  0.0116\n",
      "     23       28.4245       27.2947  0.0113\n",
      "     24       \u001b[36m28.4187\u001b[0m       27.2952  0.0125\n",
      "     25       \u001b[36m28.4154\u001b[0m       27.3072  0.0121\n",
      "     26       28.4179       27.2758  0.0119\n",
      "     27       28.4171       27.2815  0.0115\n",
      "     28       \u001b[36m28.4103\u001b[0m       27.3321  0.0114\n",
      "     29       28.4233       27.2777  0.0120\n",
      "     30       28.4379       27.2315  0.0116\n",
      "     31       28.4381       27.4131  0.0116\n",
      "     32       28.4576       27.2374  0.0111\n",
      "     33       28.5012       27.2660  0.0113\n",
      "     34       28.4821       27.3476  0.0123\n",
      "     35       28.4653       27.1833  0.0116\n",
      "     36       \u001b[36m28.3996\u001b[0m       27.3594  0.0115\n",
      "     37       28.4245       27.1739  0.0114\n",
      "     38       \u001b[36m28.3977\u001b[0m       27.3505  0.0112\n",
      "     39       28.4072       27.2453  0.0119\n",
      "     40       \u001b[36m28.3925\u001b[0m       27.3309  0.0115\n",
      "     41       28.3980       27.2317  0.0116\n",
      "     42       \u001b[36m28.3904\u001b[0m       27.3013  0.0116\n",
      "     43       28.3912       27.2653  0.0115\n",
      "     44       \u001b[36m28.3892\u001b[0m       27.2828  0.0113\n",
      "     45       \u001b[36m28.3873\u001b[0m       27.2669  0.0115\n",
      "     46       \u001b[36m28.3869\u001b[0m       27.2706  0.0120\n",
      "     47       \u001b[36m28.3844\u001b[0m       27.2688  0.0117\n",
      "     48       \u001b[36m28.3844\u001b[0m       27.2568  0.0115\n",
      "     49       \u001b[36m28.3826\u001b[0m       27.2576  0.0120\n",
      "     50       \u001b[36m28.3822\u001b[0m       27.2505  0.0116\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m42.8988\u001b[0m       \u001b[32m41.9928\u001b[0m  0.0109\n",
      "      2       \u001b[36m39.2075\u001b[0m       \u001b[32m37.5652\u001b[0m  0.0108\n",
      "      3       \u001b[36m36.0397\u001b[0m       \u001b[32m33.8187\u001b[0m  0.0105\n",
      "      4       \u001b[36m33.8481\u001b[0m       \u001b[32m31.4998\u001b[0m  0.0112\n",
      "      5       \u001b[36m32.9008\u001b[0m       \u001b[32m30.5723\u001b[0m  0.0106\n",
      "      6       \u001b[36m32.6486\u001b[0m       \u001b[32m30.2810\u001b[0m  0.0108\n",
      "      7       \u001b[36m32.5690\u001b[0m       \u001b[32m30.1789\u001b[0m  0.0105\n",
      "      8       \u001b[36m32.5236\u001b[0m       \u001b[32m30.1300\u001b[0m  0.0106\n",
      "      9       \u001b[36m32.4898\u001b[0m       \u001b[32m30.0994\u001b[0m  0.0112\n",
      "     10       \u001b[36m32.4628\u001b[0m       \u001b[32m30.0760\u001b[0m  0.0111\n",
      "     11       \u001b[36m32.4409\u001b[0m       \u001b[32m30.0568\u001b[0m  0.0110\n",
      "     12       \u001b[36m32.4226\u001b[0m       \u001b[32m30.0407\u001b[0m  0.0107\n",
      "     13       \u001b[36m32.4073\u001b[0m       \u001b[32m30.0271\u001b[0m  0.0106\n",
      "     14       \u001b[36m32.3943\u001b[0m       \u001b[32m30.0155\u001b[0m  0.0110\n",
      "     15       \u001b[36m32.3831\u001b[0m       \u001b[32m30.0053\u001b[0m  0.0115\n",
      "     16       \u001b[36m32.3735\u001b[0m       \u001b[32m29.9965\u001b[0m  0.0108\n",
      "     17       \u001b[36m32.3649\u001b[0m       \u001b[32m29.9888\u001b[0m  0.0104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     18       \u001b[36m32.3573\u001b[0m       \u001b[32m29.9821\u001b[0m  0.0110\n",
      "     19       \u001b[36m32.3505\u001b[0m       \u001b[32m29.9750\u001b[0m  0.0107\n",
      "     20       \u001b[36m32.3446\u001b[0m       \u001b[32m29.9691\u001b[0m  0.0115\n",
      "     21       \u001b[36m32.3390\u001b[0m       \u001b[32m29.9635\u001b[0m  0.0108\n",
      "     22       \u001b[36m32.3340\u001b[0m       \u001b[32m29.9588\u001b[0m  0.0106\n",
      "     23       \u001b[36m32.3294\u001b[0m       \u001b[32m29.9543\u001b[0m  0.0104\n",
      "     24       \u001b[36m32.3252\u001b[0m       \u001b[32m29.9502\u001b[0m  0.0112\n",
      "     25       \u001b[36m32.3212\u001b[0m       \u001b[32m29.9464\u001b[0m  0.0110\n",
      "     26       \u001b[36m32.3175\u001b[0m       \u001b[32m29.9425\u001b[0m  0.0108\n",
      "     27       \u001b[36m32.3142\u001b[0m       \u001b[32m29.9391\u001b[0m  0.0111\n",
      "     28       \u001b[36m32.3110\u001b[0m       \u001b[32m29.9361\u001b[0m  0.0107\n",
      "     29       \u001b[36m32.3080\u001b[0m       \u001b[32m29.9331\u001b[0m  0.0108\n",
      "     30       \u001b[36m32.3051\u001b[0m       \u001b[32m29.9307\u001b[0m  0.0115\n",
      "     31       \u001b[36m32.3024\u001b[0m       \u001b[32m29.9279\u001b[0m  0.0109\n",
      "     32       \u001b[36m32.2997\u001b[0m       \u001b[32m29.9255\u001b[0m  0.0107\n",
      "     33       \u001b[36m32.2973\u001b[0m       \u001b[32m29.9237\u001b[0m  0.0108\n",
      "     34       \u001b[36m32.2950\u001b[0m       \u001b[32m29.9210\u001b[0m  0.0114\n",
      "     35       \u001b[36m32.2928\u001b[0m       \u001b[32m29.9188\u001b[0m  0.0185\n",
      "     36       \u001b[36m32.2907\u001b[0m       \u001b[32m29.9172\u001b[0m  0.0132\n",
      "     37       \u001b[36m32.2886\u001b[0m       \u001b[32m29.9150\u001b[0m  0.0111\n",
      "     38       \u001b[36m32.2867\u001b[0m       \u001b[32m29.9135\u001b[0m  0.0113\n",
      "     39       \u001b[36m32.2848\u001b[0m       \u001b[32m29.9114\u001b[0m  0.0120\n",
      "     40       \u001b[36m32.2829\u001b[0m       \u001b[32m29.9100\u001b[0m  0.0131\n",
      "     41       \u001b[36m32.2809\u001b[0m       \u001b[32m29.9082\u001b[0m  0.0140\n",
      "     42       \u001b[36m32.2792\u001b[0m       \u001b[32m29.9070\u001b[0m  0.0119\n",
      "     43       \u001b[36m32.2774\u001b[0m       \u001b[32m29.9049\u001b[0m  0.0108\n",
      "     44       \u001b[36m32.2757\u001b[0m       \u001b[32m29.9031\u001b[0m  0.0135\n",
      "     45       \u001b[36m32.2741\u001b[0m       \u001b[32m29.9016\u001b[0m  0.0118\n",
      "     46       \u001b[36m32.2725\u001b[0m       \u001b[32m29.9000\u001b[0m  0.0117\n",
      "     47       \u001b[36m32.2710\u001b[0m       \u001b[32m29.8986\u001b[0m  0.0106\n",
      "     48       \u001b[36m32.2695\u001b[0m       \u001b[32m29.8967\u001b[0m  0.0106\n",
      "     49       \u001b[36m32.2681\u001b[0m       \u001b[32m29.8951\u001b[0m  0.0131\n",
      "     50       \u001b[36m32.2669\u001b[0m       \u001b[32m29.8937\u001b[0m  0.0117\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m34.2106\u001b[0m       \u001b[32m31.0156\u001b[0m  0.0109\n",
      "      2       \u001b[36m30.6045\u001b[0m       \u001b[32m28.6432\u001b[0m  0.0110\n",
      "      3       \u001b[36m27.5080\u001b[0m       \u001b[32m26.8534\u001b[0m  0.0113\n",
      "      4       \u001b[36m25.1555\u001b[0m       \u001b[32m26.1629\u001b[0m  0.0141\n",
      "      5       \u001b[36m24.0239\u001b[0m       26.2166  0.0117\n",
      "      6       \u001b[36m23.6744\u001b[0m       26.3249  0.0108\n",
      "      7       \u001b[36m23.5510\u001b[0m       26.3633  0.0111\n",
      "      8       \u001b[36m23.4805\u001b[0m       26.3706  0.0131\n",
      "      9       \u001b[36m23.4295\u001b[0m       26.3704  0.0124\n",
      "     10       \u001b[36m23.3885\u001b[0m       26.3684  0.0121\n",
      "     11       \u001b[36m23.3547\u001b[0m       26.3658  0.0115\n",
      "     12       \u001b[36m23.3265\u001b[0m       26.3639  0.0109\n",
      "     13       \u001b[36m23.3021\u001b[0m       26.3626  0.0134\n",
      "     14       \u001b[36m23.2813\u001b[0m       26.3623  0.0118\n",
      "     15       \u001b[36m23.2635\u001b[0m       26.3619  0.0119\n",
      "     16       \u001b[36m23.2480\u001b[0m       26.3619  0.0109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m23.2345\u001b[0m       26.3618  0.0112\n",
      "     18       \u001b[36m23.2227\u001b[0m       26.3619  0.0137\n",
      "     19       \u001b[36m23.2125\u001b[0m       26.3624  0.0121\n",
      "     20       \u001b[36m23.2033\u001b[0m       26.3633  0.0120\n",
      "     21       \u001b[36m23.1950\u001b[0m       26.3641  0.0106\n",
      "     22       \u001b[36m23.1877\u001b[0m       26.3651  0.0106\n",
      "     23       \u001b[36m23.1808\u001b[0m       26.3661  0.0131\n",
      "     24       \u001b[36m23.1748\u001b[0m       26.3673  0.0123\n",
      "     25       \u001b[36m23.1692\u001b[0m       26.3683  0.0118\n",
      "     26       \u001b[36m23.1640\u001b[0m       26.3694  0.0123\n",
      "     27       \u001b[36m23.1593\u001b[0m       26.3708  0.0122\n",
      "     28       \u001b[36m23.1551\u001b[0m       26.3724  0.0139\n",
      "     29       \u001b[36m23.1511\u001b[0m       26.3737  0.0125\n",
      "     30       \u001b[36m23.1475\u001b[0m       26.3752  0.0121\n",
      "     31       \u001b[36m23.1440\u001b[0m       26.3765  0.0108\n",
      "     32       \u001b[36m23.1409\u001b[0m       26.3776  0.0107\n",
      "     33       \u001b[36m23.1379\u001b[0m       26.3788  0.0133\n",
      "     34       \u001b[36m23.1351\u001b[0m       26.3799  0.0126\n",
      "     35       \u001b[36m23.1324\u001b[0m       26.3809  0.0120\n",
      "     36       \u001b[36m23.1299\u001b[0m       26.3820  0.0120\n",
      "     37       \u001b[36m23.1276\u001b[0m       26.3829  0.0107\n",
      "     38       \u001b[36m23.1254\u001b[0m       26.3839  0.0139\n",
      "     39       \u001b[36m23.1232\u001b[0m       26.3847  0.0119\n",
      "     40       \u001b[36m23.1212\u001b[0m       26.3855  0.0123\n",
      "     41       \u001b[36m23.1193\u001b[0m       26.3862  0.0109\n",
      "     42       \u001b[36m23.1174\u001b[0m       26.3867  0.0109\n",
      "     43       \u001b[36m23.1157\u001b[0m       26.3874  0.0138\n",
      "     44       \u001b[36m23.1140\u001b[0m       26.3879  0.0119\n",
      "     45       \u001b[36m23.1124\u001b[0m       26.3884  0.0109\n",
      "     46       \u001b[36m23.1108\u001b[0m       26.3888  0.0106\n",
      "     47       \u001b[36m23.1093\u001b[0m       26.3892  0.0135\n",
      "     48       \u001b[36m23.1079\u001b[0m       26.3896  0.0120\n",
      "     49       \u001b[36m23.1065\u001b[0m       26.3899  0.0121\n",
      "     50       \u001b[36m23.1052\u001b[0m       26.3901  0.0107\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m39.8593\u001b[0m       \u001b[32m29.6350\u001b[0m  0.0113\n",
      "      2       \u001b[36m34.8951\u001b[0m       \u001b[32m26.9799\u001b[0m  0.0142\n",
      "      3       \u001b[36m31.0115\u001b[0m       \u001b[32m26.3666\u001b[0m  0.0118\n",
      "      4       \u001b[36m29.3146\u001b[0m       27.0487  0.0122\n",
      "      5       \u001b[36m29.0222\u001b[0m       27.3240  0.0113\n",
      "      6       \u001b[36m28.9199\u001b[0m       27.3707  0.0114\n",
      "      7       \u001b[36m28.8382\u001b[0m       27.3659  0.0135\n",
      "      8       \u001b[36m28.7734\u001b[0m       27.3536  0.0124\n",
      "      9       \u001b[36m28.7230\u001b[0m       27.3418  0.0123\n",
      "     10       \u001b[36m28.6825\u001b[0m       27.3300  0.0108\n",
      "     11       \u001b[36m28.6496\u001b[0m       27.3208  0.0107\n",
      "     12       \u001b[36m28.6230\u001b[0m       27.3146  0.0134\n",
      "     13       \u001b[36m28.6009\u001b[0m       27.3097  0.0178\n",
      "     14       \u001b[36m28.5821\u001b[0m       27.3056  0.0131\n",
      "     15       \u001b[36m28.5659\u001b[0m       27.3014  0.0111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     16       \u001b[36m28.5520\u001b[0m       27.2986  0.0115\n",
      "     17       \u001b[36m28.5402\u001b[0m       27.2964  0.0152\n",
      "     18       \u001b[36m28.5298\u001b[0m       27.2940  0.0145\n",
      "     19       \u001b[36m28.5206\u001b[0m       27.2897  0.0135\n",
      "     20       \u001b[36m28.5121\u001b[0m       27.2868  0.0115\n",
      "     21       \u001b[36m28.5047\u001b[0m       27.2845  0.0111\n",
      "     22       \u001b[36m28.4979\u001b[0m       27.2822  0.0142\n",
      "     23       \u001b[36m28.4918\u001b[0m       27.2812  0.0120\n",
      "     24       \u001b[36m28.4864\u001b[0m       27.2789  0.0122\n",
      "     25       \u001b[36m28.4813\u001b[0m       27.2772  0.0112\n",
      "     26       \u001b[36m28.4766\u001b[0m       27.2755  0.0111\n",
      "     27       \u001b[36m28.4722\u001b[0m       27.2738  0.0138\n",
      "     28       \u001b[36m28.4683\u001b[0m       27.2729  0.0122\n",
      "     29       \u001b[36m28.4647\u001b[0m       27.2722  0.0124\n",
      "     30       \u001b[36m28.4614\u001b[0m       27.2706  0.0114\n",
      "     31       \u001b[36m28.4582\u001b[0m       27.2696  0.0107\n",
      "     32       \u001b[36m28.4553\u001b[0m       27.2684  0.0132\n",
      "     33       \u001b[36m28.4527\u001b[0m       27.2676  0.0118\n",
      "     34       \u001b[36m28.4502\u001b[0m       27.2668  0.0122\n",
      "     35       \u001b[36m28.4479\u001b[0m       27.2655  0.0113\n",
      "     36       \u001b[36m28.4458\u001b[0m       27.2646  0.0116\n",
      "     37       \u001b[36m28.4437\u001b[0m       27.2638  0.0141\n",
      "     38       \u001b[36m28.4418\u001b[0m       27.2629  0.0122\n",
      "     39       \u001b[36m28.4400\u001b[0m       27.2619  0.0121\n",
      "     40       \u001b[36m28.4383\u001b[0m       27.2614  0.0111\n",
      "     41       \u001b[36m28.4366\u001b[0m       27.2608  0.0111\n",
      "     42       \u001b[36m28.4352\u001b[0m       27.2601  0.0130\n",
      "     43       \u001b[36m28.4337\u001b[0m       27.2594  0.0122\n",
      "     44       \u001b[36m28.4323\u001b[0m       27.2589  0.0122\n",
      "     45       \u001b[36m28.4310\u001b[0m       27.2582  0.0111\n",
      "     46       \u001b[36m28.4298\u001b[0m       27.2578  0.0107\n",
      "     47       \u001b[36m28.4286\u001b[0m       27.2574  0.0131\n",
      "     48       \u001b[36m28.4275\u001b[0m       27.2566  0.0119\n",
      "     49       \u001b[36m28.4264\u001b[0m       27.2563  0.0121\n",
      "     50       \u001b[36m28.4253\u001b[0m       27.2557  0.0110\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m40.1502\u001b[0m       \u001b[32m31.8875\u001b[0m  0.0121\n",
      "      2       \u001b[36m34.6262\u001b[0m       \u001b[32m31.4290\u001b[0m  0.0143\n",
      "      3       \u001b[36m33.7346\u001b[0m       32.1694  0.0128\n",
      "      4       \u001b[36m33.1894\u001b[0m       \u001b[32m30.3083\u001b[0m  0.0124\n",
      "      5       \u001b[36m32.9129\u001b[0m       30.4906  0.0129\n",
      "      6       \u001b[36m32.7364\u001b[0m       30.6805  0.0131\n",
      "      7       \u001b[36m32.5235\u001b[0m       \u001b[32m30.0144\u001b[0m  0.0130\n",
      "      8       \u001b[36m32.4353\u001b[0m       30.2814  0.0113\n",
      "      9       \u001b[36m32.4046\u001b[0m       30.2901  0.0113\n",
      "     10       \u001b[36m32.3259\u001b[0m       \u001b[32m30.0108\u001b[0m  0.0141\n",
      "     11       \u001b[36m32.3087\u001b[0m       30.1756  0.0130\n",
      "     12       \u001b[36m32.2926\u001b[0m       30.0830  0.0129\n",
      "     13       \u001b[36m32.2567\u001b[0m       30.0454  0.0127\n",
      "     14       \u001b[36m32.2542\u001b[0m       30.1027  0.0116\n",
      "     15       \u001b[36m32.2456\u001b[0m       30.0156  0.0114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     16       \u001b[36m32.2288\u001b[0m       30.2315  0.0152\n",
      "     17       32.2802       30.0957  0.0125\n",
      "     18       32.3008       30.4574  0.0125\n",
      "     19       32.5227       30.1670  0.0131\n",
      "     20       32.2860       30.0188  0.0121\n",
      "     21       32.2552       30.1013  0.0120\n",
      "     22       32.2416       \u001b[32m29.9468\u001b[0m  0.0145\n",
      "     23       \u001b[36m32.2131\u001b[0m       30.0612  0.0125\n",
      "     24       \u001b[36m32.2014\u001b[0m       29.9541  0.0128\n",
      "     25       \u001b[36m32.1818\u001b[0m       29.9994  0.0116\n",
      "     26       32.1844       29.9742  0.0117\n",
      "     27       \u001b[36m32.1652\u001b[0m       29.9782  0.0149\n",
      "     28       \u001b[36m32.1634\u001b[0m       29.9832  0.0127\n",
      "     29       \u001b[36m32.1532\u001b[0m       29.9775  0.0123\n",
      "     30       \u001b[36m32.1511\u001b[0m       29.9921  0.0124\n",
      "     31       \u001b[36m32.1441\u001b[0m       29.9782  0.0141\n",
      "     32       \u001b[36m32.1402\u001b[0m       29.9908  0.0170\n",
      "     33       \u001b[36m32.1355\u001b[0m       29.9881  0.0150\n",
      "     34       \u001b[36m32.1314\u001b[0m       29.9864  0.0128\n",
      "     35       \u001b[36m32.1279\u001b[0m       29.9869  0.0120\n",
      "     36       \u001b[36m32.1237\u001b[0m       29.9869  0.0145\n",
      "     37       \u001b[36m32.1207\u001b[0m       29.9843  0.0145\n",
      "     38       \u001b[36m32.1176\u001b[0m       29.9890  0.0144\n",
      "     39       \u001b[36m32.1138\u001b[0m       29.9823  0.0142\n",
      "     40       \u001b[36m32.1114\u001b[0m       29.9880  0.0137\n",
      "     41       \u001b[36m32.1071\u001b[0m       29.9847  0.0134\n",
      "     42       \u001b[36m32.1049\u001b[0m       29.9939  0.0134\n",
      "     43       \u001b[36m32.1010\u001b[0m       29.9910  0.0124\n",
      "     44       \u001b[36m32.0987\u001b[0m       29.9973  0.0157\n",
      "     45       \u001b[36m32.0955\u001b[0m       29.9917  0.0135\n",
      "     46       \u001b[36m32.0933\u001b[0m       30.0020  0.0119\n",
      "     47       \u001b[36m32.0895\u001b[0m       30.0015  0.0116\n",
      "     48       32.0906       29.9877  0.0149\n",
      "     49       \u001b[36m32.0851\u001b[0m       30.0303  0.0127\n",
      "     50       \u001b[36m32.0848\u001b[0m       29.9911  0.0130\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m30.1731\u001b[0m       \u001b[32m27.5611\u001b[0m  0.0123\n",
      "      2       \u001b[36m25.4134\u001b[0m       \u001b[32m27.4777\u001b[0m  0.0117\n",
      "      3       \u001b[36m24.2333\u001b[0m       \u001b[32m26.4584\u001b[0m  0.0260\n",
      "      4       24.4154       26.8163  0.0199\n",
      "      5       \u001b[36m23.7410\u001b[0m       27.7532  0.0133\n",
      "      6       \u001b[36m23.5686\u001b[0m       26.8568  0.0129\n",
      "      7       \u001b[36m23.5146\u001b[0m       27.0287  0.0130\n",
      "      8       \u001b[36m23.3368\u001b[0m       27.2665  0.0125\n",
      "      9       \u001b[36m23.2725\u001b[0m       26.8087  0.0120\n",
      "     10       \u001b[36m23.2523\u001b[0m       26.9717  0.0146\n",
      "     11       \u001b[36m23.1967\u001b[0m       27.1064  0.0126\n",
      "     12       \u001b[36m23.1578\u001b[0m       26.7464  0.0131\n",
      "     13       \u001b[36m23.1533\u001b[0m       26.8573  0.0122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     14       \u001b[36m23.1246\u001b[0m       26.9233  0.0125\n",
      "     15       \u001b[36m23.1075\u001b[0m       26.7330  0.0152\n",
      "     16       \u001b[36m23.1027\u001b[0m       26.8258  0.0125\n",
      "     17       \u001b[36m23.0844\u001b[0m       26.7852  0.0127\n",
      "     18       \u001b[36m23.0801\u001b[0m       26.7407  0.0121\n",
      "     19       \u001b[36m23.0704\u001b[0m       26.7617  0.0119\n",
      "     20       \u001b[36m23.0661\u001b[0m       26.7141  0.0142\n",
      "     21       \u001b[36m23.0640\u001b[0m       26.7542  0.0129\n",
      "     22       23.0686       26.6645  0.0124\n",
      "     23       23.0683       26.7338  0.0119\n",
      "     24       23.0662       26.6530  0.0136\n",
      "     25       \u001b[36m23.0577\u001b[0m       26.7157  0.0148\n",
      "     26       23.0687       26.6762  0.0128\n",
      "     27       23.0798       26.7088  0.0131\n",
      "     28       23.1069       26.7474  0.0132\n",
      "     29       23.0848       26.5764  0.0116\n",
      "     30       23.0789       26.8936  0.0119\n",
      "     31       \u001b[36m23.0292\u001b[0m       26.5092  0.0142\n",
      "     32       23.0446       26.8434  0.0125\n",
      "     33       \u001b[36m23.0172\u001b[0m       26.5646  0.0129\n",
      "     34       23.0243       26.7143  0.0129\n",
      "     35       \u001b[36m23.0122\u001b[0m       26.6306  0.0130\n",
      "     36       \u001b[36m23.0119\u001b[0m       26.6428  0.0128\n",
      "     37       \u001b[36m23.0060\u001b[0m       26.6486  0.0128\n",
      "     38       \u001b[36m23.0043\u001b[0m       26.6174  0.0118\n",
      "     39       \u001b[36m23.0003\u001b[0m       26.6344  0.0116\n",
      "     40       \u001b[36m22.9993\u001b[0m       26.6091  0.0148\n",
      "     41       \u001b[36m22.9953\u001b[0m       26.6243  0.0138\n",
      "     42       \u001b[36m22.9943\u001b[0m       26.5992  0.0129\n",
      "     43       \u001b[36m22.9915\u001b[0m       26.6284  0.0133\n",
      "     44       \u001b[36m22.9894\u001b[0m       26.5910  0.0116\n",
      "     45       \u001b[36m22.9882\u001b[0m       26.6196  0.0115\n",
      "     46       \u001b[36m22.9855\u001b[0m       26.6110  0.0146\n",
      "     47       \u001b[36m22.9848\u001b[0m       26.6019  0.0138\n",
      "     48       \u001b[36m22.9838\u001b[0m       26.6175  0.0126\n",
      "     49       \u001b[36m22.9823\u001b[0m       26.6039  0.0117\n",
      "     50       22.9835       26.6089  0.0115\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m36.9502\u001b[0m       \u001b[32m26.9579\u001b[0m  0.0136\n",
      "      2       \u001b[36m31.3088\u001b[0m       28.0212  0.0130\n",
      "      3       \u001b[36m29.7123\u001b[0m       \u001b[32m26.3620\u001b[0m  0.0129\n",
      "      4       \u001b[36m29.2158\u001b[0m       28.2933  0.0122\n",
      "      5       29.2822       27.9320  0.0127\n",
      "      6       \u001b[36m28.7964\u001b[0m       26.9135  0.0128\n",
      "      7       \u001b[36m28.6613\u001b[0m       27.6772  0.0157\n",
      "      8       28.6843       27.3439  0.0175\n",
      "      9       \u001b[36m28.5395\u001b[0m       27.1755  0.0131\n",
      "     10       \u001b[36m28.5169\u001b[0m       27.5396  0.0126\n",
      "     11       28.5189       27.2036  0.0125\n",
      "     12       \u001b[36m28.4708\u001b[0m       27.2999  0.0126\n",
      "     13       28.4758       27.3842  0.0146\n",
      "     14       \u001b[36m28.4602\u001b[0m       27.1870  0.0127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     15       \u001b[36m28.4520\u001b[0m       27.3355  0.0133\n",
      "     16       28.4531       27.2273  0.0121\n",
      "     17       \u001b[36m28.4501\u001b[0m       27.2542  0.0125\n",
      "     18       28.4651       27.3021  0.0120\n",
      "     19       28.4750       27.1710  0.0121\n",
      "     20       28.4709       27.3181  0.0131\n",
      "     21       28.4685       27.1942  0.0117\n",
      "     22       28.4996       27.1704  0.0118\n",
      "     23       28.4636       27.4243  0.0120\n",
      "     24       28.4519       27.0074  0.0119\n",
      "     25       \u001b[36m28.4102\u001b[0m       27.3994  0.0120\n",
      "     26       28.4227       27.0973  0.0115\n",
      "     27       \u001b[36m28.4001\u001b[0m       27.2721  0.0113\n",
      "     28       28.4131       27.1547  0.0122\n",
      "     29       \u001b[36m28.3983\u001b[0m       27.1885  0.0127\n",
      "     30       28.4011       27.1941  0.0122\n",
      "     31       \u001b[36m28.3970\u001b[0m       27.1423  0.0126\n",
      "     32       \u001b[36m28.3946\u001b[0m       27.1968  0.0126\n",
      "     33       \u001b[36m28.3942\u001b[0m       27.1374  0.0129\n",
      "     34       \u001b[36m28.3915\u001b[0m       27.1703  0.0121\n",
      "     35       \u001b[36m28.3911\u001b[0m       27.1343  0.0118\n",
      "     36       \u001b[36m28.3896\u001b[0m       27.1532  0.0121\n",
      "     37       \u001b[36m28.3882\u001b[0m       27.1297  0.0120\n",
      "     38       \u001b[36m28.3864\u001b[0m       27.1353  0.0120\n",
      "     39       28.3874       27.1283  0.0116\n",
      "     40       \u001b[36m28.3837\u001b[0m       27.1240  0.0119\n",
      "     41       28.3845       27.1203  0.0122\n",
      "     42       28.3847       27.1073  0.0119\n",
      "     43       \u001b[36m28.3812\u001b[0m       27.1168  0.0116\n",
      "     44       28.3824       27.1102  0.0115\n",
      "     45       28.3868       27.0835  0.0118\n",
      "     46       28.3814       27.1148  0.0123\n",
      "     47       28.3846       27.1151  0.0118\n",
      "     48       28.3989       27.0668  0.0117\n",
      "     49       28.4052       27.1197  0.0112\n",
      "     50       28.4104       27.1385  0.0114\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m41.5152\u001b[0m       \u001b[32m40.9991\u001b[0m  0.0129\n",
      "      2       \u001b[36m38.7320\u001b[0m       \u001b[32m37.4567\u001b[0m  0.0114\n",
      "      3       \u001b[36m36.2138\u001b[0m       \u001b[32m34.4773\u001b[0m  0.0114\n",
      "      4       \u001b[36m34.4216\u001b[0m       \u001b[32m32.3680\u001b[0m  0.0113\n",
      "      5       \u001b[36m33.4026\u001b[0m       \u001b[32m31.1424\u001b[0m  0.0111\n",
      "      6       \u001b[36m32.9486\u001b[0m       \u001b[32m30.5989\u001b[0m  0.0111\n",
      "      7       \u001b[36m32.7687\u001b[0m       \u001b[32m30.3731\u001b[0m  0.0119\n",
      "      8       \u001b[36m32.6806\u001b[0m       \u001b[32m30.2652\u001b[0m  0.0119\n",
      "      9       \u001b[36m32.6221\u001b[0m       \u001b[32m30.2026\u001b[0m  0.0109\n",
      "     10       \u001b[36m32.5780\u001b[0m       \u001b[32m30.1604\u001b[0m  0.0109\n",
      "     11       \u001b[36m32.5431\u001b[0m       \u001b[32m30.1289\u001b[0m  0.0107\n",
      "     12       \u001b[36m32.5150\u001b[0m       \u001b[32m30.1039\u001b[0m  0.0108\n",
      "     13       \u001b[36m32.4910\u001b[0m       \u001b[32m30.0831\u001b[0m  0.0111\n",
      "     14       \u001b[36m32.4709\u001b[0m       \u001b[32m30.0653\u001b[0m  0.0108\n",
      "     15       \u001b[36m32.4534\u001b[0m       \u001b[32m30.0500\u001b[0m  0.0111\n",
      "     16       \u001b[36m32.4382\u001b[0m       \u001b[32m30.0363\u001b[0m  0.0106\n",
      "     17       \u001b[36m32.4244\u001b[0m       \u001b[32m30.0240\u001b[0m  0.0116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     18       \u001b[36m32.4123\u001b[0m       \u001b[32m30.0124\u001b[0m  0.0120\n",
      "     19       \u001b[36m32.4016\u001b[0m       \u001b[32m30.0022\u001b[0m  0.0110\n",
      "     20       \u001b[36m32.3917\u001b[0m       \u001b[32m29.9929\u001b[0m  0.0117\n",
      "     21       \u001b[36m32.3826\u001b[0m       \u001b[32m29.9844\u001b[0m  0.0113\n",
      "     22       \u001b[36m32.3745\u001b[0m       \u001b[32m29.9768\u001b[0m  0.0107\n",
      "     23       \u001b[36m32.3668\u001b[0m       \u001b[32m29.9695\u001b[0m  0.0121\n",
      "     24       \u001b[36m32.3597\u001b[0m       \u001b[32m29.9624\u001b[0m  0.0117\n",
      "     25       \u001b[36m32.3533\u001b[0m       \u001b[32m29.9560\u001b[0m  0.0111\n",
      "     26       \u001b[36m32.3471\u001b[0m       \u001b[32m29.9501\u001b[0m  0.0107\n",
      "     27       \u001b[36m32.3417\u001b[0m       \u001b[32m29.9444\u001b[0m  0.0107\n",
      "     28       \u001b[36m32.3364\u001b[0m       \u001b[32m29.9391\u001b[0m  0.0113\n",
      "     29       \u001b[36m32.3314\u001b[0m       \u001b[32m29.9340\u001b[0m  0.0113\n",
      "     30       \u001b[36m32.3268\u001b[0m       \u001b[32m29.9294\u001b[0m  0.0109\n",
      "     31       \u001b[36m32.3227\u001b[0m       \u001b[32m29.9248\u001b[0m  0.0109\n",
      "     32       \u001b[36m32.3185\u001b[0m       \u001b[32m29.9210\u001b[0m  0.0107\n",
      "     33       \u001b[36m32.3146\u001b[0m       \u001b[32m29.9170\u001b[0m  0.0113\n",
      "     34       \u001b[36m32.3110\u001b[0m       \u001b[32m29.9135\u001b[0m  0.0117\n",
      "     35       \u001b[36m32.3076\u001b[0m       \u001b[32m29.9101\u001b[0m  0.0114\n",
      "     36       \u001b[36m32.3044\u001b[0m       \u001b[32m29.9067\u001b[0m  0.0108\n",
      "     37       \u001b[36m32.3014\u001b[0m       \u001b[32m29.9038\u001b[0m  0.0105\n",
      "     38       \u001b[36m32.2986\u001b[0m       \u001b[32m29.9008\u001b[0m  0.0250\n",
      "     39       \u001b[36m32.2958\u001b[0m       \u001b[32m29.8982\u001b[0m  0.0184\n",
      "     40       \u001b[36m32.2932\u001b[0m       \u001b[32m29.8954\u001b[0m  0.0183\n",
      "     41       \u001b[36m32.2908\u001b[0m       \u001b[32m29.8930\u001b[0m  0.0145\n",
      "     42       \u001b[36m32.2885\u001b[0m       \u001b[32m29.8904\u001b[0m  0.0160\n",
      "     43       \u001b[36m32.2862\u001b[0m       \u001b[32m29.8880\u001b[0m  0.0175\n",
      "     44       \u001b[36m32.2839\u001b[0m       \u001b[32m29.8859\u001b[0m  0.0117\n",
      "     45       \u001b[36m32.2821\u001b[0m       \u001b[32m29.8834\u001b[0m  0.0114\n",
      "     46       \u001b[36m32.2799\u001b[0m       \u001b[32m29.8815\u001b[0m  0.0113\n",
      "     47       \u001b[36m32.2782\u001b[0m       \u001b[32m29.8796\u001b[0m  0.0115\n",
      "     48       \u001b[36m32.2763\u001b[0m       \u001b[32m29.8774\u001b[0m  0.0115\n",
      "     49       \u001b[36m32.2745\u001b[0m       \u001b[32m29.8757\u001b[0m  0.0116\n",
      "     50       \u001b[36m32.2729\u001b[0m       \u001b[32m29.8738\u001b[0m  0.0113\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m32.7610\u001b[0m       \u001b[32m30.4235\u001b[0m  0.0111\n",
      "      2       \u001b[36m30.0546\u001b[0m       \u001b[32m28.5853\u001b[0m  0.0120\n",
      "      3       \u001b[36m27.5476\u001b[0m       \u001b[32m27.1707\u001b[0m  0.0114\n",
      "      4       \u001b[36m25.6036\u001b[0m       \u001b[32m26.4360\u001b[0m  0.0112\n",
      "      5       \u001b[36m24.3929\u001b[0m       \u001b[32m26.2577\u001b[0m  0.0111\n",
      "      6       \u001b[36m23.7998\u001b[0m       26.3233  0.0107\n",
      "      7       \u001b[36m23.5679\u001b[0m       26.3896  0.0110\n",
      "      8       \u001b[36m23.4691\u001b[0m       26.4184  0.0116\n",
      "      9       \u001b[36m23.4127\u001b[0m       26.4263  0.0116\n",
      "     10       \u001b[36m23.3729\u001b[0m       26.4256  0.0117\n",
      "     11       \u001b[36m23.3421\u001b[0m       26.4223  0.0121\n",
      "     12       \u001b[36m23.3173\u001b[0m       26.4184  0.0116\n",
      "     13       \u001b[36m23.2966\u001b[0m       26.4145  0.0108\n",
      "     14       \u001b[36m23.2791\u001b[0m       26.4112  0.0108\n",
      "     15       \u001b[36m23.2639\u001b[0m       26.4085  0.0108\n",
      "     16       \u001b[36m23.2507\u001b[0m       26.4062  0.0107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m23.2389\u001b[0m       26.4043  0.0116\n",
      "     18       \u001b[36m23.2283\u001b[0m       26.4027  0.0113\n",
      "     19       \u001b[36m23.2189\u001b[0m       26.4017  0.0110\n",
      "     20       \u001b[36m23.2103\u001b[0m       26.4009  0.0109\n",
      "     21       \u001b[36m23.2026\u001b[0m       26.4002  0.0106\n",
      "     22       \u001b[36m23.1955\u001b[0m       26.3997  0.0109\n",
      "     23       \u001b[36m23.1889\u001b[0m       26.3993  0.0115\n",
      "     24       \u001b[36m23.1829\u001b[0m       26.3991  0.0110\n",
      "     25       \u001b[36m23.1773\u001b[0m       26.3989  0.0110\n",
      "     26       \u001b[36m23.1721\u001b[0m       26.3988  0.0108\n",
      "     27       \u001b[36m23.1673\u001b[0m       26.3989  0.0106\n",
      "     28       \u001b[36m23.1628\u001b[0m       26.3989  0.0111\n",
      "     29       \u001b[36m23.1586\u001b[0m       26.3990  0.0133\n",
      "     30       \u001b[36m23.1546\u001b[0m       26.3992  0.0111\n",
      "     31       \u001b[36m23.1509\u001b[0m       26.3993  0.0107\n",
      "     32       \u001b[36m23.1474\u001b[0m       26.3994  0.0106\n",
      "     33       \u001b[36m23.1441\u001b[0m       26.3995  0.0113\n",
      "     34       \u001b[36m23.1409\u001b[0m       26.3996  0.0115\n",
      "     35       \u001b[36m23.1380\u001b[0m       26.3996  0.0111\n",
      "     36       \u001b[36m23.1352\u001b[0m       26.3997  0.0108\n",
      "     37       \u001b[36m23.1326\u001b[0m       26.3997  0.0108\n",
      "     38       \u001b[36m23.1301\u001b[0m       26.3997  0.0111\n",
      "     39       \u001b[36m23.1277\u001b[0m       26.3999  0.0112\n",
      "     40       \u001b[36m23.1254\u001b[0m       26.3999  0.0109\n",
      "     41       \u001b[36m23.1232\u001b[0m       26.3999  0.0109\n",
      "     42       \u001b[36m23.1211\u001b[0m       26.3999  0.0107\n",
      "     43       \u001b[36m23.1191\u001b[0m       26.3998  0.0110\n",
      "     44       \u001b[36m23.1172\u001b[0m       26.3999  0.0111\n",
      "     45       \u001b[36m23.1154\u001b[0m       26.3999  0.0110\n",
      "     46       \u001b[36m23.1136\u001b[0m       26.3999  0.0107\n",
      "     47       \u001b[36m23.1119\u001b[0m       26.3999  0.0109\n",
      "     48       \u001b[36m23.1103\u001b[0m       26.3997  0.0115\n",
      "     49       \u001b[36m23.1088\u001b[0m       26.3996  0.0113\n",
      "     50       \u001b[36m23.1073\u001b[0m       26.3994  0.0114\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m38.9456\u001b[0m       \u001b[32m29.3647\u001b[0m  0.0107\n",
      "      2       \u001b[36m34.7770\u001b[0m       \u001b[32m26.9877\u001b[0m  0.0113\n",
      "      3       \u001b[36m31.2170\u001b[0m       \u001b[32m26.3942\u001b[0m  0.0115\n",
      "      4       \u001b[36m29.3992\u001b[0m       27.0745  0.0115\n",
      "      5       \u001b[36m28.9708\u001b[0m       27.4309  0.0115\n",
      "      6       \u001b[36m28.8637\u001b[0m       27.4890  0.0109\n",
      "      7       \u001b[36m28.7935\u001b[0m       27.4749  0.0107\n",
      "      8       \u001b[36m28.7384\u001b[0m       27.4564  0.0107\n",
      "      9       \u001b[36m28.6952\u001b[0m       27.4393  0.0113\n",
      "     10       \u001b[36m28.6611\u001b[0m       27.4292  0.0109\n",
      "     11       \u001b[36m28.6334\u001b[0m       27.4217  0.0110\n",
      "     12       \u001b[36m28.6108\u001b[0m       27.4147  0.0110\n",
      "     13       \u001b[36m28.5917\u001b[0m       27.4082  0.0107\n",
      "     14       \u001b[36m28.5753\u001b[0m       27.4024  0.0106\n",
      "     15       \u001b[36m28.5612\u001b[0m       27.3983  0.0113\n",
      "     16       \u001b[36m28.5489\u001b[0m       27.3940  0.0109\n",
      "     17       \u001b[36m28.5380\u001b[0m       27.3909  0.0117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     18       \u001b[36m28.5283\u001b[0m       27.3876  0.0144\n",
      "     19       \u001b[36m28.5198\u001b[0m       27.3850  0.0132\n",
      "     20       \u001b[36m28.5121\u001b[0m       27.3826  0.0117\n",
      "     21       \u001b[36m28.5053\u001b[0m       27.3804  0.0118\n",
      "     22       \u001b[36m28.4991\u001b[0m       27.3769  0.0117\n",
      "     23       \u001b[36m28.4934\u001b[0m       27.3744  0.0142\n",
      "     24       \u001b[36m28.4883\u001b[0m       27.3713  0.0119\n",
      "     25       \u001b[36m28.4834\u001b[0m       27.3687  0.0162\n",
      "     26       \u001b[36m28.4791\u001b[0m       27.3659  0.0110\n",
      "     27       \u001b[36m28.4749\u001b[0m       27.3627  0.0112\n",
      "     28       \u001b[36m28.4713\u001b[0m       27.3595  0.0114\n",
      "     29       \u001b[36m28.4677\u001b[0m       27.3575  0.0111\n",
      "     30       \u001b[36m28.4645\u001b[0m       27.3554  0.0111\n",
      "     31       \u001b[36m28.4616\u001b[0m       27.3528  0.0110\n",
      "     32       \u001b[36m28.4588\u001b[0m       27.3504  0.0109\n",
      "     33       \u001b[36m28.4560\u001b[0m       27.3485  0.0108\n",
      "     34       \u001b[36m28.4536\u001b[0m       27.3459  0.0110\n",
      "     35       \u001b[36m28.4513\u001b[0m       27.3440  0.0109\n",
      "     36       \u001b[36m28.4490\u001b[0m       27.3425  0.0112\n",
      "     37       \u001b[36m28.4471\u001b[0m       27.3401  0.0107\n",
      "     38       \u001b[36m28.4450\u001b[0m       27.3382  0.0110\n",
      "     39       \u001b[36m28.4431\u001b[0m       27.3364  0.0114\n",
      "     40       \u001b[36m28.4414\u001b[0m       27.3345  0.0109\n",
      "     41       \u001b[36m28.4397\u001b[0m       27.3326  0.0107\n",
      "     42       \u001b[36m28.4381\u001b[0m       27.3310  0.0106\n",
      "     43       \u001b[36m28.4366\u001b[0m       27.3296  0.0108\n",
      "     44       \u001b[36m28.4352\u001b[0m       27.3279  0.0113\n",
      "     45       \u001b[36m28.4338\u001b[0m       27.3261  0.0110\n",
      "     46       \u001b[36m28.4325\u001b[0m       27.3248  0.0111\n",
      "     47       \u001b[36m28.4313\u001b[0m       27.3231  0.0110\n",
      "     48       \u001b[36m28.4301\u001b[0m       27.3220  0.0115\n",
      "     49       \u001b[36m28.4289\u001b[0m       27.3202  0.0112\n",
      "     50       \u001b[36m28.4278\u001b[0m       27.3184  0.0110\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m40.6849\u001b[0m       \u001b[32m35.2110\u001b[0m  0.0118\n",
      "      2       \u001b[36m35.7121\u001b[0m       \u001b[32m31.7131\u001b[0m  0.0121\n",
      "      3       \u001b[36m33.9120\u001b[0m       33.7475  0.0129\n",
      "      4       \u001b[36m33.6969\u001b[0m       \u001b[32m30.7191\u001b[0m  0.0119\n",
      "      5       \u001b[36m33.1676\u001b[0m       \u001b[32m30.6572\u001b[0m  0.0118\n",
      "      6       \u001b[36m32.9022\u001b[0m       31.1487  0.0117\n",
      "      7       \u001b[36m32.7499\u001b[0m       \u001b[32m30.1647\u001b[0m  0.0115\n",
      "      8       \u001b[36m32.6005\u001b[0m       30.4132  0.0114\n",
      "      9       \u001b[36m32.5285\u001b[0m       30.5584  0.0116\n",
      "     10       \u001b[36m32.4363\u001b[0m       \u001b[32m30.1284\u001b[0m  0.0116\n",
      "     11       \u001b[36m32.4013\u001b[0m       30.4207  0.0116\n",
      "     12       \u001b[36m32.3857\u001b[0m       30.2888  0.0115\n",
      "     13       \u001b[36m32.3306\u001b[0m       30.2689  0.0121\n",
      "     14       32.3335       30.3838  0.0125\n",
      "     15       \u001b[36m32.3077\u001b[0m       30.2876  0.0212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     16       \u001b[36m32.2929\u001b[0m       30.3412  0.0130\n",
      "     17       \u001b[36m32.2863\u001b[0m       30.2882  0.0120\n",
      "     18       \u001b[36m32.2725\u001b[0m       30.3473  0.0118\n",
      "     19       \u001b[36m32.2656\u001b[0m       30.2559  0.0115\n",
      "     20       \u001b[36m32.2586\u001b[0m       30.3986  0.0114\n",
      "     21       \u001b[36m32.2526\u001b[0m       30.1964  0.0119\n",
      "     22       32.2554       30.5051  0.0118\n",
      "     23       \u001b[36m32.2513\u001b[0m       30.1438  0.0115\n",
      "     24       32.2711       30.6587  0.0117\n",
      "     25       32.2668       \u001b[32m30.0776\u001b[0m  0.0116\n",
      "     26       32.2809       30.6273  0.0115\n",
      "     27       32.2574       \u001b[32m30.0350\u001b[0m  0.0116\n",
      "     28       \u001b[36m32.2377\u001b[0m       30.4870  0.0121\n",
      "     29       \u001b[36m32.2327\u001b[0m       30.1339  0.0122\n",
      "     30       \u001b[36m32.2006\u001b[0m       30.3706  0.0121\n",
      "     31       32.2179       30.2338  0.0119\n",
      "     32       \u001b[36m32.1885\u001b[0m       30.3355  0.0127\n",
      "     33       32.2034       30.2656  0.0124\n",
      "     34       \u001b[36m32.1854\u001b[0m       30.3507  0.0122\n",
      "     35       32.1916       30.2714  0.0118\n",
      "     36       \u001b[36m32.1829\u001b[0m       30.3808  0.0115\n",
      "     37       \u001b[36m32.1823\u001b[0m       30.2687  0.0117\n",
      "     38       \u001b[36m32.1790\u001b[0m       30.4127  0.0115\n",
      "     39       \u001b[36m32.1735\u001b[0m       30.2731  0.0118\n",
      "     40       32.1754       30.4335  0.0113\n",
      "     41       \u001b[36m32.1667\u001b[0m       30.2834  0.0117\n",
      "     42       32.1726       30.4711  0.0116\n",
      "     43       \u001b[36m32.1608\u001b[0m       30.2940  0.0119\n",
      "     44       32.1734       30.5013  0.0122\n",
      "     45       \u001b[36m32.1547\u001b[0m       30.3055  0.0123\n",
      "     46       32.1724       30.5161  0.0123\n",
      "     47       \u001b[36m32.1463\u001b[0m       30.3388  0.0197\n",
      "     48       32.1677       30.4773  0.0151\n",
      "     49       \u001b[36m32.1378\u001b[0m       30.4236  0.0129\n",
      "     50       32.1590       30.4155  0.0126\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m30.8766\u001b[0m       \u001b[32m28.6309\u001b[0m  0.0136\n",
      "      2       \u001b[36m25.5146\u001b[0m       \u001b[32m26.4378\u001b[0m  0.0124\n",
      "      3       \u001b[36m24.4866\u001b[0m       \u001b[32m26.4121\u001b[0m  0.0129\n",
      "      4       \u001b[36m23.8766\u001b[0m       28.4911  0.0122\n",
      "      5       \u001b[36m23.5888\u001b[0m       26.7552  0.0119\n",
      "      6       \u001b[36m23.5887\u001b[0m       27.0501  0.0116\n",
      "      7       \u001b[36m23.3682\u001b[0m       27.4991  0.0119\n",
      "      8       \u001b[36m23.2420\u001b[0m       26.8769  0.0117\n",
      "      9       23.2535       27.2191  0.0121\n",
      "     10       \u001b[36m23.1736\u001b[0m       27.0350  0.0119\n",
      "     11       \u001b[36m23.1528\u001b[0m       26.8276  0.0117\n",
      "     12       \u001b[36m23.1426\u001b[0m       27.0935  0.0122\n",
      "     13       \u001b[36m23.1118\u001b[0m       26.8384  0.0126\n",
      "     14       \u001b[36m23.1092\u001b[0m       26.8418  0.0122\n",
      "     15       \u001b[36m23.0948\u001b[0m       26.8830  0.0118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     16       \u001b[36m23.0844\u001b[0m       26.7643  0.0119\n",
      "     17       \u001b[36m23.0805\u001b[0m       26.8495  0.0120\n",
      "     18       23.0953       26.7245  0.0119\n",
      "     19       23.1218       26.8375  0.0119\n",
      "     20       23.1138       26.7273  0.0118\n",
      "     21       23.1515       26.6170  0.0116\n",
      "     22       23.0851       26.7754  0.0116\n",
      "     23       \u001b[36m23.0668\u001b[0m       26.6417  0.0122\n",
      "     24       \u001b[36m23.0486\u001b[0m       26.7170  0.0124\n",
      "     25       \u001b[36m23.0453\u001b[0m       26.6536  0.0121\n",
      "     26       \u001b[36m23.0388\u001b[0m       26.6304  0.0116\n",
      "     27       \u001b[36m23.0315\u001b[0m       26.6341  0.0121\n",
      "     28       \u001b[36m23.0235\u001b[0m       26.5979  0.0131\n",
      "     29       \u001b[36m23.0215\u001b[0m       26.6091  0.0125\n",
      "     30       \u001b[36m23.0160\u001b[0m       26.5769  0.0125\n",
      "     31       \u001b[36m23.0139\u001b[0m       26.6065  0.0117\n",
      "     32       \u001b[36m23.0103\u001b[0m       26.5791  0.0119\n",
      "     33       \u001b[36m23.0064\u001b[0m       26.5870  0.0121\n",
      "     34       \u001b[36m23.0039\u001b[0m       26.5772  0.0120\n",
      "     35       \u001b[36m23.0011\u001b[0m       26.5826  0.0121\n",
      "     36       \u001b[36m22.9979\u001b[0m       26.5722  0.0118\n",
      "     37       \u001b[36m22.9964\u001b[0m       26.5781  0.0119\n",
      "     38       \u001b[36m22.9929\u001b[0m       26.5745  0.0132\n",
      "     39       \u001b[36m22.9914\u001b[0m       26.5689  0.0123\n",
      "     40       \u001b[36m22.9889\u001b[0m       26.5821  0.0118\n",
      "     41       \u001b[36m22.9857\u001b[0m       26.5647  0.0119\n",
      "     42       \u001b[36m22.9850\u001b[0m       26.5791  0.0113\n",
      "     43       \u001b[36m22.9833\u001b[0m       26.5780  0.0119\n",
      "     44       \u001b[36m22.9791\u001b[0m       26.5715  0.0118\n",
      "     45       22.9835       26.5764  0.0119\n",
      "     46       22.9860       26.6085  0.0116\n",
      "     47       22.9799       26.5754  0.0115\n",
      "     48       23.0158       26.5730  0.0119\n",
      "     49       23.0473       26.6528  0.0117\n",
      "     50       23.0191       26.5128  0.0116\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m36.9336\u001b[0m       \u001b[32m33.3928\u001b[0m  0.0117\n",
      "      2       \u001b[36m32.1332\u001b[0m       \u001b[32m26.5476\u001b[0m  0.0115\n",
      "      3       \u001b[36m30.4774\u001b[0m       26.9391  0.0130\n",
      "      4       \u001b[36m29.5684\u001b[0m       29.0289  0.0115\n",
      "      5       \u001b[36m29.1961\u001b[0m       27.0546  0.0118\n",
      "      6       \u001b[36m28.9851\u001b[0m       27.7613  0.0115\n",
      "      7       \u001b[36m28.9187\u001b[0m       27.5297  0.0117\n",
      "      8       \u001b[36m28.6836\u001b[0m       26.9856  0.0123\n",
      "      9       \u001b[36m28.6120\u001b[0m       27.8712  0.0117\n",
      "     10       \u001b[36m28.6115\u001b[0m       27.2112  0.0116\n",
      "     11       \u001b[36m28.5217\u001b[0m       27.3736  0.0113\n",
      "     12       28.5494       27.3419  0.0113\n",
      "     13       \u001b[36m28.4904\u001b[0m       27.3133  0.0121\n",
      "     14       28.4967       27.3374  0.0120\n",
      "     15       \u001b[36m28.4749\u001b[0m       27.2792  0.0119\n",
      "     16       \u001b[36m28.4731\u001b[0m       27.2433  0.0118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m28.4584\u001b[0m       27.3161  0.0123\n",
      "     18       \u001b[36m28.4525\u001b[0m       27.2081  0.0114\n",
      "     19       \u001b[36m28.4410\u001b[0m       27.2886  0.0113\n",
      "     20       \u001b[36m28.4383\u001b[0m       27.2324  0.0119\n",
      "     21       28.4552       27.2470  0.0118\n",
      "     22       28.4520       27.2594  0.0117\n",
      "     23       28.4563       27.1929  0.0120\n",
      "     24       \u001b[36m28.4276\u001b[0m       27.2222  0.0156\n",
      "     25       \u001b[36m28.4126\u001b[0m       27.2196  0.0152\n",
      "     26       28.4180       27.1790  0.0125\n",
      "     27       \u001b[36m28.4063\u001b[0m       27.1975  0.0122\n",
      "     28       28.4139       27.1766  0.0126\n",
      "     29       \u001b[36m28.3986\u001b[0m       27.2104  0.0129\n",
      "     30       28.4064       27.1338  0.0166\n",
      "     31       \u001b[36m28.3962\u001b[0m       27.2075  0.0132\n",
      "     32       28.3993       27.1362  0.0128\n",
      "     33       \u001b[36m28.3945\u001b[0m       27.1762  0.0119\n",
      "     34       28.3948       27.1409  0.0118\n",
      "     35       \u001b[36m28.3909\u001b[0m       27.1731  0.0122\n",
      "     36       28.3931       27.1211  0.0117\n",
      "     37       \u001b[36m28.3890\u001b[0m       27.1643  0.0117\n",
      "     38       \u001b[36m28.3870\u001b[0m       27.1487  0.0119\n",
      "     39       28.3913       27.1228  0.0118\n",
      "     40       28.3876       27.1434  0.0128\n",
      "     41       \u001b[36m28.3847\u001b[0m       27.1705  0.0117\n",
      "     42       28.3960       27.0984  0.0148\n",
      "     43       28.4020       27.0959  0.0115\n",
      "     44       28.3937       27.2271  0.0118\n",
      "     45       28.4260       27.0564  0.0115\n",
      "     46       28.4405       27.0837  0.0113\n",
      "     47       28.4332       27.2592  0.0118\n",
      "     48       28.4461       26.9539  0.0120\n",
      "     49       28.4116       27.2501  0.0117\n",
      "     50       28.4112       27.0293  0.0114\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m43.3184\u001b[0m       \u001b[32m42.3977\u001b[0m  0.0108\n",
      "      2       \u001b[36m39.7313\u001b[0m       \u001b[32m38.2024\u001b[0m  0.0118\n",
      "      3       \u001b[36m36.6720\u001b[0m       \u001b[32m34.6088\u001b[0m  0.0111\n",
      "      4       \u001b[36m34.3904\u001b[0m       \u001b[32m31.9224\u001b[0m  0.0117\n",
      "      5       \u001b[36m33.1819\u001b[0m       \u001b[32m30.7188\u001b[0m  0.0111\n",
      "      6       \u001b[36m32.7971\u001b[0m       \u001b[32m30.3407\u001b[0m  0.0111\n",
      "      7       \u001b[36m32.6654\u001b[0m       \u001b[32m30.2136\u001b[0m  0.0119\n",
      "      8       \u001b[36m32.5955\u001b[0m       \u001b[32m30.1576\u001b[0m  0.0112\n",
      "      9       \u001b[36m32.5500\u001b[0m       \u001b[32m30.1262\u001b[0m  0.0110\n",
      "     10       \u001b[36m32.5175\u001b[0m       \u001b[32m30.1046\u001b[0m  0.0109\n",
      "     11       \u001b[36m32.4925\u001b[0m       \u001b[32m30.0867\u001b[0m  0.0110\n",
      "     12       \u001b[36m32.4729\u001b[0m       \u001b[32m30.0714\u001b[0m  0.0111\n",
      "     13       \u001b[36m32.4552\u001b[0m       \u001b[32m30.0595\u001b[0m  0.0114\n",
      "     14       \u001b[36m32.4419\u001b[0m       \u001b[32m30.0476\u001b[0m  0.0112\n",
      "     15       \u001b[36m32.4283\u001b[0m       \u001b[32m30.0379\u001b[0m  0.0108\n",
      "     16       \u001b[36m32.4182\u001b[0m       \u001b[32m30.0278\u001b[0m  0.0109\n",
      "     17       \u001b[36m32.4077\u001b[0m       \u001b[32m30.0198\u001b[0m  0.0111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     18       \u001b[36m32.3999\u001b[0m       \u001b[32m30.0113\u001b[0m  0.0120\n",
      "     19       \u001b[36m32.3911\u001b[0m       \u001b[32m30.0052\u001b[0m  0.0114\n",
      "     20       \u001b[36m32.3846\u001b[0m       \u001b[32m29.9978\u001b[0m  0.0111\n",
      "     21       \u001b[36m32.3772\u001b[0m       \u001b[32m29.9926\u001b[0m  0.0108\n",
      "     22       \u001b[36m32.3714\u001b[0m       \u001b[32m29.9866\u001b[0m  0.0109\n",
      "     23       \u001b[36m32.3658\u001b[0m       \u001b[32m29.9810\u001b[0m  0.0120\n",
      "     24       \u001b[36m32.3598\u001b[0m       \u001b[32m29.9767\u001b[0m  0.0117\n",
      "     25       \u001b[36m32.3551\u001b[0m       \u001b[32m29.9715\u001b[0m  0.0114\n",
      "     26       \u001b[36m32.3499\u001b[0m       \u001b[32m29.9676\u001b[0m  0.0108\n",
      "     27       \u001b[36m32.3460\u001b[0m       \u001b[32m29.9636\u001b[0m  0.0106\n",
      "     28       \u001b[36m32.3411\u001b[0m       \u001b[32m29.9601\u001b[0m  0.0111\n",
      "     29       \u001b[36m32.3375\u001b[0m       \u001b[32m29.9566\u001b[0m  0.0113\n",
      "     30       \u001b[36m32.3332\u001b[0m       \u001b[32m29.9535\u001b[0m  0.0110\n",
      "     31       \u001b[36m32.3297\u001b[0m       \u001b[32m29.9505\u001b[0m  0.0109\n",
      "     32       \u001b[36m32.3262\u001b[0m       \u001b[32m29.9478\u001b[0m  0.0106\n",
      "     33       \u001b[36m32.3231\u001b[0m       \u001b[32m29.9451\u001b[0m  0.0109\n",
      "     34       \u001b[36m32.3195\u001b[0m       \u001b[32m29.9429\u001b[0m  0.0111\n",
      "     35       \u001b[36m32.3164\u001b[0m       \u001b[32m29.9409\u001b[0m  0.0108\n",
      "     36       \u001b[36m32.3136\u001b[0m       \u001b[32m29.9384\u001b[0m  0.0112\n",
      "     37       \u001b[36m32.3110\u001b[0m       \u001b[32m29.9363\u001b[0m  0.0112\n",
      "     38       \u001b[36m32.3081\u001b[0m       \u001b[32m29.9343\u001b[0m  0.0125\n",
      "     39       \u001b[36m32.3056\u001b[0m       \u001b[32m29.9326\u001b[0m  0.0113\n",
      "     40       \u001b[36m32.3028\u001b[0m       \u001b[32m29.9313\u001b[0m  0.0120\n",
      "     41       \u001b[36m32.3006\u001b[0m       \u001b[32m29.9295\u001b[0m  0.0114\n",
      "     42       \u001b[36m32.2981\u001b[0m       \u001b[32m29.9279\u001b[0m  0.0121\n",
      "     43       \u001b[36m32.2958\u001b[0m       \u001b[32m29.9266\u001b[0m  0.0111\n",
      "     44       \u001b[36m32.2937\u001b[0m       \u001b[32m29.9249\u001b[0m  0.0112\n",
      "     45       \u001b[36m32.2913\u001b[0m       \u001b[32m29.9235\u001b[0m  0.0110\n",
      "     46       \u001b[36m32.2895\u001b[0m       \u001b[32m29.9221\u001b[0m  0.0110\n",
      "     47       \u001b[36m32.2875\u001b[0m       \u001b[32m29.9208\u001b[0m  0.0109\n",
      "     48       \u001b[36m32.2854\u001b[0m       \u001b[32m29.9196\u001b[0m  0.0113\n",
      "     49       \u001b[36m32.2837\u001b[0m       \u001b[32m29.9183\u001b[0m  0.0115\n",
      "     50       \u001b[36m32.2819\u001b[0m       \u001b[32m29.9172\u001b[0m  0.0112\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m32.2824\u001b[0m       \u001b[32m29.8467\u001b[0m  0.0109\n",
      "      2       \u001b[36m29.1380\u001b[0m       \u001b[32m27.7567\u001b[0m  0.0112\n",
      "      3       \u001b[36m26.3756\u001b[0m       \u001b[32m26.6125\u001b[0m  0.0125\n",
      "      4       \u001b[36m24.6925\u001b[0m       \u001b[32m26.3509\u001b[0m  0.0151\n",
      "      5       \u001b[36m23.9312\u001b[0m       26.4006  0.0206\n",
      "      6       \u001b[36m23.6424\u001b[0m       26.4463  0.0175\n",
      "      7       \u001b[36m23.5193\u001b[0m       26.4545  0.0157\n",
      "      8       \u001b[36m23.4486\u001b[0m       26.4470  0.0130\n",
      "      9       \u001b[36m23.3995\u001b[0m       26.4364  0.0154\n",
      "     10       \u001b[36m23.3620\u001b[0m       26.4263  0.0133\n",
      "     11       \u001b[36m23.3322\u001b[0m       26.4175  0.0132\n",
      "     12       \u001b[36m23.3081\u001b[0m       26.4104  0.0131\n",
      "     13       \u001b[36m23.2880\u001b[0m       26.4044  0.0126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     14       \u001b[36m23.2711\u001b[0m       26.3995  0.0152\n",
      "     15       \u001b[36m23.2567\u001b[0m       26.3952  0.0121\n",
      "     16       \u001b[36m23.2441\u001b[0m       26.3914  0.0116\n",
      "     17       \u001b[36m23.2332\u001b[0m       26.3881  0.0124\n",
      "     18       \u001b[36m23.2234\u001b[0m       26.3854  0.0123\n",
      "     19       \u001b[36m23.2147\u001b[0m       26.3831  0.0124\n",
      "     20       \u001b[36m23.2068\u001b[0m       26.3810  0.0131\n",
      "     21       \u001b[36m23.1995\u001b[0m       26.3794  0.0126\n",
      "     22       \u001b[36m23.1929\u001b[0m       26.3778  0.0119\n",
      "     23       \u001b[36m23.1868\u001b[0m       26.3766  0.0112\n",
      "     24       \u001b[36m23.1811\u001b[0m       26.3755  0.0118\n",
      "     25       \u001b[36m23.1758\u001b[0m       26.3744  0.0107\n",
      "     26       \u001b[36m23.1709\u001b[0m       26.3735  0.0114\n",
      "     27       \u001b[36m23.1663\u001b[0m       26.3725  0.0115\n",
      "     28       \u001b[36m23.1621\u001b[0m       26.3716  0.0117\n",
      "     29       \u001b[36m23.1580\u001b[0m       26.3708  0.0122\n",
      "     30       \u001b[36m23.1542\u001b[0m       26.3699  0.0136\n",
      "     31       \u001b[36m23.1506\u001b[0m       26.3691  0.0118\n",
      "     32       \u001b[36m23.1472\u001b[0m       26.3683  0.0118\n",
      "     33       \u001b[36m23.1439\u001b[0m       26.3677  0.0123\n",
      "     34       \u001b[36m23.1409\u001b[0m       26.3672  0.0118\n",
      "     35       \u001b[36m23.1379\u001b[0m       26.3665  0.0120\n",
      "     36       \u001b[36m23.1352\u001b[0m       26.3661  0.0118\n",
      "     37       \u001b[36m23.1325\u001b[0m       26.3654  0.0119\n",
      "     38       \u001b[36m23.1300\u001b[0m       26.3648  0.0118\n",
      "     39       \u001b[36m23.1276\u001b[0m       26.3641  0.0127\n",
      "     40       \u001b[36m23.1252\u001b[0m       26.3637  0.0112\n",
      "     41       \u001b[36m23.1230\u001b[0m       26.3632  0.0126\n",
      "     42       \u001b[36m23.1209\u001b[0m       26.3626  0.0116\n",
      "     43       \u001b[36m23.1188\u001b[0m       26.3622  0.0119\n",
      "     44       \u001b[36m23.1169\u001b[0m       26.3616  0.0119\n",
      "     45       \u001b[36m23.1150\u001b[0m       26.3612  0.0120\n",
      "     46       \u001b[36m23.1132\u001b[0m       26.3607  0.0113\n",
      "     47       \u001b[36m23.1114\u001b[0m       26.3602  0.0113\n",
      "     48       \u001b[36m23.1098\u001b[0m       26.3598  0.0114\n",
      "     49       \u001b[36m23.1081\u001b[0m       26.3592  0.0112\n",
      "     50       \u001b[36m23.1066\u001b[0m       26.3590  0.0114\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m40.8125\u001b[0m       \u001b[32m30.2054\u001b[0m  0.0122\n",
      "      2       \u001b[36m35.7480\u001b[0m       \u001b[32m27.2064\u001b[0m  0.0115\n",
      "      3       \u001b[36m31.5153\u001b[0m       \u001b[32m26.3729\u001b[0m  0.0112\n",
      "      4       \u001b[36m29.4897\u001b[0m       27.0687  0.0112\n",
      "      5       \u001b[36m29.0243\u001b[0m       27.4345  0.0110\n",
      "      6       \u001b[36m28.9055\u001b[0m       27.4956  0.0107\n",
      "      7       \u001b[36m28.8263\u001b[0m       27.4879  0.0111\n",
      "      8       \u001b[36m28.7653\u001b[0m       27.4713  0.0113\n",
      "      9       \u001b[36m28.7177\u001b[0m       27.4582  0.0114\n",
      "     10       \u001b[36m28.6799\u001b[0m       27.4458  0.0110\n",
      "     11       \u001b[36m28.6490\u001b[0m       27.4360  0.0108\n",
      "     12       \u001b[36m28.6235\u001b[0m       27.4279  0.0122\n",
      "     13       \u001b[36m28.6024\u001b[0m       27.4212  0.0116\n",
      "     14       \u001b[36m28.5846\u001b[0m       27.4157  0.0113\n",
      "     15       \u001b[36m28.5694\u001b[0m       27.4108  0.0111\n",
      "     16       \u001b[36m28.5562\u001b[0m       27.4067  0.0118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m28.5449\u001b[0m       27.4015  0.0124\n",
      "     18       \u001b[36m28.5347\u001b[0m       27.3993  0.0119\n",
      "     19       \u001b[36m28.5259\u001b[0m       27.3968  0.0117\n",
      "     20       \u001b[36m28.5181\u001b[0m       27.3947  0.0112\n",
      "     21       \u001b[36m28.5113\u001b[0m       27.3914  0.0112\n",
      "     22       \u001b[36m28.5048\u001b[0m       27.3898  0.0119\n",
      "     23       \u001b[36m28.4991\u001b[0m       27.3878  0.0113\n",
      "     24       \u001b[36m28.4940\u001b[0m       27.3840  0.0113\n",
      "     25       \u001b[36m28.4890\u001b[0m       27.3830  0.0111\n",
      "     26       \u001b[36m28.4847\u001b[0m       27.3813  0.0112\n",
      "     27       \u001b[36m28.4806\u001b[0m       27.3796  0.0115\n",
      "     28       \u001b[36m28.4768\u001b[0m       27.3774  0.0111\n",
      "     29       \u001b[36m28.4733\u001b[0m       27.3752  0.0111\n",
      "     30       \u001b[36m28.4699\u001b[0m       27.3740  0.0112\n",
      "     31       \u001b[36m28.4668\u001b[0m       27.3722  0.0120\n",
      "     32       \u001b[36m28.4641\u001b[0m       27.3708  0.0192\n",
      "     33       \u001b[36m28.4614\u001b[0m       27.3685  0.0139\n",
      "     34       \u001b[36m28.4589\u001b[0m       27.3673  0.0127\n",
      "     35       \u001b[36m28.4566\u001b[0m       27.3651  0.0144\n",
      "     36       \u001b[36m28.4544\u001b[0m       27.3642  0.0145\n",
      "     37       \u001b[36m28.4523\u001b[0m       27.3632  0.0205\n",
      "     38       \u001b[36m28.4503\u001b[0m       27.3616  0.0145\n",
      "     39       \u001b[36m28.4485\u001b[0m       27.3608  0.0115\n",
      "     40       \u001b[36m28.4467\u001b[0m       27.3593  0.0113\n",
      "     41       \u001b[36m28.4452\u001b[0m       27.3589  0.0119\n",
      "     42       \u001b[36m28.4435\u001b[0m       27.3579  0.0115\n",
      "     43       \u001b[36m28.4421\u001b[0m       27.3574  0.0115\n",
      "     44       \u001b[36m28.4406\u001b[0m       27.3559  0.0114\n",
      "     45       \u001b[36m28.4393\u001b[0m       27.3556  0.0115\n",
      "     46       \u001b[36m28.4380\u001b[0m       27.3546  0.0116\n",
      "     47       \u001b[36m28.4367\u001b[0m       27.3538  0.0118\n",
      "     48       \u001b[36m28.4355\u001b[0m       27.3532  0.0116\n",
      "     49       \u001b[36m28.4344\u001b[0m       27.3524  0.0111\n",
      "     50       \u001b[36m28.4333\u001b[0m       27.3515  0.0106\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m39.6288\u001b[0m       \u001b[32m31.9470\u001b[0m  0.0124\n",
      "      2       \u001b[36m34.5026\u001b[0m       33.2584  0.0119\n",
      "      3       \u001b[36m33.8380\u001b[0m       \u001b[32m30.8486\u001b[0m  0.0121\n",
      "      4       \u001b[36m33.0198\u001b[0m       \u001b[32m30.2935\u001b[0m  0.0126\n",
      "      5       \u001b[36m32.9004\u001b[0m       31.1721  0.0122\n",
      "      6       \u001b[36m32.7160\u001b[0m       \u001b[32m30.0842\u001b[0m  0.0120\n",
      "      7       \u001b[36m32.5610\u001b[0m       30.3586  0.0117\n",
      "      8       \u001b[36m32.5074\u001b[0m       30.4059  0.0126\n",
      "      9       \u001b[36m32.3877\u001b[0m       \u001b[32m30.0170\u001b[0m  0.0118\n",
      "     10       \u001b[36m32.3851\u001b[0m       30.3781  0.0121\n",
      "     11       \u001b[36m32.3547\u001b[0m       30.0473  0.0123\n",
      "     12       \u001b[36m32.3186\u001b[0m       30.2169  0.0119\n",
      "     13       32.3197       30.1048  0.0118\n",
      "     14       \u001b[36m32.2842\u001b[0m       30.1696  0.0115\n",
      "     15       32.2858       30.0866  0.0123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     16       \u001b[36m32.2797\u001b[0m       30.2252  0.0122\n",
      "     17       \u001b[36m32.2727\u001b[0m       30.0605  0.0123\n",
      "     18       32.2848       30.3095  0.0118\n",
      "     19       \u001b[36m32.2703\u001b[0m       30.0290  0.0116\n",
      "     20       32.2918       30.3659  0.0117\n",
      "     21       \u001b[36m32.2697\u001b[0m       \u001b[32m29.9875\u001b[0m  0.0122\n",
      "     22       32.2815       30.3003  0.0118\n",
      "     23       \u001b[36m32.2480\u001b[0m       \u001b[32m29.9390\u001b[0m  0.0118\n",
      "     24       \u001b[36m32.2331\u001b[0m       30.2083  0.0118\n",
      "     25       \u001b[36m32.2202\u001b[0m       29.9940  0.0118\n",
      "     26       \u001b[36m32.1967\u001b[0m       30.1645  0.0115\n",
      "     27       32.2013       30.0520  0.0116\n",
      "     28       \u001b[36m32.1797\u001b[0m       30.1311  0.0154\n",
      "     29       32.1867       30.0842  0.0151\n",
      "     30       \u001b[36m32.1693\u001b[0m       30.1359  0.0122\n",
      "     31       32.1737       30.0997  0.0118\n",
      "     32       \u001b[36m32.1622\u001b[0m       30.1405  0.0117\n",
      "     33       \u001b[36m32.1608\u001b[0m       30.1365  0.0117\n",
      "     34       \u001b[36m32.1566\u001b[0m       30.1214  0.0119\n",
      "     35       \u001b[36m32.1511\u001b[0m       30.2087  0.0115\n",
      "     36       \u001b[36m32.1503\u001b[0m       30.1309  0.0115\n",
      "     37       32.1556       30.1518  0.0124\n",
      "     38       \u001b[36m32.1402\u001b[0m       30.3623  0.0116\n",
      "     39       32.1650       30.0839  0.0119\n",
      "     40       32.2082       30.2889  0.0116\n",
      "     41       32.1691       30.2727  0.0120\n",
      "     42       32.1874       30.0590  0.0115\n",
      "     43       32.1686       30.1421  0.0114\n",
      "     44       32.1608       30.0931  0.0129\n",
      "     45       \u001b[36m32.1356\u001b[0m       30.2126  0.0119\n",
      "     46       \u001b[36m32.1289\u001b[0m       30.1201  0.0117\n",
      "     47       \u001b[36m32.1232\u001b[0m       30.1706  0.0116\n",
      "     48       \u001b[36m32.1153\u001b[0m       30.1383  0.0110\n",
      "     49       \u001b[36m32.1137\u001b[0m       30.2015  0.0114\n",
      "     50       \u001b[36m32.1057\u001b[0m       30.2061  0.0119\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m29.5208\u001b[0m       \u001b[32m29.7542\u001b[0m  0.0126\n",
      "      2       \u001b[36m24.7752\u001b[0m       \u001b[32m26.4639\u001b[0m  0.0117\n",
      "      3       \u001b[36m24.1251\u001b[0m       27.6185  0.0116\n",
      "      4       \u001b[36m23.7316\u001b[0m       27.0997  0.0119\n",
      "      5       \u001b[36m23.5614\u001b[0m       26.9721  0.0128\n",
      "      6       \u001b[36m23.3438\u001b[0m       27.2312  0.0114\n",
      "      7       \u001b[36m23.2592\u001b[0m       26.8460  0.0114\n",
      "      8       23.2648       27.2441  0.0182\n",
      "      9       \u001b[36m23.1883\u001b[0m       26.7612  0.0146\n",
      "     10       \u001b[36m23.1786\u001b[0m       26.9934  0.0127\n",
      "     11       \u001b[36m23.1295\u001b[0m       26.8651  0.0125\n",
      "     12       23.1300       26.8366  0.0125\n",
      "     13       \u001b[36m23.1082\u001b[0m       26.8764  0.0139\n",
      "     14       \u001b[36m23.0980\u001b[0m       26.8236  0.0129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     15       \u001b[36m23.0856\u001b[0m       26.8760  0.0122\n",
      "     16       \u001b[36m23.0764\u001b[0m       26.7702  0.0129\n",
      "     17       \u001b[36m23.0697\u001b[0m       26.8624  0.0126\n",
      "     18       \u001b[36m23.0631\u001b[0m       26.7364  0.0120\n",
      "     19       23.0665       26.8215  0.0116\n",
      "     20       \u001b[36m23.0529\u001b[0m       26.7892  0.0127\n",
      "     21       23.0668       26.6909  0.0123\n",
      "     22       23.0823       26.8991  0.0121\n",
      "     23       23.0833       26.6576  0.0120\n",
      "     24       23.0921       26.8191  0.0119\n",
      "     25       \u001b[36m23.0347\u001b[0m       26.6805  0.0120\n",
      "     26       23.0383       26.8264  0.0121\n",
      "     27       \u001b[36m23.0216\u001b[0m       26.6924  0.0121\n",
      "     28       23.0259       26.7907  0.0120\n",
      "     29       \u001b[36m23.0090\u001b[0m       26.6724  0.0119\n",
      "     30       23.0133       26.7892  0.0120\n",
      "     31       \u001b[36m23.0009\u001b[0m       26.6839  0.0121\n",
      "     32       23.0024       26.7586  0.0117\n",
      "     33       \u001b[36m22.9944\u001b[0m       26.6892  0.0119\n",
      "     34       \u001b[36m22.9940\u001b[0m       26.7448  0.0119\n",
      "     35       \u001b[36m22.9879\u001b[0m       26.7062  0.0119\n",
      "     36       \u001b[36m22.9865\u001b[0m       26.7328  0.0115\n",
      "     37       \u001b[36m22.9820\u001b[0m       26.7209  0.0117\n",
      "     38       \u001b[36m22.9787\u001b[0m       26.7176  0.0129\n",
      "     39       \u001b[36m22.9767\u001b[0m       26.7266  0.0120\n",
      "     40       \u001b[36m22.9724\u001b[0m       26.7250  0.0122\n",
      "     41       \u001b[36m22.9700\u001b[0m       26.7252  0.0118\n",
      "     42       22.9720       26.7064  0.0115\n",
      "     43       \u001b[36m22.9683\u001b[0m       26.7915  0.0126\n",
      "     44       22.9701       26.6998  0.0121\n",
      "     45       23.0097       26.6858  0.0117\n",
      "     46       23.0820       26.9639  0.0119\n",
      "     47       23.0845       26.4904  0.0116\n",
      "     48       23.1545       26.9199  0.0113\n",
      "     49       23.0222       26.6041  0.0120\n",
      "     50       23.0198       26.8094  0.0133\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m35.5234\u001b[0m       \u001b[32m36.1504\u001b[0m  0.0125\n",
      "      2       \u001b[36m31.6831\u001b[0m       \u001b[32m26.5571\u001b[0m  0.0118\n",
      "      3       \u001b[36m30.5045\u001b[0m       27.0392  0.0120\n",
      "      4       \u001b[36m29.5011\u001b[0m       29.0860  0.0131\n",
      "      5       \u001b[36m29.0180\u001b[0m       26.7192  0.0120\n",
      "      6       \u001b[36m28.8655\u001b[0m       27.5834  0.0115\n",
      "      7       \u001b[36m28.8500\u001b[0m       27.7176  0.0116\n",
      "      8       \u001b[36m28.6233\u001b[0m       26.8857  0.0123\n",
      "      9       \u001b[36m28.5510\u001b[0m       27.7005  0.0119\n",
      "     10       28.5868       27.2653  0.0118\n",
      "     11       \u001b[36m28.4874\u001b[0m       27.1945  0.0116\n",
      "     12       28.4974       27.5198  0.0114\n",
      "     13       \u001b[36m28.4783\u001b[0m       27.1276  0.0123\n",
      "     14       \u001b[36m28.4493\u001b[0m       27.3942  0.0119\n",
      "     15       28.4574       27.2219  0.0117\n",
      "     16       \u001b[36m28.4277\u001b[0m       27.2594  0.0116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       28.4364       27.2899  0.0118\n",
      "     18       \u001b[36m28.4240\u001b[0m       27.2141  0.0121\n",
      "     19       28.4263       27.2771  0.0118\n",
      "     20       28.4322       27.2701  0.0115\n",
      "     21       28.4667       27.2493  0.0119\n",
      "     22       28.4920       27.2316  0.0118\n",
      "     23       28.5022       27.3176  0.0114\n",
      "     24       28.4998       27.2729  0.0116\n",
      "     25       28.5062       27.1363  0.0117\n",
      "     26       28.4272       27.2066  0.0117\n",
      "     27       \u001b[36m28.4170\u001b[0m       27.1815  0.0112\n",
      "     28       \u001b[36m28.4120\u001b[0m       27.1964  0.0114\n",
      "     29       \u001b[36m28.4119\u001b[0m       27.1271  0.0121\n",
      "     30       \u001b[36m28.3997\u001b[0m       27.2091  0.0119\n",
      "     31       \u001b[36m28.3968\u001b[0m       27.1551  0.0117\n",
      "     32       \u001b[36m28.3961\u001b[0m       27.1486  0.0113\n",
      "     33       \u001b[36m28.3914\u001b[0m       27.1681  0.0114\n",
      "     34       \u001b[36m28.3906\u001b[0m       27.1603  0.0120\n",
      "     35       \u001b[36m28.3875\u001b[0m       27.1560  0.0136\n",
      "     36       28.3882       27.1490  0.0183\n",
      "     37       \u001b[36m28.3838\u001b[0m       27.1676  0.0141\n",
      "     38       28.3847       27.1420  0.0140\n",
      "     39       \u001b[36m28.3825\u001b[0m       27.1626  0.0129\n",
      "     40       \u001b[36m28.3814\u001b[0m       27.1516  0.0126\n",
      "     41       \u001b[36m28.3807\u001b[0m       27.1564  0.0159\n",
      "     42       \u001b[36m28.3786\u001b[0m       27.1504  0.0144\n",
      "     43       \u001b[36m28.3779\u001b[0m       27.1565  0.0123\n",
      "     44       \u001b[36m28.3771\u001b[0m       27.1514  0.0133\n",
      "     45       \u001b[36m28.3755\u001b[0m       27.1562  0.0135\n",
      "     46       \u001b[36m28.3742\u001b[0m       27.1634  0.0127\n",
      "     47       28.3747       27.1500  0.0119\n",
      "     48       \u001b[36m28.3724\u001b[0m       27.1656  0.0128\n",
      "     49       \u001b[36m28.3715\u001b[0m       27.1679  0.0131\n",
      "     50       28.3732       27.1436  0.0124\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m42.6713\u001b[0m       \u001b[32m41.3563\u001b[0m  0.0111\n",
      "      2       \u001b[36m38.7609\u001b[0m       \u001b[32m36.8095\u001b[0m  0.0134\n",
      "      3       \u001b[36m35.6857\u001b[0m       \u001b[32m33.3864\u001b[0m  0.0118\n",
      "      4       \u001b[36m33.7986\u001b[0m       \u001b[32m31.3713\u001b[0m  0.0120\n",
      "      5       \u001b[36m33.0182\u001b[0m       \u001b[32m30.6102\u001b[0m  0.0114\n",
      "      6       \u001b[36m32.7871\u001b[0m       \u001b[32m30.3633\u001b[0m  0.0116\n",
      "      7       \u001b[36m32.6913\u001b[0m       \u001b[32m30.2625\u001b[0m  0.0123\n",
      "      8       \u001b[36m32.6302\u001b[0m       \u001b[32m30.2059\u001b[0m  0.0125\n",
      "      9       \u001b[36m32.5863\u001b[0m       \u001b[32m30.1665\u001b[0m  0.0132\n",
      "     10       \u001b[36m32.5514\u001b[0m       \u001b[32m30.1355\u001b[0m  0.0125\n",
      "     11       \u001b[36m32.5241\u001b[0m       \u001b[32m30.1102\u001b[0m  0.0146\n",
      "     12       \u001b[36m32.5005\u001b[0m       \u001b[32m30.0888\u001b[0m  0.0120\n",
      "     13       \u001b[36m32.4812\u001b[0m       \u001b[32m30.0700\u001b[0m  0.0112\n",
      "     14       \u001b[36m32.4643\u001b[0m       \u001b[32m30.0528\u001b[0m  0.0112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     15       \u001b[36m32.4494\u001b[0m       \u001b[32m30.0377\u001b[0m  0.0219\n",
      "     16       \u001b[36m32.4358\u001b[0m       \u001b[32m30.0248\u001b[0m  0.0147\n",
      "     17       \u001b[36m32.4240\u001b[0m       \u001b[32m30.0128\u001b[0m  0.0113\n",
      "     18       \u001b[36m32.4131\u001b[0m       \u001b[32m30.0024\u001b[0m  0.0112\n",
      "     19       \u001b[36m32.4036\u001b[0m       \u001b[32m29.9920\u001b[0m  0.0112\n",
      "     20       \u001b[36m32.3946\u001b[0m       \u001b[32m29.9831\u001b[0m  0.0115\n",
      "     21       \u001b[36m32.3864\u001b[0m       \u001b[32m29.9746\u001b[0m  0.0115\n",
      "     22       \u001b[36m32.3790\u001b[0m       \u001b[32m29.9674\u001b[0m  0.0115\n",
      "     23       \u001b[36m32.3720\u001b[0m       \u001b[32m29.9599\u001b[0m  0.0111\n",
      "     24       \u001b[36m32.3656\u001b[0m       \u001b[32m29.9534\u001b[0m  0.0110\n",
      "     25       \u001b[36m32.3594\u001b[0m       \u001b[32m29.9467\u001b[0m  0.0113\n",
      "     26       \u001b[36m32.3539\u001b[0m       \u001b[32m29.9410\u001b[0m  0.0112\n",
      "     27       \u001b[36m32.3486\u001b[0m       \u001b[32m29.9355\u001b[0m  0.0110\n",
      "     28       \u001b[36m32.3435\u001b[0m       \u001b[32m29.9300\u001b[0m  0.0108\n",
      "     29       \u001b[36m32.3387\u001b[0m       \u001b[32m29.9261\u001b[0m  0.0110\n",
      "     30       \u001b[36m32.3344\u001b[0m       \u001b[32m29.9219\u001b[0m  0.0114\n",
      "     31       \u001b[36m32.3302\u001b[0m       \u001b[32m29.9181\u001b[0m  0.0114\n",
      "     32       \u001b[36m32.3262\u001b[0m       \u001b[32m29.9139\u001b[0m  0.0111\n",
      "     33       \u001b[36m32.3224\u001b[0m       \u001b[32m29.9106\u001b[0m  0.0109\n",
      "     34       \u001b[36m32.3187\u001b[0m       \u001b[32m29.9076\u001b[0m  0.0107\n",
      "     35       \u001b[36m32.3153\u001b[0m       \u001b[32m29.9041\u001b[0m  0.0112\n",
      "     36       \u001b[36m32.3121\u001b[0m       \u001b[32m29.9012\u001b[0m  0.0115\n",
      "     37       \u001b[36m32.3089\u001b[0m       \u001b[32m29.8984\u001b[0m  0.0111\n",
      "     38       \u001b[36m32.3059\u001b[0m       \u001b[32m29.8960\u001b[0m  0.0109\n",
      "     39       \u001b[36m32.3031\u001b[0m       \u001b[32m29.8935\u001b[0m  0.0109\n",
      "     40       \u001b[36m32.3004\u001b[0m       \u001b[32m29.8911\u001b[0m  0.0112\n",
      "     41       \u001b[36m32.2978\u001b[0m       \u001b[32m29.8889\u001b[0m  0.0114\n",
      "     42       \u001b[36m32.2951\u001b[0m       \u001b[32m29.8869\u001b[0m  0.0109\n",
      "     43       \u001b[36m32.2928\u001b[0m       \u001b[32m29.8849\u001b[0m  0.0112\n",
      "     44       \u001b[36m32.2904\u001b[0m       \u001b[32m29.8830\u001b[0m  0.0110\n",
      "     45       \u001b[36m32.2881\u001b[0m       \u001b[32m29.8812\u001b[0m  0.0114\n",
      "     46       \u001b[36m32.2859\u001b[0m       \u001b[32m29.8795\u001b[0m  0.0112\n",
      "     47       \u001b[36m32.2839\u001b[0m       \u001b[32m29.8778\u001b[0m  0.0112\n",
      "     48       \u001b[36m32.2818\u001b[0m       \u001b[32m29.8760\u001b[0m  0.0107\n",
      "     49       \u001b[36m32.2799\u001b[0m       \u001b[32m29.8743\u001b[0m  0.0109\n",
      "     50       \u001b[36m32.2780\u001b[0m       \u001b[32m29.8725\u001b[0m  0.0112\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m32.4161\u001b[0m       \u001b[32m29.7829\u001b[0m  0.0109\n",
      "      2       \u001b[36m28.9601\u001b[0m       \u001b[32m27.5949\u001b[0m  0.0111\n",
      "      3       \u001b[36m26.1210\u001b[0m       \u001b[32m26.4485\u001b[0m  0.0109\n",
      "      4       \u001b[36m24.4736\u001b[0m       \u001b[32m26.2502\u001b[0m  0.0113\n",
      "      5       \u001b[36m23.7918\u001b[0m       26.3395  0.0110\n",
      "      6       \u001b[36m23.5572\u001b[0m       26.3998  0.0109\n",
      "      7       \u001b[36m23.4619\u001b[0m       26.4180  0.0111\n",
      "      8       \u001b[36m23.4084\u001b[0m       26.4184  0.0113\n",
      "      9       \u001b[36m23.3706\u001b[0m       26.4135  0.0113\n",
      "     10       \u001b[36m23.3408\u001b[0m       26.4074  0.0181\n",
      "     11       \u001b[36m23.3163\u001b[0m       26.4021  0.0169\n",
      "     12       \u001b[36m23.2957\u001b[0m       26.3977  0.0165\n",
      "     13       \u001b[36m23.2781\u001b[0m       26.3953  0.0184\n",
      "     14       \u001b[36m23.2628\u001b[0m       26.3931  0.0116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     15       \u001b[36m23.2493\u001b[0m       26.3919  0.0116\n",
      "     16       \u001b[36m23.2374\u001b[0m       26.3910  0.0125\n",
      "     17       \u001b[36m23.2267\u001b[0m       26.3906  0.0121\n",
      "     18       \u001b[36m23.2171\u001b[0m       26.3910  0.0136\n",
      "     19       \u001b[36m23.2083\u001b[0m       26.3915  0.0111\n",
      "     20       \u001b[36m23.2003\u001b[0m       26.3923  0.0123\n",
      "     21       \u001b[36m23.1929\u001b[0m       26.3934  0.0110\n",
      "     22       \u001b[36m23.1861\u001b[0m       26.3942  0.0112\n",
      "     23       \u001b[36m23.1799\u001b[0m       26.3952  0.0111\n",
      "     24       \u001b[36m23.1741\u001b[0m       26.3964  0.0110\n",
      "     25       \u001b[36m23.1686\u001b[0m       26.3975  0.0113\n",
      "     26       \u001b[36m23.1634\u001b[0m       26.3984  0.0113\n",
      "     27       \u001b[36m23.1587\u001b[0m       26.3998  0.0112\n",
      "     28       \u001b[36m23.1542\u001b[0m       26.4009  0.0111\n",
      "     29       \u001b[36m23.1500\u001b[0m       26.4024  0.0111\n",
      "     30       \u001b[36m23.1461\u001b[0m       26.4035  0.0111\n",
      "     31       \u001b[36m23.1424\u001b[0m       26.4048  0.0113\n",
      "     32       \u001b[36m23.1389\u001b[0m       26.4057  0.0109\n",
      "     33       \u001b[36m23.1357\u001b[0m       26.4068  0.0109\n",
      "     34       \u001b[36m23.1326\u001b[0m       26.4075  0.0108\n",
      "     35       \u001b[36m23.1296\u001b[0m       26.4087  0.0111\n",
      "     36       \u001b[36m23.1269\u001b[0m       26.4095  0.0111\n",
      "     37       \u001b[36m23.1242\u001b[0m       26.4103  0.0109\n",
      "     38       \u001b[36m23.1217\u001b[0m       26.4106  0.0107\n",
      "     39       \u001b[36m23.1194\u001b[0m       26.4112  0.0116\n",
      "     40       \u001b[36m23.1171\u001b[0m       26.4115  0.0113\n",
      "     41       \u001b[36m23.1150\u001b[0m       26.4119  0.0108\n",
      "     42       \u001b[36m23.1129\u001b[0m       26.4122  0.0108\n",
      "     43       \u001b[36m23.1110\u001b[0m       26.4123  0.0106\n",
      "     44       \u001b[36m23.1090\u001b[0m       26.4124  0.0107\n",
      "     45       \u001b[36m23.1072\u001b[0m       26.4123  0.0109\n",
      "     46       \u001b[36m23.1055\u001b[0m       26.4123  0.0115\n",
      "     47       \u001b[36m23.1039\u001b[0m       26.4124  0.0110\n",
      "     48       \u001b[36m23.1023\u001b[0m       26.4123  0.0107\n",
      "     49       \u001b[36m23.1008\u001b[0m       26.4121  0.0108\n",
      "     50       \u001b[36m23.0994\u001b[0m       26.4117  0.0109\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m38.4862\u001b[0m       \u001b[32m28.8574\u001b[0m  0.0108\n",
      "      2       \u001b[36m33.9061\u001b[0m       \u001b[32m26.6210\u001b[0m  0.0109\n",
      "      3       \u001b[36m30.5211\u001b[0m       \u001b[32m26.5421\u001b[0m  0.0106\n",
      "      4       \u001b[36m29.2431\u001b[0m       27.1775  0.0108\n",
      "      5       \u001b[36m28.9903\u001b[0m       27.3996  0.0104\n",
      "      6       \u001b[36m28.8929\u001b[0m       27.4372  0.0107\n",
      "      7       \u001b[36m28.8202\u001b[0m       27.4349  0.0109\n",
      "      8       \u001b[36m28.7632\u001b[0m       27.4275  0.0123\n",
      "      9       \u001b[36m28.7180\u001b[0m       27.4210  0.0111\n",
      "     10       \u001b[36m28.6816\u001b[0m       27.4146  0.0109\n",
      "     11       \u001b[36m28.6513\u001b[0m       27.4094  0.0107\n",
      "     12       \u001b[36m28.6261\u001b[0m       27.4043  0.0108\n",
      "     13       \u001b[36m28.6050\u001b[0m       27.4004  0.0111\n",
      "     14       \u001b[36m28.5873\u001b[0m       27.3975  0.0108\n",
      "     15       \u001b[36m28.5723\u001b[0m       27.3936  0.0104\n",
      "     16       \u001b[36m28.5591\u001b[0m       27.3901  0.0110\n",
      "     17       \u001b[36m28.5478\u001b[0m       27.3873  0.0110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     18       \u001b[36m28.5378\u001b[0m       27.3846  0.0113\n",
      "     19       \u001b[36m28.5290\u001b[0m       27.3819  0.0111\n",
      "     20       \u001b[36m28.5211\u001b[0m       27.3793  0.0108\n",
      "     21       \u001b[36m28.5141\u001b[0m       27.3775  0.0108\n",
      "     22       \u001b[36m28.5079\u001b[0m       27.3751  0.0108\n",
      "     23       \u001b[36m28.5023\u001b[0m       27.3727  0.0109\n",
      "     24       \u001b[36m28.4971\u001b[0m       27.3705  0.0111\n",
      "     25       \u001b[36m28.4923\u001b[0m       27.3683  0.0109\n",
      "     26       \u001b[36m28.4879\u001b[0m       27.3664  0.0108\n",
      "     27       \u001b[36m28.4839\u001b[0m       27.3641  0.0108\n",
      "     28       \u001b[36m28.4800\u001b[0m       27.3623  0.0108\n",
      "     29       \u001b[36m28.4765\u001b[0m       27.3596  0.0111\n",
      "     30       \u001b[36m28.4732\u001b[0m       27.3574  0.0108\n",
      "     31       \u001b[36m28.4700\u001b[0m       27.3550  0.0106\n",
      "     32       \u001b[36m28.4671\u001b[0m       27.3534  0.0107\n",
      "     33       \u001b[36m28.4643\u001b[0m       27.3517  0.0111\n",
      "     34       \u001b[36m28.4618\u001b[0m       27.3499  0.0110\n",
      "     35       \u001b[36m28.4594\u001b[0m       27.3483  0.0109\n",
      "     36       \u001b[36m28.4571\u001b[0m       27.3467  0.0107\n",
      "     37       \u001b[36m28.4550\u001b[0m       27.3450  0.0107\n",
      "     38       \u001b[36m28.4529\u001b[0m       27.3439  0.0110\n",
      "     39       \u001b[36m28.4509\u001b[0m       27.3428  0.0113\n",
      "     40       \u001b[36m28.4491\u001b[0m       27.3417  0.0111\n",
      "     41       \u001b[36m28.4473\u001b[0m       27.3401  0.0107\n",
      "     42       \u001b[36m28.4455\u001b[0m       27.3392  0.0107\n",
      "     43       \u001b[36m28.4439\u001b[0m       27.3379  0.0107\n",
      "     44       \u001b[36m28.4424\u001b[0m       27.3371  0.0116\n",
      "     45       \u001b[36m28.4409\u001b[0m       27.3360  0.0110\n",
      "     46       \u001b[36m28.4394\u001b[0m       27.3352  0.0119\n",
      "     47       \u001b[36m28.4381\u001b[0m       27.3336  0.0213\n",
      "     48       \u001b[36m28.4367\u001b[0m       27.3327  0.0118\n",
      "     49       \u001b[36m28.4354\u001b[0m       27.3318  0.0116\n",
      "     50       \u001b[36m28.4342\u001b[0m       27.3306  0.0117\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m42.4870\u001b[0m       \u001b[32m41.4468\u001b[0m  0.0144\n",
      "      2       \u001b[36m38.6131\u001b[0m       \u001b[32m34.8860\u001b[0m  0.0126\n",
      "      3       \u001b[36m34.6191\u001b[0m       \u001b[32m31.4748\u001b[0m  0.0126\n",
      "      4       \u001b[36m34.3338\u001b[0m       \u001b[32m31.1970\u001b[0m  0.0120\n",
      "      5       \u001b[36m33.3720\u001b[0m       31.8838  0.0117\n",
      "      6       \u001b[36m33.2264\u001b[0m       \u001b[32m31.0196\u001b[0m  0.0117\n",
      "      7       \u001b[36m32.8573\u001b[0m       \u001b[32m30.3367\u001b[0m  0.0123\n",
      "      8       \u001b[36m32.7505\u001b[0m       \u001b[32m30.3177\u001b[0m  0.0121\n",
      "      9       \u001b[36m32.6186\u001b[0m       30.5636  0.0118\n",
      "     10       \u001b[36m32.5408\u001b[0m       30.4246  0.0119\n",
      "     11       \u001b[36m32.4376\u001b[0m       \u001b[32m30.2203\u001b[0m  0.0117\n",
      "     12       \u001b[36m32.3975\u001b[0m       30.3558  0.0121\n",
      "     13       \u001b[36m32.3611\u001b[0m       30.3647  0.0121\n",
      "     14       \u001b[36m32.3322\u001b[0m       30.3392  0.0117\n",
      "     15       \u001b[36m32.2962\u001b[0m       \u001b[32m30.1344\u001b[0m  0.0110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     16       \u001b[36m32.2932\u001b[0m       30.3797  0.0113\n",
      "     17       \u001b[36m32.2825\u001b[0m       30.1844  0.0122\n",
      "     18       32.2872       30.5338  0.0119\n",
      "     19       32.3035       30.1797  0.0114\n",
      "     20       32.3741       30.3704  0.0116\n",
      "     21       32.2977       \u001b[32m30.0412\u001b[0m  0.0113\n",
      "     22       \u001b[36m32.2589\u001b[0m       \u001b[32m30.0052\u001b[0m  0.0115\n",
      "     23       \u001b[36m32.2317\u001b[0m       30.1835  0.0126\n",
      "     24       \u001b[36m32.2200\u001b[0m       30.0085  0.0119\n",
      "     25       \u001b[36m32.2106\u001b[0m       30.0841  0.0122\n",
      "     26       \u001b[36m32.2035\u001b[0m       30.0847  0.0118\n",
      "     27       \u001b[36m32.1894\u001b[0m       30.0518  0.0119\n",
      "     28       \u001b[36m32.1870\u001b[0m       30.0897  0.0129\n",
      "     29       \u001b[36m32.1826\u001b[0m       30.0520  0.0124\n",
      "     30       \u001b[36m32.1766\u001b[0m       30.0718  0.0124\n",
      "     31       \u001b[36m32.1738\u001b[0m       30.0664  0.0118\n",
      "     32       \u001b[36m32.1683\u001b[0m       30.0598  0.0117\n",
      "     33       \u001b[36m32.1658\u001b[0m       30.0681  0.0119\n",
      "     34       \u001b[36m32.1619\u001b[0m       30.0588  0.0120\n",
      "     35       \u001b[36m32.1588\u001b[0m       30.0641  0.0119\n",
      "     36       \u001b[36m32.1557\u001b[0m       30.0585  0.0118\n",
      "     37       \u001b[36m32.1523\u001b[0m       30.0630  0.0120\n",
      "     38       \u001b[36m32.1495\u001b[0m       30.0593  0.0121\n",
      "     39       \u001b[36m32.1461\u001b[0m       30.0667  0.0118\n",
      "     40       \u001b[36m32.1434\u001b[0m       30.0619  0.0115\n",
      "     41       \u001b[36m32.1401\u001b[0m       30.0674  0.0118\n",
      "     42       \u001b[36m32.1376\u001b[0m       30.0588  0.0112\n",
      "     43       \u001b[36m32.1350\u001b[0m       30.0622  0.0121\n",
      "     44       \u001b[36m32.1325\u001b[0m       30.0547  0.0118\n",
      "     45       \u001b[36m32.1299\u001b[0m       30.0644  0.0115\n",
      "     46       \u001b[36m32.1277\u001b[0m       30.0527  0.0112\n",
      "     47       \u001b[36m32.1255\u001b[0m       30.0600  0.0112\n",
      "     48       \u001b[36m32.1232\u001b[0m       30.0513  0.0121\n",
      "     49       \u001b[36m32.1215\u001b[0m       30.0636  0.0116\n",
      "     50       \u001b[36m32.1192\u001b[0m       30.0523  0.0118\n",
      "     51       \u001b[36m32.1178\u001b[0m       30.0631  0.0112\n",
      "     52       \u001b[36m32.1154\u001b[0m       30.0483  0.0114\n",
      "     53       \u001b[36m32.1142\u001b[0m       30.0606  0.0123\n",
      "     54       \u001b[36m32.1117\u001b[0m       30.0473  0.0118\n",
      "     55       \u001b[36m32.1110\u001b[0m       30.0591  0.0116\n",
      "     56       \u001b[36m32.1084\u001b[0m       30.0501  0.0114\n",
      "     57       \u001b[36m32.1076\u001b[0m       30.0556  0.0114\n",
      "     58       \u001b[36m32.1053\u001b[0m       30.0530  0.0120\n",
      "     59       \u001b[36m32.1044\u001b[0m       30.0584  0.0117\n",
      "     60       \u001b[36m32.1024\u001b[0m       30.0575  0.0118\n",
      "     61       \u001b[36m32.1015\u001b[0m       30.0619  0.0112\n",
      "     62       \u001b[36m32.0999\u001b[0m       30.0660  0.0112\n",
      "     63       \u001b[36m32.0984\u001b[0m       30.0653  0.0119\n",
      "     64       \u001b[36m32.0973\u001b[0m       30.0707  0.0119\n",
      "     65       \u001b[36m32.0957\u001b[0m       30.0711  0.0116\n",
      "     66       \u001b[36m32.0949\u001b[0m       30.0722  0.0114\n",
      "     67       \u001b[36m32.0935\u001b[0m       30.0776  0.0116\n",
      "     68       \u001b[36m32.0920\u001b[0m       30.0764  0.0122\n",
      "     69       \u001b[36m32.0916\u001b[0m       30.0821  0.0116\n",
      "     70       \u001b[36m32.0898\u001b[0m       30.0840  0.0114\n",
      "     71       \u001b[36m32.0891\u001b[0m       30.0861  0.0125\n",
      "     72       \u001b[36m32.0881\u001b[0m       30.0901  0.0115\n",
      "     73       \u001b[36m32.0866\u001b[0m       30.0894  0.0121\n",
      "     74       \u001b[36m32.0861\u001b[0m       30.0933  0.0117\n",
      "     75       \u001b[36m32.0845\u001b[0m       30.0958  0.0163\n",
      "     76       \u001b[36m32.0839\u001b[0m       30.0985  0.0181\n",
      "     77       \u001b[36m32.0832\u001b[0m       30.1058  0.0154\n",
      "     78       \u001b[36m32.0814\u001b[0m       30.1032  0.0133\n",
      "     79       \u001b[36m32.0814\u001b[0m       30.1064  0.0147\n",
      "     80       \u001b[36m32.0803\u001b[0m       30.1152  0.0152\n",
      "     81       \u001b[36m32.0786\u001b[0m       30.1121  0.0167\n",
      "     82       32.0789       30.1179  0.0133\n",
      "     83       \u001b[36m32.0772\u001b[0m       30.1220  0.0124\n",
      "     84       \u001b[36m32.0763\u001b[0m       30.1195  0.0122\n",
      "     85       \u001b[36m32.0761\u001b[0m       30.1298  0.0121\n",
      "     86       \u001b[36m32.0744\u001b[0m       30.1290  0.0122\n",
      "     87       \u001b[36m32.0742\u001b[0m       30.1290  0.0119\n",
      "     88       \u001b[36m32.0734\u001b[0m       30.1346  0.0122\n",
      "     89       \u001b[36m32.0720\u001b[0m       30.1392  0.0123\n",
      "     90       32.0723       30.1418  0.0121\n",
      "     91       \u001b[36m32.0714\u001b[0m       30.1427  0.0123\n",
      "     92       \u001b[36m32.0698\u001b[0m       30.1437  0.0118\n",
      "     93       32.0703       30.1452  0.0120\n",
      "     94       \u001b[36m32.0689\u001b[0m       30.1512  0.0119\n",
      "     95       \u001b[36m32.0682\u001b[0m       30.1465  0.0116\n",
      "     96       32.0689       30.1466  0.0121\n",
      "     97       \u001b[36m32.0664\u001b[0m       30.1532  0.0115\n",
      "     98       32.0668       30.1518  0.0123\n",
      "     99       32.0675       30.1549  0.0128\n",
      "    100       \u001b[36m32.0643\u001b[0m       30.1562  0.0124\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m34.3609\u001b[0m       \u001b[32m30.1346\u001b[0m  0.0119\n",
      "      2       \u001b[36m28.7479\u001b[0m       \u001b[32m27.6624\u001b[0m  0.0127\n",
      "      3       \u001b[36m25.8948\u001b[0m       29.0475  0.0125\n",
      "      4       \u001b[36m24.8529\u001b[0m       \u001b[32m26.6030\u001b[0m  0.0127\n",
      "      5       \u001b[36m24.5795\u001b[0m       \u001b[32m26.4825\u001b[0m  0.0122\n",
      "      6       \u001b[36m24.1344\u001b[0m       27.0285  0.0125\n",
      "      7       \u001b[36m23.8108\u001b[0m       27.5541  0.0123\n",
      "      8       \u001b[36m23.6407\u001b[0m       26.9860  0.0119\n",
      "      9       \u001b[36m23.5640\u001b[0m       26.8648  0.0126\n",
      "     10       \u001b[36m23.4452\u001b[0m       27.0731  0.0121\n",
      "     11       \u001b[36m23.3624\u001b[0m       27.0357  0.0123\n",
      "     12       \u001b[36m23.2986\u001b[0m       26.8640  0.0122\n",
      "     13       \u001b[36m23.2732\u001b[0m       26.8957  0.0122\n",
      "     14       \u001b[36m23.2344\u001b[0m       26.9480  0.0126\n",
      "     15       \u001b[36m23.2052\u001b[0m       26.8885  0.0126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     16       \u001b[36m23.1884\u001b[0m       26.8748  0.0152\n",
      "     17       \u001b[36m23.1703\u001b[0m       26.8862  0.0123\n",
      "     18       \u001b[36m23.1565\u001b[0m       26.8847  0.0124\n",
      "     19       \u001b[36m23.1456\u001b[0m       26.8814  0.0132\n",
      "     20       \u001b[36m23.1342\u001b[0m       26.8420  0.0124\n",
      "     21       \u001b[36m23.1278\u001b[0m       26.8275  0.0120\n",
      "     22       \u001b[36m23.1198\u001b[0m       26.8318  0.0120\n",
      "     23       \u001b[36m23.1122\u001b[0m       26.8172  0.0127\n",
      "     24       \u001b[36m23.1070\u001b[0m       26.8095  0.0156\n",
      "     25       \u001b[36m23.1010\u001b[0m       26.8018  0.0128\n",
      "     26       \u001b[36m23.0962\u001b[0m       26.8024  0.0114\n",
      "     27       \u001b[36m23.0914\u001b[0m       26.7977  0.0116\n",
      "     28       \u001b[36m23.0874\u001b[0m       26.7962  0.0118\n",
      "     29       \u001b[36m23.0834\u001b[0m       26.7946  0.0117\n",
      "     30       \u001b[36m23.0795\u001b[0m       26.7968  0.0116\n",
      "     31       \u001b[36m23.0761\u001b[0m       26.7845  0.0114\n",
      "     32       \u001b[36m23.0733\u001b[0m       26.7807  0.0115\n",
      "     33       \u001b[36m23.0697\u001b[0m       26.7724  0.0118\n",
      "     34       \u001b[36m23.0675\u001b[0m       26.7724  0.0119\n",
      "     35       \u001b[36m23.0637\u001b[0m       26.7612  0.0116\n",
      "     36       \u001b[36m23.0630\u001b[0m       26.7711  0.0115\n",
      "     37       \u001b[36m23.0582\u001b[0m       26.7533  0.0118\n",
      "     38       23.0593       26.7704  0.0120\n",
      "     39       \u001b[36m23.0528\u001b[0m       26.7462  0.0122\n",
      "     40       23.0549       26.7691  0.0121\n",
      "     41       \u001b[36m23.0483\u001b[0m       26.7372  0.0122\n",
      "     42       23.0508       26.7566  0.0123\n",
      "     43       \u001b[36m23.0442\u001b[0m       26.7293  0.0123\n",
      "     44       23.0467       26.7491  0.0120\n",
      "     45       \u001b[36m23.0403\u001b[0m       26.7298  0.0122\n",
      "     46       23.0423       26.7426  0.0123\n",
      "     47       \u001b[36m23.0370\u001b[0m       26.7231  0.0119\n",
      "     48       23.0383       26.7368  0.0118\n",
      "     49       \u001b[36m23.0338\u001b[0m       26.7275  0.0118\n",
      "     50       23.0347       26.7414  0.0146\n",
      "     51       \u001b[36m23.0305\u001b[0m       26.7322  0.0152\n",
      "     52       23.0314       26.7400  0.0126\n",
      "     53       \u001b[36m23.0275\u001b[0m       26.7310  0.0124\n",
      "     54       23.0283       26.7456  0.0126\n",
      "     55       \u001b[36m23.0246\u001b[0m       26.7362  0.0163\n",
      "     56       23.0255       26.7531  0.0124\n",
      "     57       \u001b[36m23.0219\u001b[0m       26.7357  0.0125\n",
      "     58       23.0232       26.7510  0.0119\n",
      "     59       \u001b[36m23.0194\u001b[0m       26.7459  0.0121\n",
      "     60       23.0211       26.7568  0.0120\n",
      "     61       \u001b[36m23.0168\u001b[0m       26.7388  0.0120\n",
      "     62       23.0186       26.7585  0.0120\n",
      "     63       \u001b[36m23.0144\u001b[0m       26.7463  0.0120\n",
      "     64       23.0163       26.7654  0.0119\n",
      "     65       \u001b[36m23.0121\u001b[0m       26.7453  0.0120\n",
      "     66       23.0154       26.7792  0.0120\n",
      "     67       \u001b[36m23.0097\u001b[0m       26.7534  0.0118\n",
      "     68       23.0138       26.7784  0.0121\n",
      "     69       \u001b[36m23.0075\u001b[0m       26.7547  0.0116\n",
      "     70       23.0128       26.7911  0.0120\n",
      "     71       \u001b[36m23.0044\u001b[0m       26.7539  0.0122\n",
      "     72       23.0135       26.8055  0.0125\n",
      "     73       \u001b[36m23.0038\u001b[0m       26.7391  0.0127\n",
      "     74       23.0178       26.8242  0.0122\n",
      "     75       23.0041       26.7408  0.0124\n",
      "     76       23.0183       26.8386  0.0124\n",
      "     77       \u001b[36m23.0007\u001b[0m       26.7546  0.0121\n",
      "     78       23.0127       26.8161  0.0118\n",
      "     79       \u001b[36m22.9980\u001b[0m       26.7669  0.0124\n",
      "     80       23.0078       26.8176  0.0119\n",
      "     81       \u001b[36m22.9954\u001b[0m       26.7675  0.0122\n",
      "     82       23.0031       26.8139  0.0119\n",
      "     83       \u001b[36m22.9926\u001b[0m       26.7778  0.0119\n",
      "     84       22.9989       26.8115  0.0118\n",
      "     85       \u001b[36m22.9900\u001b[0m       26.7926  0.0119\n",
      "     86       22.9959       26.8298  0.0117\n",
      "     87       \u001b[36m22.9871\u001b[0m       26.7950  0.0116\n",
      "     88       22.9916       26.8239  0.0117\n",
      "     89       \u001b[36m22.9866\u001b[0m       26.8297  0.0118\n",
      "     90       22.9879       26.8315  0.0114\n",
      "     91       \u001b[36m22.9836\u001b[0m       26.8239  0.0119\n",
      "     92       22.9858       26.8586  0.0117\n",
      "     93       \u001b[36m22.9802\u001b[0m       26.8322  0.0118\n",
      "     94       22.9837       26.8544  0.0121\n",
      "     95       22.9806       26.8601  0.0120\n",
      "     96       22.9827       26.8336  0.0123\n",
      "     97       \u001b[36m22.9791\u001b[0m       26.8974  0.0126\n",
      "     98       22.9798       26.8508  0.0125\n",
      "     99       \u001b[36m22.9749\u001b[0m       26.8767  0.0131\n",
      "    100       22.9768       26.9087  0.0121\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m38.2946\u001b[0m       \u001b[32m28.6329\u001b[0m  0.0120\n",
      "      2       \u001b[36m32.9813\u001b[0m       \u001b[32m27.9551\u001b[0m  0.0118\n",
      "      3       \u001b[36m30.8745\u001b[0m       30.0910  0.0119\n",
      "      4       \u001b[36m30.0328\u001b[0m       \u001b[32m26.7297\u001b[0m  0.0118\n",
      "      5       \u001b[36m29.3589\u001b[0m       \u001b[32m26.5370\u001b[0m  0.0119\n",
      "      6       \u001b[36m29.0675\u001b[0m       27.5605  0.0119\n",
      "      7       29.0842       27.7978  0.0117\n",
      "      8       \u001b[36m28.8526\u001b[0m       27.1982  0.0115\n",
      "      9       \u001b[36m28.7232\u001b[0m       27.2624  0.0119\n",
      "     10       \u001b[36m28.6755\u001b[0m       27.4824  0.0119\n",
      "     11       \u001b[36m28.6480\u001b[0m       27.2017  0.0117\n",
      "     12       \u001b[36m28.5676\u001b[0m       27.1153  0.0115\n",
      "     13       \u001b[36m28.5370\u001b[0m       27.3180  0.0115\n",
      "     14       \u001b[36m28.5341\u001b[0m       27.2739  0.0120\n",
      "     15       \u001b[36m28.5036\u001b[0m       27.1126  0.0116\n",
      "     16       \u001b[36m28.4818\u001b[0m       27.1653  0.0116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m28.4803\u001b[0m       27.1761  0.0119\n",
      "     18       \u001b[36m28.4670\u001b[0m       27.1039  0.0115\n",
      "     19       \u001b[36m28.4537\u001b[0m       27.1280  0.0120\n",
      "     20       \u001b[36m28.4504\u001b[0m       27.1444  0.0117\n",
      "     21       \u001b[36m28.4433\u001b[0m       27.1005  0.0115\n",
      "     22       \u001b[36m28.4353\u001b[0m       27.1186  0.0117\n",
      "     23       \u001b[36m28.4315\u001b[0m       27.1343  0.0119\n",
      "     24       \u001b[36m28.4269\u001b[0m       27.1043  0.0121\n",
      "     25       \u001b[36m28.4232\u001b[0m       27.1157  0.0119\n",
      "     26       \u001b[36m28.4185\u001b[0m       27.1253  0.0123\n",
      "     27       \u001b[36m28.4158\u001b[0m       27.0931  0.0123\n",
      "     28       28.4215       27.1059  0.0164\n",
      "     29       28.4168       27.1330  0.0159\n",
      "     30       28.4209       27.0810  0.0135\n",
      "     31       28.4800       27.0650  0.0127\n",
      "     32       28.4726       27.2800  0.0148\n",
      "     33       28.5434       26.8945  0.0141\n",
      "     34       28.4235       27.2854  0.0129\n",
      "     35       28.4454       26.9895  0.0120\n",
      "     36       \u001b[36m28.3892\u001b[0m       27.0817  0.0118\n",
      "     37       28.4141       27.1739  0.0122\n",
      "     38       28.3940       26.9969  0.0119\n",
      "     39       28.3923       27.1729  0.0117\n",
      "     40       28.3967       27.0512  0.0117\n",
      "     41       \u001b[36m28.3882\u001b[0m       27.0926  0.0116\n",
      "     42       28.3896       27.1003  0.0121\n",
      "     43       28.3883       27.0930  0.0119\n",
      "     44       \u001b[36m28.3843\u001b[0m       27.0740  0.0117\n",
      "     45       28.3849       27.1202  0.0119\n",
      "     46       \u001b[36m28.3837\u001b[0m       27.0748  0.0118\n",
      "     47       \u001b[36m28.3826\u001b[0m       27.1040  0.0121\n",
      "     48       \u001b[36m28.3807\u001b[0m       27.0897  0.0117\n",
      "     49       28.3815       27.1096  0.0118\n",
      "     50       \u001b[36m28.3798\u001b[0m       27.0864  0.0118\n",
      "     51       \u001b[36m28.3788\u001b[0m       27.1055  0.0115\n",
      "     52       28.3799       27.0917  0.0120\n",
      "     53       \u001b[36m28.3763\u001b[0m       27.1026  0.0126\n",
      "     54       28.3796       27.1007  0.0119\n",
      "     55       \u001b[36m28.3752\u001b[0m       27.0933  0.0120\n",
      "     56       28.3771       27.1038  0.0128\n",
      "     57       \u001b[36m28.3750\u001b[0m       27.0917  0.0121\n",
      "     58       28.3754       27.1025  0.0127\n",
      "     59       \u001b[36m28.3745\u001b[0m       27.0986  0.0126\n",
      "     60       \u001b[36m28.3740\u001b[0m       27.0972  0.0125\n",
      "     61       \u001b[36m28.3732\u001b[0m       27.1054  0.0122\n",
      "     62       28.3740       27.0992  0.0118\n",
      "     63       \u001b[36m28.3719\u001b[0m       27.1036  0.0123\n",
      "     64       28.3734       27.1068  0.0123\n",
      "     65       \u001b[36m28.3710\u001b[0m       27.0944  0.0119\n",
      "     66       28.3713       27.1206  0.0118\n",
      "     67       28.3720       27.0779  0.0123\n",
      "     68       \u001b[36m28.3675\u001b[0m       27.1305  0.0119\n",
      "     69       28.3740       27.0898  0.0118\n",
      "     70       \u001b[36m28.3663\u001b[0m       27.1150  0.0115\n",
      "     71       28.3715       27.1088  0.0124\n",
      "     72       28.3674       27.1055  0.0133\n",
      "     73       28.3699       27.1244  0.0120\n",
      "     74       28.3674       27.0929  0.0114\n",
      "     75       28.3666       27.1432  0.0121\n",
      "     76       28.3681       27.1016  0.0122\n",
      "     77       28.3663       27.1340  0.0117\n",
      "     78       \u001b[36m28.3659\u001b[0m       27.1088  0.0118\n",
      "     79       \u001b[36m28.3652\u001b[0m       27.1464  0.0116\n",
      "     80       28.3676       27.1066  0.0116\n",
      "     81       \u001b[36m28.3625\u001b[0m       27.1378  0.0119\n",
      "     82       28.3665       27.1282  0.0120\n",
      "     83       28.3628       27.1318  0.0115\n",
      "     84       28.3657       27.1490  0.0117\n",
      "     85       28.3629       27.1211  0.0115\n",
      "     86       28.3632       27.1646  0.0114\n",
      "     87       28.3631       27.1248  0.0116\n",
      "     88       28.3634       27.1674  0.0117\n",
      "     89       \u001b[36m28.3613\u001b[0m       27.1357  0.0117\n",
      "     90       28.3632       27.1686  0.0113\n",
      "     91       \u001b[36m28.3609\u001b[0m       27.1330  0.0116\n",
      "     92       28.3616       27.1799  0.0118\n",
      "     93       28.3618       27.1413  0.0117\n",
      "     94       28.3622       27.1817  0.0113\n",
      "     95       \u001b[36m28.3603\u001b[0m       27.1391  0.0116\n",
      "     96       \u001b[36m28.3602\u001b[0m       27.1914  0.0113\n",
      "     97       28.3621       27.1433  0.0119\n",
      "     98       \u001b[36m28.3578\u001b[0m       27.1765  0.0119\n",
      "     99       28.3604       27.1829  0.0120\n",
      "    100       28.3609       27.1703  0.0119\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m41.0363\u001b[0m       \u001b[32m40.1411\u001b[0m  0.0116\n",
      "      2       \u001b[36m38.0516\u001b[0m       \u001b[32m36.3546\u001b[0m  0.0110\n",
      "      3       \u001b[36m35.3928\u001b[0m       \u001b[32m33.1763\u001b[0m  0.0107\n",
      "      4       \u001b[36m33.6044\u001b[0m       \u001b[32m31.2746\u001b[0m  0.0110\n",
      "      5       \u001b[36m32.8805\u001b[0m       \u001b[32m30.5149\u001b[0m  0.0108\n",
      "      6       \u001b[36m32.6977\u001b[0m       \u001b[32m30.2520\u001b[0m  0.0148\n",
      "      7       \u001b[36m32.6301\u001b[0m       \u001b[32m30.1436\u001b[0m  0.0149\n",
      "      8       \u001b[36m32.5844\u001b[0m       \u001b[32m30.0846\u001b[0m  0.0114\n",
      "      9       \u001b[36m32.5488\u001b[0m       \u001b[32m30.0461\u001b[0m  0.0120\n",
      "     10       \u001b[36m32.5191\u001b[0m       \u001b[32m30.0168\u001b[0m  0.0148\n",
      "     11       \u001b[36m32.4945\u001b[0m       \u001b[32m29.9934\u001b[0m  0.0147\n",
      "     12       \u001b[36m32.4732\u001b[0m       \u001b[32m29.9736\u001b[0m  0.0113\n",
      "     13       \u001b[36m32.4550\u001b[0m       \u001b[32m29.9562\u001b[0m  0.0120\n",
      "     14       \u001b[36m32.4393\u001b[0m       \u001b[32m29.9406\u001b[0m  0.0113\n",
      "     15       \u001b[36m32.4253\u001b[0m       \u001b[32m29.9271\u001b[0m  0.0109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     16       \u001b[36m32.4129\u001b[0m       \u001b[32m29.9147\u001b[0m  0.0112\n",
      "     17       \u001b[36m32.4018\u001b[0m       \u001b[32m29.9039\u001b[0m  0.0113\n",
      "     18       \u001b[36m32.3917\u001b[0m       \u001b[32m29.8944\u001b[0m  0.0114\n",
      "     19       \u001b[36m32.3827\u001b[0m       \u001b[32m29.8859\u001b[0m  0.0110\n",
      "     20       \u001b[36m32.3744\u001b[0m       \u001b[32m29.8783\u001b[0m  0.0109\n",
      "     21       \u001b[36m32.3670\u001b[0m       \u001b[32m29.8713\u001b[0m  0.0108\n",
      "     22       \u001b[36m32.3602\u001b[0m       \u001b[32m29.8651\u001b[0m  0.0113\n",
      "     23       \u001b[36m32.3539\u001b[0m       \u001b[32m29.8593\u001b[0m  0.0114\n",
      "     24       \u001b[36m32.3481\u001b[0m       \u001b[32m29.8539\u001b[0m  0.0112\n",
      "     25       \u001b[36m32.3428\u001b[0m       \u001b[32m29.8493\u001b[0m  0.0108\n",
      "     26       \u001b[36m32.3377\u001b[0m       \u001b[32m29.8445\u001b[0m  0.0109\n",
      "     27       \u001b[36m32.3330\u001b[0m       \u001b[32m29.8404\u001b[0m  0.0112\n",
      "     28       \u001b[36m32.3286\u001b[0m       \u001b[32m29.8363\u001b[0m  0.0114\n",
      "     29       \u001b[36m32.3243\u001b[0m       \u001b[32m29.8326\u001b[0m  0.0110\n",
      "     30       \u001b[36m32.3203\u001b[0m       \u001b[32m29.8289\u001b[0m  0.0107\n",
      "     31       \u001b[36m32.3167\u001b[0m       \u001b[32m29.8253\u001b[0m  0.0109\n",
      "     32       \u001b[36m32.3133\u001b[0m       \u001b[32m29.8227\u001b[0m  0.0113\n",
      "     33       \u001b[36m32.3102\u001b[0m       \u001b[32m29.8201\u001b[0m  0.0113\n",
      "     34       \u001b[36m32.3072\u001b[0m       \u001b[32m29.8179\u001b[0m  0.0109\n",
      "     35       \u001b[36m32.3043\u001b[0m       \u001b[32m29.8151\u001b[0m  0.0107\n",
      "     36       \u001b[36m32.3017\u001b[0m       \u001b[32m29.8135\u001b[0m  0.0111\n",
      "     37       \u001b[36m32.2991\u001b[0m       \u001b[32m29.8115\u001b[0m  0.0113\n",
      "     38       \u001b[36m32.2968\u001b[0m       \u001b[32m29.8101\u001b[0m  0.0117\n",
      "     39       \u001b[36m32.2945\u001b[0m       \u001b[32m29.8081\u001b[0m  0.0113\n",
      "     40       \u001b[36m32.2922\u001b[0m       \u001b[32m29.8062\u001b[0m  0.0115\n",
      "     41       \u001b[36m32.2902\u001b[0m       \u001b[32m29.8053\u001b[0m  0.0111\n",
      "     42       \u001b[36m32.2882\u001b[0m       \u001b[32m29.8037\u001b[0m  0.0119\n",
      "     43       \u001b[36m32.2863\u001b[0m       \u001b[32m29.8025\u001b[0m  0.0111\n",
      "     44       \u001b[36m32.2845\u001b[0m       \u001b[32m29.8012\u001b[0m  0.0109\n",
      "     45       \u001b[36m32.2829\u001b[0m       \u001b[32m29.8001\u001b[0m  0.0106\n",
      "     46       \u001b[36m32.2812\u001b[0m       \u001b[32m29.7988\u001b[0m  0.0108\n",
      "     47       \u001b[36m32.2796\u001b[0m       \u001b[32m29.7976\u001b[0m  0.0111\n",
      "     48       \u001b[36m32.2782\u001b[0m       \u001b[32m29.7970\u001b[0m  0.0112\n",
      "     49       \u001b[36m32.2767\u001b[0m       \u001b[32m29.7962\u001b[0m  0.0110\n",
      "     50       \u001b[36m32.2752\u001b[0m       \u001b[32m29.7950\u001b[0m  0.0107\n",
      "     51       \u001b[36m32.2740\u001b[0m       \u001b[32m29.7945\u001b[0m  0.0106\n",
      "     52       \u001b[36m32.2726\u001b[0m       \u001b[32m29.7939\u001b[0m  0.0114\n",
      "     53       \u001b[36m32.2713\u001b[0m       \u001b[32m29.7929\u001b[0m  0.0120\n",
      "     54       \u001b[36m32.2701\u001b[0m       \u001b[32m29.7928\u001b[0m  0.0113\n",
      "     55       \u001b[36m32.2687\u001b[0m       \u001b[32m29.7923\u001b[0m  0.0109\n",
      "     56       \u001b[36m32.2675\u001b[0m       \u001b[32m29.7916\u001b[0m  0.0109\n",
      "     57       \u001b[36m32.2664\u001b[0m       \u001b[32m29.7914\u001b[0m  0.0116\n",
      "     58       \u001b[36m32.2651\u001b[0m       \u001b[32m29.7907\u001b[0m  0.0115\n",
      "     59       \u001b[36m32.2641\u001b[0m       \u001b[32m29.7905\u001b[0m  0.0114\n",
      "     60       \u001b[36m32.2629\u001b[0m       \u001b[32m29.7900\u001b[0m  0.0108\n",
      "     61       \u001b[36m32.2616\u001b[0m       \u001b[32m29.7889\u001b[0m  0.0106\n",
      "     62       \u001b[36m32.2607\u001b[0m       29.7889  0.0111\n",
      "     63       \u001b[36m32.2596\u001b[0m       \u001b[32m29.7882\u001b[0m  0.0114\n",
      "     64       \u001b[36m32.2586\u001b[0m       \u001b[32m29.7877\u001b[0m  0.0113\n",
      "     65       \u001b[36m32.2577\u001b[0m       \u001b[32m29.7875\u001b[0m  0.0109\n",
      "     66       \u001b[36m32.2565\u001b[0m       \u001b[32m29.7868\u001b[0m  0.0107\n",
      "     67       \u001b[36m32.2557\u001b[0m       \u001b[32m29.7865\u001b[0m  0.0109\n",
      "     68       \u001b[36m32.2546\u001b[0m       \u001b[32m29.7859\u001b[0m  0.0110\n",
      "     69       \u001b[36m32.2538\u001b[0m       \u001b[32m29.7858\u001b[0m  0.0111\n",
      "     70       \u001b[36m32.2530\u001b[0m       29.7860  0.0113\n",
      "     71       \u001b[36m32.2520\u001b[0m       \u001b[32m29.7852\u001b[0m  0.0108\n",
      "     72       \u001b[36m32.2512\u001b[0m       \u001b[32m29.7851\u001b[0m  0.0111\n",
      "     73       \u001b[36m32.2504\u001b[0m       \u001b[32m29.7849\u001b[0m  0.0119\n",
      "     74       \u001b[36m32.2495\u001b[0m       29.7850  0.0115\n",
      "     75       \u001b[36m32.2487\u001b[0m       \u001b[32m29.7847\u001b[0m  0.0115\n",
      "     76       \u001b[36m32.2479\u001b[0m       29.7847  0.0116\n",
      "     77       \u001b[36m32.2470\u001b[0m       \u001b[32m29.7843\u001b[0m  0.0109\n",
      "     78       \u001b[36m32.2464\u001b[0m       29.7847  0.0109\n",
      "     79       \u001b[36m32.2454\u001b[0m       \u001b[32m29.7842\u001b[0m  0.0110\n",
      "     80       \u001b[36m32.2448\u001b[0m       \u001b[32m29.7841\u001b[0m  0.0110\n",
      "     81       \u001b[36m32.2441\u001b[0m       29.7845  0.0108\n",
      "     82       \u001b[36m32.2432\u001b[0m       \u001b[32m29.7840\u001b[0m  0.0106\n",
      "     83       \u001b[36m32.2427\u001b[0m       29.7843  0.0109\n",
      "     84       \u001b[36m32.2419\u001b[0m       \u001b[32m29.7840\u001b[0m  0.0113\n",
      "     85       \u001b[36m32.2414\u001b[0m       29.7842  0.0110\n",
      "     86       \u001b[36m32.2406\u001b[0m       \u001b[32m29.7837\u001b[0m  0.0115\n",
      "     87       \u001b[36m32.2400\u001b[0m       29.7840  0.0111\n",
      "     88       \u001b[36m32.2393\u001b[0m       29.7837  0.0114\n",
      "     89       \u001b[36m32.2387\u001b[0m       29.7839  0.0168\n",
      "     90       \u001b[36m32.2381\u001b[0m       29.7838  0.0159\n",
      "     91       \u001b[36m32.2375\u001b[0m       29.7838  0.0116\n",
      "     92       \u001b[36m32.2368\u001b[0m       \u001b[32m29.7837\u001b[0m  0.0123\n",
      "     93       \u001b[36m32.2364\u001b[0m       29.7840  0.0129\n",
      "     94       \u001b[36m32.2356\u001b[0m       \u001b[32m29.7836\u001b[0m  0.0159\n",
      "     95       \u001b[36m32.2351\u001b[0m       29.7839  0.0112\n",
      "     96       \u001b[36m32.2345\u001b[0m       \u001b[32m29.7836\u001b[0m  0.0155\n",
      "     97       \u001b[36m32.2340\u001b[0m       29.7840  0.0113\n",
      "     98       \u001b[36m32.2333\u001b[0m       29.7836  0.0123\n",
      "     99       \u001b[36m32.2328\u001b[0m       29.7837  0.0113\n",
      "    100       \u001b[36m32.2322\u001b[0m       29.7837  0.0112\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m32.5590\u001b[0m       \u001b[32m30.4184\u001b[0m  0.0110\n",
      "      2       \u001b[36m30.1642\u001b[0m       \u001b[32m28.8332\u001b[0m  0.0113\n",
      "      3       \u001b[36m28.0268\u001b[0m       \u001b[32m27.4924\u001b[0m  0.0112\n",
      "      4       \u001b[36m26.1417\u001b[0m       \u001b[32m26.5377\u001b[0m  0.0113\n",
      "      5       \u001b[36m24.6908\u001b[0m       \u001b[32m26.1294\u001b[0m  0.0111\n",
      "      6       \u001b[36m23.8359\u001b[0m       26.1535  0.0111\n",
      "      7       \u001b[36m23.4775\u001b[0m       26.2874  0.0111\n",
      "      8       \u001b[36m23.3534\u001b[0m       26.3717  0.0111\n",
      "      9       \u001b[36m23.3029\u001b[0m       26.4058  0.0114\n",
      "     10       \u001b[36m23.2727\u001b[0m       26.4155  0.0107\n",
      "     11       \u001b[36m23.2502\u001b[0m       26.4163  0.0108\n",
      "     12       \u001b[36m23.2316\u001b[0m       26.4142  0.0116\n",
      "     13       \u001b[36m23.2159\u001b[0m       26.4116  0.0111\n",
      "     14       \u001b[36m23.2021\u001b[0m       26.4091  0.0113\n",
      "     15       \u001b[36m23.1900\u001b[0m       26.4070  0.0113\n",
      "     16       \u001b[36m23.1794\u001b[0m       26.4053  0.0111\n",
      "     17       \u001b[36m23.1701\u001b[0m       26.4041  0.0110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     18       \u001b[36m23.1618\u001b[0m       26.4029  0.0118\n",
      "     19       \u001b[36m23.1542\u001b[0m       26.4019  0.0110\n",
      "     20       \u001b[36m23.1476\u001b[0m       26.4012  0.0109\n",
      "     21       \u001b[36m23.1415\u001b[0m       26.4003  0.0109\n",
      "     22       \u001b[36m23.1361\u001b[0m       26.3997  0.0105\n",
      "     23       \u001b[36m23.1311\u001b[0m       26.3991  0.0107\n",
      "     24       \u001b[36m23.1266\u001b[0m       26.3984  0.0111\n",
      "     25       \u001b[36m23.1225\u001b[0m       26.3978  0.0110\n",
      "     26       \u001b[36m23.1187\u001b[0m       26.3974  0.0109\n",
      "     27       \u001b[36m23.1152\u001b[0m       26.3972  0.0108\n",
      "     28       \u001b[36m23.1120\u001b[0m       26.3970  0.0112\n",
      "     29       \u001b[36m23.1090\u001b[0m       26.3968  0.0111\n",
      "     30       \u001b[36m23.1063\u001b[0m       26.3969  0.0113\n",
      "     31       \u001b[36m23.1037\u001b[0m       26.3970  0.0110\n",
      "     32       \u001b[36m23.1014\u001b[0m       26.3971  0.0109\n",
      "     33       \u001b[36m23.0992\u001b[0m       26.3971  0.0109\n",
      "     34       \u001b[36m23.0971\u001b[0m       26.3972  0.0113\n",
      "     35       \u001b[36m23.0953\u001b[0m       26.3972  0.0125\n",
      "     36       \u001b[36m23.0935\u001b[0m       26.3974  0.0113\n",
      "     37       \u001b[36m23.0919\u001b[0m       26.3976  0.0111\n",
      "     38       \u001b[36m23.0903\u001b[0m       26.3977  0.0117\n",
      "     39       \u001b[36m23.0889\u001b[0m       26.3978  0.0117\n",
      "     40       \u001b[36m23.0875\u001b[0m       26.3978  0.0114\n",
      "     41       \u001b[36m23.0862\u001b[0m       26.3980  0.0112\n",
      "     42       \u001b[36m23.0849\u001b[0m       26.3982  0.0110\n",
      "     43       \u001b[36m23.0838\u001b[0m       26.3983  0.0114\n",
      "     44       \u001b[36m23.0826\u001b[0m       26.3986  0.0117\n",
      "     45       \u001b[36m23.0815\u001b[0m       26.3988  0.0137\n",
      "     46       \u001b[36m23.0805\u001b[0m       26.3988  0.0118\n",
      "     47       \u001b[36m23.0795\u001b[0m       26.3989  0.0111\n",
      "     48       \u001b[36m23.0786\u001b[0m       26.3990  0.0113\n",
      "     49       \u001b[36m23.0777\u001b[0m       26.3991  0.0111\n",
      "     50       \u001b[36m23.0768\u001b[0m       26.3992  0.0109\n",
      "     51       \u001b[36m23.0760\u001b[0m       26.3993  0.0109\n",
      "     52       \u001b[36m23.0751\u001b[0m       26.3995  0.0108\n",
      "     53       \u001b[36m23.0744\u001b[0m       26.3996  0.0110\n",
      "     54       \u001b[36m23.0736\u001b[0m       26.3997  0.0112\n",
      "     55       \u001b[36m23.0729\u001b[0m       26.3997  0.0110\n",
      "     56       \u001b[36m23.0721\u001b[0m       26.3998  0.0110\n",
      "     57       \u001b[36m23.0714\u001b[0m       26.3998  0.0108\n",
      "     58       \u001b[36m23.0708\u001b[0m       26.4002  0.0112\n",
      "     59       \u001b[36m23.0701\u001b[0m       26.4001  0.0117\n",
      "     60       \u001b[36m23.0695\u001b[0m       26.4002  0.0112\n",
      "     61       \u001b[36m23.0689\u001b[0m       26.4004  0.0114\n",
      "     62       \u001b[36m23.0683\u001b[0m       26.4005  0.0115\n",
      "     63       \u001b[36m23.0677\u001b[0m       26.4001  0.0120\n",
      "     64       \u001b[36m23.0672\u001b[0m       26.4002  0.0116\n",
      "     65       \u001b[36m23.0666\u001b[0m       26.4003  0.0118\n",
      "     66       \u001b[36m23.0661\u001b[0m       26.4002  0.0111\n",
      "     67       \u001b[36m23.0655\u001b[0m       26.4002  0.0111\n",
      "     68       \u001b[36m23.0650\u001b[0m       26.4002  0.0113\n",
      "     69       \u001b[36m23.0645\u001b[0m       26.4001  0.0113\n",
      "     70       \u001b[36m23.0640\u001b[0m       26.3998  0.0118\n",
      "     71       \u001b[36m23.0636\u001b[0m       26.3998  0.0200\n",
      "     72       \u001b[36m23.0631\u001b[0m       26.3998  0.0144\n",
      "     73       \u001b[36m23.0626\u001b[0m       26.3999  0.0136\n",
      "     74       \u001b[36m23.0622\u001b[0m       26.3998  0.0155\n",
      "     75       \u001b[36m23.0617\u001b[0m       26.3997  0.0171\n",
      "     76       \u001b[36m23.0613\u001b[0m       26.3995  0.0232\n",
      "     77       \u001b[36m23.0609\u001b[0m       26.3995  0.0133\n",
      "     78       \u001b[36m23.0604\u001b[0m       26.3993  0.0113\n",
      "     79       \u001b[36m23.0600\u001b[0m       26.3993  0.0111\n",
      "     80       \u001b[36m23.0596\u001b[0m       26.3992  0.0111\n",
      "     81       \u001b[36m23.0592\u001b[0m       26.3990  0.0113\n",
      "     82       \u001b[36m23.0588\u001b[0m       26.3989  0.0113\n",
      "     83       \u001b[36m23.0584\u001b[0m       26.3987  0.0113\n",
      "     84       \u001b[36m23.0581\u001b[0m       26.3986  0.0111\n",
      "     85       \u001b[36m23.0577\u001b[0m       26.3985  0.0110\n",
      "     86       \u001b[36m23.0573\u001b[0m       26.3984  0.0112\n",
      "     87       \u001b[36m23.0570\u001b[0m       26.3983  0.0113\n",
      "     88       \u001b[36m23.0566\u001b[0m       26.3981  0.0109\n",
      "     89       \u001b[36m23.0563\u001b[0m       26.3981  0.0112\n",
      "     90       \u001b[36m23.0559\u001b[0m       26.3979  0.0111\n",
      "     91       \u001b[36m23.0556\u001b[0m       26.3977  0.0114\n",
      "     92       \u001b[36m23.0552\u001b[0m       26.3977  0.0119\n",
      "     93       \u001b[36m23.0549\u001b[0m       26.3975  0.0122\n",
      "     94       \u001b[36m23.0545\u001b[0m       26.3973  0.0117\n",
      "     95       \u001b[36m23.0542\u001b[0m       26.3973  0.0119\n",
      "     96       \u001b[36m23.0539\u001b[0m       26.3971  0.0115\n",
      "     97       \u001b[36m23.0536\u001b[0m       26.3970  0.0112\n",
      "     98       \u001b[36m23.0532\u001b[0m       26.3969  0.0113\n",
      "     99       \u001b[36m23.0530\u001b[0m       26.3967  0.0112\n",
      "    100       \u001b[36m23.0526\u001b[0m       26.3968  0.0112\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m39.2752\u001b[0m       \u001b[32m30.0745\u001b[0m  0.0114\n",
      "      2       \u001b[36m36.3955\u001b[0m       \u001b[32m28.3561\u001b[0m  0.0113\n",
      "      3       \u001b[36m33.7935\u001b[0m       \u001b[32m26.9157\u001b[0m  0.0113\n",
      "      4       \u001b[36m31.3917\u001b[0m       \u001b[32m26.1022\u001b[0m  0.0109\n",
      "      5       \u001b[36m29.6411\u001b[0m       26.2257  0.0108\n",
      "      6       \u001b[36m28.8750\u001b[0m       26.7352  0.0112\n",
      "      7       \u001b[36m28.6962\u001b[0m       27.0401  0.0109\n",
      "      8       \u001b[36m28.6542\u001b[0m       27.1579  0.0110\n",
      "      9       \u001b[36m28.6262\u001b[0m       27.2023  0.0111\n",
      "     10       \u001b[36m28.6016\u001b[0m       27.2249  0.0108\n",
      "     11       \u001b[36m28.5808\u001b[0m       27.2392  0.0110\n",
      "     12       \u001b[36m28.5633\u001b[0m       27.2509  0.0114\n",
      "     13       \u001b[36m28.5487\u001b[0m       27.2583  0.0115\n",
      "     14       \u001b[36m28.5364\u001b[0m       27.2658  0.0113\n",
      "     15       \u001b[36m28.5258\u001b[0m       27.2728  0.0111\n",
      "     16       \u001b[36m28.5165\u001b[0m       27.2783  0.0110\n",
      "     17       \u001b[36m28.5085\u001b[0m       27.2809  0.0116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     18       \u001b[36m28.5013\u001b[0m       27.2853  0.0114\n",
      "     19       \u001b[36m28.4950\u001b[0m       27.2894  0.0108\n",
      "     20       \u001b[36m28.4895\u001b[0m       27.2906  0.0108\n",
      "     21       \u001b[36m28.4843\u001b[0m       27.2930  0.0107\n",
      "     22       \u001b[36m28.4798\u001b[0m       27.2943  0.0108\n",
      "     23       \u001b[36m28.4758\u001b[0m       27.2947  0.0109\n",
      "     24       \u001b[36m28.4721\u001b[0m       27.2947  0.0113\n",
      "     25       \u001b[36m28.4689\u001b[0m       27.2948  0.0112\n",
      "     26       \u001b[36m28.4658\u001b[0m       27.2941  0.0134\n",
      "     27       \u001b[36m28.4630\u001b[0m       27.2941  0.0129\n",
      "     28       \u001b[36m28.4604\u001b[0m       27.2925  0.0113\n",
      "     29       \u001b[36m28.4579\u001b[0m       27.2923  0.0114\n",
      "     30       \u001b[36m28.4556\u001b[0m       27.2914  0.0111\n",
      "     31       \u001b[36m28.4534\u001b[0m       27.2906  0.0113\n",
      "     32       \u001b[36m28.4515\u001b[0m       27.2902  0.0108\n",
      "     33       \u001b[36m28.4496\u001b[0m       27.2890  0.0111\n",
      "     34       \u001b[36m28.4479\u001b[0m       27.2874  0.0112\n",
      "     35       \u001b[36m28.4462\u001b[0m       27.2869  0.0111\n",
      "     36       \u001b[36m28.4448\u001b[0m       27.2856  0.0110\n",
      "     37       \u001b[36m28.4433\u001b[0m       27.2840  0.0107\n",
      "     38       \u001b[36m28.4420\u001b[0m       27.2825  0.0110\n",
      "     39       \u001b[36m28.4406\u001b[0m       27.2812  0.0111\n",
      "     40       \u001b[36m28.4394\u001b[0m       27.2794  0.0111\n",
      "     41       \u001b[36m28.4382\u001b[0m       27.2778  0.0111\n",
      "     42       \u001b[36m28.4370\u001b[0m       27.2758  0.0106\n",
      "     43       \u001b[36m28.4359\u001b[0m       27.2741  0.0114\n",
      "     44       \u001b[36m28.4348\u001b[0m       27.2726  0.0110\n",
      "     45       \u001b[36m28.4338\u001b[0m       27.2709  0.0108\n",
      "     46       \u001b[36m28.4329\u001b[0m       27.2694  0.0108\n",
      "     47       \u001b[36m28.4319\u001b[0m       27.2678  0.0111\n",
      "     48       \u001b[36m28.4311\u001b[0m       27.2652  0.0116\n",
      "     49       \u001b[36m28.4301\u001b[0m       27.2646  0.0119\n",
      "     50       \u001b[36m28.4293\u001b[0m       27.2629  0.0118\n",
      "     51       \u001b[36m28.4285\u001b[0m       27.2614  0.0172\n",
      "     52       \u001b[36m28.4277\u001b[0m       27.2596  0.0147\n",
      "     53       \u001b[36m28.4269\u001b[0m       27.2581  0.0125\n",
      "     54       \u001b[36m28.4262\u001b[0m       27.2562  0.0137\n",
      "     55       \u001b[36m28.4254\u001b[0m       27.2554  0.0138\n",
      "     56       \u001b[36m28.4247\u001b[0m       27.2539  0.0134\n",
      "     57       \u001b[36m28.4240\u001b[0m       27.2526  0.0131\n",
      "     58       \u001b[36m28.4234\u001b[0m       27.2507  0.0127\n",
      "     59       \u001b[36m28.4227\u001b[0m       27.2498  0.0123\n",
      "     60       \u001b[36m28.4221\u001b[0m       27.2487  0.0144\n",
      "     61       \u001b[36m28.4215\u001b[0m       27.2476  0.0116\n",
      "     62       \u001b[36m28.4209\u001b[0m       27.2462  0.0124\n",
      "     63       \u001b[36m28.4204\u001b[0m       27.2444  0.0113\n",
      "     64       \u001b[36m28.4197\u001b[0m       27.2435  0.0111\n",
      "     65       \u001b[36m28.4193\u001b[0m       27.2420  0.0114\n",
      "     66       \u001b[36m28.4187\u001b[0m       27.2409  0.0114\n",
      "     67       \u001b[36m28.4182\u001b[0m       27.2401  0.0122\n",
      "     68       \u001b[36m28.4177\u001b[0m       27.2384  0.0124\n",
      "     69       \u001b[36m28.4172\u001b[0m       27.2375  0.0106\n",
      "     70       \u001b[36m28.4167\u001b[0m       27.2366  0.0110\n",
      "     71       \u001b[36m28.4163\u001b[0m       27.2350  0.0113\n",
      "     72       \u001b[36m28.4158\u001b[0m       27.2343  0.0111\n",
      "     73       \u001b[36m28.4153\u001b[0m       27.2335  0.0105\n",
      "     74       \u001b[36m28.4150\u001b[0m       27.2320  0.0106\n",
      "     75       \u001b[36m28.4145\u001b[0m       27.2312  0.0110\n",
      "     76       \u001b[36m28.4141\u001b[0m       27.2303  0.0113\n",
      "     77       \u001b[36m28.4137\u001b[0m       27.2290  0.0115\n",
      "     78       \u001b[36m28.4133\u001b[0m       27.2282  0.0109\n",
      "     79       \u001b[36m28.4130\u001b[0m       27.2270  0.0106\n",
      "     80       \u001b[36m28.4125\u001b[0m       27.2267  0.0106\n",
      "     81       \u001b[36m28.4122\u001b[0m       27.2261  0.0114\n",
      "     82       \u001b[36m28.4119\u001b[0m       27.2248  0.0112\n",
      "     83       \u001b[36m28.4115\u001b[0m       27.2241  0.0107\n",
      "     84       \u001b[36m28.4111\u001b[0m       27.2236  0.0108\n",
      "     85       \u001b[36m28.4108\u001b[0m       27.2222  0.0107\n",
      "     86       \u001b[36m28.4104\u001b[0m       27.2218  0.0110\n",
      "     87       \u001b[36m28.4101\u001b[0m       27.2213  0.0109\n",
      "     88       \u001b[36m28.4099\u001b[0m       27.2202  0.0112\n",
      "     89       \u001b[36m28.4095\u001b[0m       27.2197  0.0107\n",
      "     90       \u001b[36m28.4092\u001b[0m       27.2189  0.0111\n",
      "     91       \u001b[36m28.4089\u001b[0m       27.2185  0.0110\n",
      "     92       \u001b[36m28.4086\u001b[0m       27.2182  0.0111\n",
      "     93       \u001b[36m28.4084\u001b[0m       27.2174  0.0107\n",
      "     94       \u001b[36m28.4081\u001b[0m       27.2168  0.0108\n",
      "     95       \u001b[36m28.4078\u001b[0m       27.2163  0.0108\n",
      "     96       \u001b[36m28.4076\u001b[0m       27.2157  0.0114\n",
      "     97       \u001b[36m28.4073\u001b[0m       27.2156  0.0121\n",
      "     98       \u001b[36m28.4071\u001b[0m       27.2150  0.0116\n",
      "     99       \u001b[36m28.4068\u001b[0m       27.2149  0.0109\n",
      "    100       \u001b[36m28.4066\u001b[0m       27.2144  0.0109\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m41.5794\u001b[0m       \u001b[32m41.3716\u001b[0m  0.0118\n",
      "      2       \u001b[36m36.8133\u001b[0m       \u001b[32m31.5588\u001b[0m  0.0117\n",
      "      3       \u001b[36m34.6995\u001b[0m       \u001b[32m31.5263\u001b[0m  0.0116\n",
      "      4       \u001b[36m33.7353\u001b[0m       31.9879  0.0117\n",
      "      5       \u001b[36m33.3726\u001b[0m       \u001b[32m31.2497\u001b[0m  0.0115\n",
      "      6       \u001b[36m33.0516\u001b[0m       \u001b[32m30.3519\u001b[0m  0.0115\n",
      "      7       \u001b[36m32.8809\u001b[0m       30.5425  0.0117\n",
      "      8       \u001b[36m32.7871\u001b[0m       30.6137  0.0115\n",
      "      9       \u001b[36m32.6479\u001b[0m       \u001b[32m30.3243\u001b[0m  0.0117\n",
      "     10       \u001b[36m32.5544\u001b[0m       \u001b[32m30.2168\u001b[0m  0.0121\n",
      "     11       \u001b[36m32.4941\u001b[0m       30.3526  0.0115\n",
      "     12       \u001b[36m32.4444\u001b[0m       30.3296  0.0118\n",
      "     13       \u001b[36m32.3919\u001b[0m       30.2283  0.0116\n",
      "     14       \u001b[36m32.3559\u001b[0m       30.2404  0.0115\n",
      "     15       \u001b[36m32.3346\u001b[0m       30.2611  0.0114\n",
      "     16       \u001b[36m32.3054\u001b[0m       \u001b[32m30.1747\u001b[0m  0.0113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m32.2845\u001b[0m       30.1830  0.0122\n",
      "     18       \u001b[36m32.2700\u001b[0m       30.1758  0.0118\n",
      "     19       \u001b[36m32.2533\u001b[0m       \u001b[32m30.1322\u001b[0m  0.0115\n",
      "     20       \u001b[36m32.2417\u001b[0m       \u001b[32m30.1268\u001b[0m  0.0112\n",
      "     21       \u001b[36m32.2316\u001b[0m       \u001b[32m30.0886\u001b[0m  0.0113\n",
      "     22       \u001b[36m32.2210\u001b[0m       \u001b[32m30.0753\u001b[0m  0.0116\n",
      "     23       \u001b[36m32.2131\u001b[0m       \u001b[32m30.0579\u001b[0m  0.0117\n",
      "     24       \u001b[36m32.2050\u001b[0m       \u001b[32m30.0461\u001b[0m  0.0117\n",
      "     25       \u001b[36m32.1978\u001b[0m       30.0542  0.0113\n",
      "     26       \u001b[36m32.1919\u001b[0m       \u001b[32m30.0396\u001b[0m  0.0114\n",
      "     27       \u001b[36m32.1864\u001b[0m       30.0397  0.0117\n",
      "     28       \u001b[36m32.1807\u001b[0m       30.0879  0.0118\n",
      "     29       \u001b[36m32.1779\u001b[0m       30.1111  0.0114\n",
      "     30       32.1783       30.0873  0.0113\n",
      "     31       32.2037       30.3705  0.0153\n",
      "     32       32.2291       30.1065  0.0204\n",
      "     33       32.2047       \u001b[32m30.0302\u001b[0m  0.0134\n",
      "     34       32.2123       30.1650  0.0133\n",
      "     35       32.1932       \u001b[32m30.0233\u001b[0m  0.0136\n",
      "     36       \u001b[36m32.1774\u001b[0m       30.1811  0.0168\n",
      "     37       32.1845       30.0305  0.0148\n",
      "     38       32.1781       30.1616  0.0118\n",
      "     39       \u001b[36m32.1708\u001b[0m       30.0363  0.0116\n",
      "     40       \u001b[36m32.1632\u001b[0m       30.1560  0.0123\n",
      "     41       \u001b[36m32.1581\u001b[0m       \u001b[32m30.0206\u001b[0m  0.0124\n",
      "     42       \u001b[36m32.1506\u001b[0m       30.1185  0.0117\n",
      "     43       \u001b[36m32.1468\u001b[0m       30.0363  0.0117\n",
      "     44       \u001b[36m32.1389\u001b[0m       30.1163  0.0119\n",
      "     45       \u001b[36m32.1375\u001b[0m       30.0777  0.0115\n",
      "     46       \u001b[36m32.1316\u001b[0m       30.1283  0.0115\n",
      "     47       \u001b[36m32.1308\u001b[0m       30.0904  0.0131\n",
      "     48       \u001b[36m32.1273\u001b[0m       30.1398  0.0118\n",
      "     49       \u001b[36m32.1259\u001b[0m       30.0917  0.0114\n",
      "     50       \u001b[36m32.1233\u001b[0m       30.1588  0.0111\n",
      "     51       \u001b[36m32.1226\u001b[0m       30.1203  0.0114\n",
      "     52       32.1236       30.1604  0.0124\n",
      "     53       32.1309       30.1528  0.0122\n",
      "     54       32.1309       30.1877  0.0114\n",
      "     55       \u001b[36m32.1157\u001b[0m       30.1395  0.0114\n",
      "     56       32.1199       30.1559  0.0114\n",
      "     57       \u001b[36m32.1096\u001b[0m       30.1662  0.0115\n",
      "     58       \u001b[36m32.1086\u001b[0m       30.1250  0.0115\n",
      "     59       \u001b[36m32.1061\u001b[0m       30.1781  0.0113\n",
      "     60       \u001b[36m32.1030\u001b[0m       30.1217  0.0110\n",
      "     61       \u001b[36m32.1009\u001b[0m       30.1534  0.0111\n",
      "     62       \u001b[36m32.1005\u001b[0m       30.1491  0.0116\n",
      "     63       \u001b[36m32.0970\u001b[0m       30.1419  0.0117\n",
      "     64       32.0973       30.1579  0.0118\n",
      "     65       \u001b[36m32.0944\u001b[0m       30.1691  0.0113\n",
      "     66       32.0946       30.1321  0.0113\n",
      "     67       32.0953       30.2005  0.0117\n",
      "     68       \u001b[36m32.0922\u001b[0m       30.1591  0.0117\n",
      "     69       32.0964       30.1376  0.0120\n",
      "     70       32.0972       30.3094  0.0115\n",
      "     71       32.1120       30.1626  0.0117\n",
      "     72       32.1209       30.1554  0.0113\n",
      "     73       32.1317       30.4643  0.0113\n",
      "     74       32.1750       30.2312  0.0119\n",
      "     75       32.1770       30.2897  0.0206\n",
      "     76       32.1275       30.1167  0.0128\n",
      "     77       32.1217       30.2917  0.0115\n",
      "     78       32.1240       30.1411  0.0112\n",
      "     79       32.1315       30.2472  0.0120\n",
      "     80       32.1163       30.1071  0.0118\n",
      "     81       32.1086       30.2040  0.0117\n",
      "     82       32.1050       30.1322  0.0115\n",
      "     83       32.0934       30.1578  0.0114\n",
      "     84       \u001b[36m32.0893\u001b[0m       30.1773  0.0121\n",
      "     85       \u001b[36m32.0818\u001b[0m       30.1596  0.0116\n",
      "     86       \u001b[36m32.0798\u001b[0m       30.1930  0.0117\n",
      "     87       \u001b[36m32.0782\u001b[0m       30.1783  0.0114\n",
      "     88       \u001b[36m32.0752\u001b[0m       30.2086  0.0114\n",
      "     89       32.0760       30.2011  0.0124\n",
      "     90       \u001b[36m32.0730\u001b[0m       30.2183  0.0115\n",
      "     91       32.0736       30.2113  0.0115\n",
      "     92       \u001b[36m32.0722\u001b[0m       30.2476  0.0118\n",
      "     93       32.0783       30.1995  0.0117\n",
      "     94       32.0979       30.3006  0.0113\n",
      "     95       32.1082       30.2438  0.0116\n",
      "     96       32.0783       30.2722  0.0112\n",
      "     97       32.0821       30.2329  0.0111\n",
      "     98       \u001b[36m32.0700\u001b[0m       30.2869  0.0121\n",
      "     99       32.0721       30.2275  0.0120\n",
      "    100       \u001b[36m32.0679\u001b[0m       30.2740  0.0116\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m30.9669\u001b[0m       \u001b[32m27.5589\u001b[0m  0.0116\n",
      "      2       \u001b[36m25.5735\u001b[0m       29.3700  0.0113\n",
      "      3       \u001b[36m24.4965\u001b[0m       \u001b[32m26.3915\u001b[0m  0.0116\n",
      "      4       \u001b[36m24.2865\u001b[0m       \u001b[32m26.3595\u001b[0m  0.0117\n",
      "      5       \u001b[36m23.7738\u001b[0m       27.0858  0.0115\n",
      "      6       \u001b[36m23.6053\u001b[0m       27.2338  0.0111\n",
      "      7       \u001b[36m23.4171\u001b[0m       26.6520  0.0112\n",
      "      8       \u001b[36m23.4036\u001b[0m       26.6573  0.0120\n",
      "      9       \u001b[36m23.2817\u001b[0m       26.9524  0.0160\n",
      "     10       \u001b[36m23.2140\u001b[0m       26.6933  0.0202\n",
      "     11       \u001b[36m23.1788\u001b[0m       26.6202  0.0145\n",
      "     12       \u001b[36m23.1489\u001b[0m       26.7658  0.0139\n",
      "     13       \u001b[36m23.1136\u001b[0m       26.6050  0.0134\n",
      "     14       \u001b[36m23.1025\u001b[0m       26.5396  0.0143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     15       \u001b[36m23.0934\u001b[0m       26.5994  0.0151\n",
      "     16       \u001b[36m23.0769\u001b[0m       26.5616  0.0125\n",
      "     17       \u001b[36m23.0696\u001b[0m       26.5011  0.0122\n",
      "     18       \u001b[36m23.0634\u001b[0m       26.5488  0.0153\n",
      "     19       \u001b[36m23.0532\u001b[0m       26.5547  0.0117\n",
      "     20       \u001b[36m23.0472\u001b[0m       26.5221  0.0117\n",
      "     21       \u001b[36m23.0437\u001b[0m       26.5406  0.0160\n",
      "     22       \u001b[36m23.0375\u001b[0m       26.5578  0.0122\n",
      "     23       \u001b[36m23.0350\u001b[0m       26.5016  0.0118\n",
      "     24       23.0384       26.5371  0.0122\n",
      "     25       \u001b[36m23.0341\u001b[0m       26.5771  0.0128\n",
      "     26       23.0482       26.4720  0.0121\n",
      "     27       23.0724       26.6022  0.0117\n",
      "     28       23.0641       26.5535  0.0124\n",
      "     29       23.0526       26.5105  0.0117\n",
      "     30       23.0353       26.5117  0.0116\n",
      "     31       23.0428       26.5795  0.0123\n",
      "     32       \u001b[36m23.0270\u001b[0m       26.5009  0.0122\n",
      "     33       \u001b[36m23.0173\u001b[0m       26.5269  0.0119\n",
      "     34       \u001b[36m23.0144\u001b[0m       26.5133  0.0119\n",
      "     35       \u001b[36m23.0080\u001b[0m       26.5182  0.0116\n",
      "     36       \u001b[36m23.0019\u001b[0m       26.5184  0.0115\n",
      "     37       \u001b[36m22.9972\u001b[0m       26.5210  0.0122\n",
      "     38       \u001b[36m22.9956\u001b[0m       26.5101  0.0119\n",
      "     39       \u001b[36m22.9926\u001b[0m       26.5172  0.0119\n",
      "     40       \u001b[36m22.9898\u001b[0m       26.5107  0.0116\n",
      "     41       \u001b[36m22.9877\u001b[0m       26.5147  0.0114\n",
      "     42       \u001b[36m22.9856\u001b[0m       26.5100  0.0121\n",
      "     43       \u001b[36m22.9835\u001b[0m       26.5135  0.0120\n",
      "     44       \u001b[36m22.9815\u001b[0m       26.5108  0.0121\n",
      "     45       \u001b[36m22.9798\u001b[0m       26.5150  0.0113\n",
      "     46       \u001b[36m22.9778\u001b[0m       26.5135  0.0113\n",
      "     47       \u001b[36m22.9761\u001b[0m       26.5147  0.0116\n",
      "     48       \u001b[36m22.9745\u001b[0m       26.5179  0.0118\n",
      "     49       \u001b[36m22.9730\u001b[0m       26.5136  0.0116\n",
      "     50       \u001b[36m22.9716\u001b[0m       26.5278  0.0112\n",
      "     51       \u001b[36m22.9698\u001b[0m       26.5128  0.0113\n",
      "     52       \u001b[36m22.9690\u001b[0m       26.5344  0.0119\n",
      "     53       \u001b[36m22.9666\u001b[0m       26.5245  0.0141\n",
      "     54       \u001b[36m22.9660\u001b[0m       26.5266  0.0130\n",
      "     55       \u001b[36m22.9645\u001b[0m       26.5454  0.0111\n",
      "     56       \u001b[36m22.9637\u001b[0m       26.5340  0.0113\n",
      "     57       22.9644       26.5235  0.0136\n",
      "     58       \u001b[36m22.9633\u001b[0m       26.5902  0.0128\n",
      "     59       22.9653       26.5282  0.0118\n",
      "     60       22.9717       26.5499  0.0120\n",
      "     61       22.9704       26.6256  0.0120\n",
      "     62       22.9694       26.5264  0.0118\n",
      "     63       22.9781       26.6032  0.0115\n",
      "     64       22.9696       26.5873  0.0120\n",
      "     65       \u001b[36m22.9624\u001b[0m       26.5426  0.0120\n",
      "     66       \u001b[36m22.9587\u001b[0m       26.6448  0.0121\n",
      "     67       \u001b[36m22.9571\u001b[0m       26.5543  0.0124\n",
      "     68       \u001b[36m22.9569\u001b[0m       26.6046  0.0113\n",
      "     69       \u001b[36m22.9503\u001b[0m       26.6121  0.0115\n",
      "     70       \u001b[36m22.9481\u001b[0m       26.5835  0.0122\n",
      "     71       \u001b[36m22.9466\u001b[0m       26.6351  0.0117\n",
      "     72       \u001b[36m22.9460\u001b[0m       26.5922  0.0116\n",
      "     73       \u001b[36m22.9453\u001b[0m       26.6197  0.0134\n",
      "     74       \u001b[36m22.9422\u001b[0m       26.6240  0.0114\n",
      "     75       22.9426       26.6086  0.0121\n",
      "     76       \u001b[36m22.9413\u001b[0m       26.6530  0.0116\n",
      "     77       \u001b[36m22.9401\u001b[0m       26.6330  0.0123\n",
      "     78       \u001b[36m22.9399\u001b[0m       26.6237  0.0119\n",
      "     79       \u001b[36m22.9391\u001b[0m       26.6718  0.0117\n",
      "     80       22.9403       26.6407  0.0114\n",
      "     81       22.9462       26.6234  0.0118\n",
      "     82       22.9460       26.7426  0.0128\n",
      "     83       22.9522       26.6303  0.0115\n",
      "     84       22.9661       26.6204  0.0117\n",
      "     85       22.9632       26.7735  0.0113\n",
      "     86       22.9610       26.6151  0.0170\n",
      "     87       22.9749       26.7040  0.0151\n",
      "     88       22.9474       26.6788  0.0126\n",
      "     89       22.9408       26.6469  0.0120\n",
      "     90       \u001b[36m22.9369\u001b[0m       26.7434  0.0122\n",
      "     91       \u001b[36m22.9339\u001b[0m       26.6131  0.0150\n",
      "     92       \u001b[36m22.9323\u001b[0m       26.7238  0.0132\n",
      "     93       \u001b[36m22.9299\u001b[0m       26.6513  0.0129\n",
      "     94       \u001b[36m22.9292\u001b[0m       26.6719  0.0120\n",
      "     95       \u001b[36m22.9270\u001b[0m       26.7263  0.0117\n",
      "     96       \u001b[36m22.9259\u001b[0m       26.6541  0.0128\n",
      "     97       \u001b[36m22.9252\u001b[0m       26.7231  0.0123\n",
      "     98       \u001b[36m22.9230\u001b[0m       26.6991  0.0116\n",
      "     99       22.9232       26.6947  0.0117\n",
      "    100       \u001b[36m22.9220\u001b[0m       26.7212  0.0116\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m39.6085\u001b[0m       \u001b[32m27.7846\u001b[0m  0.0124\n",
      "      2       \u001b[36m32.1485\u001b[0m       31.6310  0.0118\n",
      "      3       \u001b[36m31.1015\u001b[0m       \u001b[32m26.6698\u001b[0m  0.0121\n",
      "      4       \u001b[36m29.9425\u001b[0m       26.7150  0.0128\n",
      "      5       \u001b[36m29.3242\u001b[0m       28.4260  0.0118\n",
      "      6       29.3868       27.7589  0.0116\n",
      "      7       \u001b[36m28.9568\u001b[0m       27.1572  0.0126\n",
      "      8       \u001b[36m28.8014\u001b[0m       27.7066  0.0114\n",
      "      9       \u001b[36m28.7752\u001b[0m       27.6568  0.0123\n",
      "     10       \u001b[36m28.6518\u001b[0m       27.2069  0.0118\n",
      "     11       \u001b[36m28.5694\u001b[0m       27.5593  0.0120\n",
      "     12       \u001b[36m28.5694\u001b[0m       27.6180  0.0113\n",
      "     13       \u001b[36m28.5265\u001b[0m       27.3186  0.0113\n",
      "     14       \u001b[36m28.4906\u001b[0m       27.5078  0.0138\n",
      "     15       28.4945       27.5093  0.0117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     16       \u001b[36m28.4737\u001b[0m       27.3543  0.0118\n",
      "     17       \u001b[36m28.4602\u001b[0m       27.4261  0.0121\n",
      "     18       \u001b[36m28.4586\u001b[0m       27.3819  0.0139\n",
      "     19       \u001b[36m28.4451\u001b[0m       27.3203  0.0114\n",
      "     20       \u001b[36m28.4411\u001b[0m       27.3372  0.0114\n",
      "     21       \u001b[36m28.4374\u001b[0m       27.3008  0.0124\n",
      "     22       \u001b[36m28.4298\u001b[0m       27.2889  0.0119\n",
      "     23       \u001b[36m28.4279\u001b[0m       27.2894  0.0120\n",
      "     24       \u001b[36m28.4237\u001b[0m       27.2735  0.0116\n",
      "     25       \u001b[36m28.4213\u001b[0m       27.2632  0.0117\n",
      "     26       \u001b[36m28.4187\u001b[0m       27.2642  0.0126\n",
      "     27       \u001b[36m28.4176\u001b[0m       27.2753  0.0119\n",
      "     28       28.4193       27.2554  0.0119\n",
      "     29       28.4210       27.2500  0.0116\n",
      "     30       28.4260       27.2804  0.0117\n",
      "     31       28.4322       27.2640  0.0123\n",
      "     32       28.4517       27.2506  0.0118\n",
      "     33       28.4437       27.3022  0.0119\n",
      "     34       28.4237       27.2279  0.0115\n",
      "     35       \u001b[36m28.4055\u001b[0m       27.2661  0.0113\n",
      "     36       28.4066       27.2450  0.0122\n",
      "     37       \u001b[36m28.4013\u001b[0m       27.2636  0.0120\n",
      "     38       \u001b[36m28.3980\u001b[0m       27.2714  0.0120\n",
      "     39       28.4002       27.2305  0.0116\n",
      "     40       \u001b[36m28.3957\u001b[0m       27.2654  0.0115\n",
      "     41       28.3965       27.2340  0.0114\n",
      "     42       \u001b[36m28.3938\u001b[0m       27.2496  0.0132\n",
      "     43       28.3939       27.2438  0.0115\n",
      "     44       \u001b[36m28.3920\u001b[0m       27.2349  0.0117\n",
      "     45       28.3920       27.2486  0.0119\n",
      "     46       \u001b[36m28.3903\u001b[0m       27.2348  0.0116\n",
      "     47       \u001b[36m28.3897\u001b[0m       27.2468  0.0114\n",
      "     48       \u001b[36m28.3892\u001b[0m       27.2448  0.0119\n",
      "     49       \u001b[36m28.3881\u001b[0m       27.2444  0.0115\n",
      "     50       \u001b[36m28.3877\u001b[0m       27.2511  0.0114\n",
      "     51       \u001b[36m28.3871\u001b[0m       27.2497  0.0120\n",
      "     52       \u001b[36m28.3865\u001b[0m       27.2496  0.0121\n",
      "     53       \u001b[36m28.3858\u001b[0m       27.2439  0.0116\n",
      "     54       \u001b[36m28.3853\u001b[0m       27.2534  0.0114\n",
      "     55       \u001b[36m28.3851\u001b[0m       27.2403  0.0119\n",
      "     56       \u001b[36m28.3838\u001b[0m       27.2594  0.0122\n",
      "     57       28.3849       27.2380  0.0117\n",
      "     58       \u001b[36m28.3824\u001b[0m       27.2538  0.0115\n",
      "     59       28.3838       27.2587  0.0124\n",
      "     60       \u001b[36m28.3819\u001b[0m       27.2609  0.0125\n",
      "     61       28.3824       27.2705  0.0118\n",
      "     62       \u001b[36m28.3805\u001b[0m       27.2758  0.0115\n",
      "     63       28.3825       27.2769  0.0120\n",
      "     64       \u001b[36m28.3798\u001b[0m       27.2711  0.0194\n",
      "     65       28.3803       27.2853  0.0190\n",
      "     66       28.3799       27.2815  0.0135\n",
      "     67       28.3809       27.2808  0.0149\n",
      "     68       \u001b[36m28.3786\u001b[0m       27.2973  0.0160\n",
      "     69       28.3829       27.3011  0.0139\n",
      "     70       28.3845       27.2808  0.0131\n",
      "     71       28.3872       27.3005  0.0126\n",
      "     72       28.3907       27.3085  0.0121\n",
      "     73       28.4040       27.3033  0.0120\n",
      "     74       28.4055       27.2836  0.0121\n",
      "     75       28.4155       27.3085  0.0121\n",
      "     76       28.4282       27.2445  0.0121\n",
      "     77       28.4035       27.2844  0.0118\n",
      "     78       28.4179       27.2625  0.0120\n",
      "     79       28.4057       27.3006  0.0123\n",
      "     80       28.4099       27.2294  0.0120\n",
      "     81       28.3847       27.2961  0.0116\n",
      "     82       28.3812       27.2466  0.0112\n",
      "     83       \u001b[36m28.3784\u001b[0m       27.2792  0.0112\n",
      "     84       28.3788       27.2239  0.0118\n",
      "     85       \u001b[36m28.3774\u001b[0m       27.2679  0.0117\n",
      "     86       \u001b[36m28.3748\u001b[0m       27.2413  0.0117\n",
      "     87       28.3748       27.2405  0.0116\n",
      "     88       \u001b[36m28.3730\u001b[0m       27.2754  0.0114\n",
      "     89       28.3740       27.2524  0.0113\n",
      "     90       \u001b[36m28.3716\u001b[0m       27.2654  0.0117\n",
      "     91       28.3725       27.2581  0.0117\n",
      "     92       \u001b[36m28.3709\u001b[0m       27.2719  0.0117\n",
      "     93       28.3718       27.2559  0.0116\n",
      "     94       \u001b[36m28.3697\u001b[0m       27.2785  0.0116\n",
      "     95       28.3704       27.2810  0.0117\n",
      "     96       \u001b[36m28.3694\u001b[0m       27.2748  0.0116\n",
      "     97       \u001b[36m28.3689\u001b[0m       27.2898  0.0140\n",
      "     98       \u001b[36m28.3687\u001b[0m       27.2956  0.0115\n",
      "     99       \u001b[36m28.3685\u001b[0m       27.2890  0.0116\n",
      "    100       \u001b[36m28.3675\u001b[0m       27.3032  0.0112\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m42.8635\u001b[0m       \u001b[32m41.8997\u001b[0m  0.0115\n",
      "      2       \u001b[36m39.2367\u001b[0m       \u001b[32m37.6269\u001b[0m  0.0107\n",
      "      3       \u001b[36m36.1466\u001b[0m       \u001b[32m34.0994\u001b[0m  0.0109\n",
      "      4       \u001b[36m33.9547\u001b[0m       \u001b[32m31.7506\u001b[0m  0.0110\n",
      "      5       \u001b[36m32.8844\u001b[0m       \u001b[32m30.6545\u001b[0m  0.0103\n",
      "      6       \u001b[36m32.5729\u001b[0m       \u001b[32m30.2664\u001b[0m  0.0107\n",
      "      7       \u001b[36m32.4936\u001b[0m       \u001b[32m30.1271\u001b[0m  0.0110\n",
      "      8       \u001b[36m32.4567\u001b[0m       \u001b[32m30.0656\u001b[0m  0.0112\n",
      "      9       \u001b[36m32.4313\u001b[0m       \u001b[32m30.0318\u001b[0m  0.0127\n",
      "     10       \u001b[36m32.4116\u001b[0m       \u001b[32m30.0083\u001b[0m  0.0112\n",
      "     11       \u001b[36m32.3963\u001b[0m       \u001b[32m29.9921\u001b[0m  0.0105\n",
      "     12       \u001b[36m32.3835\u001b[0m       \u001b[32m29.9788\u001b[0m  0.0110\n",
      "     13       \u001b[36m32.3728\u001b[0m       \u001b[32m29.9680\u001b[0m  0.0113\n",
      "     14       \u001b[36m32.3637\u001b[0m       \u001b[32m29.9579\u001b[0m  0.0110\n",
      "     15       \u001b[36m32.3558\u001b[0m       \u001b[32m29.9488\u001b[0m  0.0108\n",
      "     16       \u001b[36m32.3486\u001b[0m       \u001b[32m29.9413\u001b[0m  0.0109\n",
      "     17       \u001b[36m32.3426\u001b[0m       \u001b[32m29.9336\u001b[0m  0.0109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     18       \u001b[36m32.3367\u001b[0m       \u001b[32m29.9268\u001b[0m  0.0118\n",
      "     19       \u001b[36m32.3316\u001b[0m       \u001b[32m29.9208\u001b[0m  0.0109\n",
      "     20       \u001b[36m32.3270\u001b[0m       \u001b[32m29.9152\u001b[0m  0.0110\n",
      "     21       \u001b[36m32.3227\u001b[0m       \u001b[32m29.9109\u001b[0m  0.0109\n",
      "     22       \u001b[36m32.3189\u001b[0m       \u001b[32m29.9067\u001b[0m  0.0104\n",
      "     23       \u001b[36m32.3151\u001b[0m       \u001b[32m29.9026\u001b[0m  0.0113\n",
      "     24       \u001b[36m32.3117\u001b[0m       \u001b[32m29.8989\u001b[0m  0.0121\n",
      "     25       \u001b[36m32.3083\u001b[0m       \u001b[32m29.8945\u001b[0m  0.0112\n",
      "     26       \u001b[36m32.3050\u001b[0m       \u001b[32m29.8917\u001b[0m  0.0105\n",
      "     27       \u001b[36m32.3021\u001b[0m       \u001b[32m29.8890\u001b[0m  0.0109\n",
      "     28       \u001b[36m32.2993\u001b[0m       \u001b[32m29.8861\u001b[0m  0.0112\n",
      "     29       \u001b[36m32.2964\u001b[0m       \u001b[32m29.8837\u001b[0m  0.0110\n",
      "     30       \u001b[36m32.2938\u001b[0m       \u001b[32m29.8818\u001b[0m  0.0109\n",
      "     31       \u001b[36m32.2913\u001b[0m       \u001b[32m29.8789\u001b[0m  0.0109\n",
      "     32       \u001b[36m32.2888\u001b[0m       \u001b[32m29.8772\u001b[0m  0.0107\n",
      "     33       \u001b[36m32.2865\u001b[0m       \u001b[32m29.8748\u001b[0m  0.0109\n",
      "     34       \u001b[36m32.2843\u001b[0m       \u001b[32m29.8732\u001b[0m  0.0112\n",
      "     35       \u001b[36m32.2823\u001b[0m       \u001b[32m29.8712\u001b[0m  0.0108\n",
      "     36       \u001b[36m32.2801\u001b[0m       \u001b[32m29.8696\u001b[0m  0.0108\n",
      "     37       \u001b[36m32.2782\u001b[0m       \u001b[32m29.8675\u001b[0m  0.0108\n",
      "     38       \u001b[36m32.2763\u001b[0m       \u001b[32m29.8656\u001b[0m  0.0110\n",
      "     39       \u001b[36m32.2745\u001b[0m       \u001b[32m29.8645\u001b[0m  0.0112\n",
      "     40       \u001b[36m32.2729\u001b[0m       \u001b[32m29.8625\u001b[0m  0.0108\n",
      "     41       \u001b[36m32.2714\u001b[0m       \u001b[32m29.8616\u001b[0m  0.0108\n",
      "     42       \u001b[36m32.2697\u001b[0m       \u001b[32m29.8602\u001b[0m  0.0108\n",
      "     43       \u001b[36m32.2682\u001b[0m       \u001b[32m29.8585\u001b[0m  0.0109\n",
      "     44       \u001b[36m32.2667\u001b[0m       \u001b[32m29.8575\u001b[0m  0.0142\n",
      "     45       \u001b[36m32.2652\u001b[0m       \u001b[32m29.8560\u001b[0m  0.0147\n",
      "     46       \u001b[36m32.2638\u001b[0m       \u001b[32m29.8556\u001b[0m  0.0117\n",
      "     47       \u001b[36m32.2626\u001b[0m       \u001b[32m29.8548\u001b[0m  0.0111\n",
      "     48       \u001b[36m32.2612\u001b[0m       \u001b[32m29.8529\u001b[0m  0.0123\n",
      "     49       \u001b[36m32.2600\u001b[0m       \u001b[32m29.8526\u001b[0m  0.0119\n",
      "     50       \u001b[36m32.2589\u001b[0m       \u001b[32m29.8514\u001b[0m  0.0128\n",
      "     51       \u001b[36m32.2577\u001b[0m       \u001b[32m29.8503\u001b[0m  0.0111\n",
      "     52       \u001b[36m32.2567\u001b[0m       \u001b[32m29.8488\u001b[0m  0.0114\n",
      "     53       \u001b[36m32.2556\u001b[0m       \u001b[32m29.8486\u001b[0m  0.0122\n",
      "     54       \u001b[36m32.2546\u001b[0m       \u001b[32m29.8475\u001b[0m  0.0113\n",
      "     55       \u001b[36m32.2537\u001b[0m       \u001b[32m29.8475\u001b[0m  0.0113\n",
      "     56       \u001b[36m32.2526\u001b[0m       \u001b[32m29.8466\u001b[0m  0.0110\n",
      "     57       \u001b[36m32.2518\u001b[0m       \u001b[32m29.8457\u001b[0m  0.0111\n",
      "     58       \u001b[36m32.2508\u001b[0m       \u001b[32m29.8454\u001b[0m  0.0113\n",
      "     59       \u001b[36m32.2500\u001b[0m       \u001b[32m29.8444\u001b[0m  0.0114\n",
      "     60       \u001b[36m32.2491\u001b[0m       \u001b[32m29.8443\u001b[0m  0.0112\n",
      "     61       \u001b[36m32.2483\u001b[0m       \u001b[32m29.8438\u001b[0m  0.0114\n",
      "     62       \u001b[36m32.2475\u001b[0m       \u001b[32m29.8432\u001b[0m  0.0113\n",
      "     63       \u001b[36m32.2467\u001b[0m       \u001b[32m29.8427\u001b[0m  0.0115\n",
      "     64       \u001b[36m32.2459\u001b[0m       \u001b[32m29.8423\u001b[0m  0.0111\n",
      "     65       \u001b[36m32.2452\u001b[0m       \u001b[32m29.8420\u001b[0m  0.0113\n",
      "     66       \u001b[36m32.2444\u001b[0m       \u001b[32m29.8412\u001b[0m  0.0104\n",
      "     67       \u001b[36m32.2437\u001b[0m       \u001b[32m29.8411\u001b[0m  0.0105\n",
      "     68       \u001b[36m32.2431\u001b[0m       \u001b[32m29.8406\u001b[0m  0.0109\n",
      "     69       \u001b[36m32.2423\u001b[0m       \u001b[32m29.8405\u001b[0m  0.0113\n",
      "     70       \u001b[36m32.2417\u001b[0m       \u001b[32m29.8398\u001b[0m  0.0108\n",
      "     71       \u001b[36m32.2410\u001b[0m       \u001b[32m29.8397\u001b[0m  0.0106\n",
      "     72       \u001b[36m32.2404\u001b[0m       \u001b[32m29.8391\u001b[0m  0.0104\n",
      "     73       \u001b[36m32.2397\u001b[0m       29.8392  0.0109\n",
      "     74       \u001b[36m32.2391\u001b[0m       \u001b[32m29.8387\u001b[0m  0.0110\n",
      "     75       \u001b[36m32.2384\u001b[0m       \u001b[32m29.8382\u001b[0m  0.0109\n",
      "     76       \u001b[36m32.2378\u001b[0m       \u001b[32m29.8379\u001b[0m  0.0110\n",
      "     77       \u001b[36m32.2372\u001b[0m       \u001b[32m29.8377\u001b[0m  0.0107\n",
      "     78       \u001b[36m32.2367\u001b[0m       \u001b[32m29.8373\u001b[0m  0.0107\n",
      "     79       \u001b[36m32.2360\u001b[0m       \u001b[32m29.8372\u001b[0m  0.0109\n",
      "     80       \u001b[36m32.2355\u001b[0m       \u001b[32m29.8369\u001b[0m  0.0114\n",
      "     81       \u001b[36m32.2349\u001b[0m       \u001b[32m29.8363\u001b[0m  0.0114\n",
      "     82       \u001b[36m32.2344\u001b[0m       29.8363  0.0107\n",
      "     83       \u001b[36m32.2338\u001b[0m       \u001b[32m29.8357\u001b[0m  0.0108\n",
      "     84       \u001b[36m32.2333\u001b[0m       29.8358  0.0112\n",
      "     85       \u001b[36m32.2327\u001b[0m       \u001b[32m29.8357\u001b[0m  0.0112\n",
      "     86       \u001b[36m32.2322\u001b[0m       \u001b[32m29.8348\u001b[0m  0.0111\n",
      "     87       \u001b[36m32.2317\u001b[0m       29.8350  0.0109\n",
      "     88       \u001b[36m32.2312\u001b[0m       29.8349  0.0107\n",
      "     89       \u001b[36m32.2307\u001b[0m       \u001b[32m29.8345\u001b[0m  0.0104\n",
      "     90       \u001b[36m32.2302\u001b[0m       \u001b[32m29.8341\u001b[0m  0.0113\n",
      "     91       \u001b[36m32.2297\u001b[0m       \u001b[32m29.8338\u001b[0m  0.0110\n",
      "     92       \u001b[36m32.2292\u001b[0m       \u001b[32m29.8337\u001b[0m  0.0110\n",
      "     93       \u001b[36m32.2287\u001b[0m       \u001b[32m29.8337\u001b[0m  0.0107\n",
      "     94       \u001b[36m32.2283\u001b[0m       \u001b[32m29.8334\u001b[0m  0.0105\n",
      "     95       \u001b[36m32.2278\u001b[0m       \u001b[32m29.8331\u001b[0m  0.0107\n",
      "     96       \u001b[36m32.2274\u001b[0m       \u001b[32m29.8331\u001b[0m  0.0112\n",
      "     97       \u001b[36m32.2269\u001b[0m       \u001b[32m29.8328\u001b[0m  0.0111\n",
      "     98       \u001b[36m32.2264\u001b[0m       \u001b[32m29.8324\u001b[0m  0.0108\n",
      "     99       \u001b[36m32.2260\u001b[0m       \u001b[32m29.8322\u001b[0m  0.0107\n",
      "    100       \u001b[36m32.2255\u001b[0m       \u001b[32m29.8322\u001b[0m  0.0110\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m34.0935\u001b[0m       \u001b[32m31.1272\u001b[0m  0.0109\n",
      "      2       \u001b[36m30.8774\u001b[0m       \u001b[32m29.0079\u001b[0m  0.0111\n",
      "      3       \u001b[36m28.1654\u001b[0m       \u001b[32m27.3664\u001b[0m  0.0110\n",
      "      4       \u001b[36m25.9693\u001b[0m       \u001b[32m26.4130\u001b[0m  0.0109\n",
      "      5       \u001b[36m24.5153\u001b[0m       \u001b[32m26.1649\u001b[0m  0.0107\n",
      "      6       \u001b[36m23.7856\u001b[0m       26.2733  0.0108\n",
      "      7       \u001b[36m23.5090\u001b[0m       26.3926  0.0113\n",
      "      8       \u001b[36m23.4017\u001b[0m       26.4452  0.0110\n",
      "      9       \u001b[36m23.3462\u001b[0m       26.4583  0.0108\n",
      "     10       \u001b[36m23.3085\u001b[0m       26.4571  0.0105\n",
      "     11       \u001b[36m23.2795\u001b[0m       26.4511  0.0103\n",
      "     12       \u001b[36m23.2565\u001b[0m       26.4451  0.0111\n",
      "     13       \u001b[36m23.2373\u001b[0m       26.4393  0.0113\n",
      "     14       \u001b[36m23.2212\u001b[0m       26.4340  0.0110\n",
      "     15       \u001b[36m23.2078\u001b[0m       26.4301  0.0106\n",
      "     16       \u001b[36m23.1965\u001b[0m       26.4265  0.0108\n",
      "     17       \u001b[36m23.1866\u001b[0m       26.4232  0.0109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     18       \u001b[36m23.1778\u001b[0m       26.4201  0.0124\n",
      "     19       \u001b[36m23.1701\u001b[0m       26.4173  0.0107\n",
      "     20       \u001b[36m23.1632\u001b[0m       26.4152  0.0112\n",
      "     21       \u001b[36m23.1570\u001b[0m       26.4136  0.0108\n",
      "     22       \u001b[36m23.1513\u001b[0m       26.4119  0.0107\n",
      "     23       \u001b[36m23.1463\u001b[0m       26.4104  0.0111\n",
      "     24       \u001b[36m23.1417\u001b[0m       26.4091  0.0113\n",
      "     25       \u001b[36m23.1374\u001b[0m       26.4080  0.0109\n",
      "     26       \u001b[36m23.1335\u001b[0m       26.4070  0.0109\n",
      "     27       \u001b[36m23.1298\u001b[0m       26.4063  0.0104\n",
      "     28       \u001b[36m23.1263\u001b[0m       26.4054  0.0134\n",
      "     29       \u001b[36m23.1231\u001b[0m       26.4049  0.0139\n",
      "     30       \u001b[36m23.1201\u001b[0m       26.4043  0.0126\n",
      "     31       \u001b[36m23.1174\u001b[0m       26.4039  0.0111\n",
      "     32       \u001b[36m23.1149\u001b[0m       26.4034  0.0113\n",
      "     33       \u001b[36m23.1125\u001b[0m       26.4034  0.0117\n",
      "     34       \u001b[36m23.1103\u001b[0m       26.4031  0.0129\n",
      "     35       \u001b[36m23.1082\u001b[0m       26.4029  0.0116\n",
      "     36       \u001b[36m23.1062\u001b[0m       26.4027  0.0112\n",
      "     37       \u001b[36m23.1043\u001b[0m       26.4026  0.0109\n",
      "     38       \u001b[36m23.1025\u001b[0m       26.4027  0.0111\n",
      "     39       \u001b[36m23.1009\u001b[0m       26.4027  0.0114\n",
      "     40       \u001b[36m23.0993\u001b[0m       26.4026  0.0110\n",
      "     41       \u001b[36m23.0978\u001b[0m       26.4026  0.0109\n",
      "     42       \u001b[36m23.0964\u001b[0m       26.4026  0.0108\n",
      "     43       \u001b[36m23.0951\u001b[0m       26.4026  0.0110\n",
      "     44       \u001b[36m23.0938\u001b[0m       26.4027  0.0111\n",
      "     45       \u001b[36m23.0926\u001b[0m       26.4026  0.0111\n",
      "     46       \u001b[36m23.0914\u001b[0m       26.4026  0.0116\n",
      "     47       \u001b[36m23.0903\u001b[0m       26.4023  0.0111\n",
      "     48       \u001b[36m23.0893\u001b[0m       26.4022  0.0112\n",
      "     49       \u001b[36m23.0883\u001b[0m       26.4020  0.0110\n",
      "     50       \u001b[36m23.0873\u001b[0m       26.4020  0.0111\n",
      "     51       \u001b[36m23.0863\u001b[0m       26.4018  0.0104\n",
      "     52       \u001b[36m23.0854\u001b[0m       26.4016  0.0104\n",
      "     53       \u001b[36m23.0845\u001b[0m       26.4014  0.0112\n",
      "     54       \u001b[36m23.0836\u001b[0m       26.4012  0.0118\n",
      "     55       \u001b[36m23.0828\u001b[0m       26.4010  0.0109\n",
      "     56       \u001b[36m23.0820\u001b[0m       26.4008  0.0107\n",
      "     57       \u001b[36m23.0812\u001b[0m       26.4008  0.0108\n",
      "     58       \u001b[36m23.0804\u001b[0m       26.4007  0.0110\n",
      "     59       \u001b[36m23.0796\u001b[0m       26.4006  0.0112\n",
      "     60       \u001b[36m23.0789\u001b[0m       26.4005  0.0111\n",
      "     61       \u001b[36m23.0782\u001b[0m       26.4005  0.0107\n",
      "     62       \u001b[36m23.0775\u001b[0m       26.4003  0.0107\n",
      "     63       \u001b[36m23.0768\u001b[0m       26.4003  0.0111\n",
      "     64       \u001b[36m23.0762\u001b[0m       26.4001  0.0112\n",
      "     65       \u001b[36m23.0756\u001b[0m       26.3999  0.0108\n",
      "     66       \u001b[36m23.0749\u001b[0m       26.3998  0.0107\n",
      "     67       \u001b[36m23.0743\u001b[0m       26.3996  0.0107\n",
      "     68       \u001b[36m23.0737\u001b[0m       26.3996  0.0110\n",
      "     69       \u001b[36m23.0732\u001b[0m       26.3994  0.0114\n",
      "     70       \u001b[36m23.0726\u001b[0m       26.3995  0.0112\n",
      "     71       \u001b[36m23.0720\u001b[0m       26.3995  0.0106\n",
      "     72       \u001b[36m23.0715\u001b[0m       26.3995  0.0105\n",
      "     73       \u001b[36m23.0710\u001b[0m       26.3995  0.0110\n",
      "     74       \u001b[36m23.0705\u001b[0m       26.3994  0.0112\n",
      "     75       \u001b[36m23.0700\u001b[0m       26.3992  0.0109\n",
      "     76       \u001b[36m23.0695\u001b[0m       26.3991  0.0107\n",
      "     77       \u001b[36m23.0690\u001b[0m       26.3989  0.0108\n",
      "     78       \u001b[36m23.0686\u001b[0m       26.3989  0.0131\n",
      "     79       \u001b[36m23.0681\u001b[0m       26.3988  0.0120\n",
      "     80       \u001b[36m23.0677\u001b[0m       26.3986  0.0110\n",
      "     81       \u001b[36m23.0673\u001b[0m       26.3987  0.0107\n",
      "     82       \u001b[36m23.0669\u001b[0m       26.3985  0.0107\n",
      "     83       \u001b[36m23.0665\u001b[0m       26.3985  0.0110\n",
      "     84       \u001b[36m23.0660\u001b[0m       26.3985  0.0111\n",
      "     85       \u001b[36m23.0657\u001b[0m       26.3983  0.0108\n",
      "     86       \u001b[36m23.0653\u001b[0m       26.3983  0.0109\n",
      "     87       \u001b[36m23.0649\u001b[0m       26.3981  0.0109\n",
      "     88       \u001b[36m23.0645\u001b[0m       26.3981  0.0109\n",
      "     89       \u001b[36m23.0642\u001b[0m       26.3980  0.0117\n",
      "     90       \u001b[36m23.0638\u001b[0m       26.3979  0.0107\n",
      "     91       \u001b[36m23.0634\u001b[0m       26.3978  0.0108\n",
      "     92       \u001b[36m23.0631\u001b[0m       26.3978  0.0106\n",
      "     93       \u001b[36m23.0627\u001b[0m       26.3978  0.0110\n",
      "     94       \u001b[36m23.0624\u001b[0m       26.3976  0.0112\n",
      "     95       \u001b[36m23.0620\u001b[0m       26.3977  0.0108\n",
      "     96       \u001b[36m23.0617\u001b[0m       26.3976  0.0107\n",
      "     97       \u001b[36m23.0614\u001b[0m       26.3976  0.0107\n",
      "     98       \u001b[36m23.0611\u001b[0m       26.3976  0.0109\n",
      "     99       \u001b[36m23.0607\u001b[0m       26.3973  0.0113\n",
      "    100       \u001b[36m23.0604\u001b[0m       26.3973  0.0107\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m39.4902\u001b[0m       \u001b[32m30.1697\u001b[0m  0.0106\n",
      "      2       \u001b[36m36.2979\u001b[0m       \u001b[32m28.4432\u001b[0m  0.0105\n",
      "      3       \u001b[36m33.7373\u001b[0m       \u001b[32m27.1732\u001b[0m  0.0110\n",
      "      4       \u001b[36m31.6369\u001b[0m       \u001b[32m26.3539\u001b[0m  0.0110\n",
      "      5       \u001b[36m29.9586\u001b[0m       \u001b[32m26.2334\u001b[0m  0.0108\n",
      "      6       \u001b[36m29.0144\u001b[0m       26.6279  0.0103\n",
      "      7       \u001b[36m28.7258\u001b[0m       26.9541  0.0104\n",
      "      8       \u001b[36m28.6629\u001b[0m       27.0939  0.0111\n",
      "      9       \u001b[36m28.6354\u001b[0m       27.1425  0.0115\n",
      "     10       \u001b[36m28.6130\u001b[0m       27.1578  0.0110\n",
      "     11       \u001b[36m28.5934\u001b[0m       27.1650  0.0104\n",
      "     12       \u001b[36m28.5770\u001b[0m       27.1663  0.0104\n",
      "     13       \u001b[36m28.5624\u001b[0m       27.1685  0.0161\n",
      "     14       \u001b[36m28.5500\u001b[0m       27.1685  0.0210\n",
      "     15       \u001b[36m28.5388\u001b[0m       27.1706  0.0173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     16       \u001b[36m28.5293\u001b[0m       27.1707  0.0176\n",
      "     17       \u001b[36m28.5207\u001b[0m       27.1697  0.0241\n",
      "     18       \u001b[36m28.5127\u001b[0m       27.1718  0.0137\n",
      "     19       \u001b[36m28.5057\u001b[0m       27.1733  0.0115\n",
      "     20       \u001b[36m28.4996\u001b[0m       27.1735  0.0111\n",
      "     21       \u001b[36m28.4939\u001b[0m       27.1719  0.0110\n",
      "     22       \u001b[36m28.4885\u001b[0m       27.1737  0.0117\n",
      "     23       \u001b[36m28.4838\u001b[0m       27.1740  0.0114\n",
      "     24       \u001b[36m28.4794\u001b[0m       27.1745  0.0112\n",
      "     25       \u001b[36m28.4755\u001b[0m       27.1727  0.0108\n",
      "     26       \u001b[36m28.4716\u001b[0m       27.1738  0.0109\n",
      "     27       \u001b[36m28.4681\u001b[0m       27.1746  0.0109\n",
      "     28       \u001b[36m28.4651\u001b[0m       27.1738  0.0115\n",
      "     29       \u001b[36m28.4619\u001b[0m       27.1745  0.0112\n",
      "     30       \u001b[36m28.4593\u001b[0m       27.1742  0.0108\n",
      "     31       \u001b[36m28.4567\u001b[0m       27.1748  0.0107\n",
      "     32       \u001b[36m28.4544\u001b[0m       27.1736  0.0108\n",
      "     33       \u001b[36m28.4521\u001b[0m       27.1747  0.0111\n",
      "     34       \u001b[36m28.4501\u001b[0m       27.1745  0.0111\n",
      "     35       \u001b[36m28.4481\u001b[0m       27.1733  0.0108\n",
      "     36       \u001b[36m28.4462\u001b[0m       27.1737  0.0108\n",
      "     37       \u001b[36m28.4445\u001b[0m       27.1736  0.0108\n",
      "     38       \u001b[36m28.4428\u001b[0m       27.1733  0.0111\n",
      "     39       \u001b[36m28.4413\u001b[0m       27.1720  0.0108\n",
      "     40       \u001b[36m28.4398\u001b[0m       27.1727  0.0107\n",
      "     41       \u001b[36m28.4384\u001b[0m       27.1728  0.0104\n",
      "     42       \u001b[36m28.4371\u001b[0m       27.1727  0.0108\n",
      "     43       \u001b[36m28.4359\u001b[0m       27.1726  0.0109\n",
      "     44       \u001b[36m28.4347\u001b[0m       27.1713  0.0110\n",
      "     45       \u001b[36m28.4335\u001b[0m       27.1718  0.0108\n",
      "     46       \u001b[36m28.4324\u001b[0m       27.1717  0.0110\n",
      "     47       \u001b[36m28.4313\u001b[0m       27.1714  0.0116\n",
      "     48       \u001b[36m28.4303\u001b[0m       27.1715  0.0116\n",
      "     49       \u001b[36m28.4294\u001b[0m       27.1702  0.0111\n",
      "     50       \u001b[36m28.4284\u001b[0m       27.1711  0.0111\n",
      "     51       \u001b[36m28.4275\u001b[0m       27.1711  0.0114\n",
      "     52       \u001b[36m28.4267\u001b[0m       27.1712  0.0117\n",
      "     53       \u001b[36m28.4259\u001b[0m       27.1713  0.0114\n",
      "     54       \u001b[36m28.4251\u001b[0m       27.1713  0.0113\n",
      "     55       \u001b[36m28.4243\u001b[0m       27.1713  0.0112\n",
      "     56       \u001b[36m28.4236\u001b[0m       27.1713  0.0113\n",
      "     57       \u001b[36m28.4229\u001b[0m       27.1711  0.0112\n",
      "     58       \u001b[36m28.4223\u001b[0m       27.1700  0.0113\n",
      "     59       \u001b[36m28.4216\u001b[0m       27.1710  0.0112\n",
      "     60       \u001b[36m28.4209\u001b[0m       27.1711  0.0111\n",
      "     61       \u001b[36m28.4203\u001b[0m       27.1712  0.0107\n",
      "     62       \u001b[36m28.4197\u001b[0m       27.1714  0.0109\n",
      "     63       \u001b[36m28.4192\u001b[0m       27.1715  0.0114\n",
      "     64       \u001b[36m28.4186\u001b[0m       27.1717  0.0109\n",
      "     65       \u001b[36m28.4181\u001b[0m       27.1716  0.0107\n",
      "     66       \u001b[36m28.4176\u001b[0m       27.1716  0.0106\n",
      "     67       \u001b[36m28.4171\u001b[0m       27.1716  0.0108\n",
      "     68       \u001b[36m28.4166\u001b[0m       27.1717  0.0113\n",
      "     69       \u001b[36m28.4161\u001b[0m       27.1716  0.0107\n",
      "     70       \u001b[36m28.4156\u001b[0m       27.1718  0.0108\n",
      "     71       \u001b[36m28.4152\u001b[0m       27.1715  0.0108\n",
      "     72       \u001b[36m28.4147\u001b[0m       27.1718  0.0110\n",
      "     73       \u001b[36m28.4143\u001b[0m       27.1717  0.0111\n",
      "     74       \u001b[36m28.4139\u001b[0m       27.1718  0.0110\n",
      "     75       \u001b[36m28.4135\u001b[0m       27.1723  0.0110\n",
      "     76       \u001b[36m28.4131\u001b[0m       27.1721  0.0112\n",
      "     77       \u001b[36m28.4127\u001b[0m       27.1718  0.0118\n",
      "     78       \u001b[36m28.4123\u001b[0m       27.1717  0.0119\n",
      "     79       \u001b[36m28.4119\u001b[0m       27.1715  0.0117\n",
      "     80       \u001b[36m28.4115\u001b[0m       27.1714  0.0115\n",
      "     81       \u001b[36m28.4111\u001b[0m       27.1715  0.0111\n",
      "     82       \u001b[36m28.4108\u001b[0m       27.1716  0.0113\n",
      "     83       \u001b[36m28.4104\u001b[0m       27.1717  0.0113\n",
      "     84       \u001b[36m28.4101\u001b[0m       27.1716  0.0112\n",
      "     85       \u001b[36m28.4098\u001b[0m       27.1717  0.0111\n",
      "     86       \u001b[36m28.4095\u001b[0m       27.1720  0.0110\n",
      "     87       \u001b[36m28.4092\u001b[0m       27.1718  0.0111\n",
      "     88       \u001b[36m28.4089\u001b[0m       27.1718  0.0113\n",
      "     89       \u001b[36m28.4086\u001b[0m       27.1721  0.0110\n",
      "     90       \u001b[36m28.4083\u001b[0m       27.1725  0.0109\n",
      "     91       \u001b[36m28.4080\u001b[0m       27.1726  0.0109\n",
      "     92       \u001b[36m28.4078\u001b[0m       27.1730  0.0109\n",
      "     93       \u001b[36m28.4075\u001b[0m       27.1731  0.0113\n",
      "     94       \u001b[36m28.4073\u001b[0m       27.1733  0.0136\n",
      "     95       \u001b[36m28.4070\u001b[0m       27.1734  0.0156\n",
      "     96       \u001b[36m28.4067\u001b[0m       27.1734  0.0115\n",
      "     97       \u001b[36m28.4065\u001b[0m       27.1738  0.0121\n",
      "     98       \u001b[36m28.4062\u001b[0m       27.1742  0.0117\n",
      "     99       \u001b[36m28.4060\u001b[0m       27.1741  0.0117\n",
      "    100       \u001b[36m28.4058\u001b[0m       27.1733  0.0122\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m41.3855\u001b[0m       \u001b[32m43.3569\u001b[0m  0.0132\n",
      "      2       \u001b[36m36.9319\u001b[0m       \u001b[32m31.6884\u001b[0m  0.0121\n",
      "      3       \u001b[36m34.5034\u001b[0m       32.3368  0.0137\n",
      "      4       \u001b[36m33.8917\u001b[0m       31.8899  0.0124\n",
      "      5       \u001b[36m33.2363\u001b[0m       \u001b[32m30.6165\u001b[0m  0.0121\n",
      "      6       \u001b[36m32.9906\u001b[0m       \u001b[32m30.3677\u001b[0m  0.0118\n",
      "      7       \u001b[36m32.7445\u001b[0m       30.6956  0.0119\n",
      "      8       \u001b[36m32.6380\u001b[0m       \u001b[32m30.2997\u001b[0m  0.0121\n",
      "      9       \u001b[36m32.4992\u001b[0m       \u001b[32m30.1013\u001b[0m  0.0120\n",
      "     10       \u001b[36m32.4464\u001b[0m       30.3369  0.0119\n",
      "     11       \u001b[36m32.4043\u001b[0m       30.3258  0.0119\n",
      "     12       \u001b[36m32.3440\u001b[0m       30.1507  0.0118\n",
      "     13       \u001b[36m32.3174\u001b[0m       30.1898  0.0116\n",
      "     14       \u001b[36m32.3055\u001b[0m       30.1857  0.0120\n",
      "     15       \u001b[36m32.2809\u001b[0m       30.1117  0.0117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     16       \u001b[36m32.2672\u001b[0m       30.1342  0.0117\n",
      "     17       \u001b[36m32.2599\u001b[0m       30.1263  0.0113\n",
      "     18       \u001b[36m32.2476\u001b[0m       \u001b[32m30.0766\u001b[0m  0.0119\n",
      "     19       \u001b[36m32.2393\u001b[0m       \u001b[32m30.0726\u001b[0m  0.0117\n",
      "     20       \u001b[36m32.2339\u001b[0m       \u001b[32m30.0682\u001b[0m  0.0118\n",
      "     21       \u001b[36m32.2260\u001b[0m       \u001b[32m30.0493\u001b[0m  0.0112\n",
      "     22       \u001b[36m32.2203\u001b[0m       \u001b[32m30.0454\u001b[0m  0.0114\n",
      "     23       \u001b[36m32.2161\u001b[0m       \u001b[32m30.0439\u001b[0m  0.0118\n",
      "     24       \u001b[36m32.2113\u001b[0m       \u001b[32m30.0347\u001b[0m  0.0116\n",
      "     25       \u001b[36m32.2068\u001b[0m       \u001b[32m30.0274\u001b[0m  0.0115\n",
      "     26       \u001b[36m32.2032\u001b[0m       \u001b[32m30.0230\u001b[0m  0.0111\n",
      "     27       \u001b[36m32.1988\u001b[0m       \u001b[32m30.0157\u001b[0m  0.0113\n",
      "     28       \u001b[36m32.1951\u001b[0m       \u001b[32m30.0123\u001b[0m  0.0121\n",
      "     29       \u001b[36m32.1919\u001b[0m       \u001b[32m30.0092\u001b[0m  0.0123\n",
      "     30       \u001b[36m32.1887\u001b[0m       \u001b[32m30.0019\u001b[0m  0.0116\n",
      "     31       \u001b[36m32.1853\u001b[0m       \u001b[32m29.9999\u001b[0m  0.0112\n",
      "     32       \u001b[36m32.1826\u001b[0m       \u001b[32m29.9983\u001b[0m  0.0112\n",
      "     33       \u001b[36m32.1797\u001b[0m       \u001b[32m29.9947\u001b[0m  0.0122\n",
      "     34       \u001b[36m32.1765\u001b[0m       29.9949  0.0117\n",
      "     35       \u001b[36m32.1743\u001b[0m       29.9948  0.0116\n",
      "     36       \u001b[36m32.1712\u001b[0m       \u001b[32m29.9936\u001b[0m  0.0116\n",
      "     37       \u001b[36m32.1691\u001b[0m       29.9947  0.0120\n",
      "     38       \u001b[36m32.1659\u001b[0m       \u001b[32m29.9901\u001b[0m  0.0113\n",
      "     39       \u001b[36m32.1637\u001b[0m       \u001b[32m29.9885\u001b[0m  0.0125\n",
      "     40       \u001b[36m32.1610\u001b[0m       \u001b[32m29.9874\u001b[0m  0.0116\n",
      "     41       \u001b[36m32.1585\u001b[0m       \u001b[32m29.9852\u001b[0m  0.0130\n",
      "     42       \u001b[36m32.1564\u001b[0m       29.9912  0.0120\n",
      "     43       \u001b[36m32.1540\u001b[0m       29.9876  0.0112\n",
      "     44       \u001b[36m32.1520\u001b[0m       29.9948  0.0117\n",
      "     45       \u001b[36m32.1499\u001b[0m       29.9909  0.0118\n",
      "     46       \u001b[36m32.1480\u001b[0m       29.9972  0.0116\n",
      "     47       \u001b[36m32.1462\u001b[0m       29.9929  0.0114\n",
      "     48       \u001b[36m32.1441\u001b[0m       29.9984  0.0113\n",
      "     49       \u001b[36m32.1424\u001b[0m       29.9900  0.0117\n",
      "     50       \u001b[36m32.1412\u001b[0m       30.0040  0.0119\n",
      "     51       \u001b[36m32.1386\u001b[0m       29.9928  0.0116\n",
      "     52       \u001b[36m32.1378\u001b[0m       30.0085  0.0121\n",
      "     53       \u001b[36m32.1349\u001b[0m       29.9928  0.0114\n",
      "     54       \u001b[36m32.1341\u001b[0m       30.0084  0.0118\n",
      "     55       \u001b[36m32.1312\u001b[0m       30.0008  0.0126\n",
      "     56       \u001b[36m32.1299\u001b[0m       30.0085  0.0145\n",
      "     57       \u001b[36m32.1275\u001b[0m       30.0118  0.0116\n",
      "     58       \u001b[36m32.1258\u001b[0m       30.0129  0.0114\n",
      "     59       \u001b[36m32.1240\u001b[0m       30.0219  0.0124\n",
      "     60       \u001b[36m32.1215\u001b[0m       30.0227  0.0115\n",
      "     61       \u001b[36m32.1207\u001b[0m       30.0329  0.0118\n",
      "     62       \u001b[36m32.1175\u001b[0m       30.0361  0.0114\n",
      "     63       \u001b[36m32.1168\u001b[0m       30.0355  0.0112\n",
      "     64       \u001b[36m32.1153\u001b[0m       30.0468  0.0117\n",
      "     65       \u001b[36m32.1129\u001b[0m       30.0505  0.0117\n",
      "     66       \u001b[36m32.1127\u001b[0m       30.0469  0.0118\n",
      "     67       \u001b[36m32.1095\u001b[0m       30.0673  0.0113\n",
      "     68       \u001b[36m32.1086\u001b[0m       30.0532  0.0112\n",
      "     69       \u001b[36m32.1082\u001b[0m       30.0603  0.0115\n",
      "     70       \u001b[36m32.1038\u001b[0m       30.0920  0.0119\n",
      "     71       32.1053       30.0447  0.0115\n",
      "     72       32.1053       30.0857  0.0111\n",
      "     73       \u001b[36m32.0998\u001b[0m       30.1122  0.0127\n",
      "     74       32.1047       30.0275  0.0225\n",
      "     75       32.1059       30.2029  0.0136\n",
      "     76       32.1073       30.0715  0.0137\n",
      "     77       32.1145       30.0188  0.0152\n",
      "     78       \u001b[36m32.0972\u001b[0m       30.1884  0.0209\n",
      "     79       32.1128       30.0452  0.0137\n",
      "     80       32.1173       30.2853  0.0125\n",
      "     81       32.1026       30.0255  0.0120\n",
      "     82       32.1080       30.2836  0.0119\n",
      "     83       \u001b[36m32.0944\u001b[0m       29.9870  0.0118\n",
      "     84       32.1105       30.3280  0.0122\n",
      "     85       32.0956       \u001b[32m29.9764\u001b[0m  0.0121\n",
      "     86       32.1043       30.3795  0.0120\n",
      "     87       32.0972       \u001b[32m29.9717\u001b[0m  0.0125\n",
      "     88       32.1013       30.3667  0.0121\n",
      "     89       32.1005       \u001b[32m29.9662\u001b[0m  0.0120\n",
      "     90       32.1033       30.3264  0.0118\n",
      "     91       32.0956       29.9700  0.0113\n",
      "     92       \u001b[36m32.0906\u001b[0m       30.2706  0.0120\n",
      "     93       32.0911       29.9944  0.0112\n",
      "     94       \u001b[36m32.0814\u001b[0m       30.2258  0.0116\n",
      "     95       32.0839       30.0370  0.0120\n",
      "     96       \u001b[36m32.0730\u001b[0m       30.1866  0.0118\n",
      "     97       32.0783       30.0722  0.0115\n",
      "     98       \u001b[36m32.0685\u001b[0m       30.1689  0.0121\n",
      "     99       32.0733       30.0967  0.0116\n",
      "    100       \u001b[36m32.0658\u001b[0m       30.1756  0.0114\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m31.0575\u001b[0m       \u001b[32m26.8297\u001b[0m  0.0118\n",
      "      2       \u001b[36m25.4212\u001b[0m       28.5311  0.0116\n",
      "      3       \u001b[36m24.0830\u001b[0m       \u001b[32m26.4226\u001b[0m  0.0121\n",
      "      4       24.1614       26.6032  0.0120\n",
      "      5       \u001b[36m23.5876\u001b[0m       27.9705  0.0117\n",
      "      6       \u001b[36m23.4399\u001b[0m       26.6872  0.0117\n",
      "      7       \u001b[36m23.4086\u001b[0m       26.5856  0.0118\n",
      "      8       \u001b[36m23.2818\u001b[0m       27.1667  0.0114\n",
      "      9       \u001b[36m23.2013\u001b[0m       26.7225  0.0114\n",
      "     10       \u001b[36m23.1771\u001b[0m       26.5859  0.0124\n",
      "     11       \u001b[36m23.1529\u001b[0m       26.8043  0.0117\n",
      "     12       \u001b[36m23.1156\u001b[0m       26.6401  0.0116\n",
      "     13       \u001b[36m23.1047\u001b[0m       26.5333  0.0115\n",
      "     14       \u001b[36m23.0969\u001b[0m       26.6529  0.0117\n",
      "     15       \u001b[36m23.0755\u001b[0m       26.5453  0.0118\n",
      "     16       \u001b[36m23.0705\u001b[0m       26.5151  0.0116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m23.0609\u001b[0m       26.5658  0.0122\n",
      "     18       \u001b[36m23.0511\u001b[0m       26.5119  0.0116\n",
      "     19       \u001b[36m23.0491\u001b[0m       26.5040  0.0112\n",
      "     20       \u001b[36m23.0419\u001b[0m       26.5264  0.0120\n",
      "     21       \u001b[36m23.0393\u001b[0m       26.4978  0.0119\n",
      "     22       23.0397       26.5080  0.0118\n",
      "     23       \u001b[36m23.0338\u001b[0m       26.5040  0.0113\n",
      "     24       23.0402       26.5019  0.0113\n",
      "     25       23.0446       26.5090  0.0117\n",
      "     26       23.0394       26.5023  0.0115\n",
      "     27       23.0428       26.5037  0.0115\n",
      "     28       \u001b[36m23.0175\u001b[0m       26.4953  0.0111\n",
      "     29       \u001b[36m23.0169\u001b[0m       26.5026  0.0113\n",
      "     30       \u001b[36m23.0124\u001b[0m       26.5082  0.0116\n",
      "     31       \u001b[36m23.0112\u001b[0m       26.5047  0.0120\n",
      "     32       \u001b[36m23.0072\u001b[0m       26.5052  0.0117\n",
      "     33       \u001b[36m23.0047\u001b[0m       26.5124  0.0115\n",
      "     34       \u001b[36m23.0020\u001b[0m       26.5119  0.0114\n",
      "     35       \u001b[36m23.0009\u001b[0m       26.5192  0.0118\n",
      "     36       \u001b[36m22.9980\u001b[0m       26.5152  0.0118\n",
      "     37       \u001b[36m22.9972\u001b[0m       26.5274  0.0120\n",
      "     38       \u001b[36m22.9950\u001b[0m       26.5194  0.0114\n",
      "     39       \u001b[36m22.9932\u001b[0m       26.5280  0.0115\n",
      "     40       \u001b[36m22.9912\u001b[0m       26.5232  0.0120\n",
      "     41       \u001b[36m22.9903\u001b[0m       26.5375  0.0122\n",
      "     42       \u001b[36m22.9875\u001b[0m       26.5332  0.0116\n",
      "     43       \u001b[36m22.9872\u001b[0m       26.5394  0.0112\n",
      "     44       \u001b[36m22.9856\u001b[0m       26.5437  0.0113\n",
      "     45       \u001b[36m22.9829\u001b[0m       26.5409  0.0121\n",
      "     46       22.9840       26.5495  0.0119\n",
      "     47       22.9833       26.5522  0.0117\n",
      "     48       \u001b[36m22.9816\u001b[0m       26.5671  0.0116\n",
      "     49       22.9852       26.5312  0.0114\n",
      "     50       22.9933       26.5801  0.0113\n",
      "     51       22.9910       26.5685  0.0171\n",
      "     52       23.0012       26.5450  0.0151\n",
      "     53       23.0098       26.5698  0.0130\n",
      "     54       22.9901       26.5677  0.0125\n",
      "     55       22.9840       26.5692  0.0124\n",
      "     56       22.9823       26.5775  0.0161\n",
      "     57       \u001b[36m22.9699\u001b[0m       26.5591  0.0123\n",
      "     58       \u001b[36m22.9695\u001b[0m       26.5686  0.0126\n",
      "     59       \u001b[36m22.9659\u001b[0m       26.5891  0.0121\n",
      "     60       \u001b[36m22.9649\u001b[0m       26.5684  0.0119\n",
      "     61       \u001b[36m22.9630\u001b[0m       26.6005  0.0118\n",
      "     62       \u001b[36m22.9608\u001b[0m       26.5798  0.0118\n",
      "     63       \u001b[36m22.9604\u001b[0m       26.5916  0.0119\n",
      "     64       \u001b[36m22.9574\u001b[0m       26.6002  0.0119\n",
      "     65       22.9574       26.6020  0.0114\n",
      "     66       \u001b[36m22.9552\u001b[0m       26.6018  0.0113\n",
      "     67       \u001b[36m22.9542\u001b[0m       26.6145  0.0123\n",
      "     68       \u001b[36m22.9532\u001b[0m       26.6125  0.0122\n",
      "     69       \u001b[36m22.9528\u001b[0m       26.6151  0.0119\n",
      "     70       \u001b[36m22.9517\u001b[0m       26.6371  0.0115\n",
      "     71       \u001b[36m22.9489\u001b[0m       26.6213  0.0116\n",
      "     72       22.9496       26.6346  0.0119\n",
      "     73       \u001b[36m22.9475\u001b[0m       26.6414  0.0120\n",
      "     74       \u001b[36m22.9474\u001b[0m       26.6440  0.0118\n",
      "     75       22.9482       26.6351  0.0118\n",
      "     76       22.9530       26.6682  0.0118\n",
      "     77       22.9495       26.6707  0.0114\n",
      "     78       22.9679       26.6395  0.0115\n",
      "     79       22.9818       26.6731  0.0119\n",
      "     80       22.9770       26.6659  0.0119\n",
      "     81       22.9871       26.6477  0.0114\n",
      "     82       22.9877       26.6742  0.0116\n",
      "     83       22.9592       26.5884  0.0121\n",
      "     84       22.9549       26.6847  0.0120\n",
      "     85       \u001b[36m22.9466\u001b[0m       26.6641  0.0118\n",
      "     86       \u001b[36m22.9465\u001b[0m       26.6655  0.0115\n",
      "     87       \u001b[36m22.9400\u001b[0m       26.6998  0.0115\n",
      "     88       \u001b[36m22.9399\u001b[0m       26.6813  0.0125\n",
      "     89       \u001b[36m22.9378\u001b[0m       26.6946  0.0122\n",
      "     90       \u001b[36m22.9373\u001b[0m       26.6933  0.0118\n",
      "     91       22.9400       26.7116  0.0115\n",
      "     92       \u001b[36m22.9315\u001b[0m       26.7088  0.0116\n",
      "     93       22.9363       26.7367  0.0129\n",
      "     94       \u001b[36m22.9299\u001b[0m       26.7128  0.0118\n",
      "     95       22.9309       26.7697  0.0118\n",
      "     96       \u001b[36m22.9288\u001b[0m       26.7356  0.0117\n",
      "     97       \u001b[36m22.9275\u001b[0m       26.7584  0.0124\n",
      "     98       \u001b[36m22.9266\u001b[0m       26.7918  0.0134\n",
      "     99       \u001b[36m22.9254\u001b[0m       26.7601  0.0122\n",
      "    100       \u001b[36m22.9249\u001b[0m       26.7982  0.0118\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m37.3221\u001b[0m       \u001b[32m26.9545\u001b[0m  0.0122\n",
      "      2       \u001b[36m31.1525\u001b[0m       28.5871  0.0119\n",
      "      3       \u001b[36m29.5433\u001b[0m       \u001b[32m26.3881\u001b[0m  0.0117\n",
      "      4       \u001b[36m29.1496\u001b[0m       27.9827  0.0119\n",
      "      5       29.1691       27.8208  0.0117\n",
      "      6       \u001b[36m28.7468\u001b[0m       27.2755  0.0119\n",
      "      7       \u001b[36m28.6478\u001b[0m       27.7400  0.0118\n",
      "      8       28.6613       27.3654  0.0119\n",
      "      9       \u001b[36m28.5461\u001b[0m       27.1203  0.0112\n",
      "     10       \u001b[36m28.5042\u001b[0m       27.5350  0.0112\n",
      "     11       28.5244       27.5127  0.0118\n",
      "     12       \u001b[36m28.4802\u001b[0m       27.2044  0.0119\n",
      "     13       \u001b[36m28.4569\u001b[0m       27.4110  0.0116\n",
      "     14       28.4690       27.3805  0.0114\n",
      "     15       \u001b[36m28.4435\u001b[0m       27.3574  0.0115\n",
      "     16       \u001b[36m28.4418\u001b[0m       27.4066  0.0118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m28.4385\u001b[0m       27.3466  0.0137\n",
      "     18       \u001b[36m28.4291\u001b[0m       27.3671  0.0125\n",
      "     19       \u001b[36m28.4282\u001b[0m       27.4028  0.0117\n",
      "     20       \u001b[36m28.4261\u001b[0m       27.3244  0.0114\n",
      "     21       \u001b[36m28.4202\u001b[0m       27.3822  0.0112\n",
      "     22       28.4207       27.3210  0.0124\n",
      "     23       \u001b[36m28.4164\u001b[0m       27.3358  0.0117\n",
      "     24       \u001b[36m28.4131\u001b[0m       27.3362  0.0115\n",
      "     25       \u001b[36m28.4128\u001b[0m       27.3038  0.0116\n",
      "     26       \u001b[36m28.4078\u001b[0m       27.3267  0.0117\n",
      "     27       28.4093       27.2918  0.0114\n",
      "     28       \u001b[36m28.4059\u001b[0m       27.3005  0.0118\n",
      "     29       \u001b[36m28.4049\u001b[0m       27.2922  0.0185\n",
      "     30       28.4055       27.2802  0.0169\n",
      "     31       \u001b[36m28.3998\u001b[0m       27.2887  0.0122\n",
      "     32       28.4050       27.2465  0.0127\n",
      "     33       \u001b[36m28.3955\u001b[0m       27.3025  0.0137\n",
      "     34       28.4032       27.2252  0.0177\n",
      "     35       28.3999       27.2939  0.0133\n",
      "     36       28.4109       27.2167  0.0120\n",
      "     37       28.4070       27.2624  0.0119\n",
      "     38       28.3974       27.2293  0.0120\n",
      "     39       28.4042       27.2280  0.0124\n",
      "     40       \u001b[36m28.3920\u001b[0m       27.3159  0.0119\n",
      "     41       28.4002       27.1834  0.0117\n",
      "     42       \u001b[36m28.3884\u001b[0m       27.2949  0.0118\n",
      "     43       28.3936       27.1912  0.0118\n",
      "     44       \u001b[36m28.3866\u001b[0m       27.2717  0.0120\n",
      "     45       28.3901       27.1864  0.0118\n",
      "     46       \u001b[36m28.3850\u001b[0m       27.2661  0.0118\n",
      "     47       28.3901       27.1863  0.0117\n",
      "     48       \u001b[36m28.3848\u001b[0m       27.2542  0.0118\n",
      "     49       28.3933       27.1816  0.0118\n",
      "     50       \u001b[36m28.3822\u001b[0m       27.2352  0.0119\n",
      "     51       28.3874       27.2145  0.0119\n",
      "     52       28.3829       27.2101  0.0118\n",
      "     53       \u001b[36m28.3807\u001b[0m       27.2021  0.0117\n",
      "     54       28.3808       27.2059  0.0115\n",
      "     55       \u001b[36m28.3801\u001b[0m       27.1961  0.0113\n",
      "     56       28.3834       27.1789  0.0119\n",
      "     57       28.3802       27.2244  0.0115\n",
      "     58       \u001b[36m28.3796\u001b[0m       27.1573  0.0115\n",
      "     59       \u001b[36m28.3791\u001b[0m       27.2132  0.0117\n",
      "     60       \u001b[36m28.3740\u001b[0m       27.1817  0.0112\n",
      "     61       28.3795       27.1718  0.0120\n",
      "     62       28.3748       27.1721  0.0120\n",
      "     63       28.3787       27.1888  0.0115\n",
      "     64       28.3780       27.1350  0.0115\n",
      "     65       \u001b[36m28.3737\u001b[0m       27.1802  0.0116\n",
      "     66       28.3784       27.1284  0.0128\n",
      "     67       \u001b[36m28.3706\u001b[0m       27.1734  0.0117\n",
      "     68       28.3726       27.1438  0.0116\n",
      "     69       28.3735       27.1315  0.0119\n",
      "     70       \u001b[36m28.3669\u001b[0m       27.1863  0.0119\n",
      "     71       28.3721       27.0952  0.0115\n",
      "     72       \u001b[36m28.3645\u001b[0m       27.1862  0.0117\n",
      "     73       28.3670       27.1217  0.0121\n",
      "     74       28.3657       27.1539  0.0122\n",
      "     75       \u001b[36m28.3638\u001b[0m       27.1443  0.0117\n",
      "     76       28.3652       27.1429  0.0112\n",
      "     77       \u001b[36m28.3617\u001b[0m       27.1536  0.0113\n",
      "     78       28.3643       27.1534  0.0120\n",
      "     79       28.3618       27.1209  0.0118\n",
      "     80       28.3627       27.1810  0.0115\n",
      "     81       28.3624       27.0962  0.0112\n",
      "     82       28.3634       27.1957  0.0112\n",
      "     83       28.3619       27.0781  0.0121\n",
      "     84       28.3641       27.1399  0.0117\n",
      "     85       28.3660       27.0721  0.0117\n",
      "     86       28.3759       27.2649  0.0116\n",
      "     87       28.3860       27.0359  0.0115\n",
      "     88       28.3774       27.1258  0.0118\n",
      "     89       28.3643       27.1276  0.0117\n",
      "     90       28.3747       27.0445  0.0115\n",
      "     91       28.3624       27.1408  0.0112\n",
      "     92       28.3633       27.0558  0.0115\n",
      "     93       28.3635       27.1116  0.0120\n",
      "     94       \u001b[36m28.3569\u001b[0m       27.0954  0.0116\n",
      "     95       28.3648       27.0756  0.0119\n",
      "     96       28.3582       27.0992  0.0114\n",
      "     97       28.3648       27.1073  0.0114\n",
      "     98       28.3623       27.0835  0.0113\n",
      "     99       28.3627       27.1271  0.0113\n",
      "    100       28.3621       27.0706  0.0122\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m42.0656\u001b[0m       \u001b[32m41.0418\u001b[0m  0.0109\n",
      "      2       \u001b[36m38.4854\u001b[0m       \u001b[32m36.7038\u001b[0m  0.0113\n",
      "      3       \u001b[36m35.4263\u001b[0m       \u001b[32m33.0603\u001b[0m  0.0113\n",
      "      4       \u001b[36m33.4883\u001b[0m       \u001b[32m31.1740\u001b[0m  0.0108\n",
      "      5       \u001b[36m32.8667\u001b[0m       \u001b[32m30.5694\u001b[0m  0.0107\n",
      "      6       \u001b[36m32.7208\u001b[0m       \u001b[32m30.3720\u001b[0m  0.0106\n",
      "      7       \u001b[36m32.6480\u001b[0m       \u001b[32m30.2816\u001b[0m  0.0109\n",
      "      8       \u001b[36m32.5931\u001b[0m       \u001b[32m30.2235\u001b[0m  0.0156\n",
      "      9       \u001b[36m32.5496\u001b[0m       \u001b[32m30.1794\u001b[0m  0.0140\n",
      "     10       \u001b[36m32.5140\u001b[0m       \u001b[32m30.1432\u001b[0m  0.0118\n",
      "     11       \u001b[36m32.4847\u001b[0m       \u001b[32m30.1119\u001b[0m  0.0114\n",
      "     12       \u001b[36m32.4603\u001b[0m       \u001b[32m30.0854\u001b[0m  0.0122\n",
      "     13       \u001b[36m32.4395\u001b[0m       \u001b[32m30.0629\u001b[0m  0.0117\n",
      "     14       \u001b[36m32.4216\u001b[0m       \u001b[32m30.0439\u001b[0m  0.0131\n",
      "     15       \u001b[36m32.4065\u001b[0m       \u001b[32m30.0272\u001b[0m  0.0138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     16       \u001b[36m32.3935\u001b[0m       \u001b[32m30.0129\u001b[0m  0.0130\n",
      "     17       \u001b[36m32.3820\u001b[0m       \u001b[32m29.9993\u001b[0m  0.0131\n",
      "     18       \u001b[36m32.3718\u001b[0m       \u001b[32m29.9873\u001b[0m  0.0125\n",
      "     19       \u001b[36m32.3629\u001b[0m       \u001b[32m29.9768\u001b[0m  0.0111\n",
      "     20       \u001b[36m32.3554\u001b[0m       \u001b[32m29.9671\u001b[0m  0.0112\n",
      "     21       \u001b[36m32.3485\u001b[0m       \u001b[32m29.9588\u001b[0m  0.0114\n",
      "     22       \u001b[36m32.3424\u001b[0m       \u001b[32m29.9509\u001b[0m  0.0112\n",
      "     23       \u001b[36m32.3369\u001b[0m       \u001b[32m29.9445\u001b[0m  0.0112\n",
      "     24       \u001b[36m32.3318\u001b[0m       \u001b[32m29.9387\u001b[0m  0.0112\n",
      "     25       \u001b[36m32.3273\u001b[0m       \u001b[32m29.9321\u001b[0m  0.0111\n",
      "     26       \u001b[36m32.3232\u001b[0m       \u001b[32m29.9270\u001b[0m  0.0115\n",
      "     27       \u001b[36m32.3194\u001b[0m       \u001b[32m29.9225\u001b[0m  0.0112\n",
      "     28       \u001b[36m32.3159\u001b[0m       \u001b[32m29.9183\u001b[0m  0.0113\n",
      "     29       \u001b[36m32.3127\u001b[0m       \u001b[32m29.9140\u001b[0m  0.0111\n",
      "     30       \u001b[36m32.3095\u001b[0m       \u001b[32m29.9098\u001b[0m  0.0108\n",
      "     31       \u001b[36m32.3068\u001b[0m       \u001b[32m29.9067\u001b[0m  0.0111\n",
      "     32       \u001b[36m32.3039\u001b[0m       \u001b[32m29.9030\u001b[0m  0.0114\n",
      "     33       \u001b[36m32.3015\u001b[0m       \u001b[32m29.8997\u001b[0m  0.0114\n",
      "     34       \u001b[36m32.2989\u001b[0m       \u001b[32m29.8966\u001b[0m  0.0109\n",
      "     35       \u001b[36m32.2966\u001b[0m       \u001b[32m29.8936\u001b[0m  0.0109\n",
      "     36       \u001b[36m32.2945\u001b[0m       \u001b[32m29.8908\u001b[0m  0.0107\n",
      "     37       \u001b[36m32.2922\u001b[0m       \u001b[32m29.8881\u001b[0m  0.0110\n",
      "     38       \u001b[36m32.2902\u001b[0m       \u001b[32m29.8858\u001b[0m  0.0112\n",
      "     39       \u001b[36m32.2881\u001b[0m       \u001b[32m29.8828\u001b[0m  0.0108\n",
      "     40       \u001b[36m32.2863\u001b[0m       \u001b[32m29.8808\u001b[0m  0.0109\n",
      "     41       \u001b[36m32.2844\u001b[0m       \u001b[32m29.8785\u001b[0m  0.0111\n",
      "     42       \u001b[36m32.2826\u001b[0m       \u001b[32m29.8758\u001b[0m  0.0111\n",
      "     43       \u001b[36m32.2811\u001b[0m       \u001b[32m29.8740\u001b[0m  0.0110\n",
      "     44       \u001b[36m32.2793\u001b[0m       \u001b[32m29.8722\u001b[0m  0.0107\n",
      "     45       \u001b[36m32.2778\u001b[0m       \u001b[32m29.8703\u001b[0m  0.0107\n",
      "     46       \u001b[36m32.2761\u001b[0m       \u001b[32m29.8686\u001b[0m  0.0108\n",
      "     47       \u001b[36m32.2747\u001b[0m       \u001b[32m29.8670\u001b[0m  0.0113\n",
      "     48       \u001b[36m32.2732\u001b[0m       \u001b[32m29.8649\u001b[0m  0.0114\n",
      "     49       \u001b[36m32.2720\u001b[0m       \u001b[32m29.8633\u001b[0m  0.0107\n",
      "     50       \u001b[36m32.2705\u001b[0m       \u001b[32m29.8615\u001b[0m  0.0108\n",
      "     51       \u001b[36m32.2694\u001b[0m       \u001b[32m29.8602\u001b[0m  0.0112\n",
      "     52       \u001b[36m32.2681\u001b[0m       \u001b[32m29.8589\u001b[0m  0.0110\n",
      "     53       \u001b[36m32.2670\u001b[0m       \u001b[32m29.8572\u001b[0m  0.0110\n",
      "     54       \u001b[36m32.2658\u001b[0m       \u001b[32m29.8561\u001b[0m  0.0109\n",
      "     55       \u001b[36m32.2646\u001b[0m       \u001b[32m29.8549\u001b[0m  0.0106\n",
      "     56       \u001b[36m32.2636\u001b[0m       \u001b[32m29.8539\u001b[0m  0.0109\n",
      "     57       \u001b[36m32.2624\u001b[0m       \u001b[32m29.8523\u001b[0m  0.0110\n",
      "     58       \u001b[36m32.2615\u001b[0m       \u001b[32m29.8514\u001b[0m  0.0117\n",
      "     59       \u001b[36m32.2604\u001b[0m       \u001b[32m29.8503\u001b[0m  0.0119\n",
      "     60       \u001b[36m32.2594\u001b[0m       \u001b[32m29.8491\u001b[0m  0.0108\n",
      "     61       \u001b[36m32.2584\u001b[0m       \u001b[32m29.8483\u001b[0m  0.0113\n",
      "     62       \u001b[36m32.2575\u001b[0m       \u001b[32m29.8470\u001b[0m  0.0110\n",
      "     63       \u001b[36m32.2565\u001b[0m       \u001b[32m29.8462\u001b[0m  0.0110\n",
      "     64       \u001b[36m32.2557\u001b[0m       \u001b[32m29.8456\u001b[0m  0.0106\n",
      "     65       \u001b[36m32.2548\u001b[0m       \u001b[32m29.8447\u001b[0m  0.0107\n",
      "     66       \u001b[36m32.2539\u001b[0m       \u001b[32m29.8439\u001b[0m  0.0109\n",
      "     67       \u001b[36m32.2531\u001b[0m       \u001b[32m29.8437\u001b[0m  0.0113\n",
      "     68       \u001b[36m32.2522\u001b[0m       \u001b[32m29.8428\u001b[0m  0.0111\n",
      "     69       \u001b[36m32.2515\u001b[0m       \u001b[32m29.8422\u001b[0m  0.0107\n",
      "     70       \u001b[36m32.2507\u001b[0m       \u001b[32m29.8415\u001b[0m  0.0110\n",
      "     71       \u001b[36m32.2499\u001b[0m       \u001b[32m29.8409\u001b[0m  0.0109\n",
      "     72       \u001b[36m32.2492\u001b[0m       \u001b[32m29.8404\u001b[0m  0.0113\n",
      "     73       \u001b[36m32.2484\u001b[0m       \u001b[32m29.8399\u001b[0m  0.0108\n",
      "     74       \u001b[36m32.2477\u001b[0m       \u001b[32m29.8393\u001b[0m  0.0109\n",
      "     75       \u001b[36m32.2470\u001b[0m       \u001b[32m29.8388\u001b[0m  0.0107\n",
      "     76       \u001b[36m32.2463\u001b[0m       \u001b[32m29.8382\u001b[0m  0.0113\n",
      "     77       \u001b[36m32.2456\u001b[0m       \u001b[32m29.8378\u001b[0m  0.0113\n",
      "     78       \u001b[36m32.2450\u001b[0m       \u001b[32m29.8374\u001b[0m  0.0110\n",
      "     79       \u001b[36m32.2443\u001b[0m       \u001b[32m29.8367\u001b[0m  0.0108\n",
      "     80       \u001b[36m32.2437\u001b[0m       \u001b[32m29.8361\u001b[0m  0.0108\n",
      "     81       \u001b[36m32.2430\u001b[0m       \u001b[32m29.8357\u001b[0m  0.0110\n",
      "     82       \u001b[36m32.2423\u001b[0m       \u001b[32m29.8353\u001b[0m  0.0115\n",
      "     83       \u001b[36m32.2417\u001b[0m       \u001b[32m29.8345\u001b[0m  0.0109\n",
      "     84       \u001b[36m32.2411\u001b[0m       \u001b[32m29.8344\u001b[0m  0.0113\n",
      "     85       \u001b[36m32.2405\u001b[0m       \u001b[32m29.8340\u001b[0m  0.0109\n",
      "     86       \u001b[36m32.2398\u001b[0m       \u001b[32m29.8338\u001b[0m  0.0110\n",
      "     87       \u001b[36m32.2392\u001b[0m       \u001b[32m29.8331\u001b[0m  0.0113\n",
      "     88       \u001b[36m32.2388\u001b[0m       \u001b[32m29.8330\u001b[0m  0.0114\n",
      "     89       \u001b[36m32.2381\u001b[0m       \u001b[32m29.8326\u001b[0m  0.0109\n",
      "     90       \u001b[36m32.2375\u001b[0m       \u001b[32m29.8318\u001b[0m  0.0109\n",
      "     91       \u001b[36m32.2370\u001b[0m       29.8319  0.0116\n",
      "     92       \u001b[36m32.2364\u001b[0m       \u001b[32m29.8312\u001b[0m  0.0216\n",
      "     93       \u001b[36m32.2358\u001b[0m       \u001b[32m29.8305\u001b[0m  0.0129\n",
      "     94       \u001b[36m32.2354\u001b[0m       \u001b[32m29.8305\u001b[0m  0.0124\n",
      "     95       \u001b[36m32.2348\u001b[0m       \u001b[32m29.8298\u001b[0m  0.0124\n",
      "     96       \u001b[36m32.2343\u001b[0m       \u001b[32m29.8297\u001b[0m  0.0122\n",
      "     97       \u001b[36m32.2337\u001b[0m       \u001b[32m29.8291\u001b[0m  0.0158\n",
      "     98       \u001b[36m32.2333\u001b[0m       \u001b[32m29.8289\u001b[0m  0.0120\n",
      "     99       \u001b[36m32.2327\u001b[0m       \u001b[32m29.8285\u001b[0m  0.0117\n",
      "    100       \u001b[36m32.2323\u001b[0m       \u001b[32m29.8284\u001b[0m  0.0111\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m32.6807\u001b[0m       \u001b[32m30.0590\u001b[0m  0.0111\n",
      "      2       \u001b[36m29.5153\u001b[0m       \u001b[32m28.2566\u001b[0m  0.0114\n",
      "      3       \u001b[36m27.1616\u001b[0m       \u001b[32m27.0723\u001b[0m  0.0110\n",
      "      4       \u001b[36m25.4434\u001b[0m       \u001b[32m26.4188\u001b[0m  0.0112\n",
      "      5       \u001b[36m24.2961\u001b[0m       \u001b[32m26.2075\u001b[0m  0.0109\n",
      "      6       \u001b[36m23.6643\u001b[0m       26.2594  0.0109\n",
      "      7       \u001b[36m23.3983\u001b[0m       26.3580  0.0112\n",
      "      8       \u001b[36m23.3011\u001b[0m       26.4196  0.0110\n",
      "      9       \u001b[36m23.2599\u001b[0m       26.4454  0.0112\n",
      "     10       \u001b[36m23.2359\u001b[0m       26.4545  0.0109\n",
      "     11       \u001b[36m23.2188\u001b[0m       26.4575  0.0107\n",
      "     12       \u001b[36m23.2051\u001b[0m       26.4585  0.0110\n",
      "     13       \u001b[36m23.1933\u001b[0m       26.4585  0.0116\n",
      "     14       \u001b[36m23.1831\u001b[0m       26.4583  0.0109\n",
      "     15       \u001b[36m23.1741\u001b[0m       26.4579  0.0109\n",
      "     16       \u001b[36m23.1662\u001b[0m       26.4574  0.0109\n",
      "     17       \u001b[36m23.1591\u001b[0m       26.4568  0.0110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     18       \u001b[36m23.1527\u001b[0m       26.4561  0.0117\n",
      "     19       \u001b[36m23.1468\u001b[0m       26.4552  0.0110\n",
      "     20       \u001b[36m23.1414\u001b[0m       26.4548  0.0112\n",
      "     21       \u001b[36m23.1366\u001b[0m       26.4542  0.0107\n",
      "     22       \u001b[36m23.1322\u001b[0m       26.4542  0.0108\n",
      "     23       \u001b[36m23.1282\u001b[0m       26.4540  0.0113\n",
      "     24       \u001b[36m23.1246\u001b[0m       26.4538  0.0111\n",
      "     25       \u001b[36m23.1213\u001b[0m       26.4534  0.0113\n",
      "     26       \u001b[36m23.1181\u001b[0m       26.4527  0.0106\n",
      "     27       \u001b[36m23.1152\u001b[0m       26.4521  0.0107\n",
      "     28       \u001b[36m23.1125\u001b[0m       26.4515  0.0113\n",
      "     29       \u001b[36m23.1099\u001b[0m       26.4511  0.0116\n",
      "     30       \u001b[36m23.1075\u001b[0m       26.4505  0.0113\n",
      "     31       \u001b[36m23.1053\u001b[0m       26.4498  0.0109\n",
      "     32       \u001b[36m23.1032\u001b[0m       26.4495  0.0105\n",
      "     33       \u001b[36m23.1013\u001b[0m       26.4490  0.0113\n",
      "     34       \u001b[36m23.0995\u001b[0m       26.4486  0.0113\n",
      "     35       \u001b[36m23.0976\u001b[0m       26.4480  0.0109\n",
      "     36       \u001b[36m23.0959\u001b[0m       26.4473  0.0109\n",
      "     37       \u001b[36m23.0943\u001b[0m       26.4465  0.0106\n",
      "     38       \u001b[36m23.0927\u001b[0m       26.4460  0.0111\n",
      "     39       \u001b[36m23.0913\u001b[0m       26.4457  0.0114\n",
      "     40       \u001b[36m23.0898\u001b[0m       26.4453  0.0110\n",
      "     41       \u001b[36m23.0885\u001b[0m       26.4446  0.0108\n",
      "     42       \u001b[36m23.0872\u001b[0m       26.4442  0.0107\n",
      "     43       \u001b[36m23.0860\u001b[0m       26.4435  0.0110\n",
      "     44       \u001b[36m23.0848\u001b[0m       26.4429  0.0113\n",
      "     45       \u001b[36m23.0837\u001b[0m       26.4424  0.0108\n",
      "     46       \u001b[36m23.0826\u001b[0m       26.4418  0.0104\n",
      "     47       \u001b[36m23.0815\u001b[0m       26.4417  0.0104\n",
      "     48       \u001b[36m23.0805\u001b[0m       26.4413  0.0111\n",
      "     49       \u001b[36m23.0795\u001b[0m       26.4409  0.0115\n",
      "     50       \u001b[36m23.0786\u001b[0m       26.4404  0.0113\n",
      "     51       \u001b[36m23.0777\u001b[0m       26.4394  0.0110\n",
      "     52       \u001b[36m23.0769\u001b[0m       26.4390  0.0108\n",
      "     53       \u001b[36m23.0760\u001b[0m       26.4385  0.0112\n",
      "     54       \u001b[36m23.0752\u001b[0m       26.4380  0.0112\n",
      "     55       \u001b[36m23.0744\u001b[0m       26.4375  0.0111\n",
      "     56       \u001b[36m23.0737\u001b[0m       26.4370  0.0105\n",
      "     57       \u001b[36m23.0730\u001b[0m       26.4365  0.0106\n",
      "     58       \u001b[36m23.0722\u001b[0m       26.4360  0.0111\n",
      "     59       \u001b[36m23.0715\u001b[0m       26.4355  0.0113\n",
      "     60       \u001b[36m23.0709\u001b[0m       26.4351  0.0110\n",
      "     61       \u001b[36m23.0702\u001b[0m       26.4347  0.0110\n",
      "     62       \u001b[36m23.0695\u001b[0m       26.4342  0.0106\n",
      "     63       \u001b[36m23.0689\u001b[0m       26.4338  0.0110\n",
      "     64       \u001b[36m23.0683\u001b[0m       26.4334  0.0114\n",
      "     65       \u001b[36m23.0677\u001b[0m       26.4330  0.0110\n",
      "     66       \u001b[36m23.0672\u001b[0m       26.4326  0.0107\n",
      "     67       \u001b[36m23.0666\u001b[0m       26.4321  0.0107\n",
      "     68       \u001b[36m23.0660\u001b[0m       26.4317  0.0111\n",
      "     69       \u001b[36m23.0655\u001b[0m       26.4314  0.0113\n",
      "     70       \u001b[36m23.0650\u001b[0m       26.4310  0.0117\n",
      "     71       \u001b[36m23.0645\u001b[0m       26.4307  0.0111\n",
      "     72       \u001b[36m23.0640\u001b[0m       26.4303  0.0107\n",
      "     73       \u001b[36m23.0635\u001b[0m       26.4300  0.0109\n",
      "     74       \u001b[36m23.0631\u001b[0m       26.4296  0.0114\n",
      "     75       \u001b[36m23.0626\u001b[0m       26.4292  0.0233\n",
      "     76       \u001b[36m23.0622\u001b[0m       26.4289  0.0183\n",
      "     77       \u001b[36m23.0618\u001b[0m       26.4284  0.0182\n",
      "     78       \u001b[36m23.0614\u001b[0m       26.4280  0.0199\n",
      "     79       \u001b[36m23.0609\u001b[0m       26.4276  0.0146\n",
      "     80       \u001b[36m23.0606\u001b[0m       26.4270  0.0187\n",
      "     81       \u001b[36m23.0602\u001b[0m       26.4267  0.0117\n",
      "     82       \u001b[36m23.0598\u001b[0m       26.4263  0.0111\n",
      "     83       \u001b[36m23.0594\u001b[0m       26.4259  0.0112\n",
      "     84       \u001b[36m23.0590\u001b[0m       26.4255  0.0111\n",
      "     85       \u001b[36m23.0587\u001b[0m       26.4251  0.0110\n",
      "     86       \u001b[36m23.0583\u001b[0m       26.4246  0.0110\n",
      "     87       \u001b[36m23.0580\u001b[0m       26.4243  0.0114\n",
      "     88       \u001b[36m23.0576\u001b[0m       26.4240  0.0113\n",
      "     89       \u001b[36m23.0573\u001b[0m       26.4237  0.0111\n",
      "     90       \u001b[36m23.0569\u001b[0m       26.4232  0.0117\n",
      "     91       \u001b[36m23.0566\u001b[0m       26.4230  0.0110\n",
      "     92       \u001b[36m23.0563\u001b[0m       26.4227  0.0108\n",
      "     93       \u001b[36m23.0560\u001b[0m       26.4222  0.0108\n",
      "     94       \u001b[36m23.0556\u001b[0m       26.4220  0.0111\n",
      "     95       \u001b[36m23.0553\u001b[0m       26.4217  0.0110\n",
      "     96       \u001b[36m23.0550\u001b[0m       26.4212  0.0108\n",
      "     97       \u001b[36m23.0547\u001b[0m       26.4210  0.0110\n",
      "     98       \u001b[36m23.0544\u001b[0m       26.4207  0.0109\n",
      "     99       \u001b[36m23.0542\u001b[0m       26.4203  0.0106\n",
      "    100       \u001b[36m23.0539\u001b[0m       26.4201  0.0108\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m39.7321\u001b[0m       \u001b[32m29.8744\u001b[0m  0.0109\n",
      "      2       \u001b[36m35.6103\u001b[0m       \u001b[32m27.7853\u001b[0m  0.0110\n",
      "      3       \u001b[36m32.5575\u001b[0m       \u001b[32m26.4253\u001b[0m  0.0112\n",
      "      4       \u001b[36m30.2484\u001b[0m       \u001b[32m26.1311\u001b[0m  0.0110\n",
      "      5       \u001b[36m29.0473\u001b[0m       26.6232  0.0107\n",
      "      6       \u001b[36m28.7605\u001b[0m       26.9694  0.0105\n",
      "      7       \u001b[36m28.7032\u001b[0m       27.0940  0.0109\n",
      "      8       \u001b[36m28.6648\u001b[0m       27.1379  0.0113\n",
      "      9       \u001b[36m28.6298\u001b[0m       27.1585  0.0106\n",
      "     10       \u001b[36m28.5992\u001b[0m       27.1749  0.0106\n",
      "     11       \u001b[36m28.5744\u001b[0m       27.1891  0.0104\n",
      "     12       \u001b[36m28.5551\u001b[0m       27.2012  0.0109\n",
      "     13       \u001b[36m28.5404\u001b[0m       27.2081  0.0110\n",
      "     14       \u001b[36m28.5280\u001b[0m       27.2132  0.0109\n",
      "     15       \u001b[36m28.5177\u001b[0m       27.2165  0.0105\n",
      "     16       \u001b[36m28.5088\u001b[0m       27.2192  0.0106\n",
      "     17       \u001b[36m28.5010\u001b[0m       27.2211  0.0108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     18       \u001b[36m28.4943\u001b[0m       27.2213  0.0118\n",
      "     19       \u001b[36m28.4881\u001b[0m       27.2214  0.0108\n",
      "     20       \u001b[36m28.4827\u001b[0m       27.2203  0.0109\n",
      "     21       \u001b[36m28.4775\u001b[0m       27.2198  0.0104\n",
      "     22       \u001b[36m28.4733\u001b[0m       27.2189  0.0107\n",
      "     23       \u001b[36m28.4693\u001b[0m       27.2186  0.0133\n",
      "     24       \u001b[36m28.4659\u001b[0m       27.2173  0.0110\n",
      "     25       \u001b[36m28.4626\u001b[0m       27.2168  0.0109\n",
      "     26       \u001b[36m28.4598\u001b[0m       27.2156  0.0106\n",
      "     27       \u001b[36m28.4570\u001b[0m       27.2151  0.0106\n",
      "     28       \u001b[36m28.4546\u001b[0m       27.2135  0.0107\n",
      "     29       \u001b[36m28.4523\u001b[0m       27.2132  0.0114\n",
      "     30       \u001b[36m28.4503\u001b[0m       27.2117  0.0110\n",
      "     31       \u001b[36m28.4483\u001b[0m       27.2106  0.0108\n",
      "     32       \u001b[36m28.4463\u001b[0m       27.2100  0.0107\n",
      "     33       \u001b[36m28.4446\u001b[0m       27.2085  0.0109\n",
      "     34       \u001b[36m28.4429\u001b[0m       27.2068  0.0113\n",
      "     35       \u001b[36m28.4412\u001b[0m       27.2063  0.0108\n",
      "     36       \u001b[36m28.4399\u001b[0m       27.2043  0.0105\n",
      "     37       \u001b[36m28.4382\u001b[0m       27.2037  0.0105\n",
      "     38       \u001b[36m28.4370\u001b[0m       27.2024  0.0108\n",
      "     39       \u001b[36m28.4359\u001b[0m       27.2006  0.0113\n",
      "     40       \u001b[36m28.4346\u001b[0m       27.1997  0.0108\n",
      "     41       \u001b[36m28.4335\u001b[0m       27.1984  0.0103\n",
      "     42       \u001b[36m28.4323\u001b[0m       27.1972  0.0105\n",
      "     43       \u001b[36m28.4313\u001b[0m       27.1963  0.0106\n",
      "     44       \u001b[36m28.4303\u001b[0m       27.1954  0.0109\n",
      "     45       \u001b[36m28.4293\u001b[0m       27.1940  0.0107\n",
      "     46       \u001b[36m28.4284\u001b[0m       27.1929  0.0105\n",
      "     47       \u001b[36m28.4274\u001b[0m       27.1918  0.0105\n",
      "     48       \u001b[36m28.4265\u001b[0m       27.1910  0.0109\n",
      "     49       \u001b[36m28.4258\u001b[0m       27.1903  0.0111\n",
      "     50       \u001b[36m28.4250\u001b[0m       27.1893  0.0110\n",
      "     51       \u001b[36m28.4242\u001b[0m       27.1888  0.0108\n",
      "     52       \u001b[36m28.4236\u001b[0m       27.1879  0.0105\n",
      "     53       \u001b[36m28.4229\u001b[0m       27.1870  0.0106\n",
      "     54       \u001b[36m28.4222\u001b[0m       27.1863  0.0113\n",
      "     55       \u001b[36m28.4216\u001b[0m       27.1856  0.0109\n",
      "     56       \u001b[36m28.4210\u001b[0m       27.1850  0.0127\n",
      "     57       \u001b[36m28.4204\u001b[0m       27.1841  0.0155\n",
      "     58       \u001b[36m28.4197\u001b[0m       27.1837  0.0118\n",
      "     59       \u001b[36m28.4192\u001b[0m       27.1828  0.0115\n",
      "     60       \u001b[36m28.4186\u001b[0m       27.1822  0.0116\n",
      "     61       \u001b[36m28.4181\u001b[0m       27.1817  0.0113\n",
      "     62       \u001b[36m28.4176\u001b[0m       27.1807  0.0129\n",
      "     63       \u001b[36m28.4171\u001b[0m       27.1801  0.0114\n",
      "     64       \u001b[36m28.4166\u001b[0m       27.1799  0.0117\n",
      "     65       \u001b[36m28.4162\u001b[0m       27.1793  0.0109\n",
      "     66       \u001b[36m28.4157\u001b[0m       27.1786  0.0110\n",
      "     67       \u001b[36m28.4152\u001b[0m       27.1784  0.0111\n",
      "     68       \u001b[36m28.4148\u001b[0m       27.1779  0.0113\n",
      "     69       \u001b[36m28.4143\u001b[0m       27.1777  0.0110\n",
      "     70       \u001b[36m28.4139\u001b[0m       27.1774  0.0106\n",
      "     71       \u001b[36m28.4135\u001b[0m       27.1767  0.0107\n",
      "     72       \u001b[36m28.4131\u001b[0m       27.1760  0.0108\n",
      "     73       \u001b[36m28.4127\u001b[0m       27.1759  0.0111\n",
      "     74       \u001b[36m28.4123\u001b[0m       27.1752  0.0112\n",
      "     75       \u001b[36m28.4119\u001b[0m       27.1748  0.0108\n",
      "     76       \u001b[36m28.4116\u001b[0m       27.1745  0.0108\n",
      "     77       \u001b[36m28.4112\u001b[0m       27.1745  0.0109\n",
      "     78       \u001b[36m28.4109\u001b[0m       27.1739  0.0107\n",
      "     79       \u001b[36m28.4105\u001b[0m       27.1740  0.0109\n",
      "     80       \u001b[36m28.4102\u001b[0m       27.1735  0.0105\n",
      "     81       \u001b[36m28.4099\u001b[0m       27.1734  0.0106\n",
      "     82       \u001b[36m28.4096\u001b[0m       27.1730  0.0106\n",
      "     83       \u001b[36m28.4093\u001b[0m       27.1728  0.0111\n",
      "     84       \u001b[36m28.4090\u001b[0m       27.1726  0.0108\n",
      "     85       \u001b[36m28.4087\u001b[0m       27.1726  0.0106\n",
      "     86       \u001b[36m28.4084\u001b[0m       27.1723  0.0106\n",
      "     87       \u001b[36m28.4081\u001b[0m       27.1722  0.0110\n",
      "     88       \u001b[36m28.4078\u001b[0m       27.1720  0.0110\n",
      "     89       \u001b[36m28.4076\u001b[0m       27.1716  0.0112\n",
      "     90       \u001b[36m28.4073\u001b[0m       27.1714  0.0105\n",
      "     91       \u001b[36m28.4070\u001b[0m       27.1717  0.0105\n",
      "     92       \u001b[36m28.4068\u001b[0m       27.1714  0.0110\n",
      "     93       \u001b[36m28.4066\u001b[0m       27.1714  0.0116\n",
      "     94       \u001b[36m28.4063\u001b[0m       27.1711  0.0110\n",
      "     95       \u001b[36m28.4061\u001b[0m       27.1708  0.0106\n",
      "     96       \u001b[36m28.4058\u001b[0m       27.1708  0.0106\n",
      "     97       \u001b[36m28.4056\u001b[0m       27.1706  0.0107\n",
      "     98       \u001b[36m28.4054\u001b[0m       27.1705  0.0114\n",
      "     99       \u001b[36m28.4052\u001b[0m       27.1704  0.0110\n",
      "    100       \u001b[36m28.4049\u001b[0m       27.1701  0.0106\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m40.0597\u001b[0m       \u001b[32m39.3791\u001b[0m  0.0114\n",
      "      2       \u001b[36m36.0889\u001b[0m       \u001b[32m31.8879\u001b[0m  0.0119\n",
      "      3       \u001b[36m34.5030\u001b[0m       32.9565  0.0117\n",
      "      4       \u001b[36m33.9700\u001b[0m       32.0139  0.0119\n",
      "      5       \u001b[36m33.1872\u001b[0m       \u001b[32m30.4765\u001b[0m  0.0118\n",
      "      6       \u001b[36m32.9617\u001b[0m       \u001b[32m30.2491\u001b[0m  0.0112\n",
      "      7       \u001b[36m32.7560\u001b[0m       30.7868  0.0112\n",
      "      8       \u001b[36m32.6840\u001b[0m       30.4627  0.0121\n",
      "      9       \u001b[36m32.5117\u001b[0m       \u001b[32m30.0412\u001b[0m  0.0116\n",
      "     10       \u001b[36m32.4592\u001b[0m       30.1721  0.0117\n",
      "     11       \u001b[36m32.4020\u001b[0m       30.3114  0.0119\n",
      "     12       \u001b[36m32.3547\u001b[0m       30.1215  0.0117\n",
      "     13       \u001b[36m32.3130\u001b[0m       30.0692  0.0115\n",
      "     14       \u001b[36m32.2962\u001b[0m       30.1349  0.0114\n",
      "     15       \u001b[36m32.2817\u001b[0m       30.1310  0.0118\n",
      "     16       \u001b[36m32.2605\u001b[0m       \u001b[32m30.0322\u001b[0m  0.0117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m32.2475\u001b[0m       30.1096  0.0123\n",
      "     18       \u001b[36m32.2410\u001b[0m       30.0725  0.0116\n",
      "     19       \u001b[36m32.2310\u001b[0m       30.1173  0.0115\n",
      "     20       \u001b[36m32.2255\u001b[0m       \u001b[32m30.0298\u001b[0m  0.0110\n",
      "     21       32.2281       30.1942  0.0112\n",
      "     22       32.2281       30.0392  0.0121\n",
      "     23       32.2447       30.1809  0.0119\n",
      "     24       32.2276       \u001b[32m29.9890\u001b[0m  0.0116\n",
      "     25       \u001b[36m32.2136\u001b[0m       30.0480  0.0113\n",
      "     26       \u001b[36m32.1984\u001b[0m       30.0235  0.0112\n",
      "     27       \u001b[36m32.1765\u001b[0m       \u001b[32m29.9816\u001b[0m  0.0121\n",
      "     28       32.1770       30.0542  0.0125\n",
      "     29       \u001b[36m32.1715\u001b[0m       \u001b[32m29.9777\u001b[0m  0.0115\n",
      "     30       \u001b[36m32.1649\u001b[0m       30.0318  0.0117\n",
      "     31       \u001b[36m32.1640\u001b[0m       29.9936  0.0117\n",
      "     32       \u001b[36m32.1568\u001b[0m       30.0166  0.0117\n",
      "     33       \u001b[36m32.1549\u001b[0m       29.9966  0.0113\n",
      "     34       \u001b[36m32.1493\u001b[0m       30.0160  0.0114\n",
      "     35       \u001b[36m32.1477\u001b[0m       29.9951  0.0119\n",
      "     36       \u001b[36m32.1433\u001b[0m       30.0137  0.0117\n",
      "     37       \u001b[36m32.1413\u001b[0m       29.9943  0.0115\n",
      "     38       \u001b[36m32.1380\u001b[0m       30.0147  0.0114\n",
      "     39       \u001b[36m32.1356\u001b[0m       29.9890  0.0182\n",
      "     40       \u001b[36m32.1331\u001b[0m       30.0160  0.0139\n",
      "     41       \u001b[36m32.1304\u001b[0m       29.9857  0.0126\n",
      "     42       \u001b[36m32.1286\u001b[0m       30.0202  0.0123\n",
      "     43       \u001b[36m32.1256\u001b[0m       29.9891  0.0124\n",
      "     44       \u001b[36m32.1243\u001b[0m       30.0271  0.0176\n",
      "     45       \u001b[36m32.1215\u001b[0m       29.9865  0.0126\n",
      "     46       \u001b[36m32.1203\u001b[0m       30.0383  0.0124\n",
      "     47       \u001b[36m32.1174\u001b[0m       29.9859  0.0117\n",
      "     48       \u001b[36m32.1168\u001b[0m       30.0548  0.0118\n",
      "     49       \u001b[36m32.1151\u001b[0m       29.9860  0.0120\n",
      "     50       32.1153       30.0845  0.0118\n",
      "     51       \u001b[36m32.1135\u001b[0m       29.9844  0.0116\n",
      "     52       32.1157       30.1257  0.0118\n",
      "     53       32.1174       29.9800  0.0117\n",
      "     54       32.1195       30.1656  0.0117\n",
      "     55       32.1224       \u001b[32m29.9675\u001b[0m  0.0115\n",
      "     56       32.1266       30.1863  0.0115\n",
      "     57       32.1286       \u001b[32m29.9547\u001b[0m  0.0117\n",
      "     58       32.1195       30.1705  0.0116\n",
      "     59       32.1222       29.9607  0.0116\n",
      "     60       \u001b[36m32.1069\u001b[0m       30.1147  0.0115\n",
      "     61       32.1089       29.9824  0.0113\n",
      "     62       \u001b[36m32.0888\u001b[0m       30.0741  0.0112\n",
      "     63       32.0949       30.0230  0.0119\n",
      "     64       \u001b[36m32.0801\u001b[0m       30.0609  0.0118\n",
      "     65       32.0848       30.0412  0.0118\n",
      "     66       \u001b[36m32.0759\u001b[0m       30.0605  0.0117\n",
      "     67       32.0782       30.0526  0.0115\n",
      "     68       \u001b[36m32.0729\u001b[0m       30.0679  0.0113\n",
      "     69       32.0729       30.0623  0.0115\n",
      "     70       \u001b[36m32.0698\u001b[0m       30.0771  0.0116\n",
      "     71       \u001b[36m32.0690\u001b[0m       30.0687  0.0114\n",
      "     72       \u001b[36m32.0667\u001b[0m       30.0846  0.0117\n",
      "     73       \u001b[36m32.0650\u001b[0m       30.0777  0.0115\n",
      "     74       \u001b[36m32.0633\u001b[0m       30.0954  0.0113\n",
      "     75       \u001b[36m32.0612\u001b[0m       30.0846  0.0120\n",
      "     76       \u001b[36m32.0600\u001b[0m       30.1011  0.0115\n",
      "     77       \u001b[36m32.0578\u001b[0m       30.0980  0.0116\n",
      "     78       \u001b[36m32.0568\u001b[0m       30.1051  0.0113\n",
      "     79       \u001b[36m32.0544\u001b[0m       30.1110  0.0113\n",
      "     80       \u001b[36m32.0539\u001b[0m       30.1071  0.0123\n",
      "     81       \u001b[36m32.0516\u001b[0m       30.1213  0.0117\n",
      "     82       \u001b[36m32.0503\u001b[0m       30.1150  0.0116\n",
      "     83       \u001b[36m32.0492\u001b[0m       30.1286  0.0112\n",
      "     84       \u001b[36m32.0465\u001b[0m       30.1285  0.0111\n",
      "     85       \u001b[36m32.0462\u001b[0m       30.1247  0.0118\n",
      "     86       \u001b[36m32.0444\u001b[0m       30.1463  0.0115\n",
      "     87       \u001b[36m32.0423\u001b[0m       30.1302  0.0117\n",
      "     88       32.0425       30.1495  0.0137\n",
      "     89       \u001b[36m32.0386\u001b[0m       30.1596  0.0118\n",
      "     90       32.0403       30.1258  0.0120\n",
      "     91       32.0394       30.1855  0.0118\n",
      "     92       \u001b[36m32.0382\u001b[0m       30.1388  0.0117\n",
      "     93       32.0425       30.1858  0.0112\n",
      "     94       \u001b[36m32.0348\u001b[0m       30.1634  0.0111\n",
      "     95       32.0380       30.1310  0.0116\n",
      "     96       32.0368       30.2513  0.0119\n",
      "     97       32.0425       30.1251  0.0116\n",
      "     98       32.0536       30.2436  0.0112\n",
      "     99       \u001b[36m32.0291\u001b[0m       30.1816  0.0111\n",
      "    100       32.0441       30.1270  0.0119\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m30.9230\u001b[0m       \u001b[32m27.5903\u001b[0m  0.0115\n",
      "      2       \u001b[36m26.5136\u001b[0m       29.1930  0.0115\n",
      "      3       \u001b[36m24.9391\u001b[0m       \u001b[32m26.8079\u001b[0m  0.0123\n",
      "      4       \u001b[36m24.3689\u001b[0m       \u001b[32m26.4696\u001b[0m  0.0128\n",
      "      5       \u001b[36m24.1146\u001b[0m       27.0508  0.0116\n",
      "      6       \u001b[36m23.6366\u001b[0m       27.5064  0.0110\n",
      "      7       \u001b[36m23.5055\u001b[0m       26.9165  0.0112\n",
      "      8       \u001b[36m23.4515\u001b[0m       26.9558  0.0116\n",
      "      9       \u001b[36m23.3631\u001b[0m       27.2763  0.0116\n",
      "     10       \u001b[36m23.2894\u001b[0m       27.1301  0.0117\n",
      "     11       \u001b[36m23.2472\u001b[0m       26.8876  0.0113\n",
      "     12       \u001b[36m23.2279\u001b[0m       26.9482  0.0112\n",
      "     13       \u001b[36m23.1886\u001b[0m       26.9731  0.0115\n",
      "     14       \u001b[36m23.1620\u001b[0m       26.8197  0.0117\n",
      "     15       \u001b[36m23.1491\u001b[0m       26.7828  0.0114\n",
      "     16       \u001b[36m23.1308\u001b[0m       26.7955  0.0114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m23.1144\u001b[0m       26.7449  0.0116\n",
      "     18       \u001b[36m23.1034\u001b[0m       26.7053  0.0160\n",
      "     19       \u001b[36m23.0922\u001b[0m       26.6934  0.0172\n",
      "     20       \u001b[36m23.0828\u001b[0m       26.6877  0.0131\n",
      "     21       \u001b[36m23.0734\u001b[0m       26.6610  0.0119\n",
      "     22       \u001b[36m23.0667\u001b[0m       26.6451  0.0123\n",
      "     23       \u001b[36m23.0598\u001b[0m       26.6469  0.0202\n",
      "     24       \u001b[36m23.0526\u001b[0m       26.6235  0.0155\n",
      "     25       \u001b[36m23.0477\u001b[0m       26.6162  0.0118\n",
      "     26       \u001b[36m23.0419\u001b[0m       26.6149  0.0117\n",
      "     27       \u001b[36m23.0372\u001b[0m       26.5995  0.0150\n",
      "     28       \u001b[36m23.0325\u001b[0m       26.5958  0.0126\n",
      "     29       \u001b[36m23.0288\u001b[0m       26.5942  0.0116\n",
      "     30       \u001b[36m23.0245\u001b[0m       26.5789  0.0119\n",
      "     31       \u001b[36m23.0219\u001b[0m       26.5759  0.0145\n",
      "     32       \u001b[36m23.0181\u001b[0m       26.5666  0.0128\n",
      "     33       \u001b[36m23.0164\u001b[0m       26.5668  0.0126\n",
      "     34       \u001b[36m23.0135\u001b[0m       26.5544  0.0116\n",
      "     35       \u001b[36m23.0110\u001b[0m       26.5611  0.0117\n",
      "     36       \u001b[36m23.0072\u001b[0m       26.5517  0.0144\n",
      "     37       \u001b[36m23.0054\u001b[0m       26.5517  0.0130\n",
      "     38       \u001b[36m23.0026\u001b[0m       26.5486  0.0127\n",
      "     39       \u001b[36m23.0003\u001b[0m       26.5429  0.0118\n",
      "     40       \u001b[36m22.9984\u001b[0m       26.5400  0.0118\n",
      "     41       \u001b[36m22.9961\u001b[0m       26.5394  0.0146\n",
      "     42       \u001b[36m22.9941\u001b[0m       26.5323  0.0126\n",
      "     43       \u001b[36m22.9926\u001b[0m       26.5371  0.0141\n",
      "     44       \u001b[36m22.9902\u001b[0m       26.5342  0.0127\n",
      "     45       \u001b[36m22.9888\u001b[0m       26.5319  0.0128\n",
      "     46       \u001b[36m22.9872\u001b[0m       26.5405  0.0116\n",
      "     47       \u001b[36m22.9849\u001b[0m       26.5357  0.0113\n",
      "     48       \u001b[36m22.9843\u001b[0m       26.5331  0.0147\n",
      "     49       \u001b[36m22.9822\u001b[0m       26.5461  0.0121\n",
      "     50       \u001b[36m22.9807\u001b[0m       26.5332  0.0116\n",
      "     51       22.9811       26.5424  0.0115\n",
      "     52       \u001b[36m22.9785\u001b[0m       26.5570  0.0113\n",
      "     53       22.9846       26.5240  0.0118\n",
      "     54       22.9891       26.5751  0.0117\n",
      "     55       23.0070       26.5050  0.0120\n",
      "     56       23.0478       26.6186  0.0113\n",
      "     57       23.0259       26.5740  0.0116\n",
      "     58       23.0086       26.6628  0.0119\n",
      "     59       22.9829       26.5365  0.0115\n",
      "     60       22.9943       26.6425  0.0116\n",
      "     61       \u001b[36m22.9718\u001b[0m       26.5332  0.0117\n",
      "     62       22.9766       26.6127  0.0113\n",
      "     63       \u001b[36m22.9669\u001b[0m       26.5884  0.0119\n",
      "     64       22.9689       26.5892  0.0115\n",
      "     65       \u001b[36m22.9648\u001b[0m       26.5840  0.0116\n",
      "     66       \u001b[36m22.9641\u001b[0m       26.5946  0.0118\n",
      "     67       \u001b[36m22.9607\u001b[0m       26.5772  0.0117\n",
      "     68       \u001b[36m22.9606\u001b[0m       26.6020  0.0113\n",
      "     69       \u001b[36m22.9582\u001b[0m       26.5877  0.0112\n",
      "     70       22.9584       26.6002  0.0120\n",
      "     71       \u001b[36m22.9555\u001b[0m       26.5926  0.0116\n",
      "     72       \u001b[36m22.9545\u001b[0m       26.6026  0.0115\n",
      "     73       \u001b[36m22.9529\u001b[0m       26.6050  0.0115\n",
      "     74       \u001b[36m22.9516\u001b[0m       26.6036  0.0112\n",
      "     75       \u001b[36m22.9505\u001b[0m       26.6076  0.0113\n",
      "     76       \u001b[36m22.9490\u001b[0m       26.6042  0.0119\n",
      "     77       \u001b[36m22.9478\u001b[0m       26.6121  0.0116\n",
      "     78       \u001b[36m22.9466\u001b[0m       26.6093  0.0115\n",
      "     79       \u001b[36m22.9455\u001b[0m       26.6184  0.0115\n",
      "     80       \u001b[36m22.9442\u001b[0m       26.6093  0.0113\n",
      "     81       \u001b[36m22.9434\u001b[0m       26.6219  0.0117\n",
      "     82       \u001b[36m22.9421\u001b[0m       26.6161  0.0114\n",
      "     83       \u001b[36m22.9415\u001b[0m       26.6295  0.0115\n",
      "     84       \u001b[36m22.9401\u001b[0m       26.6175  0.0110\n",
      "     85       \u001b[36m22.9393\u001b[0m       26.6274  0.0112\n",
      "     86       \u001b[36m22.9387\u001b[0m       26.6279  0.0119\n",
      "     87       \u001b[36m22.9367\u001b[0m       26.6321  0.0126\n",
      "     88       22.9367       26.6364  0.0115\n",
      "     89       \u001b[36m22.9352\u001b[0m       26.6364  0.0114\n",
      "     90       \u001b[36m22.9337\u001b[0m       26.6492  0.0113\n",
      "     91       \u001b[36m22.9333\u001b[0m       26.6282  0.0119\n",
      "     92       22.9333       26.6686  0.0116\n",
      "     93       \u001b[36m22.9303\u001b[0m       26.6375  0.0116\n",
      "     94       22.9310       26.6575  0.0115\n",
      "     95       22.9309       26.6681  0.0143\n",
      "     96       \u001b[36m22.9294\u001b[0m       26.6402  0.0149\n",
      "     97       22.9357       26.6791  0.0127\n",
      "     98       22.9320       26.6728  0.0121\n",
      "     99       \u001b[36m22.9261\u001b[0m       26.6760  0.0132\n",
      "    100       22.9325       26.6742  0.0124\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m40.6766\u001b[0m       \u001b[32m29.1872\u001b[0m  0.0125\n",
      "      2       \u001b[36m33.0576\u001b[0m       31.9871  0.0122\n",
      "      3       \u001b[36m32.4486\u001b[0m       \u001b[32m27.5668\u001b[0m  0.0120\n",
      "      4       \u001b[36m30.3128\u001b[0m       \u001b[32m26.5990\u001b[0m  0.0123\n",
      "      5       \u001b[36m30.0894\u001b[0m       27.4303  0.0130\n",
      "      6       \u001b[36m29.5320\u001b[0m       29.0047  0.0118\n",
      "      7       \u001b[36m29.3837\u001b[0m       27.3694  0.0119\n",
      "      8       \u001b[36m29.0648\u001b[0m       27.0417  0.0125\n",
      "      9       \u001b[36m28.8745\u001b[0m       27.7826  0.0120\n",
      "     10       \u001b[36m28.8689\u001b[0m       27.5184  0.0118\n",
      "     11       \u001b[36m28.7056\u001b[0m       27.1554  0.0119\n",
      "     12       \u001b[36m28.6117\u001b[0m       27.4428  0.0116\n",
      "     13       \u001b[36m28.5985\u001b[0m       27.4709  0.0118\n",
      "     14       \u001b[36m28.5514\u001b[0m       27.1942  0.0118\n",
      "     15       \u001b[36m28.5239\u001b[0m       27.3993  0.0117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     16       28.5310       27.3550  0.0120\n",
      "     17       \u001b[36m28.5023\u001b[0m       27.2548  0.0116\n",
      "     18       \u001b[36m28.4898\u001b[0m       27.3625  0.0125\n",
      "     19       28.5021       27.2552  0.0125\n",
      "     20       \u001b[36m28.4721\u001b[0m       27.3244  0.0119\n",
      "     21       \u001b[36m28.4549\u001b[0m       27.2836  0.0117\n",
      "     22       \u001b[36m28.4332\u001b[0m       27.2608  0.0117\n",
      "     23       \u001b[36m28.4275\u001b[0m       27.3149  0.0116\n",
      "     24       28.4293       27.2720  0.0114\n",
      "     25       \u001b[36m28.4189\u001b[0m       27.2855  0.0117\n",
      "     26       \u001b[36m28.4176\u001b[0m       27.2846  0.0115\n",
      "     27       \u001b[36m28.4143\u001b[0m       27.2635  0.0115\n",
      "     28       \u001b[36m28.4116\u001b[0m       27.2680  0.0116\n",
      "     29       \u001b[36m28.4097\u001b[0m       27.2459  0.0114\n",
      "     30       \u001b[36m28.4091\u001b[0m       27.2536  0.0116\n",
      "     31       28.4139       27.2435  0.0117\n",
      "     32       28.4173       27.2334  0.0113\n",
      "     33       28.4288       27.2701  0.0113\n",
      "     34       28.4700       27.2836  0.0110\n",
      "     35       28.5149       27.1299  0.0111\n",
      "     36       28.4722       27.2828  0.0119\n",
      "     37       28.4653       27.2623  0.0118\n",
      "     38       28.4751       27.0569  0.0115\n",
      "     39       \u001b[36m28.4027\u001b[0m       27.2422  0.0111\n",
      "     40       28.4217       27.1038  0.0112\n",
      "     41       \u001b[36m28.3978\u001b[0m       27.1967  0.0117\n",
      "     42       28.4093       27.1075  0.0115\n",
      "     43       \u001b[36m28.3888\u001b[0m       27.1552  0.0114\n",
      "     44       28.3949       27.1499  0.0135\n",
      "     45       \u001b[36m28.3873\u001b[0m       27.1223  0.0120\n",
      "     46       28.3904       27.1212  0.0116\n",
      "     47       \u001b[36m28.3851\u001b[0m       27.1249  0.0116\n",
      "     48       \u001b[36m28.3843\u001b[0m       27.1190  0.0115\n",
      "     49       \u001b[36m28.3833\u001b[0m       27.1061  0.0116\n",
      "     50       \u001b[36m28.3816\u001b[0m       27.1000  0.0111\n",
      "     51       28.3819       27.1075  0.0111\n",
      "     52       \u001b[36m28.3796\u001b[0m       27.0952  0.0119\n",
      "     53       28.3802       27.0888  0.0115\n",
      "     54       28.3811       27.0925  0.0117\n",
      "     55       \u001b[36m28.3776\u001b[0m       27.0828  0.0115\n",
      "     56       \u001b[36m28.3772\u001b[0m       27.0822  0.0113\n",
      "     57       \u001b[36m28.3768\u001b[0m       27.0792  0.0113\n",
      "     58       \u001b[36m28.3753\u001b[0m       27.0746  0.0116\n",
      "     59       \u001b[36m28.3747\u001b[0m       27.0712  0.0118\n",
      "     60       \u001b[36m28.3745\u001b[0m       27.0672  0.0116\n",
      "     61       \u001b[36m28.3732\u001b[0m       27.0640  0.0113\n",
      "     62       \u001b[36m28.3727\u001b[0m       27.0622  0.0112\n",
      "     63       \u001b[36m28.3722\u001b[0m       27.0598  0.0121\n",
      "     64       \u001b[36m28.3716\u001b[0m       27.0548  0.0114\n",
      "     65       \u001b[36m28.3708\u001b[0m       27.0508  0.0116\n",
      "     66       \u001b[36m28.3704\u001b[0m       27.0517  0.0111\n",
      "     67       \u001b[36m28.3698\u001b[0m       27.0482  0.0114\n",
      "     68       \u001b[36m28.3691\u001b[0m       27.0466  0.0120\n",
      "     69       28.3694       27.0429  0.0118\n",
      "     70       \u001b[36m28.3681\u001b[0m       27.0442  0.0114\n",
      "     71       \u001b[36m28.3675\u001b[0m       27.0422  0.0116\n",
      "     72       28.3696       27.0374  0.0113\n",
      "     73       28.3681       27.0374  0.0110\n",
      "     74       \u001b[36m28.3664\u001b[0m       27.0477  0.0111\n",
      "     75       28.3737       27.0342  0.0186\n",
      "     76       28.3809       27.0273  0.0221\n",
      "     77       28.3749       27.0591  0.0204\n",
      "     78       28.3898       27.0502  0.0208\n",
      "     79       28.4248       26.9962  0.0150\n",
      "     80       28.4199       27.0929  0.0127\n",
      "     81       28.4405       27.0659  0.0117\n",
      "     82       28.4551       27.0836  0.0129\n",
      "     83       28.4386       26.9499  0.0120\n",
      "     84       28.4100       26.9973  0.0120\n",
      "     85       28.3752       27.0950  0.0117\n",
      "     86       28.3848       26.9762  0.0117\n",
      "     87       28.3678       27.0465  0.0123\n",
      "     88       28.3769       27.0067  0.0125\n",
      "     89       \u001b[36m28.3617\u001b[0m       27.0490  0.0120\n",
      "     90       28.3691       26.9976  0.0118\n",
      "     91       \u001b[36m28.3608\u001b[0m       27.0338  0.0115\n",
      "     92       28.3652       27.0084  0.0120\n",
      "     93       28.3612       27.0201  0.0117\n",
      "     94       28.3610       27.0182  0.0120\n",
      "     95       28.3609       27.0045  0.0120\n",
      "     96       \u001b[36m28.3594\u001b[0m       27.0215  0.0120\n",
      "     97       28.3605       26.9959  0.0129\n",
      "     98       \u001b[36m28.3591\u001b[0m       27.0250  0.0123\n",
      "     99       28.3606       26.9958  0.0122\n",
      "    100       28.3603       27.0176  0.0120\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m43.2192\u001b[0m       \u001b[32m42.9598\u001b[0m  0.0113\n",
      "      2       \u001b[36m40.3031\u001b[0m       \u001b[32m39.3091\u001b[0m  0.0110\n",
      "      3       \u001b[36m37.5409\u001b[0m       \u001b[32m35.8537\u001b[0m  0.0111\n",
      "      4       \u001b[36m35.1819\u001b[0m       \u001b[32m33.0283\u001b[0m  0.0113\n",
      "      5       \u001b[36m33.6532\u001b[0m       \u001b[32m31.3338\u001b[0m  0.0109\n",
      "      6       \u001b[36m33.0049\u001b[0m       \u001b[32m30.6127\u001b[0m  0.0110\n",
      "      7       \u001b[36m32.7843\u001b[0m       \u001b[32m30.3395\u001b[0m  0.0107\n",
      "      8       \u001b[36m32.6879\u001b[0m       \u001b[32m30.2215\u001b[0m  0.0108\n",
      "      9       \u001b[36m32.6255\u001b[0m       \u001b[32m30.1564\u001b[0m  0.0109\n",
      "     10       \u001b[36m32.5786\u001b[0m       \u001b[32m30.1114\u001b[0m  0.0121\n",
      "     11       \u001b[36m32.5414\u001b[0m       \u001b[32m30.0766\u001b[0m  0.0112\n",
      "     12       \u001b[36m32.5109\u001b[0m       \u001b[32m30.0488\u001b[0m  0.0114\n",
      "     13       \u001b[36m32.4848\u001b[0m       \u001b[32m30.0257\u001b[0m  0.0110\n",
      "     14       \u001b[36m32.4628\u001b[0m       \u001b[32m30.0053\u001b[0m  0.0111\n",
      "     15       \u001b[36m32.4437\u001b[0m       \u001b[32m29.9875\u001b[0m  0.0116\n",
      "     16       \u001b[36m32.4271\u001b[0m       \u001b[32m29.9715\u001b[0m  0.0117\n",
      "     17       \u001b[36m32.4130\u001b[0m       \u001b[32m29.9572\u001b[0m  0.0114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     18       \u001b[36m32.4003\u001b[0m       \u001b[32m29.9446\u001b[0m  0.0112\n",
      "     19       \u001b[36m32.3891\u001b[0m       \u001b[32m29.9336\u001b[0m  0.0113\n",
      "     20       \u001b[36m32.3789\u001b[0m       \u001b[32m29.9232\u001b[0m  0.0116\n",
      "     21       \u001b[36m32.3702\u001b[0m       \u001b[32m29.9140\u001b[0m  0.0121\n",
      "     22       \u001b[36m32.3623\u001b[0m       \u001b[32m29.9057\u001b[0m  0.0123\n",
      "     23       \u001b[36m32.3549\u001b[0m       \u001b[32m29.8980\u001b[0m  0.0113\n",
      "     24       \u001b[36m32.3479\u001b[0m       \u001b[32m29.8905\u001b[0m  0.0116\n",
      "     25       \u001b[36m32.3415\u001b[0m       \u001b[32m29.8843\u001b[0m  0.0118\n",
      "     26       \u001b[36m32.3358\u001b[0m       \u001b[32m29.8780\u001b[0m  0.0118\n",
      "     27       \u001b[36m32.3310\u001b[0m       \u001b[32m29.8721\u001b[0m  0.0112\n",
      "     28       \u001b[36m32.3265\u001b[0m       \u001b[32m29.8679\u001b[0m  0.0111\n",
      "     29       \u001b[36m32.3222\u001b[0m       \u001b[32m29.8629\u001b[0m  0.0110\n",
      "     30       \u001b[36m32.3186\u001b[0m       \u001b[32m29.8589\u001b[0m  0.0113\n",
      "     31       \u001b[36m32.3148\u001b[0m       \u001b[32m29.8552\u001b[0m  0.0116\n",
      "     32       \u001b[36m32.3114\u001b[0m       \u001b[32m29.8525\u001b[0m  0.0113\n",
      "     33       \u001b[36m32.3083\u001b[0m       \u001b[32m29.8490\u001b[0m  0.0110\n",
      "     34       \u001b[36m32.3052\u001b[0m       \u001b[32m29.8457\u001b[0m  0.0110\n",
      "     35       \u001b[36m32.3025\u001b[0m       \u001b[32m29.8429\u001b[0m  0.0114\n",
      "     36       \u001b[36m32.3000\u001b[0m       \u001b[32m29.8407\u001b[0m  0.0112\n",
      "     37       \u001b[36m32.2974\u001b[0m       \u001b[32m29.8383\u001b[0m  0.0113\n",
      "     38       \u001b[36m32.2953\u001b[0m       \u001b[32m29.8358\u001b[0m  0.0111\n",
      "     39       \u001b[36m32.2928\u001b[0m       \u001b[32m29.8335\u001b[0m  0.0112\n",
      "     40       \u001b[36m32.2909\u001b[0m       \u001b[32m29.8319\u001b[0m  0.0112\n",
      "     41       \u001b[36m32.2888\u001b[0m       \u001b[32m29.8297\u001b[0m  0.0117\n",
      "     42       \u001b[36m32.2868\u001b[0m       \u001b[32m29.8286\u001b[0m  0.0116\n",
      "     43       \u001b[36m32.2849\u001b[0m       \u001b[32m29.8266\u001b[0m  0.0113\n",
      "     44       \u001b[36m32.2832\u001b[0m       \u001b[32m29.8248\u001b[0m  0.0109\n",
      "     45       \u001b[36m32.2814\u001b[0m       \u001b[32m29.8230\u001b[0m  0.0120\n",
      "     46       \u001b[36m32.2797\u001b[0m       \u001b[32m29.8219\u001b[0m  0.0123\n",
      "     47       \u001b[36m32.2782\u001b[0m       \u001b[32m29.8203\u001b[0m  0.0115\n",
      "     48       \u001b[36m32.2765\u001b[0m       \u001b[32m29.8191\u001b[0m  0.0111\n",
      "     49       \u001b[36m32.2750\u001b[0m       \u001b[32m29.8179\u001b[0m  0.0115\n",
      "     50       \u001b[36m32.2735\u001b[0m       \u001b[32m29.8169\u001b[0m  0.0117\n",
      "     51       \u001b[36m32.2721\u001b[0m       \u001b[32m29.8157\u001b[0m  0.0114\n",
      "     52       \u001b[36m32.2707\u001b[0m       \u001b[32m29.8145\u001b[0m  0.0114\n",
      "     53       \u001b[36m32.2694\u001b[0m       \u001b[32m29.8133\u001b[0m  0.0146\n",
      "     54       \u001b[36m32.2680\u001b[0m       \u001b[32m29.8124\u001b[0m  0.0145\n",
      "     55       \u001b[36m32.2668\u001b[0m       \u001b[32m29.8114\u001b[0m  0.0124\n",
      "     56       \u001b[36m32.2655\u001b[0m       \u001b[32m29.8106\u001b[0m  0.0120\n",
      "     57       \u001b[36m32.2643\u001b[0m       \u001b[32m29.8096\u001b[0m  0.0131\n",
      "     58       \u001b[36m32.2632\u001b[0m       \u001b[32m29.8089\u001b[0m  0.0125\n",
      "     59       \u001b[36m32.2619\u001b[0m       \u001b[32m29.8083\u001b[0m  0.0135\n",
      "     60       \u001b[36m32.2610\u001b[0m       \u001b[32m29.8076\u001b[0m  0.0124\n",
      "     61       \u001b[36m32.2597\u001b[0m       \u001b[32m29.8067\u001b[0m  0.0131\n",
      "     62       \u001b[36m32.2588\u001b[0m       \u001b[32m29.8059\u001b[0m  0.0110\n",
      "     63       \u001b[36m32.2577\u001b[0m       \u001b[32m29.8051\u001b[0m  0.0112\n",
      "     64       \u001b[36m32.2568\u001b[0m       \u001b[32m29.8043\u001b[0m  0.0137\n",
      "     65       \u001b[36m32.2558\u001b[0m       \u001b[32m29.8035\u001b[0m  0.0121\n",
      "     66       \u001b[36m32.2547\u001b[0m       \u001b[32m29.8031\u001b[0m  0.0122\n",
      "     67       \u001b[36m32.2540\u001b[0m       \u001b[32m29.8024\u001b[0m  0.0108\n",
      "     68       \u001b[36m32.2529\u001b[0m       \u001b[32m29.8017\u001b[0m  0.0108\n",
      "     69       \u001b[36m32.2522\u001b[0m       \u001b[32m29.8013\u001b[0m  0.0134\n",
      "     70       \u001b[36m32.2511\u001b[0m       \u001b[32m29.8006\u001b[0m  0.0126\n",
      "     71       \u001b[36m32.2503\u001b[0m       \u001b[32m29.8005\u001b[0m  0.0117\n",
      "     72       \u001b[36m32.2495\u001b[0m       \u001b[32m29.7997\u001b[0m  0.0109\n",
      "     73       \u001b[36m32.2486\u001b[0m       \u001b[32m29.7990\u001b[0m  0.0105\n",
      "     74       \u001b[36m32.2479\u001b[0m       \u001b[32m29.7987\u001b[0m  0.0135\n",
      "     75       \u001b[36m32.2470\u001b[0m       \u001b[32m29.7980\u001b[0m  0.0118\n",
      "     76       \u001b[36m32.2463\u001b[0m       \u001b[32m29.7973\u001b[0m  0.0120\n",
      "     77       \u001b[36m32.2455\u001b[0m       \u001b[32m29.7973\u001b[0m  0.0110\n",
      "     78       \u001b[36m32.2448\u001b[0m       \u001b[32m29.7966\u001b[0m  0.0106\n",
      "     79       \u001b[36m32.2440\u001b[0m       \u001b[32m29.7960\u001b[0m  0.0138\n",
      "     80       \u001b[36m32.2433\u001b[0m       \u001b[32m29.7960\u001b[0m  0.0123\n",
      "     81       \u001b[36m32.2426\u001b[0m       \u001b[32m29.7955\u001b[0m  0.0119\n",
      "     82       \u001b[36m32.2419\u001b[0m       \u001b[32m29.7953\u001b[0m  0.0104\n",
      "     83       \u001b[36m32.2412\u001b[0m       \u001b[32m29.7951\u001b[0m  0.0107\n",
      "     84       \u001b[36m32.2405\u001b[0m       \u001b[32m29.7947\u001b[0m  0.0128\n",
      "     85       \u001b[36m32.2399\u001b[0m       \u001b[32m29.7940\u001b[0m  0.0123\n",
      "     86       \u001b[36m32.2391\u001b[0m       \u001b[32m29.7936\u001b[0m  0.0119\n",
      "     87       \u001b[36m32.2386\u001b[0m       \u001b[32m29.7931\u001b[0m  0.0105\n",
      "     88       \u001b[36m32.2379\u001b[0m       29.7932  0.0106\n",
      "     89       \u001b[36m32.2373\u001b[0m       \u001b[32m29.7927\u001b[0m  0.0137\n",
      "     90       \u001b[36m32.2366\u001b[0m       \u001b[32m29.7923\u001b[0m  0.0117\n",
      "     91       \u001b[36m32.2361\u001b[0m       \u001b[32m29.7923\u001b[0m  0.0120\n",
      "     92       \u001b[36m32.2355\u001b[0m       \u001b[32m29.7920\u001b[0m  0.0107\n",
      "     93       \u001b[36m32.2350\u001b[0m       \u001b[32m29.7915\u001b[0m  0.0104\n",
      "     94       \u001b[36m32.2344\u001b[0m       \u001b[32m29.7911\u001b[0m  0.0137\n",
      "     95       \u001b[36m32.2338\u001b[0m       \u001b[32m29.7909\u001b[0m  0.0116\n",
      "     96       \u001b[36m32.2332\u001b[0m       \u001b[32m29.7906\u001b[0m  0.0116\n",
      "     97       \u001b[36m32.2327\u001b[0m       \u001b[32m29.7903\u001b[0m  0.0106\n",
      "     98       \u001b[36m32.2321\u001b[0m       29.7904  0.0107\n",
      "     99       \u001b[36m32.2317\u001b[0m       \u001b[32m29.7900\u001b[0m  0.0138\n",
      "    100       \u001b[36m32.2310\u001b[0m       \u001b[32m29.7897\u001b[0m  0.0122\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m34.3941\u001b[0m       \u001b[32m31.3981\u001b[0m  0.0108\n",
      "      2       \u001b[36m31.2538\u001b[0m       \u001b[32m29.0868\u001b[0m  0.0134\n",
      "      3       \u001b[36m28.0876\u001b[0m       \u001b[32m27.1503\u001b[0m  0.0113\n",
      "      4       \u001b[36m25.4708\u001b[0m       \u001b[32m26.3387\u001b[0m  0.0132\n",
      "      5       \u001b[36m24.1076\u001b[0m       26.3388  0.0119\n",
      "      6       \u001b[36m23.6591\u001b[0m       26.4381  0.0109\n",
      "      7       \u001b[36m23.5179\u001b[0m       26.4682  0.0108\n",
      "      8       \u001b[36m23.4489\u001b[0m       26.4670  0.0141\n",
      "      9       \u001b[36m23.4024\u001b[0m       26.4581  0.0118\n",
      "     10       \u001b[36m23.3664\u001b[0m       26.4500  0.0117\n",
      "     11       \u001b[36m23.3380\u001b[0m       26.4398  0.0106\n",
      "     12       \u001b[36m23.3138\u001b[0m       26.4329  0.0106\n",
      "     13       \u001b[36m23.2933\u001b[0m       26.4252  0.0132\n",
      "     14       \u001b[36m23.2758\u001b[0m       26.4205  0.0118\n",
      "     15       \u001b[36m23.2607\u001b[0m       26.4166  0.0119\n",
      "     16       \u001b[36m23.2476\u001b[0m       26.4122  0.0105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m23.2361\u001b[0m       26.4101  0.0108\n",
      "     18       \u001b[36m23.2257\u001b[0m       26.4069  0.0129\n",
      "     19       \u001b[36m23.2164\u001b[0m       26.4052  0.0157\n",
      "     20       \u001b[36m23.2077\u001b[0m       26.4032  0.0124\n",
      "     21       \u001b[36m23.2000\u001b[0m       26.4007  0.0106\n",
      "     22       \u001b[36m23.1930\u001b[0m       26.3996  0.0107\n",
      "     23       \u001b[36m23.1864\u001b[0m       26.3982  0.0133\n",
      "     24       \u001b[36m23.1803\u001b[0m       26.3964  0.0117\n",
      "     25       \u001b[36m23.1748\u001b[0m       26.3955  0.0115\n",
      "     26       \u001b[36m23.1696\u001b[0m       26.3948  0.0111\n",
      "     27       \u001b[36m23.1647\u001b[0m       26.3936  0.0107\n",
      "     28       \u001b[36m23.1603\u001b[0m       26.3930  0.0136\n",
      "     29       \u001b[36m23.1561\u001b[0m       26.3920  0.0120\n",
      "     30       \u001b[36m23.1522\u001b[0m       26.3914  0.0116\n",
      "     31       \u001b[36m23.1484\u001b[0m       26.3907  0.0124\n",
      "     32       \u001b[36m23.1449\u001b[0m       26.3899  0.0156\n",
      "     33       \u001b[36m23.1416\u001b[0m       26.3898  0.0140\n",
      "     34       \u001b[36m23.1386\u001b[0m       26.3895  0.0116\n",
      "     35       \u001b[36m23.1358\u001b[0m       26.3889  0.0114\n",
      "     36       \u001b[36m23.1331\u001b[0m       26.3884  0.0115\n",
      "     37       \u001b[36m23.1306\u001b[0m       26.3877  0.0140\n",
      "     38       \u001b[36m23.1282\u001b[0m       26.3874  0.0112\n",
      "     39       \u001b[36m23.1259\u001b[0m       26.3868  0.0116\n",
      "     40       \u001b[36m23.1237\u001b[0m       26.3866  0.0111\n",
      "     41       \u001b[36m23.1217\u001b[0m       26.3862  0.0110\n",
      "     42       \u001b[36m23.1197\u001b[0m       26.3861  0.0113\n",
      "     43       \u001b[36m23.1179\u001b[0m       26.3857  0.0113\n",
      "     44       \u001b[36m23.1161\u001b[0m       26.3856  0.0112\n",
      "     45       \u001b[36m23.1144\u001b[0m       26.3851  0.0109\n",
      "     46       \u001b[36m23.1127\u001b[0m       26.3847  0.0111\n",
      "     47       \u001b[36m23.1112\u001b[0m       26.3842  0.0114\n",
      "     48       \u001b[36m23.1097\u001b[0m       26.3837  0.0115\n",
      "     49       \u001b[36m23.1083\u001b[0m       26.3834  0.0113\n",
      "     50       \u001b[36m23.1069\u001b[0m       26.3828  0.0111\n",
      "     51       \u001b[36m23.1056\u001b[0m       26.3824  0.0108\n",
      "     52       \u001b[36m23.1042\u001b[0m       26.3818  0.0110\n",
      "     53       \u001b[36m23.1030\u001b[0m       26.3814  0.0113\n",
      "     54       \u001b[36m23.1018\u001b[0m       26.3810  0.0110\n",
      "     55       \u001b[36m23.1006\u001b[0m       26.3804  0.0108\n",
      "     56       \u001b[36m23.0995\u001b[0m       26.3800  0.0107\n",
      "     57       \u001b[36m23.0984\u001b[0m       26.3795  0.0111\n",
      "     58       \u001b[36m23.0974\u001b[0m       26.3792  0.0119\n",
      "     59       \u001b[36m23.0963\u001b[0m       26.3786  0.0109\n",
      "     60       \u001b[36m23.0953\u001b[0m       26.3782  0.0106\n",
      "     61       \u001b[36m23.0944\u001b[0m       26.3777  0.0109\n",
      "     62       \u001b[36m23.0934\u001b[0m       26.3774  0.0110\n",
      "     63       \u001b[36m23.0925\u001b[0m       26.3770  0.0113\n",
      "     64       \u001b[36m23.0917\u001b[0m       26.3764  0.0111\n",
      "     65       \u001b[36m23.0908\u001b[0m       26.3761  0.0108\n",
      "     66       \u001b[36m23.0900\u001b[0m       26.3758  0.0107\n",
      "     67       \u001b[36m23.0892\u001b[0m       26.3755  0.0110\n",
      "     68       \u001b[36m23.0884\u001b[0m       26.3750  0.0114\n",
      "     69       \u001b[36m23.0876\u001b[0m       26.3748  0.0116\n",
      "     70       \u001b[36m23.0868\u001b[0m       26.3744  0.0118\n",
      "     71       \u001b[36m23.0861\u001b[0m       26.3739  0.0112\n",
      "     72       \u001b[36m23.0854\u001b[0m       26.3738  0.0115\n",
      "     73       \u001b[36m23.0847\u001b[0m       26.3734  0.0113\n",
      "     74       \u001b[36m23.0840\u001b[0m       26.3730  0.0113\n",
      "     75       \u001b[36m23.0834\u001b[0m       26.3727  0.0112\n",
      "     76       \u001b[36m23.0827\u001b[0m       26.3725  0.0110\n",
      "     77       \u001b[36m23.0820\u001b[0m       26.3721  0.0112\n",
      "     78       \u001b[36m23.0814\u001b[0m       26.3718  0.0116\n",
      "     79       \u001b[36m23.0808\u001b[0m       26.3717  0.0119\n",
      "     80       \u001b[36m23.0802\u001b[0m       26.3714  0.0113\n",
      "     81       \u001b[36m23.0796\u001b[0m       26.3710  0.0110\n",
      "     82       \u001b[36m23.0790\u001b[0m       26.3708  0.0114\n",
      "     83       \u001b[36m23.0785\u001b[0m       26.3704  0.0114\n",
      "     84       \u001b[36m23.0779\u001b[0m       26.3702  0.0115\n",
      "     85       \u001b[36m23.0773\u001b[0m       26.3699  0.0111\n",
      "     86       \u001b[36m23.0768\u001b[0m       26.3696  0.0108\n",
      "     87       \u001b[36m23.0763\u001b[0m       26.3694  0.0113\n",
      "     88       \u001b[36m23.0757\u001b[0m       26.3692  0.0114\n",
      "     89       \u001b[36m23.0752\u001b[0m       26.3689  0.0113\n",
      "     90       \u001b[36m23.0747\u001b[0m       26.3686  0.0110\n",
      "     91       \u001b[36m23.0742\u001b[0m       26.3685  0.0111\n",
      "     92       \u001b[36m23.0737\u001b[0m       26.3682  0.0113\n",
      "     93       \u001b[36m23.0732\u001b[0m       26.3679  0.0115\n",
      "     94       \u001b[36m23.0727\u001b[0m       26.3677  0.0111\n",
      "     95       \u001b[36m23.0723\u001b[0m       26.3674  0.0110\n",
      "     96       \u001b[36m23.0718\u001b[0m       26.3671  0.0106\n",
      "     97       \u001b[36m23.0714\u001b[0m       26.3669  0.0114\n",
      "     98       \u001b[36m23.0709\u001b[0m       26.3666  0.0118\n",
      "     99       \u001b[36m23.0705\u001b[0m       26.3664  0.0117\n",
      "    100       \u001b[36m23.0700\u001b[0m       26.3663  0.0113\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m40.3999\u001b[0m       \u001b[32m30.2481\u001b[0m  0.0108\n",
      "      2       \u001b[36m35.7594\u001b[0m       \u001b[32m27.5671\u001b[0m  0.0142\n",
      "      3       \u001b[36m32.0690\u001b[0m       \u001b[32m26.2564\u001b[0m  0.0123\n",
      "      4       \u001b[36m29.7094\u001b[0m       26.5137  0.0118\n",
      "      5       \u001b[36m28.8877\u001b[0m       27.0652  0.0108\n",
      "      6       \u001b[36m28.7467\u001b[0m       27.2632  0.0107\n",
      "      7       \u001b[36m28.6952\u001b[0m       27.3085  0.0135\n",
      "      8       \u001b[36m28.6558\u001b[0m       27.3205  0.0118\n",
      "      9       \u001b[36m28.6233\u001b[0m       27.3253  0.0121\n",
      "     10       \u001b[36m28.5964\u001b[0m       27.3290  0.0105\n",
      "     11       \u001b[36m28.5745\u001b[0m       27.3334  0.0105\n",
      "     12       \u001b[36m28.5564\u001b[0m       27.3362  0.0132\n",
      "     13       \u001b[36m28.5413\u001b[0m       27.3397  0.0133\n",
      "     14       \u001b[36m28.5287\u001b[0m       27.3421  0.0172\n",
      "     15       \u001b[36m28.5177\u001b[0m       27.3446  0.0117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     16       \u001b[36m28.5082\u001b[0m       27.3465  0.0125\n",
      "     17       \u001b[36m28.5001\u001b[0m       27.3487  0.0130\n",
      "     18       \u001b[36m28.4930\u001b[0m       27.3497  0.0142\n",
      "     19       \u001b[36m28.4866\u001b[0m       27.3507  0.0130\n",
      "     20       \u001b[36m28.4811\u001b[0m       27.3515  0.0132\n",
      "     21       \u001b[36m28.4762\u001b[0m       27.3513  0.0122\n",
      "     22       \u001b[36m28.4717\u001b[0m       27.3511  0.0118\n",
      "     23       \u001b[36m28.4677\u001b[0m       27.3508  0.0125\n",
      "     24       \u001b[36m28.4641\u001b[0m       27.3498  0.0122\n",
      "     25       \u001b[36m28.4608\u001b[0m       27.3492  0.0130\n",
      "     26       \u001b[36m28.4578\u001b[0m       27.3488  0.0135\n",
      "     27       \u001b[36m28.4551\u001b[0m       27.3485  0.0118\n",
      "     28       \u001b[36m28.4525\u001b[0m       27.3479  0.0116\n",
      "     29       \u001b[36m28.4501\u001b[0m       27.3473  0.0135\n",
      "     30       \u001b[36m28.4479\u001b[0m       27.3469  0.0133\n",
      "     31       \u001b[36m28.4459\u001b[0m       27.3456  0.0128\n",
      "     32       \u001b[36m28.4439\u001b[0m       27.3446  0.0121\n",
      "     33       \u001b[36m28.4420\u001b[0m       27.3435  0.0119\n",
      "     34       \u001b[36m28.4402\u001b[0m       27.3431  0.0113\n",
      "     35       \u001b[36m28.4386\u001b[0m       27.3429  0.0123\n",
      "     36       \u001b[36m28.4372\u001b[0m       27.3421  0.0115\n",
      "     37       \u001b[36m28.4358\u001b[0m       27.3416  0.0117\n",
      "     38       \u001b[36m28.4344\u001b[0m       27.3412  0.0110\n",
      "     39       \u001b[36m28.4332\u001b[0m       27.3406  0.0107\n",
      "     40       \u001b[36m28.4320\u001b[0m       27.3403  0.0115\n",
      "     41       \u001b[36m28.4309\u001b[0m       27.3388  0.0111\n",
      "     42       \u001b[36m28.4298\u001b[0m       27.3383  0.0106\n",
      "     43       \u001b[36m28.4288\u001b[0m       27.3382  0.0106\n",
      "     44       \u001b[36m28.4278\u001b[0m       27.3372  0.0107\n",
      "     45       \u001b[36m28.4268\u001b[0m       27.3366  0.0109\n",
      "     46       \u001b[36m28.4259\u001b[0m       27.3364  0.0113\n",
      "     47       \u001b[36m28.4251\u001b[0m       27.3353  0.0110\n",
      "     48       \u001b[36m28.4242\u001b[0m       27.3349  0.0108\n",
      "     49       \u001b[36m28.4234\u001b[0m       27.3343  0.0104\n",
      "     50       \u001b[36m28.4226\u001b[0m       27.3345  0.0109\n",
      "     51       \u001b[36m28.4220\u001b[0m       27.3337  0.0112\n",
      "     52       \u001b[36m28.4212\u001b[0m       27.3327  0.0110\n",
      "     53       \u001b[36m28.4205\u001b[0m       27.3323  0.0104\n",
      "     54       \u001b[36m28.4198\u001b[0m       27.3327  0.0105\n",
      "     55       \u001b[36m28.4193\u001b[0m       27.3315  0.0110\n",
      "     56       \u001b[36m28.4186\u001b[0m       27.3313  0.0112\n",
      "     57       \u001b[36m28.4180\u001b[0m       27.3312  0.0112\n",
      "     58       \u001b[36m28.4175\u001b[0m       27.3306  0.0110\n",
      "     59       \u001b[36m28.4169\u001b[0m       27.3305  0.0108\n",
      "     60       \u001b[36m28.4164\u001b[0m       27.3308  0.0113\n",
      "     61       \u001b[36m28.4160\u001b[0m       27.3294  0.0114\n",
      "     62       \u001b[36m28.4153\u001b[0m       27.3287  0.0111\n",
      "     63       \u001b[36m28.4148\u001b[0m       27.3288  0.0106\n",
      "     64       \u001b[36m28.4143\u001b[0m       27.3289  0.0127\n",
      "     65       \u001b[36m28.4139\u001b[0m       27.3280  0.0145\n",
      "     66       \u001b[36m28.4134\u001b[0m       27.3278  0.0113\n",
      "     67       \u001b[36m28.4129\u001b[0m       27.3275  0.0111\n",
      "     68       \u001b[36m28.4124\u001b[0m       27.3272  0.0111\n",
      "     69       \u001b[36m28.4120\u001b[0m       27.3273  0.0109\n",
      "     70       \u001b[36m28.4117\u001b[0m       27.3274  0.0111\n",
      "     71       \u001b[36m28.4113\u001b[0m       27.3272  0.0113\n",
      "     72       \u001b[36m28.4109\u001b[0m       27.3270  0.0112\n",
      "     73       \u001b[36m28.4105\u001b[0m       27.3264  0.0107\n",
      "     74       \u001b[36m28.4102\u001b[0m       27.3260  0.0108\n",
      "     75       \u001b[36m28.4098\u001b[0m       27.3256  0.0112\n",
      "     76       \u001b[36m28.4094\u001b[0m       27.3252  0.0117\n",
      "     77       \u001b[36m28.4091\u001b[0m       27.3248  0.0109\n",
      "     78       \u001b[36m28.4088\u001b[0m       27.3243  0.0107\n",
      "     79       \u001b[36m28.4084\u001b[0m       27.3242  0.0107\n",
      "     80       \u001b[36m28.4082\u001b[0m       27.3237  0.0109\n",
      "     81       \u001b[36m28.4079\u001b[0m       27.3232  0.0110\n",
      "     82       \u001b[36m28.4075\u001b[0m       27.3228  0.0109\n",
      "     83       \u001b[36m28.4072\u001b[0m       27.3225  0.0108\n",
      "     84       \u001b[36m28.4070\u001b[0m       27.3223  0.0106\n",
      "     85       \u001b[36m28.4067\u001b[0m       27.3224  0.0112\n",
      "     86       \u001b[36m28.4065\u001b[0m       27.3214  0.0113\n",
      "     87       \u001b[36m28.4061\u001b[0m       27.3211  0.0106\n",
      "     88       \u001b[36m28.4059\u001b[0m       27.3205  0.0108\n",
      "     89       \u001b[36m28.4056\u001b[0m       27.3203  0.0106\n",
      "     90       \u001b[36m28.4054\u001b[0m       27.3200  0.0112\n",
      "     91       \u001b[36m28.4051\u001b[0m       27.3198  0.0113\n",
      "     92       \u001b[36m28.4049\u001b[0m       27.3195  0.0110\n",
      "     93       \u001b[36m28.4047\u001b[0m       27.3191  0.0114\n",
      "     94       \u001b[36m28.4044\u001b[0m       27.3191  0.0116\n",
      "     95       \u001b[36m28.4042\u001b[0m       27.3183  0.0187\n",
      "     96       \u001b[36m28.4040\u001b[0m       27.3175  0.0136\n",
      "     97       \u001b[36m28.4037\u001b[0m       27.3175  0.0127\n",
      "     98       \u001b[36m28.4035\u001b[0m       27.3169  0.0115\n",
      "     99       \u001b[36m28.4032\u001b[0m       27.3171  0.0123\n",
      "    100       \u001b[36m28.4031\u001b[0m       27.3167  0.0138\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m41.9471\u001b[0m       \u001b[32m38.7336\u001b[0m  0.0134\n",
      "      2       \u001b[36m36.1392\u001b[0m       \u001b[32m32.0360\u001b[0m  0.0124\n",
      "      3       \u001b[36m34.6013\u001b[0m       32.9421  0.0127\n",
      "      4       \u001b[36m33.9961\u001b[0m       32.7024  0.0140\n",
      "      5       \u001b[36m33.4349\u001b[0m       \u001b[32m30.6084\u001b[0m  0.0125\n",
      "      6       \u001b[36m33.0711\u001b[0m       \u001b[32m30.5692\u001b[0m  0.0120\n",
      "      7       \u001b[36m32.8916\u001b[0m       30.9256  0.0118\n",
      "      8       \u001b[36m32.7585\u001b[0m       \u001b[32m30.4040\u001b[0m  0.0124\n",
      "      9       \u001b[36m32.5834\u001b[0m       \u001b[32m30.1664\u001b[0m  0.0121\n",
      "     10       \u001b[36m32.5147\u001b[0m       30.4237  0.0122\n",
      "     11       \u001b[36m32.4600\u001b[0m       30.3485  0.0123\n",
      "     12       \u001b[36m32.3835\u001b[0m       \u001b[32m30.1238\u001b[0m  0.0121\n",
      "     13       \u001b[36m32.3531\u001b[0m       30.2381  0.0121\n",
      "     14       \u001b[36m32.3348\u001b[0m       30.2121  0.0118\n",
      "     15       \u001b[36m32.3011\u001b[0m       \u001b[32m30.1045\u001b[0m  0.0120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     16       \u001b[36m32.2850\u001b[0m       30.1335  0.0119\n",
      "     17       \u001b[36m32.2726\u001b[0m       30.1524  0.0121\n",
      "     18       \u001b[36m32.2572\u001b[0m       \u001b[32m30.0903\u001b[0m  0.0119\n",
      "     19       \u001b[36m32.2452\u001b[0m       30.1009  0.0120\n",
      "     20       \u001b[36m32.2387\u001b[0m       30.0981  0.0121\n",
      "     21       \u001b[36m32.2287\u001b[0m       \u001b[32m30.0876\u001b[0m  0.0119\n",
      "     22       \u001b[36m32.2211\u001b[0m       \u001b[32m30.0795\u001b[0m  0.0116\n",
      "     23       \u001b[36m32.2175\u001b[0m       30.0940  0.0112\n",
      "     24       \u001b[36m32.2075\u001b[0m       \u001b[32m30.0710\u001b[0m  0.0122\n",
      "     25       32.2076       30.0744  0.0118\n",
      "     26       \u001b[36m32.1977\u001b[0m       30.0880  0.0116\n",
      "     27       32.1990       30.0903  0.0118\n",
      "     28       32.2108       \u001b[32m30.0607\u001b[0m  0.0117\n",
      "     29       \u001b[36m32.1845\u001b[0m       30.0875  0.0120\n",
      "     30       32.1989       30.1124  0.0122\n",
      "     31       32.2199       30.1676  0.0114\n",
      "     32       32.1883       \u001b[32m30.0383\u001b[0m  0.0113\n",
      "     33       32.1971       30.0807  0.0125\n",
      "     34       \u001b[36m32.1713\u001b[0m       \u001b[32m30.0226\u001b[0m  0.0118\n",
      "     35       \u001b[36m32.1687\u001b[0m       30.0806  0.0123\n",
      "     36       \u001b[36m32.1667\u001b[0m       30.0786  0.0118\n",
      "     37       \u001b[36m32.1571\u001b[0m       30.0431  0.0120\n",
      "     38       32.1588       30.0860  0.0119\n",
      "     39       \u001b[36m32.1530\u001b[0m       30.0544  0.0117\n",
      "     40       \u001b[36m32.1514\u001b[0m       30.0726  0.0122\n",
      "     41       \u001b[36m32.1488\u001b[0m       30.0756  0.0118\n",
      "     42       \u001b[36m32.1459\u001b[0m       30.0687  0.0119\n",
      "     43       \u001b[36m32.1451\u001b[0m       30.0863  0.0113\n",
      "     44       \u001b[36m32.1402\u001b[0m       30.0848  0.0111\n",
      "     45       32.1414       30.0829  0.0122\n",
      "     46       32.1402       30.0987  0.0119\n",
      "     47       \u001b[36m32.1346\u001b[0m       30.1044  0.0122\n",
      "     48       32.1394       30.0899  0.0114\n",
      "     49       32.1420       30.1754  0.0112\n",
      "     50       32.1547       30.1474  0.0121\n",
      "     51       32.1642       30.0799  0.0118\n",
      "     52       32.1537       30.3295  0.0118\n",
      "     53       32.1924       30.2050  0.0114\n",
      "     54       32.1943       30.4114  0.0115\n",
      "     55       32.1526       \u001b[32m30.0149\u001b[0m  0.0121\n",
      "     56       32.1792       30.4382  0.0118\n",
      "     57       32.1611       \u001b[32m29.9619\u001b[0m  0.0120\n",
      "     58       32.1690       30.4603  0.0118\n",
      "     59       32.1616       \u001b[32m29.9505\u001b[0m  0.0120\n",
      "     60       32.1383       30.3027  0.0122\n",
      "     61       32.1456       30.0105  0.0117\n",
      "     62       \u001b[36m32.1138\u001b[0m       30.2029  0.0115\n",
      "     63       32.1271       30.0991  0.0114\n",
      "     64       \u001b[36m32.1029\u001b[0m       30.1400  0.0116\n",
      "     65       32.1133       30.1441  0.0123\n",
      "     66       \u001b[36m32.1002\u001b[0m       30.1388  0.0120\n",
      "     67       32.1047       30.1716  0.0157\n",
      "     68       \u001b[36m32.0985\u001b[0m       30.1552  0.0215\n",
      "     69       32.0995       30.1826  0.0129\n",
      "     70       \u001b[36m32.0962\u001b[0m       30.1706  0.0122\n",
      "     71       \u001b[36m32.0957\u001b[0m       30.1961  0.0133\n",
      "     72       \u001b[36m32.0934\u001b[0m       30.1877  0.0172\n",
      "     73       \u001b[36m32.0926\u001b[0m       30.2025  0.0161\n",
      "     74       \u001b[36m32.0907\u001b[0m       30.1990  0.0128\n",
      "     75       \u001b[36m32.0896\u001b[0m       30.2147  0.0134\n",
      "     76       \u001b[36m32.0877\u001b[0m       30.2127  0.0155\n",
      "     77       \u001b[36m32.0871\u001b[0m       30.2242  0.0126\n",
      "     78       \u001b[36m32.0851\u001b[0m       30.2229  0.0161\n",
      "     79       \u001b[36m32.0843\u001b[0m       30.2303  0.0121\n",
      "     80       \u001b[36m32.0822\u001b[0m       30.2353  0.0119\n",
      "     81       \u001b[36m32.0816\u001b[0m       30.2405  0.0118\n",
      "     82       \u001b[36m32.0797\u001b[0m       30.2450  0.0116\n",
      "     83       \u001b[36m32.0793\u001b[0m       30.2461  0.0121\n",
      "     84       \u001b[36m32.0773\u001b[0m       30.2547  0.0121\n",
      "     85       \u001b[36m32.0767\u001b[0m       30.2537  0.0119\n",
      "     86       \u001b[36m32.0750\u001b[0m       30.2596  0.0117\n",
      "     87       \u001b[36m32.0746\u001b[0m       30.2595  0.0117\n",
      "     88       \u001b[36m32.0730\u001b[0m       30.2704  0.0123\n",
      "     89       32.0731       30.2624  0.0119\n",
      "     90       \u001b[36m32.0704\u001b[0m       30.2759  0.0120\n",
      "     91       \u001b[36m32.0692\u001b[0m       30.2669  0.0122\n",
      "     92       \u001b[36m32.0676\u001b[0m       30.2812  0.0116\n",
      "     93       \u001b[36m32.0664\u001b[0m       30.2727  0.0115\n",
      "     94       \u001b[36m32.0647\u001b[0m       30.2845  0.0115\n",
      "     95       \u001b[36m32.0641\u001b[0m       30.2846  0.0133\n",
      "     96       \u001b[36m32.0617\u001b[0m       30.2987  0.0139\n",
      "     97       32.0621       30.2835  0.0126\n",
      "     98       \u001b[36m32.0598\u001b[0m       30.3047  0.0119\n",
      "     99       \u001b[36m32.0582\u001b[0m       30.3018  0.0119\n",
      "    100       \u001b[36m32.0582\u001b[0m       30.2990  0.0121\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m32.7541\u001b[0m       \u001b[32m28.6605\u001b[0m  0.0123\n",
      "      2       \u001b[36m26.8433\u001b[0m       29.0198  0.0122\n",
      "      3       \u001b[36m25.1091\u001b[0m       \u001b[32m26.6178\u001b[0m  0.0119\n",
      "      4       \u001b[36m24.7975\u001b[0m       26.6308  0.0120\n",
      "      5       \u001b[36m24.1656\u001b[0m       28.0828  0.0118\n",
      "      6       \u001b[36m23.6588\u001b[0m       27.1512  0.0119\n",
      "      7       \u001b[36m23.6450\u001b[0m       27.0311  0.0116\n",
      "      8       \u001b[36m23.4776\u001b[0m       27.5581  0.0116\n",
      "      9       \u001b[36m23.3737\u001b[0m       27.1369  0.0119\n",
      "     10       \u001b[36m23.2991\u001b[0m       26.8386  0.0119\n",
      "     11       \u001b[36m23.2813\u001b[0m       27.1743  0.0119\n",
      "     12       \u001b[36m23.2197\u001b[0m       27.1459  0.0116\n",
      "     13       \u001b[36m23.1900\u001b[0m       26.8550  0.0112\n",
      "     14       \u001b[36m23.1751\u001b[0m       26.8710  0.0123\n",
      "     15       \u001b[36m23.1515\u001b[0m       26.9110  0.0121\n",
      "     16       \u001b[36m23.1380\u001b[0m       26.9026  0.0122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m23.1225\u001b[0m       26.8537  0.0121\n",
      "     18       \u001b[36m23.1137\u001b[0m       26.8497  0.0117\n",
      "     19       \u001b[36m23.1038\u001b[0m       26.8971  0.0132\n",
      "     20       \u001b[36m23.0931\u001b[0m       26.8672  0.0132\n",
      "     21       \u001b[36m23.0889\u001b[0m       26.8782  0.0120\n",
      "     22       \u001b[36m23.0802\u001b[0m       26.8740  0.0119\n",
      "     23       \u001b[36m23.0748\u001b[0m       26.8603  0.0116\n",
      "     24       \u001b[36m23.0696\u001b[0m       26.8506  0.0124\n",
      "     25       \u001b[36m23.0646\u001b[0m       26.8389  0.0122\n",
      "     26       \u001b[36m23.0600\u001b[0m       26.8297  0.0121\n",
      "     27       \u001b[36m23.0563\u001b[0m       26.8220  0.0116\n",
      "     28       \u001b[36m23.0524\u001b[0m       26.8122  0.0116\n",
      "     29       \u001b[36m23.0489\u001b[0m       26.8000  0.0123\n",
      "     30       \u001b[36m23.0459\u001b[0m       26.7949  0.0120\n",
      "     31       \u001b[36m23.0425\u001b[0m       26.7815  0.0118\n",
      "     32       \u001b[36m23.0398\u001b[0m       26.7825  0.0119\n",
      "     33       \u001b[36m23.0368\u001b[0m       26.7817  0.0118\n",
      "     34       \u001b[36m23.0345\u001b[0m       26.7852  0.0134\n",
      "     35       \u001b[36m23.0316\u001b[0m       26.7828  0.0122\n",
      "     36       \u001b[36m23.0297\u001b[0m       26.7829  0.0121\n",
      "     37       \u001b[36m23.0269\u001b[0m       26.7771  0.0124\n",
      "     38       \u001b[36m23.0253\u001b[0m       26.7810  0.0122\n",
      "     39       \u001b[36m23.0221\u001b[0m       26.7713  0.0122\n",
      "     40       \u001b[36m23.0215\u001b[0m       26.7848  0.0122\n",
      "     41       \u001b[36m23.0184\u001b[0m       26.7680  0.0119\n",
      "     42       23.0210       26.7848  0.0119\n",
      "     43       23.0210       26.7642  0.0118\n",
      "     44       23.0446       26.8094  0.0123\n",
      "     45       23.0682       26.7234  0.0122\n",
      "     46       23.0749       26.9220  0.0121\n",
      "     47       23.0563       \u001b[32m26.5982\u001b[0m  0.0117\n",
      "     48       23.0421       27.0300  0.0163\n",
      "     49       \u001b[36m23.0130\u001b[0m       26.6136  0.0145\n",
      "     50       23.0230       26.8599  0.0118\n",
      "     51       \u001b[36m23.0079\u001b[0m       26.7708  0.0126\n",
      "     52       \u001b[36m23.0037\u001b[0m       26.7559  0.0125\n",
      "     53       \u001b[36m23.0030\u001b[0m       26.7972  0.0159\n",
      "     54       \u001b[36m22.9988\u001b[0m       26.7711  0.0123\n",
      "     55       22.9992       26.7622  0.0159\n",
      "     56       \u001b[36m22.9965\u001b[0m       26.7803  0.0123\n",
      "     57       \u001b[36m22.9954\u001b[0m       26.7540  0.0122\n",
      "     58       \u001b[36m22.9938\u001b[0m       26.7680  0.0121\n",
      "     59       \u001b[36m22.9932\u001b[0m       26.7583  0.0119\n",
      "     60       \u001b[36m22.9913\u001b[0m       26.7550  0.0121\n",
      "     61       \u001b[36m22.9908\u001b[0m       26.7633  0.0122\n",
      "     62       \u001b[36m22.9890\u001b[0m       26.7515  0.0122\n",
      "     63       \u001b[36m22.9877\u001b[0m       26.7554  0.0121\n",
      "     64       \u001b[36m22.9871\u001b[0m       26.7492  0.0120\n",
      "     65       \u001b[36m22.9852\u001b[0m       26.7534  0.0120\n",
      "     66       \u001b[36m22.9851\u001b[0m       26.7477  0.0121\n",
      "     67       \u001b[36m22.9833\u001b[0m       26.7481  0.0116\n",
      "     68       22.9838       26.7437  0.0114\n",
      "     69       22.9846       26.7527  0.0115\n",
      "     70       22.9840       26.7305  0.0115\n",
      "     71       22.9846       26.7622  0.0118\n",
      "     72       \u001b[36m22.9794\u001b[0m       26.7232  0.0116\n",
      "     73       22.9823       26.7609  0.0117\n",
      "     74       \u001b[36m22.9750\u001b[0m       26.7264  0.0120\n",
      "     75       22.9790       26.7511  0.0116\n",
      "     76       \u001b[36m22.9725\u001b[0m       26.7393  0.0116\n",
      "     77       22.9741       26.7468  0.0117\n",
      "     78       22.9728       26.7428  0.0117\n",
      "     79       \u001b[36m22.9706\u001b[0m       26.7572  0.0120\n",
      "     80       22.9740       26.7162  0.0114\n",
      "     81       22.9769       26.8074  0.0114\n",
      "     82       22.9729       26.6767  0.0121\n",
      "     83       22.9825       26.8293  0.0117\n",
      "     84       \u001b[36m22.9669\u001b[0m       26.6788  0.0116\n",
      "     85       22.9772       26.8122  0.0119\n",
      "     86       22.9676       26.7112  0.0116\n",
      "     87       22.9742       26.8176  0.0122\n",
      "     88       22.9917       26.6810  0.0118\n",
      "     89       23.0194       26.9032  0.0118\n",
      "     90       22.9907       \u001b[32m26.5949\u001b[0m  0.0116\n",
      "     91       23.0181       26.9206  0.0115\n",
      "     92       22.9782       26.6979  0.0124\n",
      "     93       23.0050       26.7827  0.0121\n",
      "     94       23.0023       26.7434  0.0119\n",
      "     95       \u001b[36m22.9659\u001b[0m       26.7551  0.0114\n",
      "     96       22.9765       26.7291  0.0113\n",
      "     97       \u001b[36m22.9641\u001b[0m       26.7477  0.0122\n",
      "     98       22.9642       26.7439  0.0119\n",
      "     99       \u001b[36m22.9594\u001b[0m       26.7449  0.0121\n",
      "    100       \u001b[36m22.9556\u001b[0m       26.7259  0.0116\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m37.8783\u001b[0m       \u001b[32m26.9136\u001b[0m  0.0115\n",
      "      2       \u001b[36m31.7013\u001b[0m       28.7336  0.0114\n",
      "      3       \u001b[36m29.9819\u001b[0m       \u001b[32m26.3600\u001b[0m  0.0113\n",
      "      4       \u001b[36m29.4956\u001b[0m       27.8759  0.0127\n",
      "      5       \u001b[36m29.3713\u001b[0m       27.8586  0.0118\n",
      "      6       \u001b[36m28.9212\u001b[0m       27.0533  0.0119\n",
      "      7       \u001b[36m28.7571\u001b[0m       27.7004  0.0114\n",
      "      8       28.7579       27.2999  0.0113\n",
      "      9       \u001b[36m28.6145\u001b[0m       27.1415  0.0118\n",
      "     10       \u001b[36m28.5671\u001b[0m       27.5275  0.0119\n",
      "     11       \u001b[36m28.5641\u001b[0m       27.3682  0.0117\n",
      "     12       \u001b[36m28.5184\u001b[0m       27.1977  0.0114\n",
      "     13       \u001b[36m28.4944\u001b[0m       27.3693  0.0114\n",
      "     14       28.4944       27.2879  0.0121\n",
      "     15       \u001b[36m28.4667\u001b[0m       27.2759  0.0119\n",
      "     16       28.4687       27.3336  0.0117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m28.4573\u001b[0m       27.2565  0.0136\n",
      "     18       \u001b[36m28.4488\u001b[0m       27.3378  0.0128\n",
      "     19       28.4489       27.2716  0.0117\n",
      "     20       \u001b[36m28.4387\u001b[0m       27.2781  0.0116\n",
      "     21       \u001b[36m28.4379\u001b[0m       27.2980  0.0112\n",
      "     22       \u001b[36m28.4328\u001b[0m       27.2459  0.0119\n",
      "     23       \u001b[36m28.4303\u001b[0m       27.2920  0.0116\n",
      "     24       \u001b[36m28.4258\u001b[0m       27.2475  0.0116\n",
      "     25       28.4297       27.2701  0.0115\n",
      "     26       \u001b[36m28.4196\u001b[0m       27.2683  0.0155\n",
      "     27       28.4257       27.2402  0.0163\n",
      "     28       28.4278       27.2587  0.0129\n",
      "     29       28.4263       27.2541  0.0129\n",
      "     30       28.4392       27.2151  0.0127\n",
      "     31       \u001b[36m28.4109\u001b[0m       27.2220  0.0147\n",
      "     32       28.4394       27.2542  0.0133\n",
      "     33       28.4650       27.2659  0.0130\n",
      "     34       28.4645       27.0977  0.0126\n",
      "     35       28.4508       27.3756  0.0122\n",
      "     36       \u001b[36m28.4057\u001b[0m       27.0578  0.0121\n",
      "     37       28.4150       27.2585  0.0128\n",
      "     38       \u001b[36m28.4024\u001b[0m       27.1653  0.0123\n",
      "     39       28.4050       27.2277  0.0122\n",
      "     40       \u001b[36m28.3983\u001b[0m       27.1609  0.0122\n",
      "     41       28.4001       27.2034  0.0122\n",
      "     42       \u001b[36m28.3974\u001b[0m       27.1927  0.0122\n",
      "     43       \u001b[36m28.3945\u001b[0m       27.1811  0.0125\n",
      "     44       28.3962       27.1957  0.0120\n",
      "     45       \u001b[36m28.3904\u001b[0m       27.1795  0.0117\n",
      "     46       28.3923       27.1891  0.0116\n",
      "     47       28.3911       27.1806  0.0124\n",
      "     48       \u001b[36m28.3889\u001b[0m       27.1807  0.0121\n",
      "     49       28.3932       27.1824  0.0121\n",
      "     50       \u001b[36m28.3875\u001b[0m       27.1742  0.0125\n",
      "     51       \u001b[36m28.3857\u001b[0m       27.1846  0.0120\n",
      "     52       28.3881       27.1705  0.0120\n",
      "     53       \u001b[36m28.3849\u001b[0m       27.1765  0.0116\n",
      "     54       \u001b[36m28.3835\u001b[0m       27.1858  0.0116\n",
      "     55       28.3932       27.1532  0.0118\n",
      "     56       28.3896       27.1872  0.0125\n",
      "     57       28.3948       27.2020  0.0121\n",
      "     58       28.4269       27.0949  0.0117\n",
      "     59       28.4179       27.2586  0.0116\n",
      "     60       28.4431       27.1655  0.0126\n",
      "     61       28.4502       27.1843  0.0123\n",
      "     62       28.4094       27.1102  0.0121\n",
      "     63       28.4152       27.2065  0.0118\n",
      "     64       \u001b[36m28.3784\u001b[0m       27.1856  0.0116\n",
      "     65       28.3963       27.0756  0.0125\n",
      "     66       28.3784       27.2370  0.0119\n",
      "     67       28.3822       27.1130  0.0118\n",
      "     68       \u001b[36m28.3754\u001b[0m       27.1722  0.0120\n",
      "     69       28.3766       27.1457  0.0119\n",
      "     70       28.3759       27.1670  0.0124\n",
      "     71       \u001b[36m28.3729\u001b[0m       27.1430  0.0123\n",
      "     72       28.3735       27.1481  0.0123\n",
      "     73       28.3734       27.1530  0.0122\n",
      "     74       \u001b[36m28.3697\u001b[0m       27.1388  0.0123\n",
      "     75       28.3722       27.1456  0.0151\n",
      "     76       \u001b[36m28.3684\u001b[0m       27.1433  0.0128\n",
      "     77       28.3699       27.1350  0.0126\n",
      "     78       28.3687       27.1389  0.0121\n",
      "     79       \u001b[36m28.3671\u001b[0m       27.1317  0.0122\n",
      "     80       28.3678       27.1328  0.0121\n",
      "     81       28.3672       27.1214  0.0118\n",
      "     82       \u001b[36m28.3648\u001b[0m       27.1345  0.0124\n",
      "     83       28.3676       27.1145  0.0122\n",
      "     84       28.3649       27.1260  0.0121\n",
      "     85       \u001b[36m28.3638\u001b[0m       27.1253  0.0122\n",
      "     86       28.3676       27.1079  0.0119\n",
      "     87       \u001b[36m28.3628\u001b[0m       27.1217  0.0117\n",
      "     88       28.3638       27.1316  0.0124\n",
      "     89       28.3708       27.0769  0.0120\n",
      "     90       28.3636       27.1367  0.0123\n",
      "     91       28.3712       27.1290  0.0121\n",
      "     92       28.3835       27.0434  0.0118\n",
      "     93       28.3698       27.1480  0.0122\n",
      "     94       28.3957       27.1449  0.0122\n",
      "     95       28.4097       27.0021  0.0120\n",
      "     96       28.3876       27.1318  0.0122\n",
      "     97       28.4016       27.0086  0.0119\n",
      "     98       28.3656       27.1469  0.0126\n",
      "     99       28.3777       26.9973  0.0120\n",
      "    100       28.3743       27.0813  0.0119\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m43.0591\u001b[0m       \u001b[32m41.8742\u001b[0m  0.0109\n",
      "      2       \u001b[36m39.1428\u001b[0m       \u001b[32m37.0208\u001b[0m  0.0115\n",
      "      3       \u001b[36m35.7152\u001b[0m       \u001b[32m33.2418\u001b[0m  0.0161\n",
      "      4       \u001b[36m33.6756\u001b[0m       \u001b[32m31.3273\u001b[0m  0.0129\n",
      "      5       \u001b[36m33.0059\u001b[0m       \u001b[32m30.6720\u001b[0m  0.0116\n",
      "      6       \u001b[36m32.8268\u001b[0m       \u001b[32m30.4533\u001b[0m  0.0114\n",
      "      7       \u001b[36m32.7388\u001b[0m       \u001b[32m30.3518\u001b[0m  0.0123\n",
      "      8       \u001b[36m32.6759\u001b[0m       \u001b[32m30.2872\u001b[0m  0.0147\n",
      "      9       \u001b[36m32.6264\u001b[0m       \u001b[32m30.2376\u001b[0m  0.0115\n",
      "     10       \u001b[36m32.5867\u001b[0m       \u001b[32m30.1972\u001b[0m  0.0120\n",
      "     11       \u001b[36m32.5540\u001b[0m       \u001b[32m30.1616\u001b[0m  0.0113\n",
      "     12       \u001b[36m32.5267\u001b[0m       \u001b[32m30.1311\u001b[0m  0.0115\n",
      "     13       \u001b[36m32.5028\u001b[0m       \u001b[32m30.1048\u001b[0m  0.0117\n",
      "     14       \u001b[36m32.4825\u001b[0m       \u001b[32m30.0821\u001b[0m  0.0117\n",
      "     15       \u001b[36m32.4646\u001b[0m       \u001b[32m30.0616\u001b[0m  0.0112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     16       \u001b[36m32.4484\u001b[0m       \u001b[32m30.0432\u001b[0m  0.0113\n",
      "     17       \u001b[36m32.4340\u001b[0m       \u001b[32m30.0272\u001b[0m  0.0113\n",
      "     18       \u001b[36m32.4213\u001b[0m       \u001b[32m30.0126\u001b[0m  0.0116\n",
      "     19       \u001b[36m32.4094\u001b[0m       \u001b[32m29.9995\u001b[0m  0.0114\n",
      "     20       \u001b[36m32.3993\u001b[0m       \u001b[32m29.9868\u001b[0m  0.0115\n",
      "     21       \u001b[36m32.3900\u001b[0m       \u001b[32m29.9758\u001b[0m  0.0110\n",
      "     22       \u001b[36m32.3813\u001b[0m       \u001b[32m29.9660\u001b[0m  0.0113\n",
      "     23       \u001b[36m32.3734\u001b[0m       \u001b[32m29.9574\u001b[0m  0.0114\n",
      "     24       \u001b[36m32.3660\u001b[0m       \u001b[32m29.9489\u001b[0m  0.0111\n",
      "     25       \u001b[36m32.3593\u001b[0m       \u001b[32m29.9420\u001b[0m  0.0113\n",
      "     26       \u001b[36m32.3533\u001b[0m       \u001b[32m29.9354\u001b[0m  0.0111\n",
      "     27       \u001b[36m32.3474\u001b[0m       \u001b[32m29.9291\u001b[0m  0.0115\n",
      "     28       \u001b[36m32.3420\u001b[0m       \u001b[32m29.9236\u001b[0m  0.0115\n",
      "     29       \u001b[36m32.3373\u001b[0m       \u001b[32m29.9183\u001b[0m  0.0113\n",
      "     30       \u001b[36m32.3325\u001b[0m       \u001b[32m29.9135\u001b[0m  0.0112\n",
      "     31       \u001b[36m32.3282\u001b[0m       \u001b[32m29.9090\u001b[0m  0.0112\n",
      "     32       \u001b[36m32.3242\u001b[0m       \u001b[32m29.9053\u001b[0m  0.0115\n",
      "     33       \u001b[36m32.3203\u001b[0m       \u001b[32m29.9011\u001b[0m  0.0115\n",
      "     34       \u001b[36m32.3169\u001b[0m       \u001b[32m29.8978\u001b[0m  0.0113\n",
      "     35       \u001b[36m32.3134\u001b[0m       \u001b[32m29.8943\u001b[0m  0.0111\n",
      "     36       \u001b[36m32.3102\u001b[0m       \u001b[32m29.8914\u001b[0m  0.0110\n",
      "     37       \u001b[36m32.3072\u001b[0m       \u001b[32m29.8881\u001b[0m  0.0114\n",
      "     38       \u001b[36m32.3043\u001b[0m       \u001b[32m29.8853\u001b[0m  0.0117\n",
      "     39       \u001b[36m32.3016\u001b[0m       \u001b[32m29.8825\u001b[0m  0.0114\n",
      "     40       \u001b[36m32.2991\u001b[0m       \u001b[32m29.8799\u001b[0m  0.0112\n",
      "     41       \u001b[36m32.2967\u001b[0m       \u001b[32m29.8774\u001b[0m  0.0110\n",
      "     42       \u001b[36m32.2944\u001b[0m       \u001b[32m29.8752\u001b[0m  0.0114\n",
      "     43       \u001b[36m32.2921\u001b[0m       \u001b[32m29.8727\u001b[0m  0.0114\n",
      "     44       \u001b[36m32.2902\u001b[0m       \u001b[32m29.8709\u001b[0m  0.0113\n",
      "     45       \u001b[36m32.2881\u001b[0m       \u001b[32m29.8690\u001b[0m  0.0111\n",
      "     46       \u001b[36m32.2862\u001b[0m       \u001b[32m29.8668\u001b[0m  0.0113\n",
      "     47       \u001b[36m32.2843\u001b[0m       \u001b[32m29.8653\u001b[0m  0.0115\n",
      "     48       \u001b[36m32.2826\u001b[0m       \u001b[32m29.8638\u001b[0m  0.0118\n",
      "     49       \u001b[36m32.2808\u001b[0m       \u001b[32m29.8621\u001b[0m  0.0119\n",
      "     50       \u001b[36m32.2792\u001b[0m       \u001b[32m29.8604\u001b[0m  0.0109\n",
      "     51       \u001b[36m32.2776\u001b[0m       \u001b[32m29.8593\u001b[0m  0.0110\n",
      "     52       \u001b[36m32.2761\u001b[0m       \u001b[32m29.8581\u001b[0m  0.0116\n",
      "     53       \u001b[36m32.2745\u001b[0m       \u001b[32m29.8567\u001b[0m  0.0116\n",
      "     54       \u001b[36m32.2730\u001b[0m       \u001b[32m29.8558\u001b[0m  0.0117\n",
      "     55       \u001b[36m32.2717\u001b[0m       \u001b[32m29.8546\u001b[0m  0.0116\n",
      "     56       \u001b[36m32.2702\u001b[0m       \u001b[32m29.8532\u001b[0m  0.0113\n",
      "     57       \u001b[36m32.2690\u001b[0m       \u001b[32m29.8521\u001b[0m  0.0114\n",
      "     58       \u001b[36m32.2677\u001b[0m       \u001b[32m29.8512\u001b[0m  0.0115\n",
      "     59       \u001b[36m32.2664\u001b[0m       \u001b[32m29.8504\u001b[0m  0.0116\n",
      "     60       \u001b[36m32.2651\u001b[0m       \u001b[32m29.8493\u001b[0m  0.0112\n",
      "     61       \u001b[36m32.2639\u001b[0m       \u001b[32m29.8486\u001b[0m  0.0111\n",
      "     62       \u001b[36m32.2628\u001b[0m       \u001b[32m29.8477\u001b[0m  0.0115\n",
      "     63       \u001b[36m32.2616\u001b[0m       \u001b[32m29.8467\u001b[0m  0.0115\n",
      "     64       \u001b[36m32.2606\u001b[0m       \u001b[32m29.8458\u001b[0m  0.0112\n",
      "     65       \u001b[36m32.2595\u001b[0m       \u001b[32m29.8452\u001b[0m  0.0112\n",
      "     66       \u001b[36m32.2584\u001b[0m       \u001b[32m29.8444\u001b[0m  0.0111\n",
      "     67       \u001b[36m32.2574\u001b[0m       \u001b[32m29.8436\u001b[0m  0.0113\n",
      "     68       \u001b[36m32.2565\u001b[0m       \u001b[32m29.8428\u001b[0m  0.0116\n",
      "     69       \u001b[36m32.2555\u001b[0m       \u001b[32m29.8419\u001b[0m  0.0115\n",
      "     70       \u001b[36m32.2546\u001b[0m       \u001b[32m29.8415\u001b[0m  0.0112\n",
      "     71       \u001b[36m32.2536\u001b[0m       \u001b[32m29.8409\u001b[0m  0.0113\n",
      "     72       \u001b[36m32.2527\u001b[0m       \u001b[32m29.8401\u001b[0m  0.0116\n",
      "     73       \u001b[36m32.2518\u001b[0m       \u001b[32m29.8396\u001b[0m  0.0114\n",
      "     74       \u001b[36m32.2510\u001b[0m       \u001b[32m29.8388\u001b[0m  0.0112\n",
      "     75       \u001b[36m32.2501\u001b[0m       \u001b[32m29.8384\u001b[0m  0.0109\n",
      "     76       \u001b[36m32.2493\u001b[0m       \u001b[32m29.8379\u001b[0m  0.0113\n",
      "     77       \u001b[36m32.2485\u001b[0m       \u001b[32m29.8373\u001b[0m  0.0114\n",
      "     78       \u001b[36m32.2477\u001b[0m       \u001b[32m29.8372\u001b[0m  0.0114\n",
      "     79       \u001b[36m32.2469\u001b[0m       \u001b[32m29.8365\u001b[0m  0.0114\n",
      "     80       \u001b[36m32.2462\u001b[0m       \u001b[32m29.8359\u001b[0m  0.0112\n",
      "     81       \u001b[36m32.2454\u001b[0m       \u001b[32m29.8355\u001b[0m  0.0112\n",
      "     82       \u001b[36m32.2447\u001b[0m       \u001b[32m29.8354\u001b[0m  0.0114\n",
      "     83       \u001b[36m32.2439\u001b[0m       \u001b[32m29.8348\u001b[0m  0.0120\n",
      "     84       \u001b[36m32.2432\u001b[0m       \u001b[32m29.8344\u001b[0m  0.0122\n",
      "     85       \u001b[36m32.2425\u001b[0m       \u001b[32m29.8338\u001b[0m  0.0196\n",
      "     86       \u001b[36m32.2418\u001b[0m       \u001b[32m29.8337\u001b[0m  0.0126\n",
      "     87       \u001b[36m32.2411\u001b[0m       \u001b[32m29.8331\u001b[0m  0.0127\n",
      "     88       \u001b[36m32.2404\u001b[0m       \u001b[32m29.8325\u001b[0m  0.0127\n",
      "     89       \u001b[36m32.2397\u001b[0m       \u001b[32m29.8323\u001b[0m  0.0126\n",
      "     90       \u001b[36m32.2390\u001b[0m       \u001b[32m29.8321\u001b[0m  0.0127\n",
      "     91       \u001b[36m32.2383\u001b[0m       \u001b[32m29.8317\u001b[0m  0.0115\n",
      "     92       \u001b[36m32.2377\u001b[0m       \u001b[32m29.8312\u001b[0m  0.0119\n",
      "     93       \u001b[36m32.2370\u001b[0m       \u001b[32m29.8309\u001b[0m  0.0113\n",
      "     94       \u001b[36m32.2364\u001b[0m       \u001b[32m29.8304\u001b[0m  0.0114\n",
      "     95       \u001b[36m32.2358\u001b[0m       \u001b[32m29.8301\u001b[0m  0.0114\n",
      "     96       \u001b[36m32.2351\u001b[0m       29.8301  0.0112\n",
      "     97       \u001b[36m32.2345\u001b[0m       \u001b[32m29.8295\u001b[0m  0.0113\n",
      "     98       \u001b[36m32.2339\u001b[0m       \u001b[32m29.8293\u001b[0m  0.0112\n",
      "     99       \u001b[36m32.2334\u001b[0m       \u001b[32m29.8291\u001b[0m  0.0114\n",
      "    100       \u001b[36m32.2327\u001b[0m       \u001b[32m29.8287\u001b[0m  0.0112\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m33.5190\u001b[0m       \u001b[32m30.9029\u001b[0m  0.0114\n",
      "      2       \u001b[36m30.5700\u001b[0m       \u001b[32m28.8019\u001b[0m  0.0118\n",
      "      3       \u001b[36m27.8056\u001b[0m       \u001b[32m27.1101\u001b[0m  0.0114\n",
      "      4       \u001b[36m25.5322\u001b[0m       \u001b[32m26.2920\u001b[0m  0.0115\n",
      "      5       \u001b[36m24.1900\u001b[0m       \u001b[32m26.2674\u001b[0m  0.0109\n",
      "      6       \u001b[36m23.6757\u001b[0m       26.4140  0.0107\n",
      "      7       \u001b[36m23.5132\u001b[0m       26.4765  0.0115\n",
      "      8       \u001b[36m23.4395\u001b[0m       26.4859  0.0115\n",
      "      9       \u001b[36m23.3913\u001b[0m       26.4784  0.0111\n",
      "     10       \u001b[36m23.3538\u001b[0m       26.4674  0.0110\n",
      "     11       \u001b[36m23.3227\u001b[0m       26.4564  0.0109\n",
      "     12       \u001b[36m23.2966\u001b[0m       26.4475  0.0112\n",
      "     13       \u001b[36m23.2748\u001b[0m       26.4397  0.0114\n",
      "     14       \u001b[36m23.2564\u001b[0m       26.4340  0.0112\n",
      "     15       \u001b[36m23.2405\u001b[0m       26.4299  0.0110\n",
      "     16       \u001b[36m23.2266\u001b[0m       26.4267  0.0110\n",
      "     17       \u001b[36m23.2146\u001b[0m       26.4235  0.0114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     18       \u001b[36m23.2040\u001b[0m       26.4210  0.0130\n",
      "     19       \u001b[36m23.1946\u001b[0m       26.4186  0.0124\n",
      "     20       \u001b[36m23.1863\u001b[0m       26.4168  0.0112\n",
      "     21       \u001b[36m23.1788\u001b[0m       26.4154  0.0110\n",
      "     22       \u001b[36m23.1722\u001b[0m       26.4143  0.0110\n",
      "     23       \u001b[36m23.1661\u001b[0m       26.4135  0.0116\n",
      "     24       \u001b[36m23.1606\u001b[0m       26.4130  0.0112\n",
      "     25       \u001b[36m23.1557\u001b[0m       26.4126  0.0115\n",
      "     26       \u001b[36m23.1513\u001b[0m       26.4121  0.0109\n",
      "     27       \u001b[36m23.1471\u001b[0m       26.4118  0.0108\n",
      "     28       \u001b[36m23.1433\u001b[0m       26.4117  0.0115\n",
      "     29       \u001b[36m23.1397\u001b[0m       26.4115  0.0116\n",
      "     30       \u001b[36m23.1365\u001b[0m       26.4115  0.0110\n",
      "     31       \u001b[36m23.1334\u001b[0m       26.4117  0.0109\n",
      "     32       \u001b[36m23.1305\u001b[0m       26.4120  0.0111\n",
      "     33       \u001b[36m23.1278\u001b[0m       26.4122  0.0120\n",
      "     34       \u001b[36m23.1252\u001b[0m       26.4125  0.0113\n",
      "     35       \u001b[36m23.1228\u001b[0m       26.4126  0.0113\n",
      "     36       \u001b[36m23.1206\u001b[0m       26.4127  0.0110\n",
      "     37       \u001b[36m23.1185\u001b[0m       26.4129  0.0134\n",
      "     38       \u001b[36m23.1164\u001b[0m       26.4129  0.0120\n",
      "     39       \u001b[36m23.1145\u001b[0m       26.4127  0.0114\n",
      "     40       \u001b[36m23.1126\u001b[0m       26.4129  0.0114\n",
      "     41       \u001b[36m23.1109\u001b[0m       26.4130  0.0111\n",
      "     42       \u001b[36m23.1092\u001b[0m       26.4130  0.0110\n",
      "     43       \u001b[36m23.1076\u001b[0m       26.4134  0.0117\n",
      "     44       \u001b[36m23.1060\u001b[0m       26.4135  0.0118\n",
      "     45       \u001b[36m23.1045\u001b[0m       26.4137  0.0113\n",
      "     46       \u001b[36m23.1031\u001b[0m       26.4138  0.0111\n",
      "     47       \u001b[36m23.1018\u001b[0m       26.4139  0.0109\n",
      "     48       \u001b[36m23.1004\u001b[0m       26.4139  0.0115\n",
      "     49       \u001b[36m23.0991\u001b[0m       26.4138  0.0121\n",
      "     50       \u001b[36m23.0979\u001b[0m       26.4134  0.0114\n",
      "     51       \u001b[36m23.0967\u001b[0m       26.4135  0.0114\n",
      "     52       \u001b[36m23.0955\u001b[0m       26.4136  0.0112\n",
      "     53       \u001b[36m23.0944\u001b[0m       26.4135  0.0115\n",
      "     54       \u001b[36m23.0933\u001b[0m       26.4136  0.0113\n",
      "     55       \u001b[36m23.0923\u001b[0m       26.4135  0.0113\n",
      "     56       \u001b[36m23.0913\u001b[0m       26.4134  0.0109\n",
      "     57       \u001b[36m23.0903\u001b[0m       26.4134  0.0110\n",
      "     58       \u001b[36m23.0894\u001b[0m       26.4131  0.0115\n",
      "     59       \u001b[36m23.0885\u001b[0m       26.4131  0.0113\n",
      "     60       \u001b[36m23.0876\u001b[0m       26.4131  0.0115\n",
      "     61       \u001b[36m23.0867\u001b[0m       26.4131  0.0111\n",
      "     62       \u001b[36m23.0859\u001b[0m       26.4131  0.0111\n",
      "     63       \u001b[36m23.0850\u001b[0m       26.4129  0.0114\n",
      "     64       \u001b[36m23.0842\u001b[0m       26.4126  0.0117\n",
      "     65       \u001b[36m23.0835\u001b[0m       26.4125  0.0126\n",
      "     66       \u001b[36m23.0827\u001b[0m       26.4123  0.0171\n",
      "     67       \u001b[36m23.0820\u001b[0m       26.4123  0.0141\n",
      "     68       \u001b[36m23.0812\u001b[0m       26.4120  0.0130\n",
      "     69       \u001b[36m23.0805\u001b[0m       26.4118  0.0139\n",
      "     70       \u001b[36m23.0799\u001b[0m       26.4114  0.0139\n",
      "     71       \u001b[36m23.0792\u001b[0m       26.4112  0.0154\n",
      "     72       \u001b[36m23.0785\u001b[0m       26.4110  0.0166\n",
      "     73       \u001b[36m23.0779\u001b[0m       26.4108  0.0114\n",
      "     74       \u001b[36m23.0773\u001b[0m       26.4106  0.0115\n",
      "     75       \u001b[36m23.0766\u001b[0m       26.4103  0.0115\n",
      "     76       \u001b[36m23.0760\u001b[0m       26.4101  0.0120\n",
      "     77       \u001b[36m23.0754\u001b[0m       26.4100  0.0117\n",
      "     78       \u001b[36m23.0748\u001b[0m       26.4096  0.0120\n",
      "     79       \u001b[36m23.0742\u001b[0m       26.4094  0.0116\n",
      "     80       \u001b[36m23.0737\u001b[0m       26.4091  0.0116\n",
      "     81       \u001b[36m23.0731\u001b[0m       26.4089  0.0120\n",
      "     82       \u001b[36m23.0725\u001b[0m       26.4088  0.0118\n",
      "     83       \u001b[36m23.0720\u001b[0m       26.4083  0.0120\n",
      "     84       \u001b[36m23.0714\u001b[0m       26.4082  0.0111\n",
      "     85       \u001b[36m23.0709\u001b[0m       26.4081  0.0114\n",
      "     86       \u001b[36m23.0704\u001b[0m       26.4076  0.0111\n",
      "     87       \u001b[36m23.0699\u001b[0m       26.4075  0.0112\n",
      "     88       \u001b[36m23.0693\u001b[0m       26.4071  0.0116\n",
      "     89       \u001b[36m23.0688\u001b[0m       26.4068  0.0115\n",
      "     90       \u001b[36m23.0683\u001b[0m       26.4067  0.0113\n",
      "     91       \u001b[36m23.0679\u001b[0m       26.4062  0.0114\n",
      "     92       \u001b[36m23.0674\u001b[0m       26.4062  0.0110\n",
      "     93       \u001b[36m23.0669\u001b[0m       26.4058  0.0113\n",
      "     94       \u001b[36m23.0664\u001b[0m       26.4057  0.0118\n",
      "     95       \u001b[36m23.0660\u001b[0m       26.4054  0.0114\n",
      "     96       \u001b[36m23.0656\u001b[0m       26.4052  0.0110\n",
      "     97       \u001b[36m23.0651\u001b[0m       26.4051  0.0112\n",
      "     98       \u001b[36m23.0647\u001b[0m       26.4047  0.0117\n",
      "     99       \u001b[36m23.0643\u001b[0m       26.4045  0.0114\n",
      "    100       \u001b[36m23.0639\u001b[0m       26.4044  0.0113\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m39.8399\u001b[0m       \u001b[32m30.1979\u001b[0m  0.0108\n",
      "      2       \u001b[36m36.1567\u001b[0m       \u001b[32m28.0201\u001b[0m  0.0111\n",
      "      3       \u001b[36m32.9455\u001b[0m       \u001b[32m26.5396\u001b[0m  0.0118\n",
      "      4       \u001b[36m30.4620\u001b[0m       \u001b[32m26.2204\u001b[0m  0.0112\n",
      "      5       \u001b[36m29.1174\u001b[0m       26.7465  0.0112\n",
      "      6       \u001b[36m28.7459\u001b[0m       27.1414  0.0108\n",
      "      7       \u001b[36m28.6701\u001b[0m       27.2783  0.0110\n",
      "      8       \u001b[36m28.6332\u001b[0m       27.3211  0.0114\n",
      "      9       \u001b[36m28.6036\u001b[0m       27.3296  0.0116\n",
      "     10       \u001b[36m28.5788\u001b[0m       27.3266  0.0114\n",
      "     11       \u001b[36m28.5575\u001b[0m       27.3249  0.0113\n",
      "     12       \u001b[36m28.5403\u001b[0m       27.3239  0.0111\n",
      "     13       \u001b[36m28.5265\u001b[0m       27.3244  0.0112\n",
      "     14       \u001b[36m28.5153\u001b[0m       27.3234  0.0116\n",
      "     15       \u001b[36m28.5059\u001b[0m       27.3223  0.0116\n",
      "     16       \u001b[36m28.4977\u001b[0m       27.3209  0.0111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m28.4907\u001b[0m       27.3204  0.0109\n",
      "     18       \u001b[36m28.4845\u001b[0m       27.3191  0.0117\n",
      "     19       \u001b[36m28.4790\u001b[0m       27.3182  0.0117\n",
      "     20       \u001b[36m28.4742\u001b[0m       27.3171  0.0116\n",
      "     21       \u001b[36m28.4700\u001b[0m       27.3149  0.0111\n",
      "     22       \u001b[36m28.4660\u001b[0m       27.3141  0.0110\n",
      "     23       \u001b[36m28.4627\u001b[0m       27.3126  0.0115\n",
      "     24       \u001b[36m28.4596\u001b[0m       27.3102  0.0118\n",
      "     25       \u001b[36m28.4568\u001b[0m       27.3076  0.0113\n",
      "     26       \u001b[36m28.4541\u001b[0m       27.3057  0.0112\n",
      "     27       \u001b[36m28.4517\u001b[0m       27.3035  0.0110\n",
      "     28       \u001b[36m28.4495\u001b[0m       27.3013  0.0113\n",
      "     29       \u001b[36m28.4474\u001b[0m       27.3000  0.0117\n",
      "     30       \u001b[36m28.4455\u001b[0m       27.2973  0.0111\n",
      "     31       \u001b[36m28.4436\u001b[0m       27.2954  0.0109\n",
      "     32       \u001b[36m28.4418\u001b[0m       27.2935  0.0111\n",
      "     33       \u001b[36m28.4402\u001b[0m       27.2916  0.0115\n",
      "     34       \u001b[36m28.4386\u001b[0m       27.2903  0.0116\n",
      "     35       \u001b[36m28.4371\u001b[0m       27.2886  0.0113\n",
      "     36       \u001b[36m28.4357\u001b[0m       27.2868  0.0112\n",
      "     37       \u001b[36m28.4343\u001b[0m       27.2854  0.0108\n",
      "     38       \u001b[36m28.4331\u001b[0m       27.2841  0.0115\n",
      "     39       \u001b[36m28.4319\u001b[0m       27.2831  0.0114\n",
      "     40       \u001b[36m28.4307\u001b[0m       27.2814  0.0115\n",
      "     41       \u001b[36m28.4296\u001b[0m       27.2800  0.0114\n",
      "     42       \u001b[36m28.4286\u001b[0m       27.2787  0.0109\n",
      "     43       \u001b[36m28.4276\u001b[0m       27.2776  0.0110\n",
      "     44       \u001b[36m28.4266\u001b[0m       27.2764  0.0117\n",
      "     45       \u001b[36m28.4257\u001b[0m       27.2756  0.0115\n",
      "     46       \u001b[36m28.4249\u001b[0m       27.2745  0.0146\n",
      "     47       \u001b[36m28.4240\u001b[0m       27.2738  0.0137\n",
      "     48       \u001b[36m28.4232\u001b[0m       27.2724  0.0130\n",
      "     49       \u001b[36m28.4224\u001b[0m       27.2718  0.0117\n",
      "     50       \u001b[36m28.4217\u001b[0m       27.2704  0.0116\n",
      "     51       \u001b[36m28.4210\u001b[0m       27.2700  0.0125\n",
      "     52       \u001b[36m28.4203\u001b[0m       27.2688  0.0133\n",
      "     53       \u001b[36m28.4196\u001b[0m       27.2683  0.0124\n",
      "     54       \u001b[36m28.4190\u001b[0m       27.2674  0.0115\n",
      "     55       \u001b[36m28.4184\u001b[0m       27.2664  0.0113\n",
      "     56       \u001b[36m28.4178\u001b[0m       27.2656  0.0113\n",
      "     57       \u001b[36m28.4172\u001b[0m       27.2654  0.0113\n",
      "     58       \u001b[36m28.4166\u001b[0m       27.2649  0.0114\n",
      "     59       \u001b[36m28.4161\u001b[0m       27.2639  0.0111\n",
      "     60       \u001b[36m28.4155\u001b[0m       27.2636  0.0109\n",
      "     61       \u001b[36m28.4150\u001b[0m       27.2627  0.0110\n",
      "     62       \u001b[36m28.4144\u001b[0m       27.2623  0.0111\n",
      "     63       \u001b[36m28.4140\u001b[0m       27.2616  0.0115\n",
      "     64       \u001b[36m28.4135\u001b[0m       27.2603  0.0115\n",
      "     65       \u001b[36m28.4129\u001b[0m       27.2601  0.0111\n",
      "     66       \u001b[36m28.4125\u001b[0m       27.2588  0.0107\n",
      "     67       \u001b[36m28.4120\u001b[0m       27.2586  0.0108\n",
      "     68       \u001b[36m28.4116\u001b[0m       27.2576  0.0114\n",
      "     69       \u001b[36m28.4111\u001b[0m       27.2572  0.0115\n",
      "     70       \u001b[36m28.4107\u001b[0m       27.2563  0.0107\n",
      "     71       \u001b[36m28.4102\u001b[0m       27.2561  0.0106\n",
      "     72       \u001b[36m28.4099\u001b[0m       27.2554  0.0109\n",
      "     73       \u001b[36m28.4095\u001b[0m       27.2549  0.0109\n",
      "     74       \u001b[36m28.4090\u001b[0m       27.2547  0.0112\n",
      "     75       \u001b[36m28.4087\u001b[0m       27.2537  0.0107\n",
      "     76       \u001b[36m28.4083\u001b[0m       27.2537  0.0107\n",
      "     77       \u001b[36m28.4079\u001b[0m       27.2528  0.0109\n",
      "     78       \u001b[36m28.4076\u001b[0m       27.2524  0.0113\n",
      "     79       \u001b[36m28.4072\u001b[0m       27.2524  0.0113\n",
      "     80       \u001b[36m28.4069\u001b[0m       27.2516  0.0107\n",
      "     81       \u001b[36m28.4065\u001b[0m       27.2510  0.0107\n",
      "     82       \u001b[36m28.4062\u001b[0m       27.2505  0.0110\n",
      "     83       \u001b[36m28.4059\u001b[0m       27.2502  0.0113\n",
      "     84       \u001b[36m28.4056\u001b[0m       27.2498  0.0110\n",
      "     85       \u001b[36m28.4052\u001b[0m       27.2495  0.0107\n",
      "     86       \u001b[36m28.4049\u001b[0m       27.2490  0.0107\n",
      "     87       \u001b[36m28.4046\u001b[0m       27.2484  0.0110\n",
      "     88       \u001b[36m28.4043\u001b[0m       27.2479  0.0115\n",
      "     89       \u001b[36m28.4040\u001b[0m       27.2475  0.0111\n",
      "     90       \u001b[36m28.4037\u001b[0m       27.2472  0.0107\n",
      "     91       \u001b[36m28.4034\u001b[0m       27.2470  0.0106\n",
      "     92       \u001b[36m28.4031\u001b[0m       27.2466  0.0111\n",
      "     93       \u001b[36m28.4029\u001b[0m       27.2464  0.0110\n",
      "     94       \u001b[36m28.4026\u001b[0m       27.2461  0.0112\n",
      "     95       \u001b[36m28.4023\u001b[0m       27.2458  0.0106\n",
      "     96       \u001b[36m28.4021\u001b[0m       27.2454  0.0109\n",
      "     97       \u001b[36m28.4018\u001b[0m       27.2452  0.0110\n",
      "     98       \u001b[36m28.4015\u001b[0m       27.2451  0.0112\n",
      "     99       \u001b[36m28.4013\u001b[0m       27.2446  0.0112\n",
      "    100       \u001b[36m28.4011\u001b[0m       27.2444  0.0112\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m39.3017\u001b[0m       \u001b[32m35.9005\u001b[0m  0.0124\n",
      "      2       \u001b[36m36.0343\u001b[0m       \u001b[32m32.3940\u001b[0m  0.0125\n",
      "      3       \u001b[36m33.8497\u001b[0m       \u001b[32m32.2462\u001b[0m  0.0120\n",
      "      4       \u001b[36m33.2784\u001b[0m       \u001b[32m30.4927\u001b[0m  0.0117\n",
      "      5       \u001b[36m32.9061\u001b[0m       31.0964  0.0114\n",
      "      6       \u001b[36m32.8096\u001b[0m       \u001b[32m30.3987\u001b[0m  0.0113\n",
      "      7       \u001b[36m32.5701\u001b[0m       \u001b[32m30.2964\u001b[0m  0.0122\n",
      "      8       \u001b[36m32.5246\u001b[0m       30.6172  0.0117\n",
      "      9       \u001b[36m32.4293\u001b[0m       30.3393  0.0120\n",
      "     10       \u001b[36m32.3867\u001b[0m       30.5077  0.0117\n",
      "     11       \u001b[36m32.3698\u001b[0m       30.3920  0.0117\n",
      "     12       \u001b[36m32.3340\u001b[0m       30.4396  0.0114\n",
      "     13       \u001b[36m32.3215\u001b[0m       30.3208  0.0120\n",
      "     14       \u001b[36m32.3025\u001b[0m       30.4096  0.0119\n",
      "     15       \u001b[36m32.2927\u001b[0m       30.3191  0.0117\n",
      "     16       \u001b[36m32.2750\u001b[0m       30.3756  0.0114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m32.2729\u001b[0m       30.3187  0.0118\n",
      "     18       \u001b[36m32.2563\u001b[0m       30.3843  0.0116\n",
      "     19       \u001b[36m32.2538\u001b[0m       30.3184  0.0118\n",
      "     20       \u001b[36m32.2439\u001b[0m       30.3897  0.0120\n",
      "     21       \u001b[36m32.2396\u001b[0m       30.2990  0.0117\n",
      "     22       \u001b[36m32.2333\u001b[0m       30.4319  0.0118\n",
      "     23       \u001b[36m32.2273\u001b[0m       \u001b[32m30.2782\u001b[0m  0.0148\n",
      "     24       \u001b[36m32.2252\u001b[0m       30.4669  0.0128\n",
      "     25       \u001b[36m32.2173\u001b[0m       \u001b[32m30.2718\u001b[0m  0.0128\n",
      "     26       32.2186       30.5118  0.0130\n",
      "     27       \u001b[36m32.2095\u001b[0m       \u001b[32m30.2523\u001b[0m  0.0191\n",
      "     28       32.2146       30.5498  0.0164\n",
      "     29       \u001b[36m32.2023\u001b[0m       30.2640  0.0157\n",
      "     30       32.2113       30.5683  0.0151\n",
      "     31       \u001b[36m32.1959\u001b[0m       30.3116  0.0162\n",
      "     32       32.2017       30.6133  0.0154\n",
      "     33       \u001b[36m32.1876\u001b[0m       30.3393  0.0142\n",
      "     34       32.1918       30.6126  0.0132\n",
      "     35       \u001b[36m32.1785\u001b[0m       30.3929  0.0131\n",
      "     36       32.1827       30.6268  0.0133\n",
      "     37       \u001b[36m32.1718\u001b[0m       30.4524  0.0128\n",
      "     38       32.1759       30.6743  0.0125\n",
      "     39       \u001b[36m32.1659\u001b[0m       30.4943  0.0132\n",
      "     40       32.1714       30.7304  0.0125\n",
      "     41       \u001b[36m32.1592\u001b[0m       30.5230  0.0127\n",
      "     42       32.1697       30.8022  0.0133\n",
      "     43       \u001b[36m32.1551\u001b[0m       30.5517  0.0133\n",
      "     44       32.1668       30.8111  0.0128\n",
      "     45       \u001b[36m32.1505\u001b[0m       30.5863  0.0124\n",
      "     46       32.1635       30.8948  0.0123\n",
      "     47       \u001b[36m32.1495\u001b[0m       30.6126  0.0124\n",
      "     48       32.1630       30.9571  0.0128\n",
      "     49       \u001b[36m32.1480\u001b[0m       30.6570  0.0125\n",
      "     50       32.1619       31.0135  0.0127\n",
      "     51       \u001b[36m32.1447\u001b[0m       30.6679  0.0123\n",
      "     52       32.1628       31.0679  0.0126\n",
      "     53       \u001b[36m32.1398\u001b[0m       30.7006  0.0123\n",
      "     54       32.1582       31.1016  0.0131\n",
      "     55       \u001b[36m32.1327\u001b[0m       30.7052  0.0127\n",
      "     56       32.1530       31.0340  0.0132\n",
      "     57       \u001b[36m32.1251\u001b[0m       30.7501  0.0125\n",
      "     58       32.1423       31.0586  0.0130\n",
      "     59       \u001b[36m32.1192\u001b[0m       30.8061  0.0127\n",
      "     60       32.1341       31.0686  0.0123\n",
      "     61       \u001b[36m32.1144\u001b[0m       30.8558  0.0129\n",
      "     62       32.1261       31.0666  0.0127\n",
      "     63       \u001b[36m32.1109\u001b[0m       30.8700  0.0125\n",
      "     64       32.1196       31.0677  0.0123\n",
      "     65       \u001b[36m32.1073\u001b[0m       30.8931  0.0122\n",
      "     66       32.1162       31.0893  0.0122\n",
      "     67       \u001b[36m32.1049\u001b[0m       30.9385  0.0128\n",
      "     68       32.1104       31.1102  0.0125\n",
      "     69       \u001b[36m32.1021\u001b[0m       30.9771  0.0121\n",
      "     70       32.1042       31.0665  0.0122\n",
      "     71       \u001b[36m32.0985\u001b[0m       30.9952  0.0119\n",
      "     72       \u001b[36m32.0978\u001b[0m       31.0349  0.0119\n",
      "     73       \u001b[36m32.0955\u001b[0m       31.0453  0.0120\n",
      "     74       \u001b[36m32.0925\u001b[0m       31.0005  0.0116\n",
      "     75       32.0937       31.0924  0.0117\n",
      "     76       \u001b[36m32.0866\u001b[0m       31.0398  0.0121\n",
      "     77       32.0936       31.0387  0.0118\n",
      "     78       \u001b[36m32.0843\u001b[0m       31.1921  0.0115\n",
      "     79       32.0884       30.9766  0.0119\n",
      "     80       32.0971       31.1196  0.0117\n",
      "     81       \u001b[36m32.0838\u001b[0m       31.3780  0.0117\n",
      "     82       32.1061       30.8086  0.0116\n",
      "     83       32.1340       31.2320  0.0115\n",
      "     84       32.1078       31.0454  0.0122\n",
      "     85       32.1176       31.0195  0.0119\n",
      "     86       32.0943       31.1387  0.0117\n",
      "     87       32.0846       31.1032  0.0117\n",
      "     88       32.0896       31.1508  0.0116\n",
      "     89       \u001b[36m32.0808\u001b[0m       31.0455  0.0121\n",
      "     90       32.0882       31.2419  0.0120\n",
      "     91       32.0855       31.0637  0.0118\n",
      "     92       \u001b[36m32.0793\u001b[0m       31.2263  0.0118\n",
      "     93       32.0846       31.0754  0.0116\n",
      "     94       \u001b[36m32.0709\u001b[0m       31.2353  0.0115\n",
      "     95       32.0771       31.0548  0.0122\n",
      "     96       \u001b[36m32.0626\u001b[0m       31.2061  0.0120\n",
      "     97       32.0698       31.1167  0.0126\n",
      "     98       \u001b[36m32.0588\u001b[0m       31.1642  0.0117\n",
      "     99       32.0639       31.1348  0.0119\n",
      "    100       \u001b[36m32.0567\u001b[0m       31.1488  0.0118\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m31.2543\u001b[0m       \u001b[32m27.7412\u001b[0m  0.0119\n",
      "      2       \u001b[36m25.5321\u001b[0m       \u001b[32m26.6395\u001b[0m  0.0183\n",
      "      3       \u001b[36m24.3859\u001b[0m       \u001b[32m26.4569\u001b[0m  0.0148\n",
      "      4       \u001b[36m23.9001\u001b[0m       27.9794  0.0132\n",
      "      5       \u001b[36m23.6089\u001b[0m       27.3234  0.0133\n",
      "      6       \u001b[36m23.5970\u001b[0m       27.1144  0.0130\n",
      "      7       \u001b[36m23.4019\u001b[0m       27.2414  0.0136\n",
      "      8       \u001b[36m23.2899\u001b[0m       26.6689  0.0122\n",
      "      9       \u001b[36m23.2494\u001b[0m       26.8909  0.0125\n",
      "     10       \u001b[36m23.1984\u001b[0m       26.9819  0.0124\n",
      "     11       \u001b[36m23.1647\u001b[0m       26.6412  0.0121\n",
      "     12       \u001b[36m23.1463\u001b[0m       26.7126  0.0118\n",
      "     13       \u001b[36m23.1193\u001b[0m       26.7103  0.0118\n",
      "     14       \u001b[36m23.1064\u001b[0m       26.7098  0.0135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     15       \u001b[36m23.0933\u001b[0m       26.6621  0.0169\n",
      "     16       \u001b[36m23.0803\u001b[0m       26.6360  0.0152\n",
      "     17       \u001b[36m23.0715\u001b[0m       26.6601  0.0151\n",
      "     18       \u001b[36m23.0626\u001b[0m       26.6391  0.0126\n",
      "     19       \u001b[36m23.0545\u001b[0m       26.6147  0.0118\n",
      "     20       \u001b[36m23.0478\u001b[0m       26.6325  0.0125\n",
      "     21       \u001b[36m23.0410\u001b[0m       26.6068  0.0121\n",
      "     22       \u001b[36m23.0359\u001b[0m       26.6080  0.0119\n",
      "     23       \u001b[36m23.0313\u001b[0m       26.6004  0.0119\n",
      "     24       \u001b[36m23.0257\u001b[0m       26.5925  0.0123\n",
      "     25       \u001b[36m23.0219\u001b[0m       26.5899  0.0121\n",
      "     26       \u001b[36m23.0179\u001b[0m       26.5927  0.0120\n",
      "     27       \u001b[36m23.0129\u001b[0m       26.5863  0.0117\n",
      "     28       \u001b[36m23.0100\u001b[0m       26.5833  0.0118\n",
      "     29       \u001b[36m23.0069\u001b[0m       26.5792  0.0120\n",
      "     30       \u001b[36m23.0023\u001b[0m       26.5796  0.0118\n",
      "     31       \u001b[36m23.0011\u001b[0m       26.5743  0.0120\n",
      "     32       \u001b[36m23.0000\u001b[0m       26.5939  0.0119\n",
      "     33       \u001b[36m22.9939\u001b[0m       26.5905  0.0113\n",
      "     34       23.0004       26.5687  0.0117\n",
      "     35       23.0113       26.6174  0.0120\n",
      "     36       22.9973       26.5991  0.0119\n",
      "     37       23.0194       26.5989  0.0114\n",
      "     38       23.0402       26.6313  0.0114\n",
      "     39       23.0176       26.6018  0.0119\n",
      "     40       23.0362       26.6243  0.0116\n",
      "     41       23.0011       26.6114  0.0118\n",
      "     42       22.9967       26.6723  0.0116\n",
      "     43       \u001b[36m22.9783\u001b[0m       26.5794  0.0116\n",
      "     44       \u001b[36m22.9782\u001b[0m       26.6329  0.0119\n",
      "     45       \u001b[36m22.9716\u001b[0m       26.6005  0.0124\n",
      "     46       \u001b[36m22.9692\u001b[0m       26.6264  0.0118\n",
      "     47       \u001b[36m22.9664\u001b[0m       26.6387  0.0113\n",
      "     48       \u001b[36m22.9603\u001b[0m       26.6001  0.0114\n",
      "     49       22.9611       26.6432  0.0123\n",
      "     50       \u001b[36m22.9555\u001b[0m       26.6050  0.0118\n",
      "     51       22.9562       26.6495  0.0117\n",
      "     52       \u001b[36m22.9524\u001b[0m       26.6241  0.0147\n",
      "     53       \u001b[36m22.9509\u001b[0m       26.6425  0.0128\n",
      "     54       \u001b[36m22.9495\u001b[0m       26.6426  0.0122\n",
      "     55       \u001b[36m22.9472\u001b[0m       26.6446  0.0118\n",
      "     56       \u001b[36m22.9458\u001b[0m       26.6440  0.0117\n",
      "     57       \u001b[36m22.9447\u001b[0m       26.6496  0.0124\n",
      "     58       \u001b[36m22.9425\u001b[0m       26.6534  0.0125\n",
      "     59       \u001b[36m22.9397\u001b[0m       26.6462  0.0119\n",
      "     60       22.9402       26.6739  0.0118\n",
      "     61       \u001b[36m22.9377\u001b[0m       26.6552  0.0120\n",
      "     62       \u001b[36m22.9350\u001b[0m       26.6616  0.0125\n",
      "     63       22.9366       26.6620  0.0121\n",
      "     64       22.9352       26.6887  0.0121\n",
      "     65       \u001b[36m22.9317\u001b[0m       26.6580  0.0122\n",
      "     66       22.9365       26.6708  0.0120\n",
      "     67       22.9428       26.7219  0.0118\n",
      "     68       22.9330       26.6963  0.0129\n",
      "     69       22.9465       26.6618  0.0119\n",
      "     70       22.9625       26.7789  0.0116\n",
      "     71       22.9550       26.6783  0.0115\n",
      "     72       22.9683       26.6986  0.0114\n",
      "     73       22.9900       26.7577  0.0122\n",
      "     74       22.9551       26.6991  0.0123\n",
      "     75       22.9519       26.7422  0.0117\n",
      "     76       22.9475       26.7157  0.0117\n",
      "     77       \u001b[36m22.9259\u001b[0m       26.6557  0.0117\n",
      "     78       22.9285       26.7981  0.0134\n",
      "     79       \u001b[36m22.9188\u001b[0m       26.6757  0.0152\n",
      "     80       22.9191       26.7459  0.0137\n",
      "     81       \u001b[36m22.9171\u001b[0m       26.7374  0.0124\n",
      "     82       \u001b[36m22.9125\u001b[0m       26.7150  0.0126\n",
      "     83       22.9145       26.7491  0.0124\n",
      "     84       \u001b[36m22.9101\u001b[0m       26.7333  0.0144\n",
      "     85       22.9102       26.7494  0.0124\n",
      "     86       \u001b[36m22.9088\u001b[0m       26.7618  0.0133\n",
      "     87       \u001b[36m22.9070\u001b[0m       26.7434  0.0123\n",
      "     88       \u001b[36m22.9061\u001b[0m       26.7690  0.0120\n",
      "     89       \u001b[36m22.9057\u001b[0m       26.7700  0.0117\n",
      "     90       \u001b[36m22.9044\u001b[0m       26.7799  0.0113\n",
      "     91       \u001b[36m22.9028\u001b[0m       26.7712  0.0120\n",
      "     92       22.9029       26.7859  0.0117\n",
      "     93       \u001b[36m22.9021\u001b[0m       26.8000  0.0119\n",
      "     94       \u001b[36m22.8997\u001b[0m       26.7913  0.0119\n",
      "     95       \u001b[36m22.8996\u001b[0m       26.7996  0.0122\n",
      "     96       22.8999       26.8299  0.0117\n",
      "     97       \u001b[36m22.8970\u001b[0m       26.8080  0.0117\n",
      "     98       \u001b[36m22.8964\u001b[0m       26.8141  0.0123\n",
      "     99       22.8975       26.8510  0.0117\n",
      "    100       \u001b[36m22.8962\u001b[0m       26.8372  0.0122\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m35.6597\u001b[0m       \u001b[32m33.4642\u001b[0m  0.0117\n",
      "      2       \u001b[36m31.7055\u001b[0m       \u001b[32m26.4027\u001b[0m  0.0119\n",
      "      3       \u001b[36m30.0244\u001b[0m       26.6107  0.0120\n",
      "      4       \u001b[36m29.2664\u001b[0m       29.4608  0.0121\n",
      "      5       \u001b[36m29.1839\u001b[0m       26.7963  0.0119\n",
      "      6       \u001b[36m28.7909\u001b[0m       26.9970  0.0116\n",
      "      7       \u001b[36m28.7281\u001b[0m       27.8078  0.0116\n",
      "      8       \u001b[36m28.6356\u001b[0m       26.9227  0.0122\n",
      "      9       \u001b[36m28.5222\u001b[0m       27.3663  0.0120\n",
      "     10       28.5544       27.3423  0.0119\n",
      "     11       \u001b[36m28.4866\u001b[0m       26.9542  0.0116\n",
      "     12       \u001b[36m28.4620\u001b[0m       27.3897  0.0127\n",
      "     13       28.4756       27.1044  0.0122\n",
      "     14       \u001b[36m28.4357\u001b[0m       27.1222  0.0117\n",
      "     15       28.4443       27.1954  0.0116\n",
      "     16       \u001b[36m28.4295\u001b[0m       27.0848  0.0121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m28.4231\u001b[0m       27.1938  0.0125\n",
      "     18       28.4238       27.0998  0.0116\n",
      "     19       \u001b[36m28.4133\u001b[0m       27.1421  0.0119\n",
      "     20       28.4150       27.1271  0.0123\n",
      "     21       \u001b[36m28.4085\u001b[0m       27.1164  0.0117\n",
      "     22       28.4094       27.1202  0.0119\n",
      "     23       \u001b[36m28.4038\u001b[0m       27.1286  0.0124\n",
      "     24       28.4095       27.1331  0.0122\n",
      "     25       28.4171       27.1100  0.0121\n",
      "     26       28.4176       27.1618  0.0118\n",
      "     27       28.4330       27.1498  0.0114\n",
      "     28       28.4539       27.1458  0.0120\n",
      "     29       28.4719       27.1054  0.0122\n",
      "     30       28.4461       27.1116  0.0120\n",
      "     31       28.4128       27.1568  0.0118\n",
      "     32       28.4245       27.0432  0.0117\n",
      "     33       \u001b[36m28.3973\u001b[0m       27.1450  0.0116\n",
      "     34       \u001b[36m28.3945\u001b[0m       27.1391  0.0121\n",
      "     35       \u001b[36m28.3906\u001b[0m       27.1093  0.0120\n",
      "     36       \u001b[36m28.3888\u001b[0m       27.1057  0.0123\n",
      "     37       \u001b[36m28.3875\u001b[0m       27.1192  0.0118\n",
      "     38       \u001b[36m28.3834\u001b[0m       27.1082  0.0116\n",
      "     39       \u001b[36m28.3823\u001b[0m       27.0933  0.0119\n",
      "     40       \u001b[36m28.3802\u001b[0m       27.1148  0.0116\n",
      "     41       \u001b[36m28.3799\u001b[0m       27.0967  0.0118\n",
      "     42       \u001b[36m28.3785\u001b[0m       27.0983  0.0116\n",
      "     43       \u001b[36m28.3763\u001b[0m       27.1039  0.0117\n",
      "     44       28.3770       27.0898  0.0122\n",
      "     45       \u001b[36m28.3757\u001b[0m       27.0907  0.0119\n",
      "     46       \u001b[36m28.3737\u001b[0m       27.0970  0.0118\n",
      "     47       28.3744       27.0881  0.0116\n",
      "     48       28.3739       27.0853  0.0113\n",
      "     49       \u001b[36m28.3714\u001b[0m       27.0950  0.0114\n",
      "     50       28.3718       27.0880  0.0122\n",
      "     51       28.3738       27.0683  0.0119\n",
      "     52       \u001b[36m28.3705\u001b[0m       27.0968  0.0163\n",
      "     53       28.3718       27.0987  0.0130\n",
      "     54       28.3772       27.0599  0.0121\n",
      "     55       28.3858       27.0879  0.0126\n",
      "     56       28.3808       27.1220  0.0182\n",
      "     57       28.4014       27.0895  0.0197\n",
      "     58       28.4257       27.0050  0.0136\n",
      "     59       28.4108       27.1796  0.0130\n",
      "     60       28.4159       26.9674  0.0184\n",
      "     61       28.3971       27.1407  0.0177\n",
      "     62       28.3920       27.0162  0.0135\n",
      "     63       28.3868       27.0122  0.0123\n",
      "     64       \u001b[36m28.3640\u001b[0m       27.1463  0.0122\n",
      "     65       28.3724       26.9670  0.0117\n",
      "     66       28.3643       27.1175  0.0118\n",
      "     67       \u001b[36m28.3610\u001b[0m       27.0222  0.0124\n",
      "     68       28.3645       27.0467  0.0120\n",
      "     69       \u001b[36m28.3568\u001b[0m       27.0769  0.0121\n",
      "     70       28.3629       27.0138  0.0121\n",
      "     71       28.3598       27.0578  0.0121\n",
      "     72       \u001b[36m28.3560\u001b[0m       27.0629  0.0121\n",
      "     73       28.3602       27.0207  0.0117\n",
      "     74       28.3569       27.0644  0.0114\n",
      "     75       \u001b[36m28.3544\u001b[0m       27.0570  0.0119\n",
      "     76       28.3576       27.0294  0.0118\n",
      "     77       28.3563       27.0500  0.0117\n",
      "     78       \u001b[36m28.3526\u001b[0m       27.0517  0.0120\n",
      "     79       28.3550       27.0490  0.0117\n",
      "     80       28.3575       27.0113  0.0113\n",
      "     81       \u001b[36m28.3523\u001b[0m       27.0708  0.0124\n",
      "     82       28.3525       27.0554  0.0122\n",
      "     83       28.3601       26.9914  0.0118\n",
      "     84       28.3556       27.0530  0.0119\n",
      "     85       28.3525       27.0841  0.0115\n",
      "     86       28.3612       27.0012  0.0119\n",
      "     87       28.3696       26.9998  0.0129\n",
      "     88       28.3551       27.1203  0.0123\n",
      "     89       28.3709       27.0534  0.0118\n",
      "     90       28.3886       26.9231  0.0123\n",
      "     91       28.3752       27.1629  0.0116\n",
      "     92       28.3763       27.0843  0.0119\n",
      "     93       28.3953       26.8899  0.0117\n",
      "     94       28.3782       27.1253  0.0119\n",
      "     95       28.3627       27.0915  0.0118\n",
      "     96       28.3898       26.8746  0.0115\n",
      "     97       28.3788       27.1248  0.0120\n",
      "     98       28.3766       27.0037  0.0122\n",
      "     99       28.3785       26.9556  0.0118\n",
      "    100       28.3658       27.1429  0.0120\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m41.9805\u001b[0m       \u001b[32m40.7426\u001b[0m  0.0111\n",
      "      2       \u001b[36m38.2870\u001b[0m       \u001b[32m36.2904\u001b[0m  0.0117\n",
      "      3       \u001b[36m35.2461\u001b[0m       \u001b[32m32.8141\u001b[0m  0.0119\n",
      "      4       \u001b[36m33.4834\u001b[0m       \u001b[32m31.0759\u001b[0m  0.0118\n",
      "      5       \u001b[36m32.9153\u001b[0m       \u001b[32m30.4885\u001b[0m  0.0110\n",
      "      6       \u001b[36m32.7613\u001b[0m       \u001b[32m30.2901\u001b[0m  0.0118\n",
      "      7       \u001b[36m32.6832\u001b[0m       \u001b[32m30.1990\u001b[0m  0.0113\n",
      "      8       \u001b[36m32.6262\u001b[0m       \u001b[32m30.1425\u001b[0m  0.0110\n",
      "      9       \u001b[36m32.5811\u001b[0m       \u001b[32m30.1012\u001b[0m  0.0109\n",
      "     10       \u001b[36m32.5451\u001b[0m       \u001b[32m30.0678\u001b[0m  0.0107\n",
      "     11       \u001b[36m32.5156\u001b[0m       \u001b[32m30.0402\u001b[0m  0.0108\n",
      "     12       \u001b[36m32.4911\u001b[0m       \u001b[32m30.0175\u001b[0m  0.0108\n",
      "     13       \u001b[36m32.4706\u001b[0m       \u001b[32m29.9978\u001b[0m  0.0113\n",
      "     14       \u001b[36m32.4532\u001b[0m       \u001b[32m29.9808\u001b[0m  0.0117\n",
      "     15       \u001b[36m32.4381\u001b[0m       \u001b[32m29.9663\u001b[0m  0.0107\n",
      "     16       \u001b[36m32.4251\u001b[0m       \u001b[32m29.9534\u001b[0m  0.0108\n",
      "     17       \u001b[36m32.4132\u001b[0m       \u001b[32m29.9418\u001b[0m  0.0109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     18       \u001b[36m32.4026\u001b[0m       \u001b[32m29.9317\u001b[0m  0.0116\n",
      "     19       \u001b[36m32.3929\u001b[0m       \u001b[32m29.9231\u001b[0m  0.0111\n",
      "     20       \u001b[36m32.3841\u001b[0m       \u001b[32m29.9144\u001b[0m  0.0109\n",
      "     21       \u001b[36m32.3763\u001b[0m       \u001b[32m29.9074\u001b[0m  0.0107\n",
      "     22       \u001b[36m32.3688\u001b[0m       \u001b[32m29.9004\u001b[0m  0.0110\n",
      "     23       \u001b[36m32.3619\u001b[0m       \u001b[32m29.8937\u001b[0m  0.0116\n",
      "     24       \u001b[36m32.3557\u001b[0m       \u001b[32m29.8879\u001b[0m  0.0111\n",
      "     25       \u001b[36m32.3498\u001b[0m       \u001b[32m29.8825\u001b[0m  0.0114\n",
      "     26       \u001b[36m32.3443\u001b[0m       \u001b[32m29.8769\u001b[0m  0.0112\n",
      "     27       \u001b[36m32.3396\u001b[0m       \u001b[32m29.8721\u001b[0m  0.0112\n",
      "     28       \u001b[36m32.3348\u001b[0m       \u001b[32m29.8674\u001b[0m  0.0119\n",
      "     29       \u001b[36m32.3305\u001b[0m       \u001b[32m29.8633\u001b[0m  0.0118\n",
      "     30       \u001b[36m32.3263\u001b[0m       \u001b[32m29.8592\u001b[0m  0.0119\n",
      "     31       \u001b[36m32.3227\u001b[0m       \u001b[32m29.8556\u001b[0m  0.0112\n",
      "     32       \u001b[36m32.3190\u001b[0m       \u001b[32m29.8523\u001b[0m  0.0111\n",
      "     33       \u001b[36m32.3154\u001b[0m       \u001b[32m29.8489\u001b[0m  0.0110\n",
      "     34       \u001b[36m32.3123\u001b[0m       \u001b[32m29.8457\u001b[0m  0.0125\n",
      "     35       \u001b[36m32.3091\u001b[0m       \u001b[32m29.8431\u001b[0m  0.0190\n",
      "     36       \u001b[36m32.3062\u001b[0m       \u001b[32m29.8402\u001b[0m  0.0115\n",
      "     37       \u001b[36m32.3034\u001b[0m       \u001b[32m29.8380\u001b[0m  0.0111\n",
      "     38       \u001b[36m32.3007\u001b[0m       \u001b[32m29.8357\u001b[0m  0.0122\n",
      "     39       \u001b[36m32.2982\u001b[0m       \u001b[32m29.8339\u001b[0m  0.0117\n",
      "     40       \u001b[36m32.2958\u001b[0m       \u001b[32m29.8322\u001b[0m  0.0139\n",
      "     41       \u001b[36m32.2934\u001b[0m       \u001b[32m29.8304\u001b[0m  0.0109\n",
      "     42       \u001b[36m32.2911\u001b[0m       \u001b[32m29.8291\u001b[0m  0.0123\n",
      "     43       \u001b[36m32.2889\u001b[0m       \u001b[32m29.8272\u001b[0m  0.0114\n",
      "     44       \u001b[36m32.2868\u001b[0m       \u001b[32m29.8260\u001b[0m  0.0112\n",
      "     45       \u001b[36m32.2847\u001b[0m       \u001b[32m29.8250\u001b[0m  0.0111\n",
      "     46       \u001b[36m32.2828\u001b[0m       \u001b[32m29.8236\u001b[0m  0.0112\n",
      "     47       \u001b[36m32.2809\u001b[0m       \u001b[32m29.8227\u001b[0m  0.0113\n",
      "     48       \u001b[36m32.2790\u001b[0m       \u001b[32m29.8215\u001b[0m  0.0114\n",
      "     49       \u001b[36m32.2772\u001b[0m       \u001b[32m29.8207\u001b[0m  0.0109\n",
      "     50       \u001b[36m32.2754\u001b[0m       \u001b[32m29.8193\u001b[0m  0.0109\n",
      "     51       \u001b[36m32.2739\u001b[0m       \u001b[32m29.8185\u001b[0m  0.0110\n",
      "     52       \u001b[36m32.2722\u001b[0m       \u001b[32m29.8176\u001b[0m  0.0113\n",
      "     53       \u001b[36m32.2706\u001b[0m       \u001b[32m29.8168\u001b[0m  0.0114\n",
      "     54       \u001b[36m32.2691\u001b[0m       \u001b[32m29.8159\u001b[0m  0.0110\n",
      "     55       \u001b[36m32.2677\u001b[0m       \u001b[32m29.8152\u001b[0m  0.0109\n",
      "     56       \u001b[36m32.2661\u001b[0m       \u001b[32m29.8143\u001b[0m  0.0107\n",
      "     57       \u001b[36m32.2648\u001b[0m       \u001b[32m29.8137\u001b[0m  0.0111\n",
      "     58       \u001b[36m32.2634\u001b[0m       \u001b[32m29.8131\u001b[0m  0.0116\n",
      "     59       \u001b[36m32.2622\u001b[0m       \u001b[32m29.8125\u001b[0m  0.0113\n",
      "     60       \u001b[36m32.2608\u001b[0m       \u001b[32m29.8119\u001b[0m  0.0109\n",
      "     61       \u001b[36m32.2596\u001b[0m       \u001b[32m29.8114\u001b[0m  0.0105\n",
      "     62       \u001b[36m32.2584\u001b[0m       \u001b[32m29.8111\u001b[0m  0.0109\n",
      "     63       \u001b[36m32.2573\u001b[0m       \u001b[32m29.8103\u001b[0m  0.0110\n",
      "     64       \u001b[36m32.2561\u001b[0m       \u001b[32m29.8097\u001b[0m  0.0109\n",
      "     65       \u001b[36m32.2551\u001b[0m       \u001b[32m29.8095\u001b[0m  0.0110\n",
      "     66       \u001b[36m32.2540\u001b[0m       \u001b[32m29.8093\u001b[0m  0.0108\n",
      "     67       \u001b[36m32.2530\u001b[0m       \u001b[32m29.8089\u001b[0m  0.0110\n",
      "     68       \u001b[36m32.2520\u001b[0m       \u001b[32m29.8086\u001b[0m  0.0112\n",
      "     69       \u001b[36m32.2509\u001b[0m       \u001b[32m29.8081\u001b[0m  0.0112\n",
      "     70       \u001b[36m32.2500\u001b[0m       \u001b[32m29.8080\u001b[0m  0.0109\n",
      "     71       \u001b[36m32.2490\u001b[0m       \u001b[32m29.8076\u001b[0m  0.0108\n",
      "     72       \u001b[36m32.2481\u001b[0m       \u001b[32m29.8073\u001b[0m  0.0110\n",
      "     73       \u001b[36m32.2471\u001b[0m       \u001b[32m29.8069\u001b[0m  0.0111\n",
      "     74       \u001b[36m32.2464\u001b[0m       \u001b[32m29.8067\u001b[0m  0.0109\n",
      "     75       \u001b[36m32.2454\u001b[0m       \u001b[32m29.8066\u001b[0m  0.0109\n",
      "     76       \u001b[36m32.2446\u001b[0m       \u001b[32m29.8063\u001b[0m  0.0108\n",
      "     77       \u001b[36m32.2437\u001b[0m       \u001b[32m29.8063\u001b[0m  0.0108\n",
      "     78       \u001b[36m32.2429\u001b[0m       \u001b[32m29.8061\u001b[0m  0.0115\n",
      "     79       \u001b[36m32.2421\u001b[0m       \u001b[32m29.8060\u001b[0m  0.0110\n",
      "     80       \u001b[36m32.2412\u001b[0m       \u001b[32m29.8056\u001b[0m  0.0108\n",
      "     81       \u001b[36m32.2406\u001b[0m       \u001b[32m29.8055\u001b[0m  0.0124\n",
      "     82       \u001b[36m32.2398\u001b[0m       \u001b[32m29.8054\u001b[0m  0.0114\n",
      "     83       \u001b[36m32.2390\u001b[0m       \u001b[32m29.8052\u001b[0m  0.0111\n",
      "     84       \u001b[36m32.2383\u001b[0m       \u001b[32m29.8050\u001b[0m  0.0109\n",
      "     85       \u001b[36m32.2376\u001b[0m       \u001b[32m29.8049\u001b[0m  0.0108\n",
      "     86       \u001b[36m32.2369\u001b[0m       \u001b[32m29.8049\u001b[0m  0.0107\n",
      "     87       \u001b[36m32.2362\u001b[0m       \u001b[32m29.8048\u001b[0m  0.0109\n",
      "     88       \u001b[36m32.2354\u001b[0m       29.8048  0.0113\n",
      "     89       \u001b[36m32.2348\u001b[0m       29.8048  0.0110\n",
      "     90       \u001b[36m32.2342\u001b[0m       \u001b[32m29.8047\u001b[0m  0.0107\n",
      "     91       \u001b[36m32.2335\u001b[0m       29.8047  0.0107\n",
      "     92       \u001b[36m32.2328\u001b[0m       \u001b[32m29.8045\u001b[0m  0.0105\n",
      "     93       \u001b[36m32.2322\u001b[0m       \u001b[32m29.8044\u001b[0m  0.0109\n",
      "     94       \u001b[36m32.2316\u001b[0m       \u001b[32m29.8043\u001b[0m  0.0114\n",
      "     95       \u001b[36m32.2310\u001b[0m       29.8044  0.0108\n",
      "     96       \u001b[36m32.2304\u001b[0m       29.8043  0.0107\n",
      "     97       \u001b[36m32.2298\u001b[0m       29.8043  0.0108\n",
      "     98       \u001b[36m32.2292\u001b[0m       29.8044  0.0109\n",
      "     99       \u001b[36m32.2286\u001b[0m       29.8045  0.0111\n",
      "    100       \u001b[36m32.2280\u001b[0m       29.8045  0.0111\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m32.9460\u001b[0m       \u001b[32m30.3373\u001b[0m  0.0106\n",
      "      2       \u001b[36m29.7337\u001b[0m       \u001b[32m28.1013\u001b[0m  0.0110\n",
      "      3       \u001b[36m26.7358\u001b[0m       \u001b[32m26.5611\u001b[0m  0.0110\n",
      "      4       \u001b[36m24.6369\u001b[0m       \u001b[32m26.1921\u001b[0m  0.0109\n",
      "      5       \u001b[36m23.7935\u001b[0m       26.3205  0.0111\n",
      "      6       \u001b[36m23.5696\u001b[0m       26.4031  0.0104\n",
      "      7       \u001b[36m23.4851\u001b[0m       26.4254  0.0105\n",
      "      8       \u001b[36m23.4308\u001b[0m       26.4279  0.0110\n",
      "      9       \u001b[36m23.3884\u001b[0m       26.4257  0.0111\n",
      "     10       \u001b[36m23.3536\u001b[0m       26.4226  0.0112\n",
      "     11       \u001b[36m23.3246\u001b[0m       26.4205  0.0111\n",
      "     12       \u001b[36m23.2997\u001b[0m       26.4190  0.0106\n",
      "     13       \u001b[36m23.2785\u001b[0m       26.4181  0.0116\n",
      "     14       \u001b[36m23.2602\u001b[0m       26.4181  0.0113\n",
      "     15       \u001b[36m23.2443\u001b[0m       26.4177  0.0116\n",
      "     16       \u001b[36m23.2305\u001b[0m       26.4180  0.0109\n",
      "     17       \u001b[36m23.2186\u001b[0m       26.4191  0.0109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     18       \u001b[36m23.2080\u001b[0m       26.4190  0.0161\n",
      "     19       \u001b[36m23.1986\u001b[0m       26.4198  0.0136\n",
      "     20       \u001b[36m23.1902\u001b[0m       26.4199  0.0120\n",
      "     21       \u001b[36m23.1826\u001b[0m       26.4204  0.0110\n",
      "     22       \u001b[36m23.1757\u001b[0m       26.4207  0.0114\n",
      "     23       \u001b[36m23.1695\u001b[0m       26.4211  0.0128\n",
      "     24       \u001b[36m23.1639\u001b[0m       26.4211  0.0137\n",
      "     25       \u001b[36m23.1587\u001b[0m       26.4211  0.0120\n",
      "     26       \u001b[36m23.1540\u001b[0m       26.4212  0.0112\n",
      "     27       \u001b[36m23.1497\u001b[0m       26.4215  0.0110\n",
      "     28       \u001b[36m23.1458\u001b[0m       26.4215  0.0112\n",
      "     29       \u001b[36m23.1421\u001b[0m       26.4216  0.0112\n",
      "     30       \u001b[36m23.1387\u001b[0m       26.4217  0.0116\n",
      "     31       \u001b[36m23.1355\u001b[0m       26.4216  0.0111\n",
      "     32       \u001b[36m23.1325\u001b[0m       26.4216  0.0118\n",
      "     33       \u001b[36m23.1297\u001b[0m       26.4218  0.0112\n",
      "     34       \u001b[36m23.1271\u001b[0m       26.4217  0.0112\n",
      "     35       \u001b[36m23.1246\u001b[0m       26.4215  0.0113\n",
      "     36       \u001b[36m23.1223\u001b[0m       26.4213  0.0111\n",
      "     37       \u001b[36m23.1201\u001b[0m       26.4211  0.0113\n",
      "     38       \u001b[36m23.1181\u001b[0m       26.4211  0.0110\n",
      "     39       \u001b[36m23.1161\u001b[0m       26.4209  0.0114\n",
      "     40       \u001b[36m23.1142\u001b[0m       26.4208  0.0110\n",
      "     41       \u001b[36m23.1124\u001b[0m       26.4205  0.0109\n",
      "     42       \u001b[36m23.1107\u001b[0m       26.4202  0.0107\n",
      "     43       \u001b[36m23.1091\u001b[0m       26.4201  0.0111\n",
      "     44       \u001b[36m23.1075\u001b[0m       26.4197  0.0113\n",
      "     45       \u001b[36m23.1060\u001b[0m       26.4194  0.0112\n",
      "     46       \u001b[36m23.1046\u001b[0m       26.4190  0.0109\n",
      "     47       \u001b[36m23.1032\u001b[0m       26.4187  0.0107\n",
      "     48       \u001b[36m23.1019\u001b[0m       26.4183  0.0113\n",
      "     49       \u001b[36m23.1006\u001b[0m       26.4179  0.0110\n",
      "     50       \u001b[36m23.0994\u001b[0m       26.4175  0.0108\n",
      "     51       \u001b[36m23.0982\u001b[0m       26.4170  0.0109\n",
      "     52       \u001b[36m23.0971\u001b[0m       26.4166  0.0107\n",
      "     53       \u001b[36m23.0960\u001b[0m       26.4162  0.0109\n",
      "     54       \u001b[36m23.0949\u001b[0m       26.4159  0.0112\n",
      "     55       \u001b[36m23.0938\u001b[0m       26.4155  0.0112\n",
      "     56       \u001b[36m23.0928\u001b[0m       26.4153  0.0107\n",
      "     57       \u001b[36m23.0918\u001b[0m       26.4148  0.0106\n",
      "     58       \u001b[36m23.0909\u001b[0m       26.4145  0.0111\n",
      "     59       \u001b[36m23.0899\u001b[0m       26.4140  0.0112\n",
      "     60       \u001b[36m23.0890\u001b[0m       26.4138  0.0109\n",
      "     61       \u001b[36m23.0881\u001b[0m       26.4132  0.0107\n",
      "     62       \u001b[36m23.0873\u001b[0m       26.4127  0.0112\n",
      "     63       \u001b[36m23.0864\u001b[0m       26.4123  0.0110\n",
      "     64       \u001b[36m23.0856\u001b[0m       26.4119  0.0113\n",
      "     65       \u001b[36m23.0848\u001b[0m       26.4116  0.0110\n",
      "     66       \u001b[36m23.0840\u001b[0m       26.4112  0.0110\n",
      "     67       \u001b[36m23.0833\u001b[0m       26.4109  0.0107\n",
      "     68       \u001b[36m23.0825\u001b[0m       26.4106  0.0112\n",
      "     69       \u001b[36m23.0818\u001b[0m       26.4103  0.0111\n",
      "     70       \u001b[36m23.0811\u001b[0m       26.4101  0.0110\n",
      "     71       \u001b[36m23.0804\u001b[0m       26.4097  0.0110\n",
      "     72       \u001b[36m23.0797\u001b[0m       26.4094  0.0108\n",
      "     73       \u001b[36m23.0790\u001b[0m       26.4092  0.0111\n",
      "     74       \u001b[36m23.0784\u001b[0m       26.4091  0.0134\n",
      "     75       \u001b[36m23.0777\u001b[0m       26.4087  0.0110\n",
      "     76       \u001b[36m23.0771\u001b[0m       26.4086  0.0105\n",
      "     77       \u001b[36m23.0765\u001b[0m       26.4083  0.0104\n",
      "     78       \u001b[36m23.0759\u001b[0m       26.4083  0.0112\n",
      "     79       \u001b[36m23.0753\u001b[0m       26.4081  0.0112\n",
      "     80       \u001b[36m23.0747\u001b[0m       26.4080  0.0110\n",
      "     81       \u001b[36m23.0742\u001b[0m       26.4077  0.0110\n",
      "     82       \u001b[36m23.0737\u001b[0m       26.4076  0.0108\n",
      "     83       \u001b[36m23.0731\u001b[0m       26.4074  0.0112\n",
      "     84       \u001b[36m23.0727\u001b[0m       26.4073  0.0114\n",
      "     85       \u001b[36m23.0721\u001b[0m       26.4070  0.0121\n",
      "     86       \u001b[36m23.0717\u001b[0m       26.4069  0.0111\n",
      "     87       \u001b[36m23.0712\u001b[0m       26.4067  0.0108\n",
      "     88       \u001b[36m23.0707\u001b[0m       26.4065  0.0109\n",
      "     89       \u001b[36m23.0703\u001b[0m       26.4064  0.0113\n",
      "     90       \u001b[36m23.0698\u001b[0m       26.4063  0.0109\n",
      "     91       \u001b[36m23.0693\u001b[0m       26.4061  0.0109\n",
      "     92       \u001b[36m23.0689\u001b[0m       26.4060  0.0108\n",
      "     93       \u001b[36m23.0685\u001b[0m       26.4058  0.0111\n",
      "     94       \u001b[36m23.0680\u001b[0m       26.4056  0.0113\n",
      "     95       \u001b[36m23.0676\u001b[0m       26.4055  0.0112\n",
      "     96       \u001b[36m23.0672\u001b[0m       26.4054  0.0108\n",
      "     97       \u001b[36m23.0668\u001b[0m       26.4052  0.0108\n",
      "     98       \u001b[36m23.0664\u001b[0m       26.4050  0.0111\n",
      "     99       \u001b[36m23.0660\u001b[0m       26.4048  0.0115\n",
      "    100       \u001b[36m23.0656\u001b[0m       26.4047  0.0109\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m39.7679\u001b[0m       \u001b[32m29.7140\u001b[0m  0.0135\n",
      "      2       \u001b[36m35.4432\u001b[0m       \u001b[32m27.4495\u001b[0m  0.0144\n",
      "      3       \u001b[36m32.0814\u001b[0m       \u001b[32m26.2778\u001b[0m  0.0120\n",
      "      4       \u001b[36m29.8872\u001b[0m       26.4988  0.0118\n",
      "      5       \u001b[36m29.0579\u001b[0m       27.0599  0.0114\n",
      "      6       \u001b[36m28.8685\u001b[0m       27.2985  0.0114\n",
      "      7       \u001b[36m28.7872\u001b[0m       27.3663  0.0133\n",
      "      8       \u001b[36m28.7269\u001b[0m       27.3877  0.0116\n",
      "      9       \u001b[36m28.6808\u001b[0m       27.3954  0.0118\n",
      "     10       \u001b[36m28.6448\u001b[0m       27.4019  0.0114\n",
      "     11       \u001b[36m28.6165\u001b[0m       27.4079  0.0113\n",
      "     12       \u001b[36m28.5936\u001b[0m       27.4121  0.0115\n",
      "     13       \u001b[36m28.5746\u001b[0m       27.4153  0.0121\n",
      "     14       \u001b[36m28.5588\u001b[0m       27.4189  0.0116\n",
      "     15       \u001b[36m28.5453\u001b[0m       27.4215  0.0110\n",
      "     16       \u001b[36m28.5338\u001b[0m       27.4241  0.0110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m28.5240\u001b[0m       27.4257  0.0113\n",
      "     18       \u001b[36m28.5156\u001b[0m       27.4272  0.0114\n",
      "     19       \u001b[36m28.5083\u001b[0m       27.4275  0.0111\n",
      "     20       \u001b[36m28.5017\u001b[0m       27.4274  0.0113\n",
      "     21       \u001b[36m28.4957\u001b[0m       27.4267  0.0110\n",
      "     22       \u001b[36m28.4905\u001b[0m       27.4261  0.0110\n",
      "     23       \u001b[36m28.4857\u001b[0m       27.4251  0.0115\n",
      "     24       \u001b[36m28.4814\u001b[0m       27.4236  0.0116\n",
      "     25       \u001b[36m28.4774\u001b[0m       27.4217  0.0113\n",
      "     26       \u001b[36m28.4738\u001b[0m       27.4201  0.0114\n",
      "     27       \u001b[36m28.4704\u001b[0m       27.4184  0.0120\n",
      "     28       \u001b[36m28.4673\u001b[0m       27.4165  0.0120\n",
      "     29       \u001b[36m28.4645\u001b[0m       27.4143  0.0120\n",
      "     30       \u001b[36m28.4617\u001b[0m       27.4125  0.0112\n",
      "     31       \u001b[36m28.4592\u001b[0m       27.4102  0.0111\n",
      "     32       \u001b[36m28.4569\u001b[0m       27.4082  0.0114\n",
      "     33       \u001b[36m28.4547\u001b[0m       27.4063  0.0114\n",
      "     34       \u001b[36m28.4526\u001b[0m       27.4045  0.0113\n",
      "     35       \u001b[36m28.4507\u001b[0m       27.4022  0.0115\n",
      "     36       \u001b[36m28.4489\u001b[0m       27.4002  0.0111\n",
      "     37       \u001b[36m28.4471\u001b[0m       27.3984  0.0115\n",
      "     38       \u001b[36m28.4456\u001b[0m       27.3963  0.0117\n",
      "     39       \u001b[36m28.4440\u001b[0m       27.3946  0.0120\n",
      "     40       \u001b[36m28.4426\u001b[0m       27.3928  0.0108\n",
      "     41       \u001b[36m28.4412\u001b[0m       27.3906  0.0111\n",
      "     42       \u001b[36m28.4399\u001b[0m       27.3895  0.0115\n",
      "     43       \u001b[36m28.4387\u001b[0m       27.3877  0.0114\n",
      "     44       \u001b[36m28.4375\u001b[0m       27.3858  0.0116\n",
      "     45       \u001b[36m28.4364\u001b[0m       27.3837  0.0116\n",
      "     46       \u001b[36m28.4353\u001b[0m       27.3821  0.0113\n",
      "     47       \u001b[36m28.4342\u001b[0m       27.3807  0.0118\n",
      "     48       \u001b[36m28.4332\u001b[0m       27.3792  0.0114\n",
      "     49       \u001b[36m28.4323\u001b[0m       27.3771  0.0116\n",
      "     50       \u001b[36m28.4313\u001b[0m       27.3760  0.0110\n",
      "     51       \u001b[36m28.4304\u001b[0m       27.3745  0.0112\n",
      "     52       \u001b[36m28.4296\u001b[0m       27.3729  0.0116\n",
      "     53       \u001b[36m28.4287\u001b[0m       27.3719  0.0114\n",
      "     54       \u001b[36m28.4280\u001b[0m       27.3703  0.0116\n",
      "     55       \u001b[36m28.4272\u001b[0m       27.3689  0.0117\n",
      "     56       \u001b[36m28.4264\u001b[0m       27.3676  0.0113\n",
      "     57       \u001b[36m28.4257\u001b[0m       27.3661  0.0118\n",
      "     58       \u001b[36m28.4250\u001b[0m       27.3646  0.0114\n",
      "     59       \u001b[36m28.4243\u001b[0m       27.3639  0.0113\n",
      "     60       \u001b[36m28.4237\u001b[0m       27.3622  0.0115\n",
      "     61       \u001b[36m28.4231\u001b[0m       27.3610  0.0116\n",
      "     62       \u001b[36m28.4225\u001b[0m       27.3596  0.0115\n",
      "     63       \u001b[36m28.4219\u001b[0m       27.3582  0.0122\n",
      "     64       \u001b[36m28.4213\u001b[0m       27.3570  0.0117\n",
      "     65       \u001b[36m28.4208\u001b[0m       27.3557  0.0115\n",
      "     66       \u001b[36m28.4202\u001b[0m       27.3543  0.0113\n",
      "     67       \u001b[36m28.4196\u001b[0m       27.3531  0.0115\n",
      "     68       \u001b[36m28.4191\u001b[0m       27.3520  0.0118\n",
      "     69       \u001b[36m28.4186\u001b[0m       27.3510  0.0116\n",
      "     70       \u001b[36m28.4181\u001b[0m       27.3497  0.0113\n",
      "     71       \u001b[36m28.4176\u001b[0m       27.3486  0.0112\n",
      "     72       \u001b[36m28.4171\u001b[0m       27.3474  0.0118\n",
      "     73       \u001b[36m28.4167\u001b[0m       27.3462  0.0118\n",
      "     74       \u001b[36m28.4162\u001b[0m       27.3452  0.0113\n",
      "     75       \u001b[36m28.4158\u001b[0m       27.3440  0.0113\n",
      "     76       \u001b[36m28.4153\u001b[0m       27.3432  0.0115\n",
      "     77       \u001b[36m28.4150\u001b[0m       27.3419  0.0117\n",
      "     78       \u001b[36m28.4145\u001b[0m       27.3411  0.0115\n",
      "     79       \u001b[36m28.4141\u001b[0m       27.3400  0.0118\n",
      "     80       \u001b[36m28.4137\u001b[0m       27.3390  0.0109\n",
      "     81       \u001b[36m28.4133\u001b[0m       27.3382  0.0114\n",
      "     82       \u001b[36m28.4129\u001b[0m       27.3374  0.0115\n",
      "     83       \u001b[36m28.4126\u001b[0m       27.3367  0.0169\n",
      "     84       \u001b[36m28.4123\u001b[0m       27.3357  0.0146\n",
      "     85       \u001b[36m28.4119\u001b[0m       27.3350  0.0117\n",
      "     86       \u001b[36m28.4115\u001b[0m       27.3341  0.0133\n",
      "     87       \u001b[36m28.4112\u001b[0m       27.3335  0.0167\n",
      "     88       \u001b[36m28.4108\u001b[0m       27.3325  0.0189\n",
      "     89       \u001b[36m28.4105\u001b[0m       27.3320  0.0142\n",
      "     90       \u001b[36m28.4102\u001b[0m       27.3312  0.0120\n",
      "     91       \u001b[36m28.4098\u001b[0m       27.3305  0.0111\n",
      "     92       \u001b[36m28.4095\u001b[0m       27.3296  0.0117\n",
      "     93       \u001b[36m28.4092\u001b[0m       27.3291  0.0114\n",
      "     94       \u001b[36m28.4089\u001b[0m       27.3282  0.0109\n",
      "     95       \u001b[36m28.4086\u001b[0m       27.3278  0.0111\n",
      "     96       \u001b[36m28.4083\u001b[0m       27.3270  0.0114\n",
      "     97       \u001b[36m28.4080\u001b[0m       27.3265  0.0113\n",
      "     98       \u001b[36m28.4077\u001b[0m       27.3259  0.0112\n",
      "     99       \u001b[36m28.4074\u001b[0m       27.3253  0.0111\n",
      "    100       \u001b[36m28.4072\u001b[0m       27.3247  0.0114\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m40.6585\u001b[0m       \u001b[32m37.4998\u001b[0m  0.0121\n",
      "      2       \u001b[36m36.1029\u001b[0m       \u001b[32m32.1132\u001b[0m  0.0116\n",
      "      3       \u001b[36m34.2407\u001b[0m       33.0672  0.0116\n",
      "      4       \u001b[36m33.6840\u001b[0m       \u001b[32m30.6820\u001b[0m  0.0116\n",
      "      5       \u001b[36m33.1637\u001b[0m       \u001b[32m30.4982\u001b[0m  0.0114\n",
      "      6       \u001b[36m32.9035\u001b[0m       31.1821  0.0117\n",
      "      7       \u001b[36m32.7805\u001b[0m       \u001b[32m30.2355\u001b[0m  0.0112\n",
      "      8       \u001b[36m32.5906\u001b[0m       \u001b[32m30.2064\u001b[0m  0.0118\n",
      "      9       \u001b[36m32.5075\u001b[0m       30.5218  0.0124\n",
      "     10       \u001b[36m32.4182\u001b[0m       \u001b[32m30.1513\u001b[0m  0.0115\n",
      "     11       \u001b[36m32.3655\u001b[0m       30.2916  0.0117\n",
      "     12       \u001b[36m32.3518\u001b[0m       30.2442  0.0116\n",
      "     13       \u001b[36m32.3110\u001b[0m       30.1993  0.0116\n",
      "     14       \u001b[36m32.2941\u001b[0m       30.1915  0.0115\n",
      "     15       \u001b[36m32.2755\u001b[0m       30.2185  0.0114\n",
      "     16       \u001b[36m32.2594\u001b[0m       30.1523  0.0120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m32.2444\u001b[0m       30.1770  0.0122\n",
      "     18       \u001b[36m32.2378\u001b[0m       \u001b[32m30.1421\u001b[0m  0.0115\n",
      "     19       \u001b[36m32.2243\u001b[0m       30.1597  0.0115\n",
      "     20       \u001b[36m32.2191\u001b[0m       \u001b[32m30.1156\u001b[0m  0.0116\n",
      "     21       \u001b[36m32.2119\u001b[0m       30.1832  0.0116\n",
      "     22       \u001b[36m32.2051\u001b[0m       \u001b[32m30.0787\u001b[0m  0.0115\n",
      "     23       32.2066       30.2119  0.0118\n",
      "     24       \u001b[36m32.1947\u001b[0m       \u001b[32m30.0556\u001b[0m  0.0116\n",
      "     25       32.2010       30.2176  0.0116\n",
      "     26       \u001b[36m32.1837\u001b[0m       \u001b[32m30.0468\u001b[0m  0.0115\n",
      "     27       32.1915       30.2004  0.0117\n",
      "     28       \u001b[36m32.1738\u001b[0m       \u001b[32m30.0436\u001b[0m  0.0116\n",
      "     29       32.1817       30.1817  0.0118\n",
      "     30       \u001b[36m32.1645\u001b[0m       30.0486  0.0114\n",
      "     31       32.1724       30.1559  0.0116\n",
      "     32       \u001b[36m32.1552\u001b[0m       30.0722  0.0116\n",
      "     33       32.1615       30.1326  0.0116\n",
      "     34       \u001b[36m32.1470\u001b[0m       30.1067  0.0117\n",
      "     35       32.1510       30.1117  0.0119\n",
      "     36       \u001b[36m32.1409\u001b[0m       30.1469  0.0131\n",
      "     37       \u001b[36m32.1404\u001b[0m       30.1101  0.0118\n",
      "     38       \u001b[36m32.1379\u001b[0m       30.1507  0.0116\n",
      "     39       \u001b[36m32.1300\u001b[0m       30.1468  0.0114\n",
      "     40       32.1322       30.1241  0.0112\n",
      "     41       \u001b[36m32.1284\u001b[0m       30.1710  0.0117\n",
      "     42       \u001b[36m32.1235\u001b[0m       30.1782  0.0120\n",
      "     43       32.1279       30.1015  0.0115\n",
      "     44       32.1289       30.2838  0.0113\n",
      "     45       32.1288       30.1853  0.0115\n",
      "     46       32.1415       30.1002  0.0120\n",
      "     47       32.1423       30.4022  0.0117\n",
      "     48       32.1540       30.1817  0.0123\n",
      "     49       32.1621       30.1388  0.0116\n",
      "     50       \u001b[36m32.1102\u001b[0m       30.2143  0.0114\n",
      "     51       32.1225       30.1410  0.0120\n",
      "     52       32.1116       30.2765  0.0117\n",
      "     53       \u001b[36m32.1064\u001b[0m       30.1634  0.0116\n",
      "     54       \u001b[36m32.1058\u001b[0m       30.2693  0.0113\n",
      "     55       \u001b[36m32.0937\u001b[0m       30.2099  0.0113\n",
      "     56       32.1012       30.2430  0.0119\n",
      "     57       \u001b[36m32.0868\u001b[0m       30.2228  0.0119\n",
      "     58       32.0942       30.2464  0.0115\n",
      "     59       \u001b[36m32.0833\u001b[0m       30.2638  0.0115\n",
      "     60       32.0888       30.2425  0.0118\n",
      "     61       32.0839       30.2903  0.0114\n",
      "     62       \u001b[36m32.0821\u001b[0m       30.2855  0.0204\n",
      "     63       32.0824       30.2727  0.0167\n",
      "     64       \u001b[36m32.0816\u001b[0m       30.3386  0.0136\n",
      "     65       \u001b[36m32.0793\u001b[0m       30.3350  0.0168\n",
      "     66       32.0891       30.2740  0.0175\n",
      "     67       \u001b[36m32.0785\u001b[0m       30.4188  0.0139\n",
      "     68       \u001b[36m32.0764\u001b[0m       30.3451  0.0141\n",
      "     69       32.0869       30.2728  0.0123\n",
      "     70       \u001b[36m32.0747\u001b[0m       30.5565  0.0121\n",
      "     71       32.0858       30.3458  0.0119\n",
      "     72       32.1006       30.2546  0.0121\n",
      "     73       32.0786       30.7862  0.0142\n",
      "     74       32.1112       30.3106  0.0129\n",
      "     75       32.1469       30.3787  0.0114\n",
      "     76       32.0781       30.6072  0.0117\n",
      "     77       32.1184       30.2498  0.0119\n",
      "     78       32.1222       30.6723  0.0118\n",
      "     79       32.0899       30.2968  0.0114\n",
      "     80       32.0863       30.4297  0.0110\n",
      "     81       \u001b[36m32.0560\u001b[0m       30.3695  0.0117\n",
      "     82       32.0705       30.4257  0.0120\n",
      "     83       \u001b[36m32.0539\u001b[0m       30.3549  0.0118\n",
      "     84       32.0636       30.4148  0.0118\n",
      "     85       \u001b[36m32.0497\u001b[0m       30.4312  0.0118\n",
      "     86       32.0525       30.3993  0.0117\n",
      "     87       32.0507       30.4181  0.0119\n",
      "     88       \u001b[36m32.0442\u001b[0m       30.4802  0.0120\n",
      "     89       32.0504       30.4044  0.0121\n",
      "     90       \u001b[36m32.0421\u001b[0m       30.5190  0.0119\n",
      "     91       32.0467       30.4335  0.0122\n",
      "     92       \u001b[36m32.0389\u001b[0m       30.5042  0.0120\n",
      "     93       32.0434       30.4693  0.0120\n",
      "     94       \u001b[36m32.0334\u001b[0m       30.5099  0.0123\n",
      "     95       32.0420       30.4847  0.0114\n",
      "     96       \u001b[36m32.0278\u001b[0m       30.5422  0.0116\n",
      "     97       32.0408       30.4511  0.0118\n",
      "     98       \u001b[36m32.0263\u001b[0m       30.5706  0.0114\n",
      "     99       32.0342       30.4799  0.0119\n",
      "    100       \u001b[36m32.0259\u001b[0m       30.5389  0.0128\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m30.5999\u001b[0m       \u001b[32m28.5499\u001b[0m  0.0130\n",
      "      2       \u001b[36m25.7304\u001b[0m       \u001b[32m26.6426\u001b[0m  0.0135\n",
      "      3       \u001b[36m24.5489\u001b[0m       \u001b[32m26.4975\u001b[0m  0.0122\n",
      "      4       \u001b[36m23.9384\u001b[0m       28.1151  0.0126\n",
      "      5       \u001b[36m23.7293\u001b[0m       27.5566  0.0120\n",
      "      6       \u001b[36m23.6774\u001b[0m       26.8785  0.0121\n",
      "      7       \u001b[36m23.6331\u001b[0m       27.3535  0.0131\n",
      "      8       \u001b[36m23.3580\u001b[0m       27.0815  0.0216\n",
      "      9       \u001b[36m23.2679\u001b[0m       26.9542  0.0177\n",
      "     10       \u001b[36m23.2640\u001b[0m       27.0607  0.0121\n",
      "     11       \u001b[36m23.2020\u001b[0m       26.8439  0.0117\n",
      "     12       \u001b[36m23.1576\u001b[0m       26.8190  0.0117\n",
      "     13       \u001b[36m23.1461\u001b[0m       26.8490  0.0119\n",
      "     14       \u001b[36m23.1245\u001b[0m       26.7848  0.0130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     15       \u001b[36m23.1046\u001b[0m       26.6990  0.0136\n",
      "     16       \u001b[36m23.0965\u001b[0m       26.7017  0.0139\n",
      "     17       \u001b[36m23.0845\u001b[0m       26.6904  0.0127\n",
      "     18       \u001b[36m23.0741\u001b[0m       26.6374  0.0122\n",
      "     19       \u001b[36m23.0688\u001b[0m       26.6678  0.0119\n",
      "     20       \u001b[36m23.0586\u001b[0m       26.6280  0.0121\n",
      "     21       \u001b[36m23.0548\u001b[0m       26.6298  0.0121\n",
      "     22       \u001b[36m23.0470\u001b[0m       26.6184  0.0125\n",
      "     23       \u001b[36m23.0425\u001b[0m       26.6163  0.0133\n",
      "     24       \u001b[36m23.0367\u001b[0m       26.6083  0.0129\n",
      "     25       \u001b[36m23.0326\u001b[0m       26.6103  0.0124\n",
      "     26       \u001b[36m23.0281\u001b[0m       26.6024  0.0116\n",
      "     27       \u001b[36m23.0245\u001b[0m       26.6027  0.0114\n",
      "     28       \u001b[36m23.0208\u001b[0m       26.5967  0.0116\n",
      "     29       \u001b[36m23.0170\u001b[0m       26.5951  0.0120\n",
      "     30       \u001b[36m23.0145\u001b[0m       26.5956  0.0127\n",
      "     31       \u001b[36m23.0112\u001b[0m       26.5939  0.0118\n",
      "     32       \u001b[36m23.0081\u001b[0m       26.5920  0.0119\n",
      "     33       \u001b[36m23.0072\u001b[0m       26.5855  0.0121\n",
      "     34       \u001b[36m23.0031\u001b[0m       26.6080  0.0135\n",
      "     35       23.0038       26.5626  0.0143\n",
      "     36       23.0058       26.6207  0.0189\n",
      "     37       \u001b[36m22.9979\u001b[0m       26.5946  0.0142\n",
      "     38       23.0120       26.5693  0.0133\n",
      "     39       23.0236       26.6482  0.0138\n",
      "     40       23.0107       26.5746  0.0145\n",
      "     41       23.0302       26.6145  0.0132\n",
      "     42       23.0257       26.6519  0.0129\n",
      "     43       23.0223       26.5747  0.0115\n",
      "     44       23.0224       26.6097  0.0123\n",
      "     45       \u001b[36m22.9889\u001b[0m       26.6229  0.0120\n",
      "     46       \u001b[36m22.9885\u001b[0m       26.6097  0.0116\n",
      "     47       \u001b[36m22.9791\u001b[0m       26.5979  0.0120\n",
      "     48       22.9807       26.6202  0.0115\n",
      "     49       \u001b[36m22.9747\u001b[0m       26.6034  0.0114\n",
      "     50       \u001b[36m22.9739\u001b[0m       26.6094  0.0118\n",
      "     51       \u001b[36m22.9733\u001b[0m       26.5964  0.0120\n",
      "     52       \u001b[36m22.9669\u001b[0m       26.6173  0.0115\n",
      "     53       22.9691       26.5978  0.0120\n",
      "     54       \u001b[36m22.9653\u001b[0m       26.6102  0.0117\n",
      "     55       \u001b[36m22.9630\u001b[0m       26.5942  0.0118\n",
      "     56       22.9642       26.5992  0.0118\n",
      "     57       \u001b[36m22.9610\u001b[0m       26.6206  0.0116\n",
      "     58       \u001b[36m22.9587\u001b[0m       26.5815  0.0118\n",
      "     59       22.9633       26.6292  0.0116\n",
      "     60       \u001b[36m22.9563\u001b[0m       26.6170  0.0120\n",
      "     61       22.9591       26.5670  0.0115\n",
      "     62       22.9659       26.6586  0.0121\n",
      "     63       \u001b[36m22.9563\u001b[0m       26.6171  0.0119\n",
      "     64       22.9703       26.5503  0.0120\n",
      "     65       22.9830       26.7069  0.0118\n",
      "     66       22.9638       26.5876  0.0116\n",
      "     67       22.9876       26.5686  0.0115\n",
      "     68       23.0002       26.7166  0.0114\n",
      "     69       22.9681       \u001b[32m26.4960\u001b[0m  0.0122\n",
      "     70       22.9774       26.7371  0.0122\n",
      "     71       22.9619       26.5935  0.0116\n",
      "     72       \u001b[36m22.9542\u001b[0m       26.5813  0.0121\n",
      "     73       \u001b[36m22.9523\u001b[0m       26.6699  0.0114\n",
      "     74       \u001b[36m22.9381\u001b[0m       26.5906  0.0113\n",
      "     75       22.9442       26.6533  0.0119\n",
      "     76       \u001b[36m22.9338\u001b[0m       26.6088  0.0117\n",
      "     77       22.9366       26.6348  0.0118\n",
      "     78       22.9341       26.6264  0.0119\n",
      "     79       \u001b[36m22.9295\u001b[0m       26.6271  0.0119\n",
      "     80       22.9315       26.6270  0.0115\n",
      "     81       \u001b[36m22.9271\u001b[0m       26.6349  0.0115\n",
      "     82       \u001b[36m22.9264\u001b[0m       26.6228  0.0121\n",
      "     83       \u001b[36m22.9258\u001b[0m       26.6271  0.0116\n",
      "     84       \u001b[36m22.9243\u001b[0m       26.6512  0.0114\n",
      "     85       \u001b[36m22.9209\u001b[0m       26.6008  0.0120\n",
      "     86       22.9247       26.6440  0.0117\n",
      "     87       22.9217       26.6519  0.0120\n",
      "     88       \u001b[36m22.9206\u001b[0m       26.6006  0.0119\n",
      "     89       22.9244       26.6423  0.0141\n",
      "     90       22.9223       26.6785  0.0117\n",
      "     91       \u001b[36m22.9168\u001b[0m       26.5731  0.0118\n",
      "     92       22.9309       26.6590  0.0113\n",
      "     93       22.9276       26.7112  0.0115\n",
      "     94       22.9283       26.5622  0.0120\n",
      "     95       22.9644       26.6759  0.0119\n",
      "     96       22.9776       26.7479  0.0115\n",
      "     97       22.9579       \u001b[32m26.4773\u001b[0m  0.0115\n",
      "     98       23.0040       26.8325  0.0119\n",
      "     99       22.9854       26.6184  0.0119\n",
      "    100       22.9873       \u001b[32m26.4651\u001b[0m  0.0118\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m37.2624\u001b[0m       \u001b[32m26.8179\u001b[0m  0.0117\n",
      "      2       \u001b[36m31.1554\u001b[0m       29.8715  0.0116\n",
      "      3       \u001b[36m29.9378\u001b[0m       \u001b[32m26.4565\u001b[0m  0.0122\n",
      "      4       \u001b[36m29.4027\u001b[0m       27.1214  0.0118\n",
      "      5       \u001b[36m29.1584\u001b[0m       28.5848  0.0120\n",
      "      6       \u001b[36m28.9840\u001b[0m       27.2658  0.0117\n",
      "      7       \u001b[36m28.7457\u001b[0m       27.3282  0.0115\n",
      "      8       \u001b[36m28.7058\u001b[0m       27.6106  0.0112\n",
      "      9       \u001b[36m28.6141\u001b[0m       27.2218  0.0117\n",
      "     10       \u001b[36m28.5318\u001b[0m       27.4434  0.0122\n",
      "     11       28.5410       27.4481  0.0127\n",
      "     12       \u001b[36m28.5131\u001b[0m       27.1898  0.0115\n",
      "     13       28.5208       27.3725  0.0117\n",
      "     14       28.5473       27.2397  0.0182\n",
      "     15       28.5133       27.2809  0.0142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     16       28.5299       27.3876  0.0155\n",
      "     17       28.5723       27.0065  0.0125\n",
      "     18       \u001b[36m28.4533\u001b[0m       27.4283  0.0124\n",
      "     19       28.4865       27.1157  0.0144\n",
      "     20       \u001b[36m28.4325\u001b[0m       27.2480  0.0127\n",
      "     21       28.4610       27.1769  0.0123\n",
      "     22       \u001b[36m28.4170\u001b[0m       27.1968  0.0122\n",
      "     23       28.4287       27.2181  0.0118\n",
      "     24       \u001b[36m28.4125\u001b[0m       27.2095  0.0122\n",
      "     25       28.4173       27.1768  0.0122\n",
      "     26       \u001b[36m28.4041\u001b[0m       27.2510  0.0118\n",
      "     27       28.4079       27.1813  0.0121\n",
      "     28       \u001b[36m28.3977\u001b[0m       27.2384  0.0124\n",
      "     29       28.4014       27.2003  0.0122\n",
      "     30       \u001b[36m28.3943\u001b[0m       27.2388  0.0123\n",
      "     31       28.3949       27.2177  0.0122\n",
      "     32       \u001b[36m28.3926\u001b[0m       27.2228  0.0122\n",
      "     33       \u001b[36m28.3905\u001b[0m       27.2470  0.0116\n",
      "     34       \u001b[36m28.3893\u001b[0m       27.2163  0.0120\n",
      "     35       \u001b[36m28.3883\u001b[0m       27.2565  0.0117\n",
      "     36       \u001b[36m28.3864\u001b[0m       27.2376  0.0111\n",
      "     37       \u001b[36m28.3856\u001b[0m       27.2460  0.0122\n",
      "     38       28.3869       27.2591  0.0125\n",
      "     39       \u001b[36m28.3832\u001b[0m       27.2471  0.0118\n",
      "     40       28.3848       27.2683  0.0119\n",
      "     41       28.3885       27.2684  0.0119\n",
      "     42       28.3845       27.2633  0.0119\n",
      "     43       28.3910       27.2936  0.0117\n",
      "     44       28.3997       27.2856  0.0115\n",
      "     45       28.4009       27.2924  0.0119\n",
      "     46       28.4165       27.2901  0.0121\n",
      "     47       28.4255       27.3114  0.0119\n",
      "     48       28.4222       27.3279  0.0118\n",
      "     49       28.4494       27.1988  0.0116\n",
      "     50       28.4032       27.4227  0.0121\n",
      "     51       28.3859       27.1504  0.0120\n",
      "     52       28.3875       27.3881  0.0117\n",
      "     53       \u001b[36m28.3751\u001b[0m       27.2633  0.0118\n",
      "     54       28.3799       27.3071  0.0116\n",
      "     55       \u001b[36m28.3713\u001b[0m       27.2892  0.0120\n",
      "     56       28.3745       27.3190  0.0121\n",
      "     57       \u001b[36m28.3687\u001b[0m       27.3040  0.0117\n",
      "     58       28.3711       27.3116  0.0115\n",
      "     59       \u001b[36m28.3680\u001b[0m       27.3331  0.0116\n",
      "     60       28.3683       27.3201  0.0116\n",
      "     61       \u001b[36m28.3674\u001b[0m       27.3208  0.0117\n",
      "     62       \u001b[36m28.3661\u001b[0m       27.3334  0.0117\n",
      "     63       28.3670       27.3182  0.0115\n",
      "     64       \u001b[36m28.3646\u001b[0m       27.3322  0.0113\n",
      "     65       28.3646       27.3191  0.0113\n",
      "     66       28.3648       27.3217  0.0117\n",
      "     67       \u001b[36m28.3626\u001b[0m       27.3216  0.0121\n",
      "     68       28.3632       27.3244  0.0113\n",
      "     69       28.3634       27.3220  0.0119\n",
      "     70       \u001b[36m28.3617\u001b[0m       27.3256  0.0115\n",
      "     71       28.3626       27.3285  0.0114\n",
      "     72       \u001b[36m28.3604\u001b[0m       27.3207  0.0114\n",
      "     73       \u001b[36m28.3600\u001b[0m       27.3303  0.0118\n",
      "     74       \u001b[36m28.3596\u001b[0m       27.3191  0.0120\n",
      "     75       \u001b[36m28.3593\u001b[0m       27.3299  0.0115\n",
      "     76       \u001b[36m28.3584\u001b[0m       27.3206  0.0110\n",
      "     77       \u001b[36m28.3577\u001b[0m       27.3306  0.0112\n",
      "     78       28.3588       27.3206  0.0121\n",
      "     79       28.3594       27.3271  0.0119\n",
      "     80       \u001b[36m28.3549\u001b[0m       27.3425  0.0120\n",
      "     81       28.3612       27.3041  0.0115\n",
      "     82       28.3622       27.3294  0.0115\n",
      "     83       28.3565       27.3453  0.0118\n",
      "     84       28.3658       27.3288  0.0119\n",
      "     85       28.3820       27.2721  0.0116\n",
      "     86       28.3699       27.4100  0.0112\n",
      "     87       28.3906       27.2973  0.0117\n",
      "     88       28.4121       27.2052  0.0118\n",
      "     89       28.3813       27.4669  0.0119\n",
      "     90       28.3905       27.2368  0.0117\n",
      "     91       28.3946       27.2866  0.0115\n",
      "     92       28.3667       27.3450  0.0119\n",
      "     93       28.3866       27.2187  0.0184\n",
      "     94       28.3657       27.3662  0.0159\n",
      "     95       28.3620       27.2314  0.0125\n",
      "     96       28.3646       27.2908  0.0120\n",
      "     97       \u001b[36m28.3497\u001b[0m       27.3192  0.0121\n",
      "     98       28.3559       27.2219  0.0177\n",
      "     99       \u001b[36m28.3490\u001b[0m       27.3313  0.0128\n",
      "    100       \u001b[36m28.3473\u001b[0m       27.2409  0.0119\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m43.8061\u001b[0m       \u001b[32m43.1829\u001b[0m  0.0111\n",
      "      2       \u001b[36m40.2967\u001b[0m       \u001b[32m38.9225\u001b[0m  0.0111\n",
      "      3       \u001b[36m37.1183\u001b[0m       \u001b[32m35.1194\u001b[0m  0.0108\n",
      "      4       \u001b[36m34.7119\u001b[0m       \u001b[32m32.3540\u001b[0m  0.0124\n",
      "      5       \u001b[36m33.3799\u001b[0m       \u001b[32m30.9530\u001b[0m  0.0109\n",
      "      6       \u001b[36m32.9010\u001b[0m       \u001b[32m30.4642\u001b[0m  0.0110\n",
      "      7       \u001b[36m32.7503\u001b[0m       \u001b[32m30.2962\u001b[0m  0.0108\n",
      "      8       \u001b[36m32.6753\u001b[0m       \u001b[32m30.2192\u001b[0m  0.0109\n",
      "      9       \u001b[36m32.6221\u001b[0m       \u001b[32m30.1721\u001b[0m  0.0113\n",
      "     10       \u001b[36m32.5800\u001b[0m       \u001b[32m30.1376\u001b[0m  0.0125\n",
      "     11       \u001b[36m32.5460\u001b[0m       \u001b[32m30.1098\u001b[0m  0.0117\n",
      "     12       \u001b[36m32.5179\u001b[0m       \u001b[32m30.0864\u001b[0m  0.0117\n",
      "     13       \u001b[36m32.4937\u001b[0m       \u001b[32m30.0663\u001b[0m  0.0125\n",
      "     14       \u001b[36m32.4729\u001b[0m       \u001b[32m30.0478\u001b[0m  0.0116\n",
      "     15       \u001b[36m32.4549\u001b[0m       \u001b[32m30.0318\u001b[0m  0.0109\n",
      "     16       \u001b[36m32.4390\u001b[0m       \u001b[32m30.0169\u001b[0m  0.0106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m32.4251\u001b[0m       \u001b[32m30.0038\u001b[0m  0.0110\n",
      "     18       \u001b[36m32.4124\u001b[0m       \u001b[32m29.9919\u001b[0m  0.0111\n",
      "     19       \u001b[36m32.4012\u001b[0m       \u001b[32m29.9818\u001b[0m  0.0119\n",
      "     20       \u001b[36m32.3910\u001b[0m       \u001b[32m29.9721\u001b[0m  0.0113\n",
      "     21       \u001b[36m32.3817\u001b[0m       \u001b[32m29.9635\u001b[0m  0.0109\n",
      "     22       \u001b[36m32.3733\u001b[0m       \u001b[32m29.9553\u001b[0m  0.0111\n",
      "     23       \u001b[36m32.3656\u001b[0m       \u001b[32m29.9481\u001b[0m  0.0114\n",
      "     24       \u001b[36m32.3585\u001b[0m       \u001b[32m29.9413\u001b[0m  0.0114\n",
      "     25       \u001b[36m32.3520\u001b[0m       \u001b[32m29.9354\u001b[0m  0.0112\n",
      "     26       \u001b[36m32.3460\u001b[0m       \u001b[32m29.9295\u001b[0m  0.0116\n",
      "     27       \u001b[36m32.3404\u001b[0m       \u001b[32m29.9242\u001b[0m  0.0110\n",
      "     28       \u001b[36m32.3352\u001b[0m       \u001b[32m29.9195\u001b[0m  0.0112\n",
      "     29       \u001b[36m32.3305\u001b[0m       \u001b[32m29.9150\u001b[0m  0.0115\n",
      "     30       \u001b[36m32.3259\u001b[0m       \u001b[32m29.9110\u001b[0m  0.0113\n",
      "     31       \u001b[36m32.3215\u001b[0m       \u001b[32m29.9075\u001b[0m  0.0109\n",
      "     32       \u001b[36m32.3176\u001b[0m       \u001b[32m29.9040\u001b[0m  0.0109\n",
      "     33       \u001b[36m32.3138\u001b[0m       \u001b[32m29.9008\u001b[0m  0.0116\n",
      "     34       \u001b[36m32.3103\u001b[0m       \u001b[32m29.8978\u001b[0m  0.0120\n",
      "     35       \u001b[36m32.3068\u001b[0m       \u001b[32m29.8951\u001b[0m  0.0114\n",
      "     36       \u001b[36m32.3037\u001b[0m       \u001b[32m29.8925\u001b[0m  0.0112\n",
      "     37       \u001b[36m32.3006\u001b[0m       \u001b[32m29.8900\u001b[0m  0.0108\n",
      "     38       \u001b[36m32.2976\u001b[0m       \u001b[32m29.8878\u001b[0m  0.0112\n",
      "     39       \u001b[36m32.2947\u001b[0m       \u001b[32m29.8858\u001b[0m  0.0117\n",
      "     40       \u001b[36m32.2920\u001b[0m       \u001b[32m29.8840\u001b[0m  0.0111\n",
      "     41       \u001b[36m32.2894\u001b[0m       \u001b[32m29.8821\u001b[0m  0.0111\n",
      "     42       \u001b[36m32.2869\u001b[0m       \u001b[32m29.8804\u001b[0m  0.0133\n",
      "     43       \u001b[36m32.2845\u001b[0m       \u001b[32m29.8788\u001b[0m  0.0115\n",
      "     44       \u001b[36m32.2822\u001b[0m       \u001b[32m29.8772\u001b[0m  0.0113\n",
      "     45       \u001b[36m32.2800\u001b[0m       \u001b[32m29.8756\u001b[0m  0.0112\n",
      "     46       \u001b[36m32.2780\u001b[0m       \u001b[32m29.8743\u001b[0m  0.0106\n",
      "     47       \u001b[36m32.2760\u001b[0m       \u001b[32m29.8731\u001b[0m  0.0111\n",
      "     48       \u001b[36m32.2741\u001b[0m       \u001b[32m29.8720\u001b[0m  0.0126\n",
      "     49       \u001b[36m32.2723\u001b[0m       \u001b[32m29.8709\u001b[0m  0.0114\n",
      "     50       \u001b[36m32.2705\u001b[0m       \u001b[32m29.8699\u001b[0m  0.0111\n",
      "     51       \u001b[36m32.2689\u001b[0m       \u001b[32m29.8691\u001b[0m  0.0110\n",
      "     52       \u001b[36m32.2672\u001b[0m       \u001b[32m29.8682\u001b[0m  0.0116\n",
      "     53       \u001b[36m32.2657\u001b[0m       \u001b[32m29.8674\u001b[0m  0.0111\n",
      "     54       \u001b[36m32.2642\u001b[0m       \u001b[32m29.8668\u001b[0m  0.0113\n",
      "     55       \u001b[36m32.2626\u001b[0m       \u001b[32m29.8662\u001b[0m  0.0112\n",
      "     56       \u001b[36m32.2613\u001b[0m       \u001b[32m29.8656\u001b[0m  0.0108\n",
      "     57       \u001b[36m32.2599\u001b[0m       \u001b[32m29.8650\u001b[0m  0.0108\n",
      "     58       \u001b[36m32.2586\u001b[0m       \u001b[32m29.8644\u001b[0m  0.0110\n",
      "     59       \u001b[36m32.2572\u001b[0m       \u001b[32m29.8640\u001b[0m  0.0114\n",
      "     60       \u001b[36m32.2561\u001b[0m       \u001b[32m29.8635\u001b[0m  0.0110\n",
      "     61       \u001b[36m32.2548\u001b[0m       \u001b[32m29.8631\u001b[0m  0.0111\n",
      "     62       \u001b[36m32.2536\u001b[0m       \u001b[32m29.8628\u001b[0m  0.0106\n",
      "     63       \u001b[36m32.2525\u001b[0m       \u001b[32m29.8624\u001b[0m  0.0111\n",
      "     64       \u001b[36m32.2513\u001b[0m       \u001b[32m29.8623\u001b[0m  0.0117\n",
      "     65       \u001b[36m32.2503\u001b[0m       \u001b[32m29.8619\u001b[0m  0.0114\n",
      "     66       \u001b[36m32.2493\u001b[0m       \u001b[32m29.8616\u001b[0m  0.0116\n",
      "     67       \u001b[36m32.2482\u001b[0m       \u001b[32m29.8614\u001b[0m  0.0112\n",
      "     68       \u001b[36m32.2472\u001b[0m       \u001b[32m29.8613\u001b[0m  0.0120\n",
      "     69       \u001b[36m32.2462\u001b[0m       \u001b[32m29.8611\u001b[0m  0.0117\n",
      "     70       \u001b[36m32.2453\u001b[0m       \u001b[32m29.8610\u001b[0m  0.0111\n",
      "     71       \u001b[36m32.2444\u001b[0m       \u001b[32m29.8609\u001b[0m  0.0109\n",
      "     72       \u001b[36m32.2435\u001b[0m       29.8609  0.0107\n",
      "     73       \u001b[36m32.2426\u001b[0m       \u001b[32m29.8608\u001b[0m  0.0110\n",
      "     74       \u001b[36m32.2417\u001b[0m       29.8611  0.0152\n",
      "     75       \u001b[36m32.2409\u001b[0m       29.8608  0.0136\n",
      "     76       \u001b[36m32.2400\u001b[0m       29.8610  0.0121\n",
      "     77       \u001b[36m32.2392\u001b[0m       29.8609  0.0116\n",
      "     78       \u001b[36m32.2384\u001b[0m       29.8610  0.0125\n",
      "     79       \u001b[36m32.2376\u001b[0m       29.8609  0.0158\n",
      "     80       \u001b[36m32.2368\u001b[0m       29.8611  0.0127\n",
      "     81       \u001b[36m32.2361\u001b[0m       29.8611  0.0132\n",
      "     82       \u001b[36m32.2353\u001b[0m       29.8612  0.0111\n",
      "     83       \u001b[36m32.2346\u001b[0m       29.8612  0.0110\n",
      "     84       \u001b[36m32.2339\u001b[0m       29.8613  0.0135\n",
      "     85       \u001b[36m32.2332\u001b[0m       29.8614  0.0124\n",
      "     86       \u001b[36m32.2324\u001b[0m       29.8615  0.0121\n",
      "     87       \u001b[36m32.2318\u001b[0m       29.8617  0.0108\n",
      "     88       \u001b[36m32.2311\u001b[0m       29.8619  0.0107\n",
      "     89       \u001b[36m32.2304\u001b[0m       29.8619  0.0131\n",
      "     90       \u001b[36m32.2297\u001b[0m       29.8622  0.0122\n",
      "     91       \u001b[36m32.2291\u001b[0m       29.8622  0.0127\n",
      "     92       \u001b[36m32.2284\u001b[0m       29.8624  0.0107\n",
      "     93       \u001b[36m32.2278\u001b[0m       29.8625  0.0106\n",
      "     94       \u001b[36m32.2272\u001b[0m       29.8629  0.0137\n",
      "     95       \u001b[36m32.2266\u001b[0m       29.8632  0.0116\n",
      "     96       \u001b[36m32.2259\u001b[0m       29.8634  0.0120\n",
      "     97       \u001b[36m32.2254\u001b[0m       29.8636  0.0107\n",
      "     98       \u001b[36m32.2247\u001b[0m       29.8638  0.0107\n",
      "     99       \u001b[36m32.2242\u001b[0m       29.8640  0.0140\n",
      "    100       \u001b[36m32.2235\u001b[0m       29.8644  0.0119\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m32.3385\u001b[0m       \u001b[32m29.7171\u001b[0m  0.0108\n",
      "      2       \u001b[36m28.9662\u001b[0m       \u001b[32m27.6486\u001b[0m  0.0107\n",
      "      3       \u001b[36m26.2915\u001b[0m       \u001b[32m26.5913\u001b[0m  0.0107\n",
      "      4       \u001b[36m24.6400\u001b[0m       \u001b[32m26.3739\u001b[0m  0.0141\n",
      "      5       \u001b[36m23.8479\u001b[0m       26.4604  0.0122\n",
      "      6       \u001b[36m23.5595\u001b[0m       26.5139  0.0106\n",
      "      7       \u001b[36m23.4474\u001b[0m       26.5114  0.0106\n",
      "      8       \u001b[36m23.3864\u001b[0m       26.4905  0.0128\n",
      "      9       \u001b[36m23.3457\u001b[0m       26.4687  0.0127\n",
      "     10       \u001b[36m23.3155\u001b[0m       26.4497  0.0120\n",
      "     11       \u001b[36m23.2921\u001b[0m       26.4346  0.0109\n",
      "     12       \u001b[36m23.2732\u001b[0m       26.4223  0.0107\n",
      "     13       \u001b[36m23.2576\u001b[0m       26.4128  0.0132\n",
      "     14       \u001b[36m23.2445\u001b[0m       26.4053  0.0122\n",
      "     15       \u001b[36m23.2332\u001b[0m       26.3995  0.0125\n",
      "     16       \u001b[36m23.2233\u001b[0m       26.3950  0.0104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m23.2145\u001b[0m       26.3913  0.0109\n",
      "     18       \u001b[36m23.2065\u001b[0m       26.3887  0.0143\n",
      "     19       \u001b[36m23.1993\u001b[0m       26.3867  0.0124\n",
      "     20       \u001b[36m23.1927\u001b[0m       26.3854  0.0116\n",
      "     21       \u001b[36m23.1868\u001b[0m       26.3843  0.0105\n",
      "     22       \u001b[36m23.1813\u001b[0m       26.3834  0.0105\n",
      "     23       \u001b[36m23.1763\u001b[0m       26.3829  0.0135\n",
      "     24       \u001b[36m23.1716\u001b[0m       26.3825  0.0118\n",
      "     25       \u001b[36m23.1672\u001b[0m       26.3824  0.0120\n",
      "     26       \u001b[36m23.1631\u001b[0m       26.3824  0.0107\n",
      "     27       \u001b[36m23.1593\u001b[0m       26.3824  0.0107\n",
      "     28       \u001b[36m23.1556\u001b[0m       26.3826  0.0138\n",
      "     29       \u001b[36m23.1522\u001b[0m       26.3826  0.0118\n",
      "     30       \u001b[36m23.1490\u001b[0m       26.3828  0.0117\n",
      "     31       \u001b[36m23.1460\u001b[0m       26.3828  0.0110\n",
      "     32       \u001b[36m23.1431\u001b[0m       26.3828  0.0114\n",
      "     33       \u001b[36m23.1403\u001b[0m       26.3828  0.0130\n",
      "     34       \u001b[36m23.1377\u001b[0m       26.3826  0.0114\n",
      "     35       \u001b[36m23.1352\u001b[0m       26.3825  0.0119\n",
      "     36       \u001b[36m23.1329\u001b[0m       26.3824  0.0105\n",
      "     37       \u001b[36m23.1307\u001b[0m       26.3825  0.0107\n",
      "     38       \u001b[36m23.1285\u001b[0m       26.3824  0.0135\n",
      "     39       \u001b[36m23.1265\u001b[0m       26.3823  0.0124\n",
      "     40       \u001b[36m23.1245\u001b[0m       26.3823  0.0122\n",
      "     41       \u001b[36m23.1226\u001b[0m       26.3823  0.0105\n",
      "     42       \u001b[36m23.1207\u001b[0m       26.3824  0.0107\n",
      "     43       \u001b[36m23.1190\u001b[0m       26.3824  0.0131\n",
      "     44       \u001b[36m23.1173\u001b[0m       26.3824  0.0119\n",
      "     45       \u001b[36m23.1157\u001b[0m       26.3824  0.0124\n",
      "     46       \u001b[36m23.1141\u001b[0m       26.3823  0.0105\n",
      "     47       \u001b[36m23.1126\u001b[0m       26.3823  0.0105\n",
      "     48       \u001b[36m23.1112\u001b[0m       26.3823  0.0146\n",
      "     49       \u001b[36m23.1098\u001b[0m       26.3824  0.0127\n",
      "     50       \u001b[36m23.1085\u001b[0m       26.3823  0.0120\n",
      "     51       \u001b[36m23.1072\u001b[0m       26.3823  0.0116\n",
      "     52       \u001b[36m23.1060\u001b[0m       26.3823  0.0169\n",
      "     53       \u001b[36m23.1047\u001b[0m       26.3821  0.0175\n",
      "     54       \u001b[36m23.1036\u001b[0m       26.3822  0.0150\n",
      "     55       \u001b[36m23.1025\u001b[0m       26.3821  0.0130\n",
      "     56       \u001b[36m23.1014\u001b[0m       26.3821  0.0179\n",
      "     57       \u001b[36m23.1003\u001b[0m       26.3820  0.0149\n",
      "     58       \u001b[36m23.0993\u001b[0m       26.3820  0.0128\n",
      "     59       \u001b[36m23.0983\u001b[0m       26.3818  0.0164\n",
      "     60       \u001b[36m23.0973\u001b[0m       26.3819  0.0112\n",
      "     61       \u001b[36m23.0964\u001b[0m       26.3817  0.0113\n",
      "     62       \u001b[36m23.0954\u001b[0m       26.3816  0.0112\n",
      "     63       \u001b[36m23.0945\u001b[0m       26.3815  0.0112\n",
      "     64       \u001b[36m23.0936\u001b[0m       26.3814  0.0110\n",
      "     65       \u001b[36m23.0928\u001b[0m       26.3814  0.0114\n",
      "     66       \u001b[36m23.0919\u001b[0m       26.3812  0.0114\n",
      "     67       \u001b[36m23.0911\u001b[0m       26.3811  0.0113\n",
      "     68       \u001b[36m23.0903\u001b[0m       26.3810  0.0111\n",
      "     69       \u001b[36m23.0895\u001b[0m       26.3809  0.0112\n",
      "     70       \u001b[36m23.0888\u001b[0m       26.3808  0.0112\n",
      "     71       \u001b[36m23.0880\u001b[0m       26.3806  0.0111\n",
      "     72       \u001b[36m23.0873\u001b[0m       26.3806  0.0109\n",
      "     73       \u001b[36m23.0866\u001b[0m       26.3805  0.0105\n",
      "     74       \u001b[36m23.0859\u001b[0m       26.3805  0.0110\n",
      "     75       \u001b[36m23.0852\u001b[0m       26.3805  0.0110\n",
      "     76       \u001b[36m23.0845\u001b[0m       26.3804  0.0118\n",
      "     77       \u001b[36m23.0839\u001b[0m       26.3803  0.0112\n",
      "     78       \u001b[36m23.0832\u001b[0m       26.3802  0.0108\n",
      "     79       \u001b[36m23.0826\u001b[0m       26.3801  0.0107\n",
      "     80       \u001b[36m23.0820\u001b[0m       26.3800  0.0111\n",
      "     81       \u001b[36m23.0814\u001b[0m       26.3799  0.0114\n",
      "     82       \u001b[36m23.0808\u001b[0m       26.3798  0.0110\n",
      "     83       \u001b[36m23.0802\u001b[0m       26.3798  0.0111\n",
      "     84       \u001b[36m23.0796\u001b[0m       26.3796  0.0111\n",
      "     85       \u001b[36m23.0790\u001b[0m       26.3795  0.0114\n",
      "     86       \u001b[36m23.0785\u001b[0m       26.3793  0.0114\n",
      "     87       \u001b[36m23.0779\u001b[0m       26.3792  0.0115\n",
      "     88       \u001b[36m23.0774\u001b[0m       26.3790  0.0108\n",
      "     89       \u001b[36m23.0768\u001b[0m       26.3790  0.0108\n",
      "     90       \u001b[36m23.0763\u001b[0m       26.3787  0.0110\n",
      "     91       \u001b[36m23.0758\u001b[0m       26.3788  0.0111\n",
      "     92       \u001b[36m23.0753\u001b[0m       26.3786  0.0110\n",
      "     93       \u001b[36m23.0748\u001b[0m       26.3786  0.0107\n",
      "     94       \u001b[36m23.0743\u001b[0m       26.3786  0.0108\n",
      "     95       \u001b[36m23.0738\u001b[0m       26.3784  0.0108\n",
      "     96       \u001b[36m23.0733\u001b[0m       26.3783  0.0116\n",
      "     97       \u001b[36m23.0728\u001b[0m       26.3782  0.0108\n",
      "     98       \u001b[36m23.0724\u001b[0m       26.3781  0.0108\n",
      "     99       \u001b[36m23.0719\u001b[0m       26.3779  0.0108\n",
      "    100       \u001b[36m23.0715\u001b[0m       26.3779  0.0116\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m41.4118\u001b[0m       \u001b[32m31.0624\u001b[0m  0.0114\n",
      "      2       \u001b[36m37.1407\u001b[0m       \u001b[32m28.3013\u001b[0m  0.0114\n",
      "      3       \u001b[36m33.1983\u001b[0m       \u001b[32m26.5016\u001b[0m  0.0115\n",
      "      4       \u001b[36m30.3336\u001b[0m       26.5190  0.0110\n",
      "      5       \u001b[36m29.1644\u001b[0m       27.1743  0.0109\n",
      "      6       \u001b[36m28.9460\u001b[0m       27.4018  0.0108\n",
      "      7       \u001b[36m28.8667\u001b[0m       27.4391  0.0110\n",
      "      8       \u001b[36m28.8063\u001b[0m       27.4331  0.0137\n",
      "      9       \u001b[36m28.7582\u001b[0m       27.4212  0.0124\n",
      "     10       \u001b[36m28.7191\u001b[0m       27.4105  0.0111\n",
      "     11       \u001b[36m28.6872\u001b[0m       27.4012  0.0108\n",
      "     12       \u001b[36m28.6608\u001b[0m       27.3936  0.0140\n",
      "     13       \u001b[36m28.6382\u001b[0m       27.3874  0.0124\n",
      "     14       \u001b[36m28.6186\u001b[0m       27.3821  0.0122\n",
      "     15       \u001b[36m28.6017\u001b[0m       27.3778  0.0109\n",
      "     16       \u001b[36m28.5869\u001b[0m       27.3738  0.0109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m28.5737\u001b[0m       27.3700  0.0135\n",
      "     18       \u001b[36m28.5620\u001b[0m       27.3670  0.0112\n",
      "     19       \u001b[36m28.5515\u001b[0m       27.3641  0.0109\n",
      "     20       \u001b[36m28.5421\u001b[0m       27.3613  0.0109\n",
      "     21       \u001b[36m28.5335\u001b[0m       27.3582  0.0109\n",
      "     22       \u001b[36m28.5258\u001b[0m       27.3552  0.0111\n",
      "     23       \u001b[36m28.5187\u001b[0m       27.3523  0.0113\n",
      "     24       \u001b[36m28.5123\u001b[0m       27.3496  0.0111\n",
      "     25       \u001b[36m28.5065\u001b[0m       27.3458  0.0107\n",
      "     26       \u001b[36m28.5009\u001b[0m       27.3431  0.0109\n",
      "     27       \u001b[36m28.4958\u001b[0m       27.3407  0.0113\n",
      "     28       \u001b[36m28.4912\u001b[0m       27.3383  0.0135\n",
      "     29       \u001b[36m28.4869\u001b[0m       27.3361  0.0117\n",
      "     30       \u001b[36m28.4829\u001b[0m       27.3343  0.0108\n",
      "     31       \u001b[36m28.4792\u001b[0m       27.3324  0.0106\n",
      "     32       \u001b[36m28.4758\u001b[0m       27.3306  0.0151\n",
      "     33       \u001b[36m28.4727\u001b[0m       27.3289  0.0151\n",
      "     34       \u001b[36m28.4697\u001b[0m       27.3270  0.0133\n",
      "     35       \u001b[36m28.4668\u001b[0m       27.3253  0.0121\n",
      "     36       \u001b[36m28.4642\u001b[0m       27.3236  0.0123\n",
      "     37       \u001b[36m28.4617\u001b[0m       27.3215  0.0158\n",
      "     38       \u001b[36m28.4593\u001b[0m       27.3203  0.0136\n",
      "     39       \u001b[36m28.4572\u001b[0m       27.3184  0.0133\n",
      "     40       \u001b[36m28.4551\u001b[0m       27.3168  0.0112\n",
      "     41       \u001b[36m28.4531\u001b[0m       27.3151  0.0112\n",
      "     42       \u001b[36m28.4512\u001b[0m       27.3139  0.0141\n",
      "     43       \u001b[36m28.4495\u001b[0m       27.3122  0.0121\n",
      "     44       \u001b[36m28.4477\u001b[0m       27.3111  0.0121\n",
      "     45       \u001b[36m28.4461\u001b[0m       27.3096  0.0111\n",
      "     46       \u001b[36m28.4445\u001b[0m       27.3084  0.0112\n",
      "     47       \u001b[36m28.4431\u001b[0m       27.3073  0.0137\n",
      "     48       \u001b[36m28.4417\u001b[0m       27.3057  0.0125\n",
      "     49       \u001b[36m28.4403\u001b[0m       27.3047  0.0131\n",
      "     50       \u001b[36m28.4390\u001b[0m       27.3037  0.0111\n",
      "     51       \u001b[36m28.4377\u001b[0m       27.3024  0.0106\n",
      "     52       \u001b[36m28.4365\u001b[0m       27.3011  0.0141\n",
      "     53       \u001b[36m28.4354\u001b[0m       27.3001  0.0119\n",
      "     54       \u001b[36m28.4342\u001b[0m       27.2987  0.0124\n",
      "     55       \u001b[36m28.4332\u001b[0m       27.2978  0.0116\n",
      "     56       \u001b[36m28.4321\u001b[0m       27.2967  0.0113\n",
      "     57       \u001b[36m28.4311\u001b[0m       27.2956  0.0152\n",
      "     58       \u001b[36m28.4301\u001b[0m       27.2947  0.0122\n",
      "     59       \u001b[36m28.4292\u001b[0m       27.2937  0.0124\n",
      "     60       \u001b[36m28.4283\u001b[0m       27.2928  0.0111\n",
      "     61       \u001b[36m28.4274\u001b[0m       27.2918  0.0109\n",
      "     62       \u001b[36m28.4266\u001b[0m       27.2909  0.0138\n",
      "     63       \u001b[36m28.4258\u001b[0m       27.2900  0.0126\n",
      "     64       \u001b[36m28.4250\u001b[0m       27.2890  0.0124\n",
      "     65       \u001b[36m28.4242\u001b[0m       27.2883  0.0112\n",
      "     66       \u001b[36m28.4235\u001b[0m       27.2870  0.0107\n",
      "     67       \u001b[36m28.4227\u001b[0m       27.2865  0.0140\n",
      "     68       \u001b[36m28.4221\u001b[0m       27.2859  0.0122\n",
      "     69       \u001b[36m28.4214\u001b[0m       27.2849  0.0122\n",
      "     70       \u001b[36m28.4207\u001b[0m       27.2840  0.0114\n",
      "     71       \u001b[36m28.4201\u001b[0m       27.2837  0.0113\n",
      "     72       \u001b[36m28.4195\u001b[0m       27.2824  0.0142\n",
      "     73       \u001b[36m28.4188\u001b[0m       27.2821  0.0123\n",
      "     74       \u001b[36m28.4182\u001b[0m       27.2813  0.0126\n",
      "     75       \u001b[36m28.4177\u001b[0m       27.2809  0.0111\n",
      "     76       \u001b[36m28.4171\u001b[0m       27.2798  0.0109\n",
      "     77       \u001b[36m28.4165\u001b[0m       27.2793  0.0138\n",
      "     78       \u001b[36m28.4160\u001b[0m       27.2783  0.0123\n",
      "     79       \u001b[36m28.4154\u001b[0m       27.2780  0.0123\n",
      "     80       \u001b[36m28.4150\u001b[0m       27.2770  0.0108\n",
      "     81       \u001b[36m28.4144\u001b[0m       27.2766  0.0109\n",
      "     82       \u001b[36m28.4140\u001b[0m       27.2759  0.0133\n",
      "     83       \u001b[36m28.4135\u001b[0m       27.2750  0.0125\n",
      "     84       \u001b[36m28.4130\u001b[0m       27.2746  0.0123\n",
      "     85       \u001b[36m28.4126\u001b[0m       27.2738  0.0110\n",
      "     86       \u001b[36m28.4121\u001b[0m       27.2731  0.0113\n",
      "     87       \u001b[36m28.4117\u001b[0m       27.2728  0.0152\n",
      "     88       \u001b[36m28.4113\u001b[0m       27.2718  0.0128\n",
      "     89       \u001b[36m28.4108\u001b[0m       27.2712  0.0124\n",
      "     90       \u001b[36m28.4104\u001b[0m       27.2705  0.0108\n",
      "     91       \u001b[36m28.4100\u001b[0m       27.2696  0.0109\n",
      "     92       \u001b[36m28.4096\u001b[0m       27.2691  0.0144\n",
      "     93       \u001b[36m28.4092\u001b[0m       27.2680  0.0122\n",
      "     94       \u001b[36m28.4088\u001b[0m       27.2675  0.0123\n",
      "     95       \u001b[36m28.4085\u001b[0m       27.2667  0.0109\n",
      "     96       \u001b[36m28.4081\u001b[0m       27.2662  0.0108\n",
      "     97       \u001b[36m28.4078\u001b[0m       27.2655  0.0131\n",
      "     98       \u001b[36m28.4074\u001b[0m       27.2650  0.0124\n",
      "     99       \u001b[36m28.4071\u001b[0m       27.2642  0.0123\n",
      "    100       \u001b[36m28.4067\u001b[0m       27.2641  0.0112\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m38.4924\u001b[0m       \u001b[32m31.5936\u001b[0m  0.0125\n",
      "      2       \u001b[36m34.2727\u001b[0m       32.8259  0.0151\n",
      "      3       \u001b[36m33.5964\u001b[0m       \u001b[32m30.9118\u001b[0m  0.0127\n",
      "      4       \u001b[36m32.9156\u001b[0m       \u001b[32m30.2999\u001b[0m  0.0129\n",
      "      5       \u001b[36m32.7986\u001b[0m       30.8484  0.0132\n",
      "      6       \u001b[36m32.6371\u001b[0m       \u001b[32m30.1465\u001b[0m  0.0118\n",
      "      7       \u001b[36m32.5011\u001b[0m       30.2641  0.0117\n",
      "      8       \u001b[36m32.4630\u001b[0m       30.2948  0.0171\n",
      "      9       \u001b[36m32.3901\u001b[0m       \u001b[32m30.1218\u001b[0m  0.0162\n",
      "     10       \u001b[36m32.3457\u001b[0m       30.1859  0.0132\n",
      "     11       \u001b[36m32.3165\u001b[0m       30.1506  0.0126\n",
      "     12       \u001b[36m32.2920\u001b[0m       \u001b[32m30.1214\u001b[0m  0.0169\n",
      "     13       \u001b[36m32.2768\u001b[0m       \u001b[32m30.0760\u001b[0m  0.0158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     14       \u001b[36m32.2663\u001b[0m       30.1451  0.0133\n",
      "     15       \u001b[36m32.2487\u001b[0m       \u001b[32m30.0150\u001b[0m  0.0131\n",
      "     16       \u001b[36m32.2427\u001b[0m       30.1542  0.0125\n",
      "     17       \u001b[36m32.2318\u001b[0m       \u001b[32m29.9968\u001b[0m  0.0121\n",
      "     18       \u001b[36m32.2279\u001b[0m       30.1799  0.0122\n",
      "     19       \u001b[36m32.2206\u001b[0m       \u001b[32m29.9943\u001b[0m  0.0127\n",
      "     20       32.2212       30.1977  0.0122\n",
      "     21       \u001b[36m32.2073\u001b[0m       \u001b[32m29.9770\u001b[0m  0.0121\n",
      "     22       32.2126       30.1934  0.0122\n",
      "     23       \u001b[36m32.1906\u001b[0m       \u001b[32m29.9572\u001b[0m  0.0122\n",
      "     24       32.1977       30.1462  0.0123\n",
      "     25       \u001b[36m32.1732\u001b[0m       29.9578  0.0122\n",
      "     26       32.1792       30.0855  0.0119\n",
      "     27       \u001b[36m32.1569\u001b[0m       29.9835  0.0121\n",
      "     28       32.1615       30.0202  0.0123\n",
      "     29       \u001b[36m32.1482\u001b[0m       30.0598  0.0118\n",
      "     30       32.1490       30.0129  0.0122\n",
      "     31       32.1535       30.0167  0.0119\n",
      "     32       \u001b[36m32.1328\u001b[0m       30.0740  0.0121\n",
      "     33       32.1443       29.9991  0.0112\n",
      "     34       32.1503       30.1325  0.0114\n",
      "     35       32.1452       30.0370  0.0121\n",
      "     36       32.1552       29.9667  0.0121\n",
      "     37       \u001b[36m32.1274\u001b[0m       30.2088  0.0119\n",
      "     38       32.1576       30.0384  0.0115\n",
      "     39       32.1712       30.1773  0.0113\n",
      "     40       32.1277       30.0359  0.0120\n",
      "     41       32.1394       30.0019  0.0117\n",
      "     42       \u001b[36m32.1015\u001b[0m       30.0448  0.0116\n",
      "     43       32.1121       30.0200  0.0115\n",
      "     44       \u001b[36m32.0892\u001b[0m       30.0440  0.0113\n",
      "     45       32.0982       30.0217  0.0134\n",
      "     46       \u001b[36m32.0849\u001b[0m       30.0687  0.0120\n",
      "     47       32.0861       30.0300  0.0117\n",
      "     48       \u001b[36m32.0843\u001b[0m       30.0310  0.0117\n",
      "     49       \u001b[36m32.0729\u001b[0m       30.0774  0.0117\n",
      "     50       32.0771       30.0043  0.0116\n",
      "     51       \u001b[36m32.0723\u001b[0m       30.1085  0.0115\n",
      "     52       \u001b[36m32.0680\u001b[0m       30.0546  0.0113\n",
      "     53       32.0752       30.0202  0.0121\n",
      "     54       \u001b[36m32.0638\u001b[0m       30.1800  0.0119\n",
      "     55       32.0698       30.0626  0.0121\n",
      "     56       32.0860       30.0008  0.0118\n",
      "     57       \u001b[36m32.0606\u001b[0m       30.3135  0.0118\n",
      "     58       32.0857       30.0662  0.0116\n",
      "     59       32.1136       30.0612  0.0116\n",
      "     60       32.0618       30.2315  0.0119\n",
      "     61       32.0893       30.0276  0.0118\n",
      "     62       32.1089       30.3080  0.0120\n",
      "     63       32.0923       30.0637  0.0145\n",
      "     64       32.0980       30.0741  0.0116\n",
      "     65       \u001b[36m32.0451\u001b[0m       30.1202  0.0124\n",
      "     66       32.0630       30.0031  0.0119\n",
      "     67       \u001b[36m32.0336\u001b[0m       30.1354  0.0122\n",
      "     68       32.0487       30.0125  0.0121\n",
      "     69       \u001b[36m32.0300\u001b[0m       30.1501  0.0118\n",
      "     70       32.0343       30.0265  0.0119\n",
      "     71       32.0308       30.1401  0.0119\n",
      "     72       \u001b[36m32.0229\u001b[0m       30.0808  0.0120\n",
      "     73       32.0277       30.0936  0.0116\n",
      "     74       \u001b[36m32.0204\u001b[0m       30.1315  0.0116\n",
      "     75       32.0218       30.0978  0.0125\n",
      "     76       32.0231       30.0926  0.0121\n",
      "     77       \u001b[36m32.0140\u001b[0m       30.1572  0.0118\n",
      "     78       32.0154       30.0630  0.0115\n",
      "     79       32.0183       30.1295  0.0116\n",
      "     80       \u001b[36m32.0045\u001b[0m       30.1611  0.0120\n",
      "     81       32.0163       30.0646  0.0117\n",
      "     82       32.0151       30.1548  0.0117\n",
      "     83       \u001b[36m32.0028\u001b[0m       30.1878  0.0119\n",
      "     84       32.0175       30.0545  0.0117\n",
      "     85       32.0263       30.1987  0.0114\n",
      "     86       32.0044       30.2751  0.0161\n",
      "     87       32.0344       30.0832  0.0214\n",
      "     88       32.0591       30.2024  0.0182\n",
      "     89       32.0213       30.3372  0.0204\n",
      "     90       32.0584       30.0108  0.0244\n",
      "     91       32.0685       30.2608  0.0130\n",
      "     92       32.0266       30.1434  0.0124\n",
      "     93       32.0589       29.9845  0.0121\n",
      "     94       32.0053       30.2952  0.0121\n",
      "     95       32.0144       29.9949  0.0120\n",
      "     96       32.0186       30.2323  0.0122\n",
      "     97       \u001b[36m31.9946\u001b[0m       30.1305  0.0122\n",
      "     98       32.0111       30.0583  0.0122\n",
      "     99       \u001b[36m31.9837\u001b[0m       30.1993  0.0120\n",
      "    100       31.9924       30.0514  0.0124\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m30.0311\u001b[0m       \u001b[32m29.9609\u001b[0m  0.0123\n",
      "      2       \u001b[36m25.0446\u001b[0m       \u001b[32m26.5595\u001b[0m  0.0119\n",
      "      3       \u001b[36m24.3924\u001b[0m       26.7702  0.0121\n",
      "      4       \u001b[36m23.8355\u001b[0m       28.2856  0.0119\n",
      "      5       \u001b[36m23.5149\u001b[0m       26.6938  0.0120\n",
      "      6       \u001b[36m23.4588\u001b[0m       27.0812  0.0116\n",
      "      7       \u001b[36m23.3178\u001b[0m       27.2202  0.0118\n",
      "      8       \u001b[36m23.2496\u001b[0m       26.7226  0.0124\n",
      "      9       \u001b[36m23.2236\u001b[0m       27.0577  0.0120\n",
      "     10       \u001b[36m23.1559\u001b[0m       26.8346  0.0120\n",
      "     11       \u001b[36m23.1377\u001b[0m       26.7136  0.0116\n",
      "     12       \u001b[36m23.1214\u001b[0m       26.8423  0.0118\n",
      "     13       \u001b[36m23.0989\u001b[0m       26.6465  0.0119\n",
      "     14       \u001b[36m23.0927\u001b[0m       26.7243  0.0120\n",
      "     15       \u001b[36m23.0779\u001b[0m       26.6901  0.0118\n",
      "     16       \u001b[36m23.0687\u001b[0m       26.6379  0.0120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m23.0622\u001b[0m       26.6853  0.0118\n",
      "     18       \u001b[36m23.0544\u001b[0m       26.6078  0.0123\n",
      "     19       23.0553       26.7094  0.0120\n",
      "     20       23.0675       26.5682  0.0116\n",
      "     21       23.0780       26.6903  0.0119\n",
      "     22       23.0729       26.6074  0.0114\n",
      "     23       23.0808       26.6567  0.0116\n",
      "     24       23.0688       26.6706  0.0119\n",
      "     25       \u001b[36m23.0515\u001b[0m       26.6340  0.0117\n",
      "     26       \u001b[36m23.0426\u001b[0m       26.6557  0.0123\n",
      "     27       \u001b[36m23.0235\u001b[0m       26.6397  0.0117\n",
      "     28       \u001b[36m23.0179\u001b[0m       26.6445  0.0119\n",
      "     29       \u001b[36m23.0127\u001b[0m       26.6307  0.0119\n",
      "     30       \u001b[36m23.0100\u001b[0m       26.6415  0.0123\n",
      "     31       \u001b[36m23.0031\u001b[0m       26.6130  0.0121\n",
      "     32       \u001b[36m23.0005\u001b[0m       26.6298  0.0119\n",
      "     33       \u001b[36m22.9960\u001b[0m       26.6215  0.0118\n",
      "     34       \u001b[36m22.9928\u001b[0m       26.6210  0.0116\n",
      "     35       \u001b[36m22.9908\u001b[0m       26.6282  0.0122\n",
      "     36       \u001b[36m22.9863\u001b[0m       26.6171  0.0120\n",
      "     37       \u001b[36m22.9852\u001b[0m       26.6281  0.0117\n",
      "     38       \u001b[36m22.9837\u001b[0m       26.6138  0.0114\n",
      "     39       \u001b[36m22.9800\u001b[0m       26.6504  0.0114\n",
      "     40       22.9812       26.5903  0.0124\n",
      "     41       22.9861       26.6841  0.0121\n",
      "     42       22.9859       26.6388  0.0119\n",
      "     43       23.0026       26.6515  0.0114\n",
      "     44       23.0332       26.6971  0.0116\n",
      "     45       23.0859       26.6799  0.0121\n",
      "     46       23.0737       26.6542  0.0121\n",
      "     47       23.0857       26.7637  0.0120\n",
      "     48       22.9968       26.5722  0.0118\n",
      "     49       23.0016       26.8080  0.0115\n",
      "     50       \u001b[36m22.9783\u001b[0m       26.6226  0.0124\n",
      "     51       22.9823       26.7958  0.0115\n",
      "     52       \u001b[36m22.9648\u001b[0m       26.5871  0.0118\n",
      "     53       22.9681       26.7459  0.0121\n",
      "     54       \u001b[36m22.9567\u001b[0m       26.6209  0.0118\n",
      "     55       22.9579       26.7719  0.0115\n",
      "     56       \u001b[36m22.9515\u001b[0m       26.6364  0.0118\n",
      "     57       22.9516       26.7295  0.0121\n",
      "     58       \u001b[36m22.9471\u001b[0m       26.6484  0.0116\n",
      "     59       \u001b[36m22.9461\u001b[0m       26.7377  0.0129\n",
      "     60       \u001b[36m22.9437\u001b[0m       26.6686  0.0119\n",
      "     61       \u001b[36m22.9418\u001b[0m       26.7197  0.0122\n",
      "     62       \u001b[36m22.9401\u001b[0m       26.6849  0.0160\n",
      "     63       \u001b[36m22.9377\u001b[0m       26.7110  0.0149\n",
      "     64       \u001b[36m22.9375\u001b[0m       26.7034  0.0124\n",
      "     65       22.9382       26.6994  0.0121\n",
      "     66       \u001b[36m22.9343\u001b[0m       26.7277  0.0120\n",
      "     67       \u001b[36m22.9301\u001b[0m       26.6957  0.0178\n",
      "     68       22.9305       26.7237  0.0139\n",
      "     69       \u001b[36m22.9274\u001b[0m       26.7116  0.0131\n",
      "     70       \u001b[36m22.9262\u001b[0m       26.7229  0.0130\n",
      "     71       \u001b[36m22.9244\u001b[0m       26.7210  0.0138\n",
      "     72       \u001b[36m22.9233\u001b[0m       26.7214  0.0120\n",
      "     73       \u001b[36m22.9222\u001b[0m       26.7327  0.0116\n",
      "     74       \u001b[36m22.9204\u001b[0m       26.7330  0.0151\n",
      "     75       \u001b[36m22.9190\u001b[0m       26.7271  0.0132\n",
      "     76       \u001b[36m22.9178\u001b[0m       26.7466  0.0132\n",
      "     77       \u001b[36m22.9163\u001b[0m       26.7376  0.0121\n",
      "     78       \u001b[36m22.9147\u001b[0m       26.7515  0.0118\n",
      "     79       \u001b[36m22.9135\u001b[0m       26.7490  0.0148\n",
      "     80       \u001b[36m22.9125\u001b[0m       26.7471  0.0124\n",
      "     81       \u001b[36m22.9119\u001b[0m       26.7766  0.0124\n",
      "     82       \u001b[36m22.9095\u001b[0m       26.7530  0.0114\n",
      "     83       22.9098       26.7477  0.0112\n",
      "     84       \u001b[36m22.9087\u001b[0m       26.7937  0.0151\n",
      "     85       \u001b[36m22.9069\u001b[0m       26.7570  0.0132\n",
      "     86       \u001b[36m22.9061\u001b[0m       26.7598  0.0133\n",
      "     87       22.9068       26.8013  0.0114\n",
      "     88       \u001b[36m22.9042\u001b[0m       26.7588  0.0117\n",
      "     89       22.9048       26.8112  0.0151\n",
      "     90       \u001b[36m22.9003\u001b[0m       26.7683  0.0131\n",
      "     91       22.9023       26.7549  0.0130\n",
      "     92       22.9022       26.8429  0.0130\n",
      "     93       \u001b[36m22.8977\u001b[0m       26.8218  0.0133\n",
      "     94       22.8977       26.7394  0.0115\n",
      "     95       \u001b[36m22.8970\u001b[0m       26.7795  0.0115\n",
      "     96       22.9013       26.8923  0.0150\n",
      "     97       \u001b[36m22.8950\u001b[0m       26.8082  0.0131\n",
      "     98       \u001b[36m22.8930\u001b[0m       26.7823  0.0130\n",
      "     99       22.8953       26.7661  0.0130\n",
      "    100       22.9022       26.7986  0.0115\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m37.1495\u001b[0m       \u001b[32m29.0714\u001b[0m  0.0134\n",
      "      2       \u001b[36m31.6709\u001b[0m       \u001b[32m26.9593\u001b[0m  0.0136\n",
      "      3       \u001b[36m30.3460\u001b[0m       \u001b[32m26.5125\u001b[0m  0.0131\n",
      "      4       \u001b[36m29.3889\u001b[0m       29.3990  0.0129\n",
      "      5       29.3996       27.2931  0.0124\n",
      "      6       \u001b[36m28.9672\u001b[0m       27.0234  0.0121\n",
      "      7       \u001b[36m28.8104\u001b[0m       27.8791  0.0153\n",
      "      8       \u001b[36m28.7590\u001b[0m       26.9584  0.0126\n",
      "      9       \u001b[36m28.6051\u001b[0m       27.4720  0.0131\n",
      "     10       28.6167       27.4924  0.0114\n",
      "     11       \u001b[36m28.5372\u001b[0m       27.0741  0.0114\n",
      "     12       \u001b[36m28.5101\u001b[0m       27.3778  0.0163\n",
      "     13       \u001b[36m28.5022\u001b[0m       27.2484  0.0142\n",
      "     14       \u001b[36m28.4719\u001b[0m       27.2616  0.0131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     15       \u001b[36m28.4679\u001b[0m       27.2873  0.0119\n",
      "     16       \u001b[36m28.4538\u001b[0m       27.2548  0.0115\n",
      "     17       \u001b[36m28.4429\u001b[0m       27.2708  0.0157\n",
      "     18       \u001b[36m28.4388\u001b[0m       27.2589  0.0136\n",
      "     19       \u001b[36m28.4289\u001b[0m       27.2732  0.0132\n",
      "     20       \u001b[36m28.4269\u001b[0m       27.2607  0.0115\n",
      "     21       \u001b[36m28.4207\u001b[0m       27.2668  0.0115\n",
      "     22       \u001b[36m28.4167\u001b[0m       27.2558  0.0152\n",
      "     23       28.4172       27.2481  0.0131\n",
      "     24       \u001b[36m28.4131\u001b[0m       27.2685  0.0131\n",
      "     25       28.4135       27.2443  0.0130\n",
      "     26       28.4305       27.2313  0.0128\n",
      "     27       28.4290       27.2760  0.0132\n",
      "     28       28.4531       27.2606  0.0114\n",
      "     29       28.4888       27.2291  0.0115\n",
      "     30       28.4813       27.1956  0.0153\n",
      "     31       28.4808       27.1624  0.0131\n",
      "     32       28.4245       27.2862  0.0134\n",
      "     33       28.4423       27.0034  0.0119\n",
      "     34       \u001b[36m28.4010\u001b[0m       27.2925  0.0173\n",
      "     35       28.4027       27.1011  0.0176\n",
      "     36       \u001b[36m28.3953\u001b[0m       27.2518  0.0171\n",
      "     37       \u001b[36m28.3950\u001b[0m       27.0776  0.0160\n",
      "     38       \u001b[36m28.3896\u001b[0m       27.2431  0.0157\n",
      "     39       \u001b[36m28.3863\u001b[0m       27.1153  0.0210\n",
      "     40       28.3879       27.1688  0.0154\n",
      "     41       \u001b[36m28.3820\u001b[0m       27.1601  0.0132\n",
      "     42       28.3844       27.1492  0.0134\n",
      "     43       \u001b[36m28.3799\u001b[0m       27.1558  0.0131\n",
      "     44       28.3805       27.1512  0.0138\n",
      "     45       \u001b[36m28.3789\u001b[0m       27.1456  0.0136\n",
      "     46       \u001b[36m28.3772\u001b[0m       27.1450  0.0134\n",
      "     47       \u001b[36m28.3765\u001b[0m       27.1426  0.0130\n",
      "     48       \u001b[36m28.3762\u001b[0m       27.1320  0.0124\n",
      "     49       \u001b[36m28.3734\u001b[0m       27.1481  0.0119\n",
      "     50       28.3742       27.1217  0.0150\n",
      "     51       \u001b[36m28.3726\u001b[0m       27.1438  0.0131\n",
      "     52       \u001b[36m28.3710\u001b[0m       27.1268  0.0131\n",
      "     53       28.3715       27.1323  0.0131\n",
      "     54       \u001b[36m28.3707\u001b[0m       27.1112  0.0121\n",
      "     55       \u001b[36m28.3674\u001b[0m       27.1525  0.0126\n",
      "     56       28.3707       27.0758  0.0156\n",
      "     57       28.3678       27.1540  0.0135\n",
      "     58       \u001b[36m28.3646\u001b[0m       27.1068  0.0131\n",
      "     59       28.3733       27.0938  0.0116\n",
      "     60       28.3658       27.1191  0.0114\n",
      "     61       28.3666       27.1847  0.0150\n",
      "     62       28.3827       26.9807  0.0128\n",
      "     63       28.3920       27.1672  0.0132\n",
      "     64       28.3834       27.2029  0.0116\n",
      "     65       28.4470       26.9486  0.0115\n",
      "     66       28.4790       27.1213  0.0150\n",
      "     67       28.4614       27.2844  0.0130\n",
      "     68       28.4995       26.7505  0.0129\n",
      "     69       28.4188       27.3819  0.0116\n",
      "     70       28.4043       26.8380  0.0117\n",
      "     71       28.3939       27.1395  0.0149\n",
      "     72       28.3665       27.0777  0.0132\n",
      "     73       28.3833       26.9528  0.0130\n",
      "     74       \u001b[36m28.3576\u001b[0m       27.1848  0.0119\n",
      "     75       28.3652       26.9712  0.0125\n",
      "     76       28.3629       27.0825  0.0156\n",
      "     77       \u001b[36m28.3539\u001b[0m       27.0752  0.0130\n",
      "     78       28.3613       26.9873  0.0128\n",
      "     79       28.3559       27.0818  0.0119\n",
      "     80       \u001b[36m28.3534\u001b[0m       27.0452  0.0117\n",
      "     81       28.3582       26.9939  0.0118\n",
      "     82       \u001b[36m28.3515\u001b[0m       27.0850  0.0113\n",
      "     83       28.3530       27.0105  0.0114\n",
      "     84       28.3549       27.0199  0.0118\n",
      "     85       \u001b[36m28.3497\u001b[0m       27.0625  0.0116\n",
      "     86       28.3514       27.0126  0.0114\n",
      "     87       28.3518       27.0256  0.0113\n",
      "     88       \u001b[36m28.3493\u001b[0m       27.0389  0.0120\n",
      "     89       \u001b[36m28.3490\u001b[0m       27.0326  0.0121\n",
      "     90       28.3497       27.0246  0.0122\n",
      "     91       \u001b[36m28.3490\u001b[0m       27.0247  0.0112\n",
      "     92       \u001b[36m28.3471\u001b[0m       27.0566  0.0110\n",
      "     93       28.3478       27.0255  0.0118\n",
      "     94       28.3482       27.0181  0.0118\n",
      "     95       \u001b[36m28.3456\u001b[0m       27.0679  0.0116\n",
      "     96       28.3457       27.0284  0.0117\n",
      "     97       28.3483       26.9985  0.0116\n",
      "     98       \u001b[36m28.3448\u001b[0m       27.0706  0.0122\n",
      "     99       \u001b[36m28.3432\u001b[0m       27.0607  0.0119\n",
      "    100       28.3474       26.9725  0.0118\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m41.9121\u001b[0m       \u001b[32m41.3944\u001b[0m  0.0111\n",
      "      2       \u001b[36m39.0235\u001b[0m       \u001b[32m37.8033\u001b[0m  0.0109\n",
      "      3       \u001b[36m36.3882\u001b[0m       \u001b[32m34.5609\u001b[0m  0.0112\n",
      "      4       \u001b[36m34.4029\u001b[0m       \u001b[32m32.3054\u001b[0m  0.0122\n",
      "      5       \u001b[36m33.3064\u001b[0m       \u001b[32m31.0249\u001b[0m  0.0118\n",
      "      6       \u001b[36m32.8412\u001b[0m       \u001b[32m30.4611\u001b[0m  0.0117\n",
      "      7       \u001b[36m32.6713\u001b[0m       \u001b[32m30.2373\u001b[0m  0.0150\n",
      "      8       \u001b[36m32.5946\u001b[0m       \u001b[32m30.1393\u001b[0m  0.0142\n",
      "      9       \u001b[36m32.5473\u001b[0m       \u001b[32m30.0868\u001b[0m  0.0115\n",
      "     10       \u001b[36m32.5118\u001b[0m       \u001b[32m30.0529\u001b[0m  0.0119\n",
      "     11       \u001b[36m32.4837\u001b[0m       \u001b[32m30.0277\u001b[0m  0.0116\n",
      "     12       \u001b[36m32.4607\u001b[0m       \u001b[32m30.0075\u001b[0m  0.0119\n",
      "     13       \u001b[36m32.4413\u001b[0m       \u001b[32m29.9905\u001b[0m  0.0133\n",
      "     14       \u001b[36m32.4248\u001b[0m       \u001b[32m29.9758\u001b[0m  0.0116\n",
      "     15       \u001b[36m32.4103\u001b[0m       \u001b[32m29.9627\u001b[0m  0.0113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     16       \u001b[36m32.3977\u001b[0m       \u001b[32m29.9510\u001b[0m  0.0112\n",
      "     17       \u001b[36m32.3866\u001b[0m       \u001b[32m29.9403\u001b[0m  0.0109\n",
      "     18       \u001b[36m32.3769\u001b[0m       \u001b[32m29.9308\u001b[0m  0.0113\n",
      "     19       \u001b[36m32.3679\u001b[0m       \u001b[32m29.9223\u001b[0m  0.0115\n",
      "     20       \u001b[36m32.3600\u001b[0m       \u001b[32m29.9146\u001b[0m  0.0111\n",
      "     21       \u001b[36m32.3529\u001b[0m       \u001b[32m29.9075\u001b[0m  0.0107\n",
      "     22       \u001b[36m32.3462\u001b[0m       \u001b[32m29.9011\u001b[0m  0.0106\n",
      "     23       \u001b[36m32.3400\u001b[0m       \u001b[32m29.8953\u001b[0m  0.0108\n",
      "     24       \u001b[36m32.3344\u001b[0m       \u001b[32m29.8900\u001b[0m  0.0112\n",
      "     25       \u001b[36m32.3290\u001b[0m       \u001b[32m29.8850\u001b[0m  0.0113\n",
      "     26       \u001b[36m32.3241\u001b[0m       \u001b[32m29.8803\u001b[0m  0.0110\n",
      "     27       \u001b[36m32.3195\u001b[0m       \u001b[32m29.8760\u001b[0m  0.0106\n",
      "     28       \u001b[36m32.3153\u001b[0m       \u001b[32m29.8720\u001b[0m  0.0110\n",
      "     29       \u001b[36m32.3112\u001b[0m       \u001b[32m29.8684\u001b[0m  0.0114\n",
      "     30       \u001b[36m32.3076\u001b[0m       \u001b[32m29.8652\u001b[0m  0.0111\n",
      "     31       \u001b[36m32.3040\u001b[0m       \u001b[32m29.8624\u001b[0m  0.0108\n",
      "     32       \u001b[36m32.3007\u001b[0m       \u001b[32m29.8596\u001b[0m  0.0108\n",
      "     33       \u001b[36m32.2975\u001b[0m       \u001b[32m29.8571\u001b[0m  0.0109\n",
      "     34       \u001b[36m32.2946\u001b[0m       \u001b[32m29.8547\u001b[0m  0.0114\n",
      "     35       \u001b[36m32.2919\u001b[0m       \u001b[32m29.8524\u001b[0m  0.0110\n",
      "     36       \u001b[36m32.2890\u001b[0m       \u001b[32m29.8504\u001b[0m  0.0106\n",
      "     37       \u001b[36m32.2867\u001b[0m       \u001b[32m29.8485\u001b[0m  0.0117\n",
      "     38       \u001b[36m32.2841\u001b[0m       \u001b[32m29.8466\u001b[0m  0.0115\n",
      "     39       \u001b[36m32.2819\u001b[0m       \u001b[32m29.8448\u001b[0m  0.0114\n",
      "     40       \u001b[36m32.2796\u001b[0m       \u001b[32m29.8432\u001b[0m  0.0113\n",
      "     41       \u001b[36m32.2775\u001b[0m       \u001b[32m29.8417\u001b[0m  0.0111\n",
      "     42       \u001b[36m32.2755\u001b[0m       \u001b[32m29.8403\u001b[0m  0.0112\n",
      "     43       \u001b[36m32.2735\u001b[0m       \u001b[32m29.8390\u001b[0m  0.0118\n",
      "     44       \u001b[36m32.2716\u001b[0m       \u001b[32m29.8375\u001b[0m  0.0115\n",
      "     45       \u001b[36m32.2698\u001b[0m       \u001b[32m29.8364\u001b[0m  0.0109\n",
      "     46       \u001b[36m32.2680\u001b[0m       \u001b[32m29.8352\u001b[0m  0.0107\n",
      "     47       \u001b[36m32.2664\u001b[0m       \u001b[32m29.8343\u001b[0m  0.0118\n",
      "     48       \u001b[36m32.2647\u001b[0m       \u001b[32m29.8334\u001b[0m  0.0109\n",
      "     49       \u001b[36m32.2631\u001b[0m       \u001b[32m29.8325\u001b[0m  0.0114\n",
      "     50       \u001b[36m32.2615\u001b[0m       \u001b[32m29.8318\u001b[0m  0.0111\n",
      "     51       \u001b[36m32.2601\u001b[0m       \u001b[32m29.8310\u001b[0m  0.0110\n",
      "     52       \u001b[36m32.2586\u001b[0m       \u001b[32m29.8302\u001b[0m  0.0106\n",
      "     53       \u001b[36m32.2572\u001b[0m       \u001b[32m29.8294\u001b[0m  0.0108\n",
      "     54       \u001b[36m32.2559\u001b[0m       \u001b[32m29.8286\u001b[0m  0.0108\n",
      "     55       \u001b[36m32.2546\u001b[0m       \u001b[32m29.8279\u001b[0m  0.0112\n",
      "     56       \u001b[36m32.2533\u001b[0m       \u001b[32m29.8273\u001b[0m  0.0109\n",
      "     57       \u001b[36m32.2521\u001b[0m       \u001b[32m29.8266\u001b[0m  0.0106\n",
      "     58       \u001b[36m32.2509\u001b[0m       \u001b[32m29.8260\u001b[0m  0.0109\n",
      "     59       \u001b[36m32.2498\u001b[0m       \u001b[32m29.8253\u001b[0m  0.0111\n",
      "     60       \u001b[36m32.2486\u001b[0m       \u001b[32m29.8248\u001b[0m  0.0112\n",
      "     61       \u001b[36m32.2475\u001b[0m       \u001b[32m29.8240\u001b[0m  0.0109\n",
      "     62       \u001b[36m32.2465\u001b[0m       \u001b[32m29.8236\u001b[0m  0.0112\n",
      "     63       \u001b[36m32.2455\u001b[0m       \u001b[32m29.8233\u001b[0m  0.0131\n",
      "     64       \u001b[36m32.2445\u001b[0m       \u001b[32m29.8229\u001b[0m  0.0117\n",
      "     65       \u001b[36m32.2435\u001b[0m       \u001b[32m29.8224\u001b[0m  0.0117\n",
      "     66       \u001b[36m32.2425\u001b[0m       \u001b[32m29.8221\u001b[0m  0.0131\n",
      "     67       \u001b[36m32.2416\u001b[0m       \u001b[32m29.8217\u001b[0m  0.0115\n",
      "     68       \u001b[36m32.2407\u001b[0m       \u001b[32m29.8213\u001b[0m  0.0110\n",
      "     69       \u001b[36m32.2398\u001b[0m       \u001b[32m29.8210\u001b[0m  0.0117\n",
      "     70       \u001b[36m32.2389\u001b[0m       \u001b[32m29.8204\u001b[0m  0.0112\n",
      "     71       \u001b[36m32.2381\u001b[0m       \u001b[32m29.8203\u001b[0m  0.0113\n",
      "     72       \u001b[36m32.2372\u001b[0m       \u001b[32m29.8200\u001b[0m  0.0110\n",
      "     73       \u001b[36m32.2364\u001b[0m       \u001b[32m29.8199\u001b[0m  0.0110\n",
      "     74       \u001b[36m32.2356\u001b[0m       \u001b[32m29.8196\u001b[0m  0.0113\n",
      "     75       \u001b[36m32.2348\u001b[0m       \u001b[32m29.8192\u001b[0m  0.0115\n",
      "     76       \u001b[36m32.2340\u001b[0m       \u001b[32m29.8189\u001b[0m  0.0111\n",
      "     77       \u001b[36m32.2333\u001b[0m       \u001b[32m29.8188\u001b[0m  0.0109\n",
      "     78       \u001b[36m32.2325\u001b[0m       \u001b[32m29.8185\u001b[0m  0.0109\n",
      "     79       \u001b[36m32.2318\u001b[0m       \u001b[32m29.8184\u001b[0m  0.0111\n",
      "     80       \u001b[36m32.2311\u001b[0m       \u001b[32m29.8181\u001b[0m  0.0113\n",
      "     81       \u001b[36m32.2303\u001b[0m       \u001b[32m29.8177\u001b[0m  0.0111\n",
      "     82       \u001b[36m32.2296\u001b[0m       29.8178  0.0109\n",
      "     83       \u001b[36m32.2289\u001b[0m       \u001b[32m29.8176\u001b[0m  0.0107\n",
      "     84       \u001b[36m32.2282\u001b[0m       \u001b[32m29.8174\u001b[0m  0.0109\n",
      "     85       \u001b[36m32.2276\u001b[0m       \u001b[32m29.8173\u001b[0m  0.0114\n",
      "     86       \u001b[36m32.2269\u001b[0m       \u001b[32m29.8170\u001b[0m  0.0109\n",
      "     87       \u001b[36m32.2263\u001b[0m       \u001b[32m29.8167\u001b[0m  0.0109\n",
      "     88       \u001b[36m32.2257\u001b[0m       29.8167  0.0111\n",
      "     89       \u001b[36m32.2250\u001b[0m       \u001b[32m29.8166\u001b[0m  0.0112\n",
      "     90       \u001b[36m32.2244\u001b[0m       \u001b[32m29.8164\u001b[0m  0.0127\n",
      "     91       \u001b[36m32.2238\u001b[0m       \u001b[32m29.8163\u001b[0m  0.0148\n",
      "     92       \u001b[36m32.2232\u001b[0m       \u001b[32m29.8162\u001b[0m  0.0128\n",
      "     93       \u001b[36m32.2226\u001b[0m       \u001b[32m29.8158\u001b[0m  0.0165\n",
      "     94       \u001b[36m32.2220\u001b[0m       29.8159  0.0129\n",
      "     95       \u001b[36m32.2214\u001b[0m       29.8158  0.0132\n",
      "     96       \u001b[36m32.2209\u001b[0m       \u001b[32m29.8156\u001b[0m  0.0129\n",
      "     97       \u001b[36m32.2203\u001b[0m       \u001b[32m29.8156\u001b[0m  0.0137\n",
      "     98       \u001b[36m32.2197\u001b[0m       \u001b[32m29.8153\u001b[0m  0.0123\n",
      "     99       \u001b[36m32.2192\u001b[0m       \u001b[32m29.8152\u001b[0m  0.0115\n",
      "    100       \u001b[36m32.2187\u001b[0m       \u001b[32m29.8150\u001b[0m  0.0113\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m33.9181\u001b[0m       \u001b[32m31.0422\u001b[0m  0.0115\n",
      "      2       \u001b[36m30.6059\u001b[0m       \u001b[32m28.6476\u001b[0m  0.0111\n",
      "      3       \u001b[36m27.4253\u001b[0m       \u001b[32m26.8733\u001b[0m  0.0115\n",
      "      4       \u001b[36m25.1651\u001b[0m       \u001b[32m26.2509\u001b[0m  0.0111\n",
      "      5       \u001b[36m24.1029\u001b[0m       \u001b[32m26.2445\u001b[0m  0.0117\n",
      "      6       \u001b[36m23.7063\u001b[0m       26.3184  0.0112\n",
      "      7       \u001b[36m23.5464\u001b[0m       26.3548  0.0116\n",
      "      8       \u001b[36m23.4622\u001b[0m       26.3670  0.0109\n",
      "      9       \u001b[36m23.4070\u001b[0m       26.3708  0.0114\n",
      "     10       \u001b[36m23.3667\u001b[0m       26.3715  0.0115\n",
      "     11       \u001b[36m23.3355\u001b[0m       26.3711  0.0114\n",
      "     12       \u001b[36m23.3107\u001b[0m       26.3714  0.0118\n",
      "     13       \u001b[36m23.2905\u001b[0m       26.3706  0.0110\n",
      "     14       \u001b[36m23.2735\u001b[0m       26.3704  0.0114\n",
      "     15       \u001b[36m23.2589\u001b[0m       26.3703  0.0110\n",
      "     16       \u001b[36m23.2463\u001b[0m       26.3703  0.0110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m23.2352\u001b[0m       26.3707  0.0116\n",
      "     18       \u001b[36m23.2253\u001b[0m       26.3711  0.0120\n",
      "     19       \u001b[36m23.2165\u001b[0m       26.3713  0.0112\n",
      "     20       \u001b[36m23.2086\u001b[0m       26.3719  0.0110\n",
      "     21       \u001b[36m23.2014\u001b[0m       26.3725  0.0108\n",
      "     22       \u001b[36m23.1948\u001b[0m       26.3732  0.0109\n",
      "     23       \u001b[36m23.1889\u001b[0m       26.3735  0.0120\n",
      "     24       \u001b[36m23.1834\u001b[0m       26.3742  0.0114\n",
      "     25       \u001b[36m23.1784\u001b[0m       26.3746  0.0106\n",
      "     26       \u001b[36m23.1737\u001b[0m       26.3750  0.0110\n",
      "     27       \u001b[36m23.1692\u001b[0m       26.3755  0.0112\n",
      "     28       \u001b[36m23.1651\u001b[0m       26.3758  0.0119\n",
      "     29       \u001b[36m23.1612\u001b[0m       26.3761  0.0116\n",
      "     30       \u001b[36m23.1575\u001b[0m       26.3761  0.0116\n",
      "     31       \u001b[36m23.1541\u001b[0m       26.3761  0.0111\n",
      "     32       \u001b[36m23.1508\u001b[0m       26.3763  0.0123\n",
      "     33       \u001b[36m23.1477\u001b[0m       26.3765  0.0120\n",
      "     34       \u001b[36m23.1448\u001b[0m       26.3765  0.0123\n",
      "     35       \u001b[36m23.1420\u001b[0m       26.3768  0.0120\n",
      "     36       \u001b[36m23.1393\u001b[0m       26.3767  0.0113\n",
      "     37       \u001b[36m23.1368\u001b[0m       26.3767  0.0119\n",
      "     38       \u001b[36m23.1343\u001b[0m       26.3766  0.0131\n",
      "     39       \u001b[36m23.1319\u001b[0m       26.3765  0.0112\n",
      "     40       \u001b[36m23.1297\u001b[0m       26.3765  0.0123\n",
      "     41       \u001b[36m23.1275\u001b[0m       26.3762  0.0111\n",
      "     42       \u001b[36m23.1255\u001b[0m       26.3760  0.0122\n",
      "     43       \u001b[36m23.1235\u001b[0m       26.3759  0.0117\n",
      "     44       \u001b[36m23.1215\u001b[0m       26.3758  0.0121\n",
      "     45       \u001b[36m23.1196\u001b[0m       26.3755  0.0113\n",
      "     46       \u001b[36m23.1179\u001b[0m       26.3753  0.0109\n",
      "     47       \u001b[36m23.1161\u001b[0m       26.3750  0.0113\n",
      "     48       \u001b[36m23.1144\u001b[0m       26.3747  0.0112\n",
      "     49       \u001b[36m23.1128\u001b[0m       26.3742  0.0111\n",
      "     50       \u001b[36m23.1113\u001b[0m       26.3741  0.0122\n",
      "     51       \u001b[36m23.1098\u001b[0m       26.3742  0.0113\n",
      "     52       \u001b[36m23.1083\u001b[0m       26.3733  0.0119\n",
      "     53       \u001b[36m23.1070\u001b[0m       26.3732  0.0122\n",
      "     54       \u001b[36m23.1056\u001b[0m       26.3732  0.0124\n",
      "     55       \u001b[36m23.1043\u001b[0m       26.3728  0.0122\n",
      "     56       \u001b[36m23.1030\u001b[0m       26.3720  0.0108\n",
      "     57       \u001b[36m23.1018\u001b[0m       26.3720  0.0113\n",
      "     58       \u001b[36m23.1006\u001b[0m       26.3716  0.0124\n",
      "     59       \u001b[36m23.0994\u001b[0m       26.3714  0.0109\n",
      "     60       \u001b[36m23.0982\u001b[0m       26.3711  0.0107\n",
      "     61       \u001b[36m23.0971\u001b[0m       26.3707  0.0107\n",
      "     62       \u001b[36m23.0960\u001b[0m       26.3701  0.0112\n",
      "     63       \u001b[36m23.0950\u001b[0m       26.3700  0.0118\n",
      "     64       \u001b[36m23.0940\u001b[0m       26.3696  0.0122\n",
      "     65       \u001b[36m23.0930\u001b[0m       26.3694  0.0109\n",
      "     66       \u001b[36m23.0920\u001b[0m       26.3690  0.0126\n",
      "     67       \u001b[36m23.0910\u001b[0m       26.3686  0.0122\n",
      "     68       \u001b[36m23.0901\u001b[0m       26.3684  0.0120\n",
      "     69       \u001b[36m23.0892\u001b[0m       26.3681  0.0125\n",
      "     70       \u001b[36m23.0884\u001b[0m       26.3679  0.0159\n",
      "     71       \u001b[36m23.0875\u001b[0m       26.3675  0.0154\n",
      "     72       \u001b[36m23.0867\u001b[0m       26.3671  0.0119\n",
      "     73       \u001b[36m23.0858\u001b[0m       26.3668  0.0115\n",
      "     74       \u001b[36m23.0850\u001b[0m       26.3664  0.0118\n",
      "     75       \u001b[36m23.0842\u001b[0m       26.3661  0.0121\n",
      "     76       \u001b[36m23.0835\u001b[0m       26.3657  0.0124\n",
      "     77       \u001b[36m23.0827\u001b[0m       26.3653  0.0122\n",
      "     78       \u001b[36m23.0819\u001b[0m       26.3650  0.0126\n",
      "     79       \u001b[36m23.0812\u001b[0m       26.3647  0.0128\n",
      "     80       \u001b[36m23.0804\u001b[0m       26.3645  0.0115\n",
      "     81       \u001b[36m23.0797\u001b[0m       26.3641  0.0145\n",
      "     82       \u001b[36m23.0790\u001b[0m       26.3638  0.0124\n",
      "     83       \u001b[36m23.0783\u001b[0m       26.3636  0.0112\n",
      "     84       \u001b[36m23.0776\u001b[0m       26.3632  0.0111\n",
      "     85       \u001b[36m23.0769\u001b[0m       26.3631  0.0109\n",
      "     86       \u001b[36m23.0762\u001b[0m       26.3627  0.0111\n",
      "     87       \u001b[36m23.0756\u001b[0m       26.3626  0.0114\n",
      "     88       \u001b[36m23.0749\u001b[0m       26.3623  0.0126\n",
      "     89       \u001b[36m23.0743\u001b[0m       26.3621  0.0109\n",
      "     90       \u001b[36m23.0736\u001b[0m       26.3620  0.0135\n",
      "     91       \u001b[36m23.0730\u001b[0m       26.3617  0.0118\n",
      "     92       \u001b[36m23.0724\u001b[0m       26.3616  0.0148\n",
      "     93       \u001b[36m23.0718\u001b[0m       26.3615  0.0148\n",
      "     94       \u001b[36m23.0713\u001b[0m       26.3613  0.0148\n",
      "     95       \u001b[36m23.0707\u001b[0m       26.3611  0.0136\n",
      "     96       \u001b[36m23.0702\u001b[0m       26.3610  0.0124\n",
      "     97       \u001b[36m23.0696\u001b[0m       26.3609  0.0122\n",
      "     98       \u001b[36m23.0691\u001b[0m       26.3606  0.0127\n",
      "     99       \u001b[36m23.0686\u001b[0m       26.3605  0.0133\n",
      "    100       \u001b[36m23.0681\u001b[0m       26.3604  0.0117\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m39.5497\u001b[0m       \u001b[32m29.6206\u001b[0m  0.0115\n",
      "      2       \u001b[36m35.1350\u001b[0m       \u001b[32m27.1336\u001b[0m  0.0114\n",
      "      3       \u001b[36m31.4332\u001b[0m       \u001b[32m26.2697\u001b[0m  0.0112\n",
      "      4       \u001b[36m29.4478\u001b[0m       26.8765  0.0126\n",
      "      5       \u001b[36m28.9565\u001b[0m       27.3065  0.0118\n",
      "      6       \u001b[36m28.8513\u001b[0m       27.4137  0.0126\n",
      "      7       \u001b[36m28.7829\u001b[0m       27.4279  0.0122\n",
      "      8       \u001b[36m28.7277\u001b[0m       27.4264  0.0123\n",
      "      9       \u001b[36m28.6838\u001b[0m       27.4210  0.0124\n",
      "     10       \u001b[36m28.6482\u001b[0m       27.4188  0.0109\n",
      "     11       \u001b[36m28.6197\u001b[0m       27.4167  0.0109\n",
      "     12       \u001b[36m28.5962\u001b[0m       27.4143  0.0122\n",
      "     13       \u001b[36m28.5767\u001b[0m       27.4130  0.0128\n",
      "     14       \u001b[36m28.5604\u001b[0m       27.4123  0.0141\n",
      "     15       \u001b[36m28.5468\u001b[0m       27.4124  0.0112\n",
      "     16       \u001b[36m28.5352\u001b[0m       27.4119  0.0117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m28.5252\u001b[0m       27.4122  0.0120\n",
      "     18       \u001b[36m28.5167\u001b[0m       27.4107  0.0120\n",
      "     19       \u001b[36m28.5090\u001b[0m       27.4102  0.0125\n",
      "     20       \u001b[36m28.5023\u001b[0m       27.4098  0.0120\n",
      "     21       \u001b[36m28.4964\u001b[0m       27.4085  0.0110\n",
      "     22       \u001b[36m28.4910\u001b[0m       27.4080  0.0141\n",
      "     23       \u001b[36m28.4862\u001b[0m       27.4072  0.0112\n",
      "     24       \u001b[36m28.4819\u001b[0m       27.4056  0.0139\n",
      "     25       \u001b[36m28.4779\u001b[0m       27.4036  0.0114\n",
      "     26       \u001b[36m28.4742\u001b[0m       27.4024  0.0108\n",
      "     27       \u001b[36m28.4707\u001b[0m       27.4006  0.0127\n",
      "     28       \u001b[36m28.4677\u001b[0m       27.3994  0.0118\n",
      "     29       \u001b[36m28.4648\u001b[0m       27.3973  0.0126\n",
      "     30       \u001b[36m28.4621\u001b[0m       27.3960  0.0111\n",
      "     31       \u001b[36m28.4596\u001b[0m       27.3945  0.0114\n",
      "     32       \u001b[36m28.4572\u001b[0m       27.3928  0.0113\n",
      "     33       \u001b[36m28.4550\u001b[0m       27.3911  0.0140\n",
      "     34       \u001b[36m28.4529\u001b[0m       27.3893  0.0116\n",
      "     35       \u001b[36m28.4509\u001b[0m       27.3879  0.0134\n",
      "     36       \u001b[36m28.4490\u001b[0m       27.3863  0.0107\n",
      "     37       \u001b[36m28.4472\u001b[0m       27.3848  0.0116\n",
      "     38       \u001b[36m28.4455\u001b[0m       27.3834  0.0126\n",
      "     39       \u001b[36m28.4439\u001b[0m       27.3818  0.0119\n",
      "     40       \u001b[36m28.4424\u001b[0m       27.3804  0.0117\n",
      "     41       \u001b[36m28.4409\u001b[0m       27.3795  0.0139\n",
      "     42       \u001b[36m28.4396\u001b[0m       27.3785  0.0125\n",
      "     43       \u001b[36m28.4383\u001b[0m       27.3772  0.0117\n",
      "     44       \u001b[36m28.4370\u001b[0m       27.3756  0.0113\n",
      "     45       \u001b[36m28.4358\u001b[0m       27.3746  0.0140\n",
      "     46       \u001b[36m28.4347\u001b[0m       27.3731  0.0133\n",
      "     47       \u001b[36m28.4336\u001b[0m       27.3721  0.0220\n",
      "     48       \u001b[36m28.4326\u001b[0m       27.3708  0.0173\n",
      "     49       \u001b[36m28.4315\u001b[0m       27.3696  0.0143\n",
      "     50       \u001b[36m28.4306\u001b[0m       27.3684  0.0115\n",
      "     51       \u001b[36m28.4296\u001b[0m       27.3668  0.0118\n",
      "     52       \u001b[36m28.4286\u001b[0m       27.3656  0.0133\n",
      "     53       \u001b[36m28.4277\u001b[0m       27.3646  0.0117\n",
      "     54       \u001b[36m28.4269\u001b[0m       27.3635  0.0114\n",
      "     55       \u001b[36m28.4261\u001b[0m       27.3627  0.0114\n",
      "     56       \u001b[36m28.4253\u001b[0m       27.3614  0.0114\n",
      "     57       \u001b[36m28.4245\u001b[0m       27.3604  0.0113\n",
      "     58       \u001b[36m28.4237\u001b[0m       27.3599  0.0121\n",
      "     59       \u001b[36m28.4230\u001b[0m       27.3587  0.0114\n",
      "     60       \u001b[36m28.4223\u001b[0m       27.3573  0.0115\n",
      "     61       \u001b[36m28.4216\u001b[0m       27.3568  0.0113\n",
      "     62       \u001b[36m28.4210\u001b[0m       27.3559  0.0112\n",
      "     63       \u001b[36m28.4203\u001b[0m       27.3547  0.0112\n",
      "     64       \u001b[36m28.4196\u001b[0m       27.3545  0.0112\n",
      "     65       \u001b[36m28.4191\u001b[0m       27.3535  0.0112\n",
      "     66       \u001b[36m28.4185\u001b[0m       27.3530  0.0113\n",
      "     67       \u001b[36m28.4179\u001b[0m       27.3525  0.0112\n",
      "     68       \u001b[36m28.4173\u001b[0m       27.3520  0.0111\n",
      "     69       \u001b[36m28.4168\u001b[0m       27.3512  0.0109\n",
      "     70       \u001b[36m28.4163\u001b[0m       27.3508  0.0113\n",
      "     71       \u001b[36m28.4158\u001b[0m       27.3501  0.0121\n",
      "     72       \u001b[36m28.4153\u001b[0m       27.3494  0.0115\n",
      "     73       \u001b[36m28.4148\u001b[0m       27.3490  0.0110\n",
      "     74       \u001b[36m28.4143\u001b[0m       27.3486  0.0108\n",
      "     75       \u001b[36m28.4139\u001b[0m       27.3481  0.0119\n",
      "     76       \u001b[36m28.4134\u001b[0m       27.3478  0.0114\n",
      "     77       \u001b[36m28.4130\u001b[0m       27.3474  0.0110\n",
      "     78       \u001b[36m28.4125\u001b[0m       27.3468  0.0111\n",
      "     79       \u001b[36m28.4121\u001b[0m       27.3469  0.0109\n",
      "     80       \u001b[36m28.4117\u001b[0m       27.3459  0.0110\n",
      "     81       \u001b[36m28.4113\u001b[0m       27.3456  0.0115\n",
      "     82       \u001b[36m28.4109\u001b[0m       27.3448  0.0109\n",
      "     83       \u001b[36m28.4105\u001b[0m       27.3447  0.0108\n",
      "     84       \u001b[36m28.4102\u001b[0m       27.3443  0.0107\n",
      "     85       \u001b[36m28.4098\u001b[0m       27.3439  0.0121\n",
      "     86       \u001b[36m28.4094\u001b[0m       27.3435  0.0113\n",
      "     87       \u001b[36m28.4091\u001b[0m       27.3434  0.0113\n",
      "     88       \u001b[36m28.4088\u001b[0m       27.3428  0.0108\n",
      "     89       \u001b[36m28.4084\u001b[0m       27.3427  0.0105\n",
      "     90       \u001b[36m28.4081\u001b[0m       27.3422  0.0108\n",
      "     91       \u001b[36m28.4078\u001b[0m       27.3424  0.0114\n",
      "     92       \u001b[36m28.4075\u001b[0m       27.3415  0.0111\n",
      "     93       \u001b[36m28.4072\u001b[0m       27.3416  0.0108\n",
      "     94       \u001b[36m28.4069\u001b[0m       27.3414  0.0108\n",
      "     95       \u001b[36m28.4066\u001b[0m       27.3408  0.0113\n",
      "     96       \u001b[36m28.4063\u001b[0m       27.3406  0.0117\n",
      "     97       \u001b[36m28.4061\u001b[0m       27.3405  0.0108\n",
      "     98       \u001b[36m28.4058\u001b[0m       27.3399  0.0108\n",
      "     99       \u001b[36m28.4055\u001b[0m       27.3398  0.0106\n",
      "    100       \u001b[36m28.4053\u001b[0m       27.3395  0.0110\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m39.2585\u001b[0m       \u001b[32m31.6765\u001b[0m  0.0118\n",
      "      2       \u001b[36m34.3944\u001b[0m       33.2448  0.0120\n",
      "      3       \u001b[36m33.7346\u001b[0m       \u001b[32m30.6260\u001b[0m  0.0122\n",
      "      4       \u001b[36m32.9889\u001b[0m       \u001b[32m30.3447\u001b[0m  0.0118\n",
      "      5       \u001b[36m32.8542\u001b[0m       30.9083  0.0119\n",
      "      6       \u001b[36m32.6377\u001b[0m       \u001b[32m30.0616\u001b[0m  0.0112\n",
      "      7       \u001b[36m32.5449\u001b[0m       30.4672  0.0113\n",
      "      8       \u001b[36m32.4990\u001b[0m       30.2004  0.0120\n",
      "      9       \u001b[36m32.3796\u001b[0m       30.1128  0.0116\n",
      "     10       32.3803       30.3402  0.0115\n",
      "     11       \u001b[36m32.3351\u001b[0m       30.0909  0.0115\n",
      "     12       \u001b[36m32.3209\u001b[0m       30.2815  0.0115\n",
      "     13       \u001b[36m32.3106\u001b[0m       30.0924  0.0117\n",
      "     14       \u001b[36m32.2846\u001b[0m       30.1752  0.0115\n",
      "     15       32.2860       30.0987  0.0118\n",
      "     16       \u001b[36m32.2642\u001b[0m       30.1487  0.0112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m32.2633\u001b[0m       30.0817  0.0118\n",
      "     18       \u001b[36m32.2508\u001b[0m       30.1449  0.0120\n",
      "     19       \u001b[36m32.2447\u001b[0m       30.0619  0.0121\n",
      "     20       \u001b[36m32.2423\u001b[0m       30.1528  0.0119\n",
      "     21       \u001b[36m32.2289\u001b[0m       \u001b[32m30.0492\u001b[0m  0.0114\n",
      "     22       32.2365       30.1493  0.0118\n",
      "     23       \u001b[36m32.2139\u001b[0m       \u001b[32m30.0440\u001b[0m  0.0131\n",
      "     24       32.2291       30.1386  0.0120\n",
      "     25       \u001b[36m32.1975\u001b[0m       30.0544  0.0119\n",
      "     26       32.2178       30.1049  0.0131\n",
      "     27       \u001b[36m32.1849\u001b[0m       30.0923  0.0218\n",
      "     28       32.2029       30.1038  0.0197\n",
      "     29       32.1981       30.1367  0.0206\n",
      "     30       32.1949       30.1382  0.0191\n",
      "     31       32.2056       30.0517  0.0180\n",
      "     32       \u001b[36m32.1708\u001b[0m       30.1745  0.0132\n",
      "     33       32.1978       30.1002  0.0119\n",
      "     34       32.2050       30.2584  0.0121\n",
      "     35       32.1867       30.1387  0.0120\n",
      "     36       32.1990       30.1674  0.0119\n",
      "     37       \u001b[36m32.1541\u001b[0m       30.1222  0.0118\n",
      "     38       32.1773       30.1268  0.0123\n",
      "     39       \u001b[36m32.1421\u001b[0m       30.1496  0.0123\n",
      "     40       32.1635       30.1430  0.0123\n",
      "     41       \u001b[36m32.1360\u001b[0m       30.1592  0.0122\n",
      "     42       32.1523       30.1360  0.0122\n",
      "     43       \u001b[36m32.1324\u001b[0m       30.1980  0.0117\n",
      "     44       32.1405       30.1525  0.0117\n",
      "     45       32.1359       30.1582  0.0117\n",
      "     46       \u001b[36m32.1272\u001b[0m       30.2675  0.0116\n",
      "     47       32.1329       30.1653  0.0116\n",
      "     48       32.1527       30.1981  0.0114\n",
      "     49       \u001b[36m32.1224\u001b[0m       30.3598  0.0125\n",
      "     50       32.1532       30.1331  0.0122\n",
      "     51       32.1904       30.3818  0.0121\n",
      "     52       32.1601       30.1954  0.0122\n",
      "     53       32.1676       30.2227  0.0122\n",
      "     54       \u001b[36m32.1119\u001b[0m       30.2463  0.0119\n",
      "     55       32.1308       30.1734  0.0129\n",
      "     56       \u001b[36m32.1025\u001b[0m       30.2857  0.0151\n",
      "     57       32.1153       30.2492  0.0128\n",
      "     58       \u001b[36m32.0960\u001b[0m       30.2946  0.0122\n",
      "     59       32.1033       30.2599  0.0133\n",
      "     60       \u001b[36m32.0922\u001b[0m       30.3074  0.0122\n",
      "     61       32.0934       30.2867  0.0124\n",
      "     62       \u001b[36m32.0875\u001b[0m       30.3103  0.0130\n",
      "     63       \u001b[36m32.0870\u001b[0m       30.3189  0.0118\n",
      "     64       \u001b[36m32.0825\u001b[0m       30.3439  0.0126\n",
      "     65       \u001b[36m32.0802\u001b[0m       30.3455  0.0118\n",
      "     66       \u001b[36m32.0788\u001b[0m       30.3694  0.0145\n",
      "     67       \u001b[36m32.0732\u001b[0m       30.3975  0.0120\n",
      "     68       32.0737       30.3825  0.0115\n",
      "     69       \u001b[36m32.0699\u001b[0m       30.4154  0.0118\n",
      "     70       \u001b[36m32.0659\u001b[0m       30.4484  0.0119\n",
      "     71       \u001b[36m32.0653\u001b[0m       30.4392  0.0118\n",
      "     72       \u001b[36m32.0644\u001b[0m       30.4459  0.0113\n",
      "     73       \u001b[36m32.0575\u001b[0m       30.5854  0.0114\n",
      "     74       32.0582       30.4916  0.0120\n",
      "     75       32.0701       30.3783  0.0120\n",
      "     76       \u001b[36m32.0563\u001b[0m       30.8774  0.0116\n",
      "     77       32.0760       30.6756  0.0117\n",
      "     78       32.1385       30.1843  0.0114\n",
      "     79       32.1825       31.0937  0.0120\n",
      "     80       32.1766       30.5293  0.0149\n",
      "     81       32.2120       30.4983  0.0120\n",
      "     82       32.0893       30.4917  0.0116\n",
      "     83       32.1002       30.2803  0.0117\n",
      "     84       32.0638       30.4560  0.0122\n",
      "     85       32.0739       30.4112  0.0119\n",
      "     86       32.0638       30.3970  0.0119\n",
      "     87       \u001b[36m32.0553\u001b[0m       30.5304  0.0117\n",
      "     88       \u001b[36m32.0523\u001b[0m       30.3800  0.0116\n",
      "     89       \u001b[36m32.0491\u001b[0m       30.5585  0.0120\n",
      "     90       \u001b[36m32.0429\u001b[0m       30.3898  0.0121\n",
      "     91       32.0446       30.5871  0.0121\n",
      "     92       \u001b[36m32.0313\u001b[0m       30.4427  0.0117\n",
      "     93       32.0396       30.5883  0.0119\n",
      "     94       \u001b[36m32.0261\u001b[0m       30.5045  0.0130\n",
      "     95       32.0302       30.6179  0.0127\n",
      "     96       \u001b[36m32.0223\u001b[0m       30.5079  0.0124\n",
      "     97       32.0233       30.6472  0.0126\n",
      "     98       \u001b[36m32.0180\u001b[0m       30.5195  0.0119\n",
      "     99       \u001b[36m32.0177\u001b[0m       30.6404  0.0121\n",
      "    100       \u001b[36m32.0110\u001b[0m       30.5606  0.0121\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m29.5041\u001b[0m       \u001b[32m29.1551\u001b[0m  0.0172\n",
      "      2       \u001b[36m24.6234\u001b[0m       \u001b[32m26.5106\u001b[0m  0.0136\n",
      "      3       \u001b[36m23.9932\u001b[0m       27.7361  0.0127\n",
      "      4       \u001b[36m23.7225\u001b[0m       26.9037  0.0126\n",
      "      5       \u001b[36m23.5880\u001b[0m       26.8695  0.0128\n",
      "      6       \u001b[36m23.3730\u001b[0m       26.8433  0.0141\n",
      "      7       \u001b[36m23.2854\u001b[0m       26.7062  0.0125\n",
      "      8       \u001b[36m23.2599\u001b[0m       26.9776  0.0131\n",
      "      9       \u001b[36m23.1918\u001b[0m       26.5901  0.0122\n",
      "     10       \u001b[36m23.1844\u001b[0m       26.8005  0.0120\n",
      "     11       \u001b[36m23.1296\u001b[0m       26.6387  0.0118\n",
      "     12       \u001b[36m23.1271\u001b[0m       26.6626  0.0122\n",
      "     13       \u001b[36m23.1021\u001b[0m       26.6242  0.0119\n",
      "     14       \u001b[36m23.0910\u001b[0m       26.5984  0.0121\n",
      "     15       \u001b[36m23.0802\u001b[0m       26.6303  0.0117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     16       \u001b[36m23.0684\u001b[0m       26.5783  0.0118\n",
      "     17       \u001b[36m23.0634\u001b[0m       26.6188  0.0121\n",
      "     18       \u001b[36m23.0532\u001b[0m       26.5673  0.0122\n",
      "     19       \u001b[36m23.0488\u001b[0m       26.5972  0.0120\n",
      "     20       \u001b[36m23.0420\u001b[0m       26.5642  0.0123\n",
      "     21       \u001b[36m23.0364\u001b[0m       26.5867  0.0119\n",
      "     22       \u001b[36m23.0322\u001b[0m       26.5598  0.0125\n",
      "     23       \u001b[36m23.0292\u001b[0m       26.5780  0.0126\n",
      "     24       \u001b[36m23.0228\u001b[0m       26.5788  0.0124\n",
      "     25       23.0276       26.5568  0.0123\n",
      "     26       23.0353       26.6010  0.0120\n",
      "     27       23.0308       26.6039  0.0124\n",
      "     28       23.0804       26.5889  0.0124\n",
      "     29       23.1230       26.6842  0.0124\n",
      "     30       23.0679       26.5466  0.0121\n",
      "     31       23.0728       26.6647  0.0120\n",
      "     32       \u001b[36m23.0187\u001b[0m       26.5460  0.0125\n",
      "     33       23.0296       26.6547  0.0122\n",
      "     34       \u001b[36m23.0063\u001b[0m       26.5488  0.0123\n",
      "     35       \u001b[36m23.0063\u001b[0m       26.6341  0.0123\n",
      "     36       \u001b[36m22.9927\u001b[0m       26.5669  0.0121\n",
      "     37       22.9946       26.6326  0.0117\n",
      "     38       \u001b[36m22.9849\u001b[0m       26.5832  0.0119\n",
      "     39       22.9856       26.6277  0.0127\n",
      "     40       \u001b[36m22.9792\u001b[0m       26.5789  0.0124\n",
      "     41       \u001b[36m22.9786\u001b[0m       26.6175  0.0120\n",
      "     42       \u001b[36m22.9737\u001b[0m       26.5922  0.0121\n",
      "     43       \u001b[36m22.9722\u001b[0m       26.6150  0.0120\n",
      "     44       \u001b[36m22.9687\u001b[0m       26.6039  0.0118\n",
      "     45       \u001b[36m22.9663\u001b[0m       26.6136  0.0126\n",
      "     46       \u001b[36m22.9641\u001b[0m       26.6137  0.0122\n",
      "     47       \u001b[36m22.9611\u001b[0m       26.6148  0.0121\n",
      "     48       \u001b[36m22.9593\u001b[0m       26.6270  0.0117\n",
      "     49       \u001b[36m22.9568\u001b[0m       26.6213  0.0117\n",
      "     50       \u001b[36m22.9550\u001b[0m       26.6407  0.0121\n",
      "     51       \u001b[36m22.9511\u001b[0m       26.6321  0.0120\n",
      "     52       \u001b[36m22.9504\u001b[0m       26.6453  0.0120\n",
      "     53       \u001b[36m22.9476\u001b[0m       26.6622  0.0117\n",
      "     54       \u001b[36m22.9441\u001b[0m       26.6371  0.0117\n",
      "     55       22.9444       26.6646  0.0122\n",
      "     56       \u001b[36m22.9421\u001b[0m       26.6891  0.0118\n",
      "     57       \u001b[36m22.9374\u001b[0m       26.6540  0.0119\n",
      "     58       22.9376       26.6695  0.0119\n",
      "     59       22.9385       26.6963  0.0121\n",
      "     60       \u001b[36m22.9354\u001b[0m       26.7056  0.0119\n",
      "     61       \u001b[36m22.9295\u001b[0m       26.6897  0.0126\n",
      "     62       22.9384       26.6218  0.0121\n",
      "     63       22.9571       26.7835  0.0122\n",
      "     64       22.9507       26.8267  0.0122\n",
      "     65       22.9580       26.5173  0.0123\n",
      "     66       23.0118       26.6855  0.0120\n",
      "     67       23.0462       26.9701  0.0130\n",
      "     68       23.0056       26.5348  0.0121\n",
      "     69       23.0369       26.7222  0.0123\n",
      "     70       23.0107       26.7828  0.0122\n",
      "     71       22.9747       26.5293  0.0116\n",
      "     72       22.9791       26.9029  0.0123\n",
      "     73       22.9322       \u001b[32m26.5031\u001b[0m  0.0124\n",
      "     74       22.9408       26.8005  0.0123\n",
      "     75       \u001b[36m22.9285\u001b[0m       26.6637  0.0121\n",
      "     76       \u001b[36m22.9212\u001b[0m       26.6759  0.0120\n",
      "     77       22.9225       26.7096  0.0121\n",
      "     78       \u001b[36m22.9132\u001b[0m       26.7153  0.0132\n",
      "     79       \u001b[36m22.9116\u001b[0m       26.6961  0.0143\n",
      "     80       \u001b[36m22.9098\u001b[0m       26.7297  0.0132\n",
      "     81       \u001b[36m22.9068\u001b[0m       26.7093  0.0128\n",
      "     82       \u001b[36m22.9034\u001b[0m       26.7097  0.0128\n",
      "     83       22.9045       26.7396  0.0134\n",
      "     84       \u001b[36m22.9007\u001b[0m       26.7283  0.0140\n",
      "     85       \u001b[36m22.8987\u001b[0m       26.7316  0.0133\n",
      "     86       22.8989       26.7363  0.0125\n",
      "     87       \u001b[36m22.8973\u001b[0m       26.7559  0.0119\n",
      "     88       \u001b[36m22.8950\u001b[0m       26.7521  0.0117\n",
      "     89       \u001b[36m22.8931\u001b[0m       26.7407  0.0125\n",
      "     90       22.8934       26.7592  0.0131\n",
      "     91       \u001b[36m22.8922\u001b[0m       26.7809  0.0119\n",
      "     92       \u001b[36m22.8896\u001b[0m       26.7589  0.0120\n",
      "     93       \u001b[36m22.8887\u001b[0m       26.7700  0.0119\n",
      "     94       22.8888       26.7875  0.0118\n",
      "     95       \u001b[36m22.8879\u001b[0m       26.7926  0.0117\n",
      "     96       \u001b[36m22.8856\u001b[0m       26.7882  0.0121\n",
      "     97       \u001b[36m22.8841\u001b[0m       26.7885  0.0117\n",
      "     98       \u001b[36m22.8840\u001b[0m       26.7798  0.0124\n",
      "     99       22.8847       26.8139  0.0116\n",
      "    100       \u001b[36m22.8829\u001b[0m       26.8370  0.0115\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m36.3142\u001b[0m       \u001b[32m36.3817\u001b[0m  0.0123\n",
      "      2       \u001b[36m31.8645\u001b[0m       \u001b[32m26.6606\u001b[0m  0.0117\n",
      "      3       \u001b[36m30.4991\u001b[0m       26.7317  0.0118\n",
      "      4       \u001b[36m29.5232\u001b[0m       29.3615  0.0118\n",
      "      5       \u001b[36m29.1193\u001b[0m       26.7834  0.0117\n",
      "      6       \u001b[36m28.8762\u001b[0m       27.6823  0.0114\n",
      "      7       28.8789       27.6160  0.0114\n",
      "      8       \u001b[36m28.6175\u001b[0m       26.9007  0.0117\n",
      "      9       \u001b[36m28.5572\u001b[0m       27.9045  0.0125\n",
      "     10       28.5927       27.2625  0.0117\n",
      "     11       \u001b[36m28.4811\u001b[0m       27.3323  0.0117\n",
      "     12       28.5035       27.6113  0.0113\n",
      "     13       \u001b[36m28.4701\u001b[0m       27.2739  0.0120\n",
      "     14       \u001b[36m28.4507\u001b[0m       27.5220  0.0118\n",
      "     15       28.4571       27.3669  0.0118\n",
      "     16       \u001b[36m28.4379\u001b[0m       27.4472  0.0117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       28.4592       27.3995  0.0116\n",
      "     18       28.4626       27.3934  0.0120\n",
      "     19       28.4828       27.4311  0.0117\n",
      "     20       28.5341       27.3609  0.0115\n",
      "     21       28.4993       27.5053  0.0114\n",
      "     22       28.4932       27.1803  0.0115\n",
      "     23       \u001b[36m28.4214\u001b[0m       27.5281  0.0120\n",
      "     24       28.4247       27.2522  0.0121\n",
      "     25       \u001b[36m28.4070\u001b[0m       27.4480  0.0117\n",
      "     26       28.4168       27.2464  0.0120\n",
      "     27       \u001b[36m28.3996\u001b[0m       27.4603  0.0116\n",
      "     28       28.4035       27.2857  0.0115\n",
      "     29       \u001b[36m28.3953\u001b[0m       27.3759  0.0116\n",
      "     30       28.3971       27.3001  0.0117\n",
      "     31       \u001b[36m28.3919\u001b[0m       27.3612  0.0118\n",
      "     32       \u001b[36m28.3911\u001b[0m       27.2989  0.0117\n",
      "     33       \u001b[36m28.3901\u001b[0m       27.3496  0.0115\n",
      "     34       \u001b[36m28.3871\u001b[0m       27.3234  0.0117\n",
      "     35       \u001b[36m28.3857\u001b[0m       27.3417  0.0128\n",
      "     36       \u001b[36m28.3857\u001b[0m       27.3283  0.0125\n",
      "     37       \u001b[36m28.3820\u001b[0m       27.3530  0.0116\n",
      "     38       28.3821       27.3416  0.0116\n",
      "     39       28.3830       27.3362  0.0113\n",
      "     40       \u001b[36m28.3775\u001b[0m       27.3636  0.0114\n",
      "     41       28.3807       27.3499  0.0118\n",
      "     42       28.3821       27.3168  0.0115\n",
      "     43       \u001b[36m28.3741\u001b[0m       27.4133  0.0115\n",
      "     44       28.3841       27.3027  0.0115\n",
      "     45       28.3876       27.3676  0.0113\n",
      "     46       28.3772       27.3436  0.0118\n",
      "     47       28.3949       27.3141  0.0117\n",
      "     48       28.3985       27.3009  0.0115\n",
      "     49       28.3872       27.3916  0.0114\n",
      "     50       28.4132       27.2260  0.0115\n",
      "     51       28.3928       27.4113  0.0116\n",
      "     52       28.3897       27.2792  0.0119\n",
      "     53       28.3996       27.2224  0.0118\n",
      "     54       \u001b[36m28.3655\u001b[0m       27.4579  0.0113\n",
      "     55       28.3761       27.1778  0.0115\n",
      "     56       28.3734       27.3754  0.0122\n",
      "     57       \u001b[36m28.3599\u001b[0m       27.3082  0.0157\n",
      "     58       28.3730       27.2306  0.0176\n",
      "     59       28.3601       27.3892  0.0125\n",
      "     60       28.3638       27.2391  0.0129\n",
      "     61       28.3653       27.3080  0.0121\n",
      "     62       \u001b[36m28.3561\u001b[0m       27.3489  0.0177\n",
      "     63       28.3636       27.2522  0.0126\n",
      "     64       28.3591       27.3183  0.0145\n",
      "     65       \u001b[36m28.3553\u001b[0m       27.3148  0.0119\n",
      "     66       28.3596       27.2508  0.0117\n",
      "     67       \u001b[36m28.3552\u001b[0m       27.3414  0.0117\n",
      "     68       \u001b[36m28.3537\u001b[0m       27.2719  0.0128\n",
      "     69       28.3556       27.2868  0.0145\n",
      "     70       \u001b[36m28.3532\u001b[0m       27.3030  0.0118\n",
      "     71       \u001b[36m28.3529\u001b[0m       27.2697  0.0117\n",
      "     72       \u001b[36m28.3516\u001b[0m       27.3108  0.0118\n",
      "     73       28.3525       27.2862  0.0118\n",
      "     74       28.3574       27.2232  0.0119\n",
      "     75       \u001b[36m28.3504\u001b[0m       27.3324  0.0114\n",
      "     76       \u001b[36m28.3494\u001b[0m       27.3753  0.0117\n",
      "     77       28.3652       27.1510  0.0121\n",
      "     78       28.3661       27.2645  0.0118\n",
      "     79       \u001b[36m28.3490\u001b[0m       27.4432  0.0118\n",
      "     80       28.3666       27.2853  0.0115\n",
      "     81       28.4004       27.0872  0.0115\n",
      "     82       28.3816       27.3936  0.0120\n",
      "     83       28.3913       27.3809  0.0119\n",
      "     84       28.4261       27.1982  0.0117\n",
      "     85       28.4401       27.0975  0.0115\n",
      "     86       28.3931       27.5590  0.0115\n",
      "     87       28.4104       27.1261  0.0119\n",
      "     88       28.4092       27.0562  0.0117\n",
      "     89       28.3969       27.5317  0.0116\n",
      "     90       28.3837       27.1413  0.0115\n",
      "     91       28.3968       27.1285  0.0112\n",
      "     92       28.3697       27.3635  0.0118\n",
      "     93       28.3630       27.1765  0.0116\n",
      "     94       28.3789       27.1415  0.0118\n",
      "     95       28.3555       27.3909  0.0114\n",
      "     96       28.3559       27.1661  0.0113\n",
      "     97       28.3572       27.1850  0.0120\n",
      "     98       \u001b[36m28.3471\u001b[0m       27.2947  0.0118\n",
      "     99       28.3480       27.1836  0.0117\n",
      "    100       \u001b[36m28.3462\u001b[0m       27.1903  0.0117\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m42.1629\u001b[0m       \u001b[32m40.4050\u001b[0m  0.0113\n",
      "      2       \u001b[36m38.0152\u001b[0m       \u001b[32m35.6667\u001b[0m  0.0111\n",
      "      3       \u001b[36m34.9957\u001b[0m       \u001b[32m32.5034\u001b[0m  0.0110\n",
      "      4       \u001b[36m33.5053\u001b[0m       \u001b[32m31.0605\u001b[0m  0.0107\n",
      "      5       \u001b[36m33.0202\u001b[0m       \u001b[32m30.5962\u001b[0m  0.0106\n",
      "      6       \u001b[36m32.8461\u001b[0m       \u001b[32m30.4269\u001b[0m  0.0105\n",
      "      7       \u001b[36m32.7481\u001b[0m       \u001b[32m30.3444\u001b[0m  0.0106\n",
      "      8       \u001b[36m32.6792\u001b[0m       \u001b[32m30.2925\u001b[0m  0.0117\n",
      "      9       \u001b[36m32.6273\u001b[0m       \u001b[32m30.2546\u001b[0m  0.0110\n",
      "     10       \u001b[36m32.5865\u001b[0m       \u001b[32m30.2241\u001b[0m  0.0109\n",
      "     11       \u001b[36m32.5528\u001b[0m       \u001b[32m30.1985\u001b[0m  0.0107\n",
      "     12       \u001b[36m32.5246\u001b[0m       \u001b[32m30.1768\u001b[0m  0.0110\n",
      "     13       \u001b[36m32.5004\u001b[0m       \u001b[32m30.1581\u001b[0m  0.0117\n",
      "     14       \u001b[36m32.4796\u001b[0m       \u001b[32m30.1412\u001b[0m  0.0111\n",
      "     15       \u001b[36m32.4611\u001b[0m       \u001b[32m30.1268\u001b[0m  0.0107\n",
      "     16       \u001b[36m32.4461\u001b[0m       \u001b[32m30.1134\u001b[0m  0.0111\n",
      "     17       \u001b[36m32.4318\u001b[0m       \u001b[32m30.1022\u001b[0m  0.0109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     18       \u001b[36m32.4197\u001b[0m       \u001b[32m30.0913\u001b[0m  0.0119\n",
      "     19       \u001b[36m32.4089\u001b[0m       \u001b[32m30.0814\u001b[0m  0.0108\n",
      "     20       \u001b[36m32.3986\u001b[0m       \u001b[32m30.0731\u001b[0m  0.0108\n",
      "     21       \u001b[36m32.3895\u001b[0m       \u001b[32m30.0650\u001b[0m  0.0107\n",
      "     22       \u001b[36m32.3813\u001b[0m       \u001b[32m30.0580\u001b[0m  0.0106\n",
      "     23       \u001b[36m32.3739\u001b[0m       \u001b[32m30.0500\u001b[0m  0.0111\n",
      "     24       \u001b[36m32.3669\u001b[0m       \u001b[32m30.0433\u001b[0m  0.0111\n",
      "     25       \u001b[36m32.3604\u001b[0m       \u001b[32m30.0373\u001b[0m  0.0109\n",
      "     26       \u001b[36m32.3545\u001b[0m       \u001b[32m30.0310\u001b[0m  0.0107\n",
      "     27       \u001b[36m32.3489\u001b[0m       \u001b[32m30.0255\u001b[0m  0.0109\n",
      "     28       \u001b[36m32.3437\u001b[0m       \u001b[32m30.0204\u001b[0m  0.0109\n",
      "     29       \u001b[36m32.3392\u001b[0m       \u001b[32m30.0147\u001b[0m  0.0114\n",
      "     30       \u001b[36m32.3346\u001b[0m       \u001b[32m30.0102\u001b[0m  0.0111\n",
      "     31       \u001b[36m32.3305\u001b[0m       \u001b[32m30.0056\u001b[0m  0.0106\n",
      "     32       \u001b[36m32.3266\u001b[0m       \u001b[32m30.0014\u001b[0m  0.0105\n",
      "     33       \u001b[36m32.3229\u001b[0m       \u001b[32m29.9973\u001b[0m  0.0107\n",
      "     34       \u001b[36m32.3195\u001b[0m       \u001b[32m29.9931\u001b[0m  0.0114\n",
      "     35       \u001b[36m32.3163\u001b[0m       \u001b[32m29.9894\u001b[0m  0.0113\n",
      "     36       \u001b[36m32.3132\u001b[0m       \u001b[32m29.9860\u001b[0m  0.0105\n",
      "     37       \u001b[36m32.3103\u001b[0m       \u001b[32m29.9824\u001b[0m  0.0115\n",
      "     38       \u001b[36m32.3076\u001b[0m       \u001b[32m29.9789\u001b[0m  0.0210\n",
      "     39       \u001b[36m32.3049\u001b[0m       \u001b[32m29.9759\u001b[0m  0.0176\n",
      "     40       \u001b[36m32.3023\u001b[0m       \u001b[32m29.9725\u001b[0m  0.0169\n",
      "     41       \u001b[36m32.2999\u001b[0m       \u001b[32m29.9696\u001b[0m  0.0208\n",
      "     42       \u001b[36m32.2976\u001b[0m       \u001b[32m29.9667\u001b[0m  0.0191\n",
      "     43       \u001b[36m32.2954\u001b[0m       \u001b[32m29.9638\u001b[0m  0.0122\n",
      "     44       \u001b[36m32.2932\u001b[0m       \u001b[32m29.9614\u001b[0m  0.0113\n",
      "     45       \u001b[36m32.2911\u001b[0m       \u001b[32m29.9591\u001b[0m  0.0111\n",
      "     46       \u001b[36m32.2891\u001b[0m       \u001b[32m29.9565\u001b[0m  0.0122\n",
      "     47       \u001b[36m32.2871\u001b[0m       \u001b[32m29.9536\u001b[0m  0.0124\n",
      "     48       \u001b[36m32.2853\u001b[0m       \u001b[32m29.9512\u001b[0m  0.0113\n",
      "     49       \u001b[36m32.2834\u001b[0m       \u001b[32m29.9484\u001b[0m  0.0111\n",
      "     50       \u001b[36m32.2817\u001b[0m       \u001b[32m29.9462\u001b[0m  0.0107\n",
      "     51       \u001b[36m32.2800\u001b[0m       \u001b[32m29.9440\u001b[0m  0.0106\n",
      "     52       \u001b[36m32.2783\u001b[0m       \u001b[32m29.9419\u001b[0m  0.0110\n",
      "     53       \u001b[36m32.2767\u001b[0m       \u001b[32m29.9397\u001b[0m  0.0114\n",
      "     54       \u001b[36m32.2751\u001b[0m       \u001b[32m29.9377\u001b[0m  0.0110\n",
      "     55       \u001b[36m32.2736\u001b[0m       \u001b[32m29.9354\u001b[0m  0.0108\n",
      "     56       \u001b[36m32.2721\u001b[0m       \u001b[32m29.9335\u001b[0m  0.0106\n",
      "     57       \u001b[36m32.2707\u001b[0m       \u001b[32m29.9312\u001b[0m  0.0108\n",
      "     58       \u001b[36m32.2693\u001b[0m       \u001b[32m29.9293\u001b[0m  0.0113\n",
      "     59       \u001b[36m32.2679\u001b[0m       \u001b[32m29.9277\u001b[0m  0.0110\n",
      "     60       \u001b[36m32.2666\u001b[0m       \u001b[32m29.9259\u001b[0m  0.0106\n",
      "     61       \u001b[36m32.2653\u001b[0m       \u001b[32m29.9239\u001b[0m  0.0107\n",
      "     62       \u001b[36m32.2641\u001b[0m       \u001b[32m29.9223\u001b[0m  0.0120\n",
      "     63       \u001b[36m32.2628\u001b[0m       \u001b[32m29.9206\u001b[0m  0.0112\n",
      "     64       \u001b[36m32.2617\u001b[0m       \u001b[32m29.9190\u001b[0m  0.0108\n",
      "     65       \u001b[36m32.2604\u001b[0m       \u001b[32m29.9172\u001b[0m  0.0107\n",
      "     66       \u001b[36m32.2593\u001b[0m       \u001b[32m29.9157\u001b[0m  0.0106\n",
      "     67       \u001b[36m32.2582\u001b[0m       \u001b[32m29.9139\u001b[0m  0.0106\n",
      "     68       \u001b[36m32.2571\u001b[0m       \u001b[32m29.9125\u001b[0m  0.0110\n",
      "     69       \u001b[36m32.2559\u001b[0m       \u001b[32m29.9109\u001b[0m  0.0110\n",
      "     70       \u001b[36m32.2549\u001b[0m       \u001b[32m29.9096\u001b[0m  0.0106\n",
      "     71       \u001b[36m32.2539\u001b[0m       \u001b[32m29.9083\u001b[0m  0.0106\n",
      "     72       \u001b[36m32.2528\u001b[0m       \u001b[32m29.9069\u001b[0m  0.0112\n",
      "     73       \u001b[36m32.2518\u001b[0m       \u001b[32m29.9058\u001b[0m  0.0114\n",
      "     74       \u001b[36m32.2509\u001b[0m       \u001b[32m29.9044\u001b[0m  0.0113\n",
      "     75       \u001b[36m32.2499\u001b[0m       \u001b[32m29.9032\u001b[0m  0.0106\n",
      "     76       \u001b[36m32.2489\u001b[0m       \u001b[32m29.9021\u001b[0m  0.0103\n",
      "     77       \u001b[36m32.2480\u001b[0m       \u001b[32m29.9006\u001b[0m  0.0108\n",
      "     78       \u001b[36m32.2471\u001b[0m       \u001b[32m29.8996\u001b[0m  0.0111\n",
      "     79       \u001b[36m32.2462\u001b[0m       \u001b[32m29.8984\u001b[0m  0.0107\n",
      "     80       \u001b[36m32.2453\u001b[0m       \u001b[32m29.8974\u001b[0m  0.0105\n",
      "     81       \u001b[36m32.2444\u001b[0m       \u001b[32m29.8962\u001b[0m  0.0104\n",
      "     82       \u001b[36m32.2435\u001b[0m       \u001b[32m29.8953\u001b[0m  0.0109\n",
      "     83       \u001b[36m32.2426\u001b[0m       \u001b[32m29.8941\u001b[0m  0.0116\n",
      "     84       \u001b[36m32.2418\u001b[0m       \u001b[32m29.8931\u001b[0m  0.0112\n",
      "     85       \u001b[36m32.2409\u001b[0m       \u001b[32m29.8921\u001b[0m  0.0110\n",
      "     86       \u001b[36m32.2401\u001b[0m       \u001b[32m29.8911\u001b[0m  0.0111\n",
      "     87       \u001b[36m32.2393\u001b[0m       \u001b[32m29.8899\u001b[0m  0.0114\n",
      "     88       \u001b[36m32.2385\u001b[0m       \u001b[32m29.8891\u001b[0m  0.0118\n",
      "     89       \u001b[36m32.2377\u001b[0m       \u001b[32m29.8881\u001b[0m  0.0110\n",
      "     90       \u001b[36m32.2369\u001b[0m       \u001b[32m29.8872\u001b[0m  0.0107\n",
      "     91       \u001b[36m32.2361\u001b[0m       \u001b[32m29.8862\u001b[0m  0.0107\n",
      "     92       \u001b[36m32.2354\u001b[0m       \u001b[32m29.8853\u001b[0m  0.0111\n",
      "     93       \u001b[36m32.2346\u001b[0m       \u001b[32m29.8844\u001b[0m  0.0113\n",
      "     94       \u001b[36m32.2339\u001b[0m       \u001b[32m29.8837\u001b[0m  0.0115\n",
      "     95       \u001b[36m32.2331\u001b[0m       \u001b[32m29.8827\u001b[0m  0.0108\n",
      "     96       \u001b[36m32.2324\u001b[0m       \u001b[32m29.8817\u001b[0m  0.0108\n",
      "     97       \u001b[36m32.2317\u001b[0m       \u001b[32m29.8808\u001b[0m  0.0127\n",
      "     98       \u001b[36m32.2310\u001b[0m       \u001b[32m29.8798\u001b[0m  0.0112\n",
      "     99       \u001b[36m32.2303\u001b[0m       \u001b[32m29.8789\u001b[0m  0.0111\n",
      "    100       \u001b[36m32.2296\u001b[0m       \u001b[32m29.8781\u001b[0m  0.0106\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m33.5557\u001b[0m       \u001b[32m30.4105\u001b[0m  0.0107\n",
      "      2       \u001b[36m29.5367\u001b[0m       \u001b[32m27.9215\u001b[0m  0.0115\n",
      "      3       \u001b[36m26.3804\u001b[0m       \u001b[32m26.6358\u001b[0m  0.0111\n",
      "      4       \u001b[36m24.6085\u001b[0m       \u001b[32m26.3643\u001b[0m  0.0110\n",
      "      5       \u001b[36m23.8955\u001b[0m       26.4061  0.0109\n",
      "      6       \u001b[36m23.6443\u001b[0m       26.4427  0.0106\n",
      "      7       \u001b[36m23.5302\u001b[0m       26.4504  0.0111\n",
      "      8       \u001b[36m23.4608\u001b[0m       26.4489  0.0115\n",
      "      9       \u001b[36m23.4118\u001b[0m       26.4447  0.0113\n",
      "     10       \u001b[36m23.3744\u001b[0m       26.4403  0.0107\n",
      "     11       \u001b[36m23.3446\u001b[0m       26.4364  0.0106\n",
      "     12       \u001b[36m23.3204\u001b[0m       26.4331  0.0106\n",
      "     13       \u001b[36m23.3002\u001b[0m       26.4305  0.0116\n",
      "     14       \u001b[36m23.2830\u001b[0m       26.4292  0.0111\n",
      "     15       \u001b[36m23.2681\u001b[0m       26.4280  0.0105\n",
      "     16       \u001b[36m23.2551\u001b[0m       26.4270  0.0106\n",
      "     17       \u001b[36m23.2437\u001b[0m       26.4269  0.0109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     18       \u001b[36m23.2335\u001b[0m       26.4265  0.0117\n",
      "     19       \u001b[36m23.2245\u001b[0m       26.4263  0.0167\n",
      "     20       \u001b[36m23.2164\u001b[0m       26.4260  0.0176\n",
      "     21       \u001b[36m23.2090\u001b[0m       26.4260  0.0118\n",
      "     22       \u001b[36m23.2023\u001b[0m       26.4259  0.0120\n",
      "     23       \u001b[36m23.1962\u001b[0m       26.4258  0.0172\n",
      "     24       \u001b[36m23.1905\u001b[0m       26.4259  0.0148\n",
      "     25       \u001b[36m23.1853\u001b[0m       26.4259  0.0184\n",
      "     26       \u001b[36m23.1804\u001b[0m       26.4256  0.0120\n",
      "     27       \u001b[36m23.1759\u001b[0m       26.4254  0.0112\n",
      "     28       \u001b[36m23.1717\u001b[0m       26.4252  0.0118\n",
      "     29       \u001b[36m23.1677\u001b[0m       26.4252  0.0117\n",
      "     30       \u001b[36m23.1641\u001b[0m       26.4249  0.0109\n",
      "     31       \u001b[36m23.1606\u001b[0m       26.4247  0.0111\n",
      "     32       \u001b[36m23.1573\u001b[0m       26.4243  0.0113\n",
      "     33       \u001b[36m23.1543\u001b[0m       26.4238  0.0115\n",
      "     34       \u001b[36m23.1513\u001b[0m       26.4233  0.0112\n",
      "     35       \u001b[36m23.1485\u001b[0m       26.4227  0.0112\n",
      "     36       \u001b[36m23.1459\u001b[0m       26.4223  0.0111\n",
      "     37       \u001b[36m23.1434\u001b[0m       26.4216  0.0115\n",
      "     38       \u001b[36m23.1410\u001b[0m       26.4212  0.0110\n",
      "     39       \u001b[36m23.1387\u001b[0m       26.4205  0.0111\n",
      "     40       \u001b[36m23.1365\u001b[0m       26.4197  0.0110\n",
      "     41       \u001b[36m23.1344\u001b[0m       26.4190  0.0108\n",
      "     42       \u001b[36m23.1324\u001b[0m       26.4184  0.0114\n",
      "     43       \u001b[36m23.1305\u001b[0m       26.4176  0.0119\n",
      "     44       \u001b[36m23.1286\u001b[0m       26.4168  0.0109\n",
      "     45       \u001b[36m23.1269\u001b[0m       26.4160  0.0108\n",
      "     46       \u001b[36m23.1251\u001b[0m       26.4154  0.0123\n",
      "     47       \u001b[36m23.1235\u001b[0m       26.4146  0.0116\n",
      "     48       \u001b[36m23.1219\u001b[0m       26.4139  0.0115\n",
      "     49       \u001b[36m23.1203\u001b[0m       26.4130  0.0112\n",
      "     50       \u001b[36m23.1188\u001b[0m       26.4122  0.0114\n",
      "     51       \u001b[36m23.1174\u001b[0m       26.4115  0.0109\n",
      "     52       \u001b[36m23.1160\u001b[0m       26.4107  0.0112\n",
      "     53       \u001b[36m23.1146\u001b[0m       26.4099  0.0112\n",
      "     54       \u001b[36m23.1133\u001b[0m       26.4092  0.0112\n",
      "     55       \u001b[36m23.1120\u001b[0m       26.4085  0.0116\n",
      "     56       \u001b[36m23.1107\u001b[0m       26.4078  0.0109\n",
      "     57       \u001b[36m23.1096\u001b[0m       26.4070  0.0110\n",
      "     58       \u001b[36m23.1084\u001b[0m       26.4064  0.0118\n",
      "     59       \u001b[36m23.1072\u001b[0m       26.4059  0.0109\n",
      "     60       \u001b[36m23.1061\u001b[0m       26.4052  0.0128\n",
      "     61       \u001b[36m23.1051\u001b[0m       26.4047  0.0107\n",
      "     62       \u001b[36m23.1040\u001b[0m       26.4041  0.0119\n",
      "     63       \u001b[36m23.1030\u001b[0m       26.4036  0.0116\n",
      "     64       \u001b[36m23.1020\u001b[0m       26.4031  0.0111\n",
      "     65       \u001b[36m23.1011\u001b[0m       26.4025  0.0112\n",
      "     66       \u001b[36m23.1001\u001b[0m       26.4022  0.0112\n",
      "     67       \u001b[36m23.0992\u001b[0m       26.4019  0.0119\n",
      "     68       \u001b[36m23.0983\u001b[0m       26.4014  0.0135\n",
      "     69       \u001b[36m23.0975\u001b[0m       26.4010  0.0123\n",
      "     70       \u001b[36m23.0966\u001b[0m       26.4006  0.0122\n",
      "     71       \u001b[36m23.0958\u001b[0m       26.4001  0.0110\n",
      "     72       \u001b[36m23.0950\u001b[0m       26.3998  0.0113\n",
      "     73       \u001b[36m23.0942\u001b[0m       26.3992  0.0115\n",
      "     74       \u001b[36m23.0934\u001b[0m       26.3990  0.0115\n",
      "     75       \u001b[36m23.0926\u001b[0m       26.3985  0.0109\n",
      "     76       \u001b[36m23.0919\u001b[0m       26.3982  0.0136\n",
      "     77       \u001b[36m23.0911\u001b[0m       26.3977  0.0118\n",
      "     78       \u001b[36m23.0904\u001b[0m       26.3974  0.0113\n",
      "     79       \u001b[36m23.0897\u001b[0m       26.3970  0.0112\n",
      "     80       \u001b[36m23.0890\u001b[0m       26.3966  0.0117\n",
      "     81       \u001b[36m23.0883\u001b[0m       26.3963  0.0109\n",
      "     82       \u001b[36m23.0877\u001b[0m       26.3959  0.0112\n",
      "     83       \u001b[36m23.0870\u001b[0m       26.3956  0.0114\n",
      "     84       \u001b[36m23.0864\u001b[0m       26.3953  0.0108\n",
      "     85       \u001b[36m23.0857\u001b[0m       26.3950  0.0110\n",
      "     86       \u001b[36m23.0851\u001b[0m       26.3947  0.0106\n",
      "     87       \u001b[36m23.0845\u001b[0m       26.3944  0.0113\n",
      "     88       \u001b[36m23.0839\u001b[0m       26.3941  0.0118\n",
      "     89       \u001b[36m23.0833\u001b[0m       26.3938  0.0113\n",
      "     90       \u001b[36m23.0827\u001b[0m       26.3935  0.0106\n",
      "     91       \u001b[36m23.0821\u001b[0m       26.3933  0.0108\n",
      "     92       \u001b[36m23.0816\u001b[0m       26.3930  0.0108\n",
      "     93       \u001b[36m23.0810\u001b[0m       26.3928  0.0114\n",
      "     94       \u001b[36m23.0804\u001b[0m       26.3926  0.0107\n",
      "     95       \u001b[36m23.0799\u001b[0m       26.3923  0.0108\n",
      "     96       \u001b[36m23.0794\u001b[0m       26.3921  0.0107\n",
      "     97       \u001b[36m23.0789\u001b[0m       26.3918  0.0110\n",
      "     98       \u001b[36m23.0783\u001b[0m       26.3916  0.0116\n",
      "     99       \u001b[36m23.0778\u001b[0m       26.3913  0.0120\n",
      "    100       \u001b[36m23.0773\u001b[0m       26.3911  0.0141\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m39.2243\u001b[0m       \u001b[32m29.3464\u001b[0m  0.0119\n",
      "      2       \u001b[36m34.7082\u001b[0m       \u001b[32m26.8847\u001b[0m  0.0116\n",
      "      3       \u001b[36m30.9842\u001b[0m       \u001b[32m26.5325\u001b[0m  0.0116\n",
      "      4       \u001b[36m29.3623\u001b[0m       27.2490  0.0117\n",
      "      5       \u001b[36m29.0365\u001b[0m       27.5106  0.0117\n",
      "      6       \u001b[36m28.9254\u001b[0m       27.5430  0.0109\n",
      "      7       \u001b[36m28.8453\u001b[0m       27.5274  0.0120\n",
      "      8       \u001b[36m28.7826\u001b[0m       27.5042  0.0116\n",
      "      9       \u001b[36m28.7320\u001b[0m       27.4827  0.0113\n",
      "     10       \u001b[36m28.6909\u001b[0m       27.4658  0.0110\n",
      "     11       \u001b[36m28.6576\u001b[0m       27.4528  0.0107\n",
      "     12       \u001b[36m28.6303\u001b[0m       27.4420  0.0112\n",
      "     13       \u001b[36m28.6076\u001b[0m       27.4341  0.0116\n",
      "     14       \u001b[36m28.5888\u001b[0m       27.4273  0.0112\n",
      "     15       \u001b[36m28.5726\u001b[0m       27.4212  0.0110\n",
      "     16       \u001b[36m28.5587\u001b[0m       27.4158  0.0109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17       \u001b[36m28.5467\u001b[0m       27.4108  0.0114\n",
      "     18       \u001b[36m28.5362\u001b[0m       27.4067  0.0119\n",
      "     19       \u001b[36m28.5271\u001b[0m       27.4029  0.0112\n",
      "     20       \u001b[36m28.5190\u001b[0m       27.3993  0.0107\n",
      "     21       \u001b[36m28.5118\u001b[0m       27.3952  0.0106\n",
      "     22       \u001b[36m28.5052\u001b[0m       27.3925  0.0111\n",
      "     23       \u001b[36m28.4994\u001b[0m       27.3892  0.0115\n",
      "     24       \u001b[36m28.4942\u001b[0m       27.3861  0.0109\n",
      "     25       \u001b[36m28.4895\u001b[0m       27.3834  0.0107\n",
      "     26       \u001b[36m28.4851\u001b[0m       27.3803  0.0107\n",
      "     27       \u001b[36m28.4811\u001b[0m       27.3775  0.0110\n",
      "     28       \u001b[36m28.4774\u001b[0m       27.3751  0.0114\n",
      "     29       \u001b[36m28.4741\u001b[0m       27.3722  0.0112\n",
      "     30       \u001b[36m28.4709\u001b[0m       27.3694  0.0104\n",
      "     31       \u001b[36m28.4679\u001b[0m       27.3670  0.0107\n",
      "     32       \u001b[36m28.4651\u001b[0m       27.3652  0.0109\n",
      "     33       \u001b[36m28.4626\u001b[0m       27.3634  0.0117\n",
      "     34       \u001b[36m28.4602\u001b[0m       27.3617  0.0108\n",
      "     35       \u001b[36m28.4580\u001b[0m       27.3600  0.0108\n",
      "     36       \u001b[36m28.4559\u001b[0m       27.3582  0.0107\n",
      "     37       \u001b[36m28.4539\u001b[0m       27.3562  0.0108\n",
      "     38       \u001b[36m28.4520\u001b[0m       27.3547  0.0113\n",
      "     39       \u001b[36m28.4503\u001b[0m       27.3528  0.0110\n",
      "     40       \u001b[36m28.4486\u001b[0m       27.3514  0.0107\n",
      "     41       \u001b[36m28.4470\u001b[0m       27.3498  0.0105\n",
      "     42       \u001b[36m28.4455\u001b[0m       27.3486  0.0111\n",
      "     43       \u001b[36m28.4440\u001b[0m       27.3475  0.0114\n",
      "     44       \u001b[36m28.4427\u001b[0m       27.3459  0.0112\n",
      "     45       \u001b[36m28.4413\u001b[0m       27.3446  0.0106\n",
      "     46       \u001b[36m28.4400\u001b[0m       27.3434  0.0106\n",
      "     47       \u001b[36m28.4388\u001b[0m       27.3425  0.0111\n",
      "     48       \u001b[36m28.4376\u001b[0m       27.3417  0.0117\n",
      "     49       \u001b[36m28.4366\u001b[0m       27.3408  0.0109\n",
      "     50       \u001b[36m28.4354\u001b[0m       27.3401  0.0107\n",
      "     51       \u001b[36m28.4344\u001b[0m       27.3393  0.0108\n",
      "     52       \u001b[36m28.4334\u001b[0m       27.3382  0.0112\n",
      "     53       \u001b[36m28.4324\u001b[0m       27.3372  0.0113\n",
      "     54       \u001b[36m28.4315\u001b[0m       27.3366  0.0108\n",
      "     55       \u001b[36m28.4306\u001b[0m       27.3358  0.0107\n",
      "     56       \u001b[36m28.4297\u001b[0m       27.3354  0.0105\n",
      "     57       \u001b[36m28.4288\u001b[0m       27.3346  0.0109\n",
      "     58       \u001b[36m28.4280\u001b[0m       27.3339  0.0116\n",
      "     59       \u001b[36m28.4272\u001b[0m       27.3333  0.0109\n",
      "     60       \u001b[36m28.4265\u001b[0m       27.3323  0.0105\n",
      "     61       \u001b[36m28.4257\u001b[0m       27.3316  0.0120\n",
      "     62       \u001b[36m28.4250\u001b[0m       27.3308  0.0118\n",
      "     63       \u001b[36m28.4243\u001b[0m       27.3299  0.0113\n",
      "     64       \u001b[36m28.4236\u001b[0m       27.3294  0.0109\n",
      "     65       \u001b[36m28.4230\u001b[0m       27.3285  0.0106\n",
      "     66       \u001b[36m28.4223\u001b[0m       27.3279  0.0107\n",
      "     67       \u001b[36m28.4217\u001b[0m       27.3272  0.0109\n",
      "     68       \u001b[36m28.4211\u001b[0m       27.3266  0.0112\n",
      "     69       \u001b[36m28.4205\u001b[0m       27.3258  0.0112\n",
      "     70       \u001b[36m28.4199\u001b[0m       27.3251  0.0107\n",
      "     71       \u001b[36m28.4193\u001b[0m       27.3245  0.0108\n",
      "     72       \u001b[36m28.4188\u001b[0m       27.3239  0.0110\n",
      "     73       \u001b[36m28.4183\u001b[0m       27.3234  0.0114\n",
      "     74       \u001b[36m28.4177\u001b[0m       27.3225  0.0110\n",
      "     75       \u001b[36m28.4172\u001b[0m       27.3221  0.0108\n",
      "     76       \u001b[36m28.4167\u001b[0m       27.3217  0.0108\n",
      "     77       \u001b[36m28.4163\u001b[0m       27.3211  0.0110\n",
      "     78       \u001b[36m28.4158\u001b[0m       27.3204  0.0113\n",
      "     79       \u001b[36m28.4153\u001b[0m       27.3199  0.0108\n",
      "     80       \u001b[36m28.4149\u001b[0m       27.3193  0.0106\n",
      "     81       \u001b[36m28.4144\u001b[0m       27.3186  0.0108\n",
      "     82       \u001b[36m28.4140\u001b[0m       27.3181  0.0113\n",
      "     83       \u001b[36m28.4136\u001b[0m       27.3174  0.0120\n",
      "     84       \u001b[36m28.4132\u001b[0m       27.3169  0.0145\n",
      "     85       \u001b[36m28.4128\u001b[0m       27.3164  0.0134\n",
      "     86       \u001b[36m28.4124\u001b[0m       27.3157  0.0115\n",
      "     87       \u001b[36m28.4120\u001b[0m       27.3154  0.0135\n",
      "     88       \u001b[36m28.4116\u001b[0m       27.3150  0.0124\n",
      "     89       \u001b[36m28.4112\u001b[0m       27.3141  0.0129\n",
      "     90       \u001b[36m28.4109\u001b[0m       27.3137  0.0114\n",
      "     91       \u001b[36m28.4105\u001b[0m       27.3130  0.0123\n",
      "     92       \u001b[36m28.4101\u001b[0m       27.3127  0.0113\n",
      "     93       \u001b[36m28.4098\u001b[0m       27.3120  0.0112\n",
      "     94       \u001b[36m28.4095\u001b[0m       27.3118  0.0113\n",
      "     95       \u001b[36m28.4091\u001b[0m       27.3112  0.0113\n",
      "     96       \u001b[36m28.4088\u001b[0m       27.3105  0.0114\n",
      "     97       \u001b[36m28.4085\u001b[0m       27.3101  0.0114\n",
      "     98       \u001b[36m28.4082\u001b[0m       27.3096  0.0111\n",
      "     99       \u001b[36m28.4079\u001b[0m       27.3089  0.0114\n",
      "    100       \u001b[36m28.4076\u001b[0m       27.3084  0.0112\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m37.3590\u001b[0m       \u001b[32m26.5421\u001b[0m  0.0166\n",
      "      2       \u001b[36m32.5580\u001b[0m       \u001b[32m23.8937\u001b[0m  0.0159\n",
      "      3       \u001b[36m30.0904\u001b[0m       \u001b[32m23.4966\u001b[0m  0.0155\n",
      "      4       \u001b[36m29.5046\u001b[0m       \u001b[32m23.4948\u001b[0m  0.0159\n",
      "      5       \u001b[36m29.3698\u001b[0m       \u001b[32m23.4536\u001b[0m  0.0157\n",
      "      6       \u001b[36m29.2962\u001b[0m       \u001b[32m23.4121\u001b[0m  0.0161\n",
      "      7       \u001b[36m29.2441\u001b[0m       \u001b[32m23.3796\u001b[0m  0.0163\n",
      "      8       \u001b[36m29.2048\u001b[0m       \u001b[32m23.3545\u001b[0m  0.0154\n",
      "      9       \u001b[36m29.1743\u001b[0m       \u001b[32m23.3365\u001b[0m  0.0155\n",
      "     10       \u001b[36m29.1500\u001b[0m       \u001b[32m23.3220\u001b[0m  0.0158\n",
      "     11       \u001b[36m29.1301\u001b[0m       \u001b[32m23.3112\u001b[0m  0.0157\n",
      "     12       \u001b[36m29.1138\u001b[0m       \u001b[32m23.3022\u001b[0m  0.0159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([63])) that is different to the input size (torch.Size([63, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/med/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([80])) that is different to the input size (torch.Size([80, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     13       \u001b[36m29.1004\u001b[0m       \u001b[32m23.2952\u001b[0m  0.0152\n",
      "     14       \u001b[36m29.0891\u001b[0m       \u001b[32m23.2893\u001b[0m  0.0155\n",
      "     15       \u001b[36m29.0796\u001b[0m       \u001b[32m23.2843\u001b[0m  0.0158\n",
      "     16       \u001b[36m29.0714\u001b[0m       \u001b[32m23.2801\u001b[0m  0.0154\n",
      "     17       \u001b[36m29.0643\u001b[0m       \u001b[32m23.2763\u001b[0m  0.0152\n",
      "     18       \u001b[36m29.0582\u001b[0m       \u001b[32m23.2733\u001b[0m  0.0157\n",
      "     19       \u001b[36m29.0528\u001b[0m       \u001b[32m23.2706\u001b[0m  0.0158\n",
      "     20       \u001b[36m29.0482\u001b[0m       \u001b[32m23.2677\u001b[0m  0.0156\n",
      "     21       \u001b[36m29.0440\u001b[0m       \u001b[32m23.2650\u001b[0m  0.0151\n",
      "     22       \u001b[36m29.0405\u001b[0m       \u001b[32m23.2638\u001b[0m  0.0162\n",
      "     23       \u001b[36m29.0374\u001b[0m       \u001b[32m23.2622\u001b[0m  0.0159\n",
      "     24       \u001b[36m29.0347\u001b[0m       \u001b[32m23.2609\u001b[0m  0.0158\n",
      "     25       \u001b[36m29.0322\u001b[0m       \u001b[32m23.2596\u001b[0m  0.0154\n",
      "     26       \u001b[36m29.0300\u001b[0m       \u001b[32m23.2581\u001b[0m  0.0160\n",
      "     27       \u001b[36m29.0279\u001b[0m       \u001b[32m23.2569\u001b[0m  0.0158\n",
      "     28       \u001b[36m29.0259\u001b[0m       \u001b[32m23.2557\u001b[0m  0.0158\n",
      "     29       \u001b[36m29.0242\u001b[0m       \u001b[32m23.2546\u001b[0m  0.0151\n",
      "     30       \u001b[36m29.0225\u001b[0m       \u001b[32m23.2535\u001b[0m  0.0157\n",
      "     31       \u001b[36m29.0211\u001b[0m       \u001b[32m23.2526\u001b[0m  0.0157\n",
      "     32       \u001b[36m29.0196\u001b[0m       \u001b[32m23.2516\u001b[0m  0.0158\n",
      "     33       \u001b[36m29.0183\u001b[0m       \u001b[32m23.2506\u001b[0m  0.0152\n",
      "     34       \u001b[36m29.0171\u001b[0m       \u001b[32m23.2498\u001b[0m  0.0162\n",
      "     35       \u001b[36m29.0160\u001b[0m       \u001b[32m23.2490\u001b[0m  0.0158\n",
      "     36       \u001b[36m29.0149\u001b[0m       \u001b[32m23.2484\u001b[0m  0.0161\n",
      "     37       \u001b[36m29.0140\u001b[0m       \u001b[32m23.2477\u001b[0m  0.0149\n",
      "     38       \u001b[36m29.0131\u001b[0m       \u001b[32m23.2470\u001b[0m  0.0152\n",
      "     39       \u001b[36m29.0121\u001b[0m       \u001b[32m23.2462\u001b[0m  0.0158\n",
      "     40       \u001b[36m29.0113\u001b[0m       \u001b[32m23.2456\u001b[0m  0.0153\n",
      "     41       \u001b[36m29.0105\u001b[0m       \u001b[32m23.2450\u001b[0m  0.0154\n",
      "     42       \u001b[36m29.0097\u001b[0m       \u001b[32m23.2444\u001b[0m  0.0158\n",
      "     43       \u001b[36m29.0090\u001b[0m       \u001b[32m23.2439\u001b[0m  0.0155\n",
      "     44       \u001b[36m29.0083\u001b[0m       \u001b[32m23.2435\u001b[0m  0.0156\n",
      "     45       \u001b[36m29.0077\u001b[0m       \u001b[32m23.2428\u001b[0m  0.0151\n",
      "     46       \u001b[36m29.0070\u001b[0m       \u001b[32m23.2423\u001b[0m  0.0156\n",
      "     47       \u001b[36m29.0064\u001b[0m       \u001b[32m23.2419\u001b[0m  0.0168\n",
      "     48       \u001b[36m29.0058\u001b[0m       \u001b[32m23.2415\u001b[0m  0.0235\n",
      "     49       \u001b[36m29.0053\u001b[0m       \u001b[32m23.2412\u001b[0m  0.0183\n",
      "     50       \u001b[36m29.0047\u001b[0m       \u001b[32m23.2408\u001b[0m  0.0179\n",
      "     51       \u001b[36m29.0042\u001b[0m       \u001b[32m23.2404\u001b[0m  0.0211\n",
      "     52       \u001b[36m29.0037\u001b[0m       \u001b[32m23.2399\u001b[0m  0.0177\n",
      "     53       \u001b[36m29.0032\u001b[0m       \u001b[32m23.2395\u001b[0m  0.0173\n",
      "     54       \u001b[36m29.0027\u001b[0m       \u001b[32m23.2391\u001b[0m  0.0160\n",
      "     55       \u001b[36m29.0022\u001b[0m       \u001b[32m23.2387\u001b[0m  0.0161\n",
      "     56       \u001b[36m29.0017\u001b[0m       \u001b[32m23.2385\u001b[0m  0.0171\n",
      "     57       \u001b[36m29.0014\u001b[0m       \u001b[32m23.2380\u001b[0m  0.0172\n",
      "     58       \u001b[36m29.0009\u001b[0m       \u001b[32m23.2377\u001b[0m  0.0189\n",
      "     59       \u001b[36m29.0005\u001b[0m       \u001b[32m23.2373\u001b[0m  0.0186\n",
      "     60       \u001b[36m29.0001\u001b[0m       \u001b[32m23.2370\u001b[0m  0.0191\n",
      "     61       \u001b[36m28.9997\u001b[0m       \u001b[32m23.2366\u001b[0m  0.0156\n",
      "     62       \u001b[36m28.9994\u001b[0m       \u001b[32m23.2362\u001b[0m  0.0183\n",
      "     63       \u001b[36m28.9990\u001b[0m       \u001b[32m23.2358\u001b[0m  0.0157\n",
      "     64       \u001b[36m28.9987\u001b[0m       \u001b[32m23.2354\u001b[0m  0.0160\n",
      "     65       \u001b[36m28.9983\u001b[0m       \u001b[32m23.2352\u001b[0m  0.0162\n",
      "     66       \u001b[36m28.9980\u001b[0m       \u001b[32m23.2348\u001b[0m  0.0218\n",
      "     67       \u001b[36m28.9977\u001b[0m       \u001b[32m23.2345\u001b[0m  0.0165\n",
      "     68       \u001b[36m28.9973\u001b[0m       \u001b[32m23.2339\u001b[0m  0.0179\n",
      "     69       \u001b[36m28.9971\u001b[0m       \u001b[32m23.2336\u001b[0m  0.0169\n",
      "     70       \u001b[36m28.9967\u001b[0m       \u001b[32m23.2332\u001b[0m  0.0164\n",
      "     71       \u001b[36m28.9965\u001b[0m       \u001b[32m23.2329\u001b[0m  0.0167\n",
      "     72       \u001b[36m28.9961\u001b[0m       \u001b[32m23.2325\u001b[0m  0.0168\n",
      "     73       \u001b[36m28.9959\u001b[0m       \u001b[32m23.2323\u001b[0m  0.0191\n",
      "     74       \u001b[36m28.9956\u001b[0m       \u001b[32m23.2320\u001b[0m  0.0160\n",
      "     75       \u001b[36m28.9953\u001b[0m       \u001b[32m23.2315\u001b[0m  0.0160\n",
      "     76       \u001b[36m28.9950\u001b[0m       \u001b[32m23.2314\u001b[0m  0.0161\n",
      "     77       \u001b[36m28.9948\u001b[0m       \u001b[32m23.2309\u001b[0m  0.0171\n",
      "     78       \u001b[36m28.9945\u001b[0m       \u001b[32m23.2308\u001b[0m  0.0160\n",
      "     79       \u001b[36m28.9944\u001b[0m       \u001b[32m23.2305\u001b[0m  0.0172\n",
      "     80       \u001b[36m28.9941\u001b[0m       \u001b[32m23.2304\u001b[0m  0.0161\n",
      "     81       \u001b[36m28.9939\u001b[0m       \u001b[32m23.2300\u001b[0m  0.0162\n",
      "     82       \u001b[36m28.9937\u001b[0m       \u001b[32m23.2298\u001b[0m  0.0168\n",
      "     83       \u001b[36m28.9935\u001b[0m       \u001b[32m23.2294\u001b[0m  0.0182\n",
      "     84       \u001b[36m28.9932\u001b[0m       \u001b[32m23.2293\u001b[0m  0.0161\n",
      "     85       \u001b[36m28.9931\u001b[0m       \u001b[32m23.2289\u001b[0m  0.0171\n",
      "     86       \u001b[36m28.9928\u001b[0m       \u001b[32m23.2288\u001b[0m  0.0168\n",
      "     87       \u001b[36m28.9927\u001b[0m       \u001b[32m23.2284\u001b[0m  0.0170\n",
      "     88       \u001b[36m28.9925\u001b[0m       \u001b[32m23.2283\u001b[0m  0.0160\n",
      "     89       \u001b[36m28.9923\u001b[0m       \u001b[32m23.2280\u001b[0m  0.0188\n",
      "     90       \u001b[36m28.9921\u001b[0m       \u001b[32m23.2278\u001b[0m  0.0157\n",
      "     91       \u001b[36m28.9919\u001b[0m       \u001b[32m23.2276\u001b[0m  0.0156\n",
      "     92       \u001b[36m28.9918\u001b[0m       \u001b[32m23.2274\u001b[0m  0.0155\n",
      "     93       \u001b[36m28.9916\u001b[0m       \u001b[32m23.2271\u001b[0m  0.0155\n",
      "     94       \u001b[36m28.9914\u001b[0m       \u001b[32m23.2269\u001b[0m  0.0154\n",
      "     95       \u001b[36m28.9913\u001b[0m       \u001b[32m23.2267\u001b[0m  0.0154\n",
      "     96       \u001b[36m28.9910\u001b[0m       \u001b[32m23.2264\u001b[0m  0.0156\n",
      "     97       \u001b[36m28.9909\u001b[0m       \u001b[32m23.2263\u001b[0m  0.0158\n",
      "     98       \u001b[36m28.9907\u001b[0m       \u001b[32m23.2261\u001b[0m  0.0154\n",
      "     99       \u001b[36m28.9906\u001b[0m       \u001b[32m23.2259\u001b[0m  0.0154\n",
      "    100       \u001b[36m28.9904\u001b[0m       \u001b[32m23.2255\u001b[0m  0.0156\n",
      "Best Hyperparameters: {'lr': 0.01, 'max_epochs': 100, 'module': <function create_model at 0x718e8d67bb00>, 'module__hidden1': 32, 'module__hidden2': 64, 'optimizer': <class 'torch.optim.sgd.SGD'>}\n"
     ]
    }
   ],
   "source": [
    "#!pip install skorch\n",
    "from skorch import NeuralNetRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define a wrapper to initialize the model with the correct input_dim\n",
    "def create_model(hidden1=64, hidden2=32):\n",
    "    return RegressionModel(input_dim=X_train.shape[1], hidden1=hidden1, hidden2=hidden2)\n",
    "\n",
    "# Define Hyperparameter Grid\n",
    "params = {\n",
    "    'module': [create_model],  # Use the wrapper function\n",
    "    'module__hidden1': [32, 64, 128],\n",
    "    'module__hidden2': [16, 32, 64],\n",
    "    'lr': [0.001, 0.005, 0.01],\n",
    "    'max_epochs': [50, 100],\n",
    "    'optimizer': [optim.Adam, optim.SGD]\n",
    "}\n",
    "\n",
    "# Initialize skorch NeuralNetRegressor\n",
    "net = NeuralNetRegressor(\n",
    "    module=create_model,  # Ensure input_dim is passed\n",
    "    criterion=nn.MSELoss,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Perform Grid Search\n",
    "gs = GridSearchCV(net, params, scoring='neg_mean_squared_error', cv=3)\n",
    "gs.fit(X_train.astype(np.float32), y_train.astype(np.float32))  # Ensure dtype is compatible\n",
    "\n",
    "# Get Best Parameters\n",
    "best_params = gs.best_params_\n",
    "print(\"Best Hyperparameters:\", best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c34215d-4fac-438f-bfbc-6de6f9a133bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'lr': 0.01, 'max_epochs': 100, 'module': <function create_model at 0x718e8d67bb00>, 'module__hidden1': 32, 'module__hidden2': 64, 'optimizer': <class 'torch.optim.sgd.SGD'>}\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'input_dim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m best_params \u001b[38;5;241m=\u001b[39m gs\u001b[38;5;241m.\u001b[39mbest_params_\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest Hyperparameters:\u001b[39m\u001b[38;5;124m\"\u001b[39m, best_params)\n\u001b[0;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m RegressionModel(\u001b[43minput_dim\u001b[49m, best_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodule__hidden1\u001b[39m\u001b[38;5;124m'\u001b[39m], best_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodule__hidden2\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      5\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m best_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimizer\u001b[39m\u001b[38;5;124m'\u001b[39m](model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mbest_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      6\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'input_dim' is not defined"
     ]
    }
   ],
   "source": [
    "# Train with Best Parameters\n",
    "best_params = gs.best_params_\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "model = RegressionModel(input_dim, best_params['module__hidden1'], best_params['module__hidden2']).to(device)\n",
    "optimizer = best_params['optimizer'](model.parameters(), lr=best_params['lr'])\n",
    "criterion = nn.MSELoss()\n",
    "train_losses, test_losses, train_r2, test_r2 = train_model(model, X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "39764c8b-0754-4e65-8090-c8d46e8db634",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_r2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Plot Accuracy (R² Score) vs Epochs\u001b[39;00m\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m,\u001b[38;5;241m5\u001b[39m))\n\u001b[0;32m----> 3\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43mtrain_r2\u001b[49m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain R² Score\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(test_r2, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest R² Score\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpochs\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_r2' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot Accuracy (R² Score) vs Epochs\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(train_r2, label='Train R² Score')\n",
    "plt.plot(test_r2, label='Test R² Score')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('R² Score')\n",
    "plt.legend()\n",
    "plt.title('R² Score vs Epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8e1a78b-9fa2-42c6-9ac1-425c2bbed228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularization with Dropout & L2\n",
    "class RegularizedModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden1=64, hidden2=32):\n",
    "        super(RegularizedModel, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden1, hidden2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden2, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d68d44c-b3db-4e94-957c-628f88f6a7d4",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (937591138.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[17], line 4\u001b[0;36m\u001b[0m\n\u001b[0;31m    train_losses_reg, test_losses_reg, train_r2_reg, test_r2_reg = train_model(regularized_model, X_train_tensor, y_train_tensor, X_test\u001b[0m\n\u001b[0m                                                                                                                                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "# Train Regularized Model\n",
    "regularized_model = RegularizedModel(input_dim, best_params['module__hidden1'], best_params['module__hidden2']).to(device)\n",
    "optimizer = best_params['optimizer'](regularized_model.parameters(), lr=best_params['lr'], weight_decay=1e-4)\n",
    "train_losses_reg, test_losses_reg, train_r2_reg, test_r2_reg = train_model(regularized_model, X_train_tensor, y_train_tensor, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "accdfa8d-3bda-4efa-93c5-85f505a0ebba",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Compare Results\u001b[39;00m\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m,\u001b[38;5;241m5\u001b[39m))\n\u001b[0;32m----> 3\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43mtest_losses\u001b[49m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOriginal Test Loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(test_losses_reg, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRegularized Test Loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpochs\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_losses' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compare Results\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(test_losses, label='Original Test Loss')\n",
    "plt.plot(test_losses_reg, label='Regularized Test Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Comparison: Regularization Effect')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac41931-0c0c-4bcd-85a7-a5c092feeb26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454b39cf-6605-4d04-acbb-a233cd8d9c7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
